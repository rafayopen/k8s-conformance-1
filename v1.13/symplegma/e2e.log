I1213 19:51:08.709516      14 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-413709859
I1213 19:51:08.709797      14 e2e.go:224] Starting e2e run "6ef568b8-ff10-11e8-b666-eabb5e37e61c" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1544730668 - Will randomize all specs
Will run 201 of 1946 specs

Dec 13 19:51:08.869: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 19:51:08.871: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 13 19:51:08.887: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 13 19:51:08.923: INFO: 25 / 25 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 13 19:51:08.923: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Dec 13 19:51:08.923: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 13 19:51:08.934: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 13 19:51:08.934: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 13 19:51:08.934: INFO: e2e test version: v1.13.0
Dec 13 19:51:08.935: INFO: kube-apiserver version: v1.13.1
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 19:51:08.935: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-probe
Dec 13 19:51:09.013: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-zsrvv
Dec 13 19:51:11.028: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-zsrvv
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 19:51:11.031: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 19:55:11.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-zsrvv" for this suite.
Dec 13 19:55:17.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:55:17.627: INFO: namespace: e2e-tests-container-probe-zsrvv, resource: bindings, ignored listing per whitelist
Dec 13 19:55:17.759: INFO: namespace e2e-tests-container-probe-zsrvv deletion completed in 6.157078459s

• [SLOW TEST:248.823 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 19:55:17.759: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 19:55:17.842: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 19:55:20.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-gfs6c" for this suite.
Dec 13 19:56:12.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:56:12.085: INFO: namespace: e2e-tests-pods-gfs6c, resource: bindings, ignored listing per whitelist
Dec 13 19:56:12.125: INFO: namespace e2e-tests-pods-gfs6c deletion completed in 52.096857445s

• [SLOW TEST:54.366 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 19:56:12.125: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec 13 19:56:14.736: INFO: Successfully updated pod "annotationupdate243806de-ff11-11e8-b666-eabb5e37e61c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 19:56:16.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zvd75" for this suite.
Dec 13 19:56:38.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:56:38.840: INFO: namespace: e2e-tests-downward-api-zvd75, resource: bindings, ignored listing per whitelist
Dec 13 19:56:38.854: INFO: namespace e2e-tests-downward-api-zvd75 deletion completed in 22.098353546s

• [SLOW TEST:26.729 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 19:56:38.854: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 13 19:56:38.932: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 13 19:56:43.936: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 19:56:44.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-n6lcp" for this suite.
Dec 13 19:56:50.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:56:51.007: INFO: namespace: e2e-tests-replication-controller-n6lcp, resource: bindings, ignored listing per whitelist
Dec 13 19:56:51.047: INFO: namespace e2e-tests-replication-controller-n6lcp deletion completed in 6.094916642s

• [SLOW TEST:12.193 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 19:56:51.047: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Dec 13 19:56:51.119: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 13 19:56:51.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:56:51.446: INFO: stderr: ""
Dec 13 19:56:51.446: INFO: stdout: "service/redis-slave created\n"
Dec 13 19:56:51.446: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 13 19:56:51.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:56:51.621: INFO: stderr: ""
Dec 13 19:56:51.621: INFO: stdout: "service/redis-master created\n"
Dec 13 19:56:51.622: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 13 19:56:51.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:56:51.795: INFO: stderr: ""
Dec 13 19:56:51.795: INFO: stdout: "service/frontend created\n"
Dec 13 19:56:51.795: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 13 19:56:51.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:56:51.952: INFO: stderr: ""
Dec 13 19:56:51.952: INFO: stdout: "deployment.extensions/frontend created\n"
Dec 13 19:56:51.952: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 13 19:56:51.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:56:52.098: INFO: stderr: ""
Dec 13 19:56:52.098: INFO: stdout: "deployment.extensions/redis-master created\n"
Dec 13 19:56:52.098: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 13 19:56:52.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:56:52.275: INFO: stderr: ""
Dec 13 19:56:52.276: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Dec 13 19:56:52.276: INFO: Waiting for all frontend pods to be Running.
Dec 13 19:56:57.326: INFO: Waiting for frontend to serve content.
Dec 13 19:56:57.338: INFO: Trying to add a new entry to the guestbook.
Dec 13 19:56:57.348: INFO: Verifying that added entry can be retrieved.
Dec 13 19:56:57.359: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:02.375: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:07.387: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:12.404: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:17.416: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:22.433: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:27.445: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:32.461: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:37.473: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:42.489: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:47.501: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec 13 19:57:52.517: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Dec 13 19:57:57.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:57:57.613: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:57:57.614: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:57:57.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:57:57.776: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:57:57.776: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:57:57.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:57:57.872: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:57:57.872: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:57:57.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:57:57.942: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:57:57.942: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:57:57.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:57:58.059: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:57:58.059: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 13 19:57:58.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-wmr2h'
Dec 13 19:57:58.216: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 19:57:58.216: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 19:57:58.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wmr2h" for this suite.
Dec 13 19:58:36.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 19:58:36.301: INFO: namespace: e2e-tests-kubectl-wmr2h, resource: bindings, ignored listing per whitelist
Dec 13 19:58:36.317: INFO: namespace e2e-tests-kubectl-wmr2h deletion completed in 38.097424654s

• [SLOW TEST:105.270 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 19:58:36.317: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-5hsb7
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Dec 13 19:58:36.403: INFO: Found 0 stateful pods, waiting for 3
Dec 13 19:58:46.412: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:58:46.412: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:58:46.412: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 19:58:46.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-5hsb7 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 19:58:46.611: INFO: stderr: ""
Dec 13 19:58:46.611: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 19:58:46.611: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 13 19:58:56.657: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 13 19:59:06.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-5hsb7 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 13 19:59:06.920: INFO: stderr: ""
Dec 13 19:59:06.920: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 13 19:59:06.920: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 13 19:59:16.943: INFO: Waiting for StatefulSet e2e-tests-statefulset-5hsb7/ss2 to complete update
Dec 13 19:59:16.943: INFO: Waiting for Pod e2e-tests-statefulset-5hsb7/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec 13 19:59:16.943: INFO: Waiting for Pod e2e-tests-statefulset-5hsb7/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec 13 19:59:16.943: INFO: Waiting for Pod e2e-tests-statefulset-5hsb7/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec 13 19:59:26.953: INFO: Waiting for StatefulSet e2e-tests-statefulset-5hsb7/ss2 to complete update
Dec 13 19:59:26.953: INFO: Waiting for Pod e2e-tests-statefulset-5hsb7/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec 13 19:59:26.953: INFO: Waiting for Pod e2e-tests-statefulset-5hsb7/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec 13 19:59:36.949: INFO: Waiting for StatefulSet e2e-tests-statefulset-5hsb7/ss2 to complete update
Dec 13 19:59:36.949: INFO: Waiting for Pod e2e-tests-statefulset-5hsb7/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Dec 13 19:59:46.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-5hsb7 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 19:59:47.151: INFO: stderr: ""
Dec 13 19:59:47.151: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 19:59:47.151: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 13 19:59:57.185: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 13 20:00:07.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-5hsb7 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 13 20:00:07.401: INFO: stderr: ""
Dec 13 20:00:07.401: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 13 20:00:07.401: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 13 20:00:27.423: INFO: Waiting for StatefulSet e2e-tests-statefulset-5hsb7/ss2 to complete update
Dec 13 20:00:27.423: INFO: Waiting for Pod e2e-tests-statefulset-5hsb7/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec 13 20:00:37.435: INFO: Deleting all statefulset in ns e2e-tests-statefulset-5hsb7
Dec 13 20:00:37.437: INFO: Scaling statefulset ss2 to 0
Dec 13 20:00:57.451: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 20:00:57.454: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:00:57.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-5hsb7" for this suite.
Dec 13 20:01:03.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:01:03.526: INFO: namespace: e2e-tests-statefulset-5hsb7, resource: bindings, ignored listing per whitelist
Dec 13 20:01:03.568: INFO: namespace e2e-tests-statefulset-5hsb7 deletion completed in 6.095371914s

• [SLOW TEST:147.251 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:01:03.569: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-d1ee6979-ff11-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:01:03.651: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1ef3b21-ff11-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-configmap-72rmz" to be "success or failure"
Dec 13 20:01:03.657: INFO: Pod "pod-configmaps-d1ef3b21-ff11-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.793456ms
Dec 13 20:01:05.661: INFO: Pod "pod-configmaps-d1ef3b21-ff11-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009341649s
STEP: Saw pod success
Dec 13 20:01:05.661: INFO: Pod "pod-configmaps-d1ef3b21-ff11-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:01:05.664: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-d1ef3b21-ff11-11e8-b666-eabb5e37e61c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:01:05.682: INFO: Waiting for pod pod-configmaps-d1ef3b21-ff11-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:01:05.684: INFO: Pod pod-configmaps-d1ef3b21-ff11-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:01:05.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-72rmz" for this suite.
Dec 13 20:01:11.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:01:11.763: INFO: namespace: e2e-tests-configmap-72rmz, resource: bindings, ignored listing per whitelist
Dec 13 20:01:11.793: INFO: namespace e2e-tests-configmap-72rmz deletion completed in 6.105464817s

• [SLOW TEST:8.224 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:01:11.793: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-cjcxd
Dec 13 20:01:13.879: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-cjcxd
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 20:01:13.882: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:05:14.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-cjcxd" for this suite.
Dec 13 20:05:20.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:05:20.521: INFO: namespace: e2e-tests-container-probe-cjcxd, resource: bindings, ignored listing per whitelist
Dec 13 20:05:20.536: INFO: namespace e2e-tests-container-probe-cjcxd deletion completed in 6.102151221s

• [SLOW TEST:248.743 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:05:20.536: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:05:20.608: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 13 20:05:20.615: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 13 20:05:25.619: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 13 20:05:25.619: INFO: Creating deployment "test-rolling-update-deployment"
Dec 13 20:05:25.625: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 13 20:05:25.631: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 13 20:05:27.637: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 13 20:05:27.642: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec 13 20:05:27.655: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-w294j,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-w294j/deployments/test-rolling-update-deployment,UID:6e1577ea-ff12-11e8-b982-02b9355b966e,ResourceVersion:47878,Generation:1,CreationTimestamp:2018-12-13 20:05:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2018-12-13 20:05:25 +0000 UTC 2018-12-13 20:05:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2018-12-13 20:05:26 +0000 UTC 2018-12-13 20:05:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 13 20:05:27.663: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-w294j,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-w294j/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:6e1ae4c0-ff12-11e8-b31c-0a2b404fde34,ResourceVersion:47869,Generation:1,CreationTimestamp:2018-12-13 20:05:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 6e1577ea-ff12-11e8-b982-02b9355b966e 0xc002554a77 0xc002554a78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 13 20:05:27.663: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 13 20:05:27.663: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-w294j,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-w294j/replicasets/test-rolling-update-controller,UID:6b18cb3b-ff12-11e8-b982-02b9355b966e,ResourceVersion:47877,Generation:2,CreationTimestamp:2018-12-13 20:05:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 6e1577ea-ff12-11e8-b982-02b9355b966e 0xc0025549b7 0xc0025549b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 13 20:05:27.673: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-llthf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-llthf,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-w294j,SelfLink:/api/v1/namespaces/e2e-tests-deployment-w294j/pods/test-rolling-update-deployment-68b55d7bc6-llthf,UID:6e1bc7e1-ff12-11e8-b31c-0a2b404fde34,ResourceVersion:47868,Generation:0,CreationTimestamp:2018-12-13 20:05:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.96/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 6e1ae4c0-ff12-11e8-b31c-0a2b404fde34 0xc002555a17 0xc002555a18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-lv9xx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lv9xx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lv9xx true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002555ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002555af0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:05:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:05:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:05:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:05:25 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:192.168.3.96,StartTime:2018-12-13 20:05:25 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2018-12-13 20:05:26 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://bf28f85ba343a2bc8f6125d20b5724c4a0dc232e5d59754a98bbaca433326bb7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:05:27.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-w294j" for this suite.
Dec 13 20:05:33.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:05:33.736: INFO: namespace: e2e-tests-deployment-w294j, resource: bindings, ignored listing per whitelist
Dec 13 20:05:33.785: INFO: namespace e2e-tests-deployment-w294j deletion completed in 6.106236859s

• [SLOW TEST:13.249 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:05:33.786: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Dec 13 20:05:35.957: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:06:00.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-q8l2s" for this suite.
Dec 13 20:06:06.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:06:06.093: INFO: namespace: e2e-tests-namespaces-q8l2s, resource: bindings, ignored listing per whitelist
Dec 13 20:06:06.126: INFO: namespace e2e-tests-namespaces-q8l2s deletion completed in 6.095655166s
STEP: Destroying namespace "e2e-tests-nsdeletetest-87hcd" for this suite.
Dec 13 20:06:06.129: INFO: Namespace e2e-tests-nsdeletetest-87hcd was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-z8www" for this suite.
Dec 13 20:06:12.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:06:12.223: INFO: namespace: e2e-tests-nsdeletetest-z8www, resource: bindings, ignored listing per whitelist
Dec 13 20:06:12.228: INFO: namespace e2e-tests-nsdeletetest-z8www deletion completed in 6.098975505s

• [SLOW TEST:38.442 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:06:12.228: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1213 20:06:52.339727      14 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 20:06:52.339: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:06:52.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mzwnx" for this suite.
Dec 13 20:06:58.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:06:58.522: INFO: namespace: e2e-tests-gc-mzwnx, resource: bindings, ignored listing per whitelist
Dec 13 20:06:58.657: INFO: namespace e2e-tests-gc-mzwnx deletion completed in 6.308626511s

• [SLOW TEST:46.429 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:06:58.657: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-l4f9l
Dec 13 20:07:02.837: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-l4f9l
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 20:07:02.840: INFO: Initial restart count of pod liveness-http is 0
Dec 13 20:07:16.871: INFO: Restart count of pod e2e-tests-container-probe-l4f9l/liveness-http is now 1 (14.030468643s elapsed)
Dec 13 20:07:36.914: INFO: Restart count of pod e2e-tests-container-probe-l4f9l/liveness-http is now 2 (34.07364793s elapsed)
Dec 13 20:07:56.954: INFO: Restart count of pod e2e-tests-container-probe-l4f9l/liveness-http is now 3 (54.113897298s elapsed)
Dec 13 20:08:16.998: INFO: Restart count of pod e2e-tests-container-probe-l4f9l/liveness-http is now 4 (1m14.158052595s elapsed)
Dec 13 20:09:29.173: INFO: Restart count of pod e2e-tests-container-probe-l4f9l/liveness-http is now 5 (2m26.332900676s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:09:29.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-l4f9l" for this suite.
Dec 13 20:09:35.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:09:35.235: INFO: namespace: e2e-tests-container-probe-l4f9l, resource: bindings, ignored listing per whitelist
Dec 13 20:09:35.287: INFO: namespace e2e-tests-container-probe-l4f9l deletion completed in 6.098109761s

• [SLOW TEST:156.630 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:09:35.287: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Dec 13 20:09:35.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-lx424'
Dec 13 20:09:35.645: INFO: stderr: ""
Dec 13 20:09:35.645: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 13 20:09:36.649: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:09:36.649: INFO: Found 0 / 1
Dec 13 20:09:37.650: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:09:37.650: INFO: Found 1 / 1
Dec 13 20:09:37.650: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 13 20:09:37.654: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:09:37.654: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 13 20:09:37.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 patch pod redis-master-rnrdt --namespace=e2e-tests-kubectl-lx424 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 13 20:09:37.732: INFO: stderr: ""
Dec 13 20:09:37.732: INFO: stdout: "pod/redis-master-rnrdt patched\n"
STEP: checking annotations
Dec 13 20:09:37.739: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:09:37.739: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:09:37.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lx424" for this suite.
Dec 13 20:09:59.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:09:59.807: INFO: namespace: e2e-tests-kubectl-lx424, resource: bindings, ignored listing per whitelist
Dec 13 20:09:59.869: INFO: namespace e2e-tests-kubectl-lx424 deletion completed in 22.112189997s

• [SLOW TEST:24.582 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:09:59.869: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:10:02.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-cwlt8" for this suite.
Dec 13 20:10:24.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:10:25.009: INFO: namespace: e2e-tests-replication-controller-cwlt8, resource: bindings, ignored listing per whitelist
Dec 13 20:10:25.070: INFO: namespace e2e-tests-replication-controller-cwlt8 deletion completed in 22.096255437s

• [SLOW TEST:25.202 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:10:25.071: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Dec 13 20:10:25.157: INFO: namespace e2e-tests-kubectl-j5kw7
Dec 13 20:10:25.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-j5kw7'
Dec 13 20:10:25.381: INFO: stderr: ""
Dec 13 20:10:25.381: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 13 20:10:26.385: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:10:26.385: INFO: Found 0 / 1
Dec 13 20:10:27.385: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:10:27.385: INFO: Found 1 / 1
Dec 13 20:10:27.385: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 13 20:10:27.388: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:10:27.388: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 13 20:10:27.388: INFO: wait on redis-master startup in e2e-tests-kubectl-j5kw7 
Dec 13 20:10:27.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 logs redis-master-k85zm redis-master --namespace=e2e-tests-kubectl-j5kw7'
Dec 13 20:10:27.468: INFO: stderr: ""
Dec 13 20:10:27.468: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Dec 20:10:26.202 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Dec 20:10:26.202 # Server started, Redis version 3.2.12\n1:M 13 Dec 20:10:26.202 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Dec 20:10:26.202 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec 13 20:10:27.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-j5kw7'
Dec 13 20:10:27.554: INFO: stderr: ""
Dec 13 20:10:27.554: INFO: stdout: "service/rm2 exposed\n"
Dec 13 20:10:27.559: INFO: Service rm2 in namespace e2e-tests-kubectl-j5kw7 found.
STEP: exposing service
Dec 13 20:10:29.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-j5kw7'
Dec 13 20:10:29.660: INFO: stderr: ""
Dec 13 20:10:29.660: INFO: stdout: "service/rm3 exposed\n"
Dec 13 20:10:29.664: INFO: Service rm3 in namespace e2e-tests-kubectl-j5kw7 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:10:31.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-j5kw7" for this suite.
Dec 13 20:10:53.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:10:53.719: INFO: namespace: e2e-tests-kubectl-j5kw7, resource: bindings, ignored listing per whitelist
Dec 13 20:10:53.769: INFO: namespace e2e-tests-kubectl-j5kw7 deletion completed in 22.096694885s

• [SLOW TEST:28.699 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:10:53.770: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:11:53.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-fhhq2" for this suite.
Dec 13 20:12:15.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:12:15.876: INFO: namespace: e2e-tests-container-probe-fhhq2, resource: bindings, ignored listing per whitelist
Dec 13 20:12:15.953: INFO: namespace e2e-tests-container-probe-fhhq2 deletion completed in 22.098750776s

• [SLOW TEST:82.183 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:12:15.953: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-p5h8
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 20:12:16.042: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-p5h8" in namespace "e2e-tests-subpath-skg7j" to be "success or failure"
Dec 13 20:12:16.046: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.284729ms
Dec 13 20:12:18.049: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007572588s
Dec 13 20:12:20.056: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 4.013976896s
Dec 13 20:12:22.059: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 6.017360963s
Dec 13 20:12:24.063: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 8.020876889s
Dec 13 20:12:26.070: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 10.028253452s
Dec 13 20:12:28.073: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 12.031778283s
Dec 13 20:12:30.079: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 14.037337666s
Dec 13 20:12:32.083: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 16.040966926s
Dec 13 20:12:34.086: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 18.044653551s
Dec 13 20:12:36.095: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 20.052902278s
Dec 13 20:12:38.098: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Running", Reason="", readiness=false. Elapsed: 22.056615116s
Dec 13 20:12:40.102: INFO: Pod "pod-subpath-test-downwardapi-p5h8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.060421083s
STEP: Saw pod success
Dec 13 20:12:40.102: INFO: Pod "pod-subpath-test-downwardapi-p5h8" satisfied condition "success or failure"
Dec 13 20:12:40.105: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-subpath-test-downwardapi-p5h8 container test-container-subpath-downwardapi-p5h8: <nil>
STEP: delete the pod
Dec 13 20:12:40.124: INFO: Waiting for pod pod-subpath-test-downwardapi-p5h8 to disappear
Dec 13 20:12:40.127: INFO: Pod pod-subpath-test-downwardapi-p5h8 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-p5h8
Dec 13 20:12:40.127: INFO: Deleting pod "pod-subpath-test-downwardapi-p5h8" in namespace "e2e-tests-subpath-skg7j"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:12:40.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-skg7j" for this suite.
Dec 13 20:12:46.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:12:46.211: INFO: namespace: e2e-tests-subpath-skg7j, resource: bindings, ignored listing per whitelist
Dec 13 20:12:46.230: INFO: namespace e2e-tests-subpath-skg7j deletion completed in 6.0973439s

• [SLOW TEST:30.278 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:12:46.231: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Dec 13 20:12:46.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:12:46.431: INFO: stderr: ""
Dec 13 20:12:46.431: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:12:46.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:12:46.509: INFO: stderr: ""
Dec 13 20:12:46.509: INFO: stdout: "update-demo-nautilus-flslg update-demo-nautilus-zgv4x "
Dec 13 20:12:46.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-flslg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:12:46.569: INFO: stderr: ""
Dec 13 20:12:46.569: INFO: stdout: ""
Dec 13 20:12:46.569: INFO: update-demo-nautilus-flslg is created but not running
Dec 13 20:12:51.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:12:51.640: INFO: stderr: ""
Dec 13 20:12:51.640: INFO: stdout: "update-demo-nautilus-flslg update-demo-nautilus-zgv4x "
Dec 13 20:12:51.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-flslg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:12:51.697: INFO: stderr: ""
Dec 13 20:12:51.697: INFO: stdout: "true"
Dec 13 20:12:51.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-flslg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:12:51.758: INFO: stderr: ""
Dec 13 20:12:51.758: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:12:51.758: INFO: validating pod update-demo-nautilus-flslg
Dec 13 20:12:51.763: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:12:51.763: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:12:51.763: INFO: update-demo-nautilus-flslg is verified up and running
Dec 13 20:12:51.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-zgv4x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:12:51.822: INFO: stderr: ""
Dec 13 20:12:51.822: INFO: stdout: "true"
Dec 13 20:12:51.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-zgv4x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:12:51.880: INFO: stderr: ""
Dec 13 20:12:51.880: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:12:51.880: INFO: validating pod update-demo-nautilus-zgv4x
Dec 13 20:12:51.885: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:12:51.885: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:12:51.885: INFO: update-demo-nautilus-zgv4x is verified up and running
STEP: rolling-update to new replication controller
Dec 13 20:12:51.887: INFO: scanned /root for discovery docs: <nil>
Dec 13 20:12:51.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:13:14.339: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 13 20:13:14.339: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:13:14.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:13:14.409: INFO: stderr: ""
Dec 13 20:13:14.409: INFO: stdout: "update-demo-kitten-fd8k9 update-demo-kitten-tqzmz "
Dec 13 20:13:14.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-kitten-fd8k9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:13:14.469: INFO: stderr: ""
Dec 13 20:13:14.469: INFO: stdout: "true"
Dec 13 20:13:14.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-kitten-fd8k9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:13:14.526: INFO: stderr: ""
Dec 13 20:13:14.526: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 13 20:13:14.526: INFO: validating pod update-demo-kitten-fd8k9
Dec 13 20:13:14.531: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 13 20:13:14.531: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 13 20:13:14.531: INFO: update-demo-kitten-fd8k9 is verified up and running
Dec 13 20:13:14.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-kitten-tqzmz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:13:14.597: INFO: stderr: ""
Dec 13 20:13:14.597: INFO: stdout: "true"
Dec 13 20:13:14.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-kitten-tqzmz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-crq4d'
Dec 13 20:13:14.654: INFO: stderr: ""
Dec 13 20:13:14.654: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 13 20:13:14.654: INFO: validating pod update-demo-kitten-tqzmz
Dec 13 20:13:14.660: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 13 20:13:14.660: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 13 20:13:14.660: INFO: update-demo-kitten-tqzmz is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:13:14.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-crq4d" for this suite.
Dec 13 20:13:36.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:13:36.769: INFO: namespace: e2e-tests-kubectl-crq4d, resource: bindings, ignored listing per whitelist
Dec 13 20:13:36.774: INFO: namespace e2e-tests-kubectl-crq4d deletion completed in 22.110111873s

• [SLOW TEST:50.542 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:13:36.774: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Dec 13 20:13:36.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-vgr26'
Dec 13 20:13:36.990: INFO: stderr: ""
Dec 13 20:13:36.990: INFO: stdout: "pod/pause created\n"
Dec 13 20:13:36.990: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 13 20:13:36.990: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-vgr26" to be "running and ready"
Dec 13 20:13:36.996: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.222395ms
Dec 13 20:13:39.003: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.012837506s
Dec 13 20:13:39.003: INFO: Pod "pause" satisfied condition "running and ready"
Dec 13 20:13:39.003: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 13 20:13:39.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-vgr26'
Dec 13 20:13:39.092: INFO: stderr: ""
Dec 13 20:13:39.092: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 13 20:13:39.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pod pause -L testing-label --namespace=e2e-tests-kubectl-vgr26'
Dec 13 20:13:39.152: INFO: stderr: ""
Dec 13 20:13:39.152: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 13 20:13:39.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 label pods pause testing-label- --namespace=e2e-tests-kubectl-vgr26'
Dec 13 20:13:39.217: INFO: stderr: ""
Dec 13 20:13:39.217: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 13 20:13:39.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pod pause -L testing-label --namespace=e2e-tests-kubectl-vgr26'
Dec 13 20:13:39.277: INFO: stderr: ""
Dec 13 20:13:39.277: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Dec 13 20:13:39.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vgr26'
Dec 13 20:13:39.348: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 20:13:39.348: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 13 20:13:39.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-vgr26'
Dec 13 20:13:39.414: INFO: stderr: "No resources found.\n"
Dec 13 20:13:39.414: INFO: stdout: ""
Dec 13 20:13:39.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -l name=pause --namespace=e2e-tests-kubectl-vgr26 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 13 20:13:39.474: INFO: stderr: ""
Dec 13 20:13:39.474: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:13:39.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vgr26" for this suite.
Dec 13 20:13:45.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:13:45.544: INFO: namespace: e2e-tests-kubectl-vgr26, resource: bindings, ignored listing per whitelist
Dec 13 20:13:45.572: INFO: namespace e2e-tests-kubectl-vgr26 deletion completed in 6.094656971s

• [SLOW TEST:8.799 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:13:45.573: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec 13 20:13:45.651: INFO: Waiting up to 5m0s for pod "downward-api-981eee85-ff13-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-mqb9k" to be "success or failure"
Dec 13 20:13:45.655: INFO: Pod "downward-api-981eee85-ff13-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170377ms
Dec 13 20:13:47.659: INFO: Pod "downward-api-981eee85-ff13-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007457157s
STEP: Saw pod success
Dec 13 20:13:47.659: INFO: Pod "downward-api-981eee85-ff13-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:13:47.663: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downward-api-981eee85-ff13-11e8-b666-eabb5e37e61c container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:13:47.692: INFO: Waiting for pod downward-api-981eee85-ff13-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:13:47.709: INFO: Pod downward-api-981eee85-ff13-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:13:47.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mqb9k" for this suite.
Dec 13 20:13:53.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:13:53.795: INFO: namespace: e2e-tests-downward-api-mqb9k, resource: bindings, ignored listing per whitelist
Dec 13 20:13:53.814: INFO: namespace e2e-tests-downward-api-mqb9k deletion completed in 6.100331066s

• [SLOW TEST:8.241 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:13:53.815: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 13 20:13:53.897: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-7wgfb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7wgfb/configmaps/e2e-watch-test-watch-closed,UID:9d08dd84-ff13-11e8-b982-02b9355b966e,ResourceVersion:49529,Generation:0,CreationTimestamp:2018-12-13 20:13:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 20:13:53.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-7wgfb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7wgfb/configmaps/e2e-watch-test-watch-closed,UID:9d08dd84-ff13-11e8-b982-02b9355b966e,ResourceVersion:49530,Generation:0,CreationTimestamp:2018-12-13 20:13:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 13 20:13:53.912: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-7wgfb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7wgfb/configmaps/e2e-watch-test-watch-closed,UID:9d08dd84-ff13-11e8-b982-02b9355b966e,ResourceVersion:49531,Generation:0,CreationTimestamp:2018-12-13 20:13:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 20:13:53.912: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-7wgfb,SelfLink:/api/v1/namespaces/e2e-tests-watch-7wgfb/configmaps/e2e-watch-test-watch-closed,UID:9d08dd84-ff13-11e8-b982-02b9355b966e,ResourceVersion:49532,Generation:0,CreationTimestamp:2018-12-13 20:13:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:13:53.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-7wgfb" for this suite.
Dec 13 20:13:59.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:13:59.986: INFO: namespace: e2e-tests-watch-7wgfb, resource: bindings, ignored listing per whitelist
Dec 13 20:14:00.046: INFO: namespace e2e-tests-watch-7wgfb deletion completed in 6.129342906s

• [SLOW TEST:6.231 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:14:00.046: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:14:00.129: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:14:01.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-kdlhn" for this suite.
Dec 13 20:14:07.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:14:07.260: INFO: namespace: e2e-tests-custom-resource-definition-kdlhn, resource: bindings, ignored listing per whitelist
Dec 13 20:14:07.272: INFO: namespace e2e-tests-custom-resource-definition-kdlhn deletion completed in 6.095557886s

• [SLOW TEST:7.226 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:14:07.272: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Dec 13 20:14:07.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 cluster-info'
Dec 13 20:14:07.409: INFO: stderr: ""
Dec 13 20:14:07.409: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:14:07.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rkm87" for this suite.
Dec 13 20:14:13.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:14:13.484: INFO: namespace: e2e-tests-kubectl-rkm87, resource: bindings, ignored listing per whitelist
Dec 13 20:14:13.508: INFO: namespace e2e-tests-kubectl-rkm87 deletion completed in 6.095360271s

• [SLOW TEST:6.236 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:14:13.509: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Dec 13 20:14:13.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:13.725: INFO: stderr: ""
Dec 13 20:14:13.725: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:14:13.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:13.803: INFO: stderr: ""
Dec 13 20:14:13.803: INFO: stdout: "update-demo-nautilus-28kfq update-demo-nautilus-57fqm "
Dec 13 20:14:13.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-28kfq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:13.862: INFO: stderr: ""
Dec 13 20:14:13.862: INFO: stdout: ""
Dec 13 20:14:13.862: INFO: update-demo-nautilus-28kfq is created but not running
Dec 13 20:14:18.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:18.925: INFO: stderr: ""
Dec 13 20:14:18.925: INFO: stdout: "update-demo-nautilus-28kfq update-demo-nautilus-57fqm "
Dec 13 20:14:18.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-28kfq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:18.984: INFO: stderr: ""
Dec 13 20:14:18.984: INFO: stdout: "true"
Dec 13 20:14:18.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-28kfq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:19.047: INFO: stderr: ""
Dec 13 20:14:19.047: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:14:19.047: INFO: validating pod update-demo-nautilus-28kfq
Dec 13 20:14:19.052: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:14:19.052: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:14:19.052: INFO: update-demo-nautilus-28kfq is verified up and running
Dec 13 20:14:19.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-57fqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:19.119: INFO: stderr: ""
Dec 13 20:14:19.119: INFO: stdout: "true"
Dec 13 20:14:19.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-57fqm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:19.181: INFO: stderr: ""
Dec 13 20:14:19.181: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:14:19.181: INFO: validating pod update-demo-nautilus-57fqm
Dec 13 20:14:19.186: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:14:19.187: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:14:19.187: INFO: update-demo-nautilus-57fqm is verified up and running
STEP: scaling down the replication controller
Dec 13 20:14:19.188: INFO: scanned /root for discovery docs: <nil>
Dec 13 20:14:19.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:20.277: INFO: stderr: ""
Dec 13 20:14:20.277: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:14:20.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:20.346: INFO: stderr: ""
Dec 13 20:14:20.346: INFO: stdout: "update-demo-nautilus-28kfq update-demo-nautilus-57fqm "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 13 20:14:25.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:25.416: INFO: stderr: ""
Dec 13 20:14:25.416: INFO: stdout: "update-demo-nautilus-28kfq update-demo-nautilus-57fqm "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 13 20:14:30.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:30.477: INFO: stderr: ""
Dec 13 20:14:30.477: INFO: stdout: "update-demo-nautilus-57fqm "
Dec 13 20:14:30.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-57fqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:30.544: INFO: stderr: ""
Dec 13 20:14:30.544: INFO: stdout: "true"
Dec 13 20:14:30.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-57fqm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:30.618: INFO: stderr: ""
Dec 13 20:14:30.618: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:14:30.618: INFO: validating pod update-demo-nautilus-57fqm
Dec 13 20:14:30.622: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:14:30.622: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:14:30.622: INFO: update-demo-nautilus-57fqm is verified up and running
STEP: scaling up the replication controller
Dec 13 20:14:30.624: INFO: scanned /root for discovery docs: <nil>
Dec 13 20:14:30.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:31.713: INFO: stderr: ""
Dec 13 20:14:31.713: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:14:31.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:31.777: INFO: stderr: ""
Dec 13 20:14:31.777: INFO: stdout: "update-demo-nautilus-57fqm update-demo-nautilus-5fvg5 "
Dec 13 20:14:31.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-57fqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:31.837: INFO: stderr: ""
Dec 13 20:14:31.837: INFO: stdout: "true"
Dec 13 20:14:31.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-57fqm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:31.900: INFO: stderr: ""
Dec 13 20:14:31.900: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:14:31.900: INFO: validating pod update-demo-nautilus-57fqm
Dec 13 20:14:31.904: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:14:31.904: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:14:31.904: INFO: update-demo-nautilus-57fqm is verified up and running
Dec 13 20:14:31.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-5fvg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:31.963: INFO: stderr: ""
Dec 13 20:14:31.963: INFO: stdout: ""
Dec 13 20:14:31.963: INFO: update-demo-nautilus-5fvg5 is created but not running
Dec 13 20:14:36.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:37.046: INFO: stderr: ""
Dec 13 20:14:37.046: INFO: stdout: "update-demo-nautilus-57fqm update-demo-nautilus-5fvg5 "
Dec 13 20:14:37.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-57fqm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:37.107: INFO: stderr: ""
Dec 13 20:14:37.107: INFO: stdout: "true"
Dec 13 20:14:37.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-57fqm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:37.165: INFO: stderr: ""
Dec 13 20:14:37.165: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:14:37.165: INFO: validating pod update-demo-nautilus-57fqm
Dec 13 20:14:37.170: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:14:37.170: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:14:37.170: INFO: update-demo-nautilus-57fqm is verified up and running
Dec 13 20:14:37.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-5fvg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:37.226: INFO: stderr: ""
Dec 13 20:14:37.226: INFO: stdout: "true"
Dec 13 20:14:37.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-5fvg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:37.283: INFO: stderr: ""
Dec 13 20:14:37.283: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:14:37.283: INFO: validating pod update-demo-nautilus-5fvg5
Dec 13 20:14:37.288: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:14:37.288: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:14:37.288: INFO: update-demo-nautilus-5fvg5 is verified up and running
STEP: using delete to clean up resources
Dec 13 20:14:37.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:37.353: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 20:14:37.353: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 13 20:14:37.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-xbkg9'
Dec 13 20:14:37.485: INFO: stderr: "No resources found.\n"
Dec 13 20:14:37.485: INFO: stdout: ""
Dec 13 20:14:37.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -l name=update-demo --namespace=e2e-tests-kubectl-xbkg9 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 13 20:14:37.621: INFO: stderr: ""
Dec 13 20:14:37.621: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:14:37.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xbkg9" for this suite.
Dec 13 20:14:59.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:14:59.678: INFO: namespace: e2e-tests-kubectl-xbkg9, resource: bindings, ignored listing per whitelist
Dec 13 20:14:59.720: INFO: namespace e2e-tests-kubectl-xbkg9 deletion completed in 22.094942086s

• [SLOW TEST:46.211 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:14:59.720: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-c450e789-ff13-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:14:59.800: INFO: Waiting up to 5m0s for pod "pod-secrets-c451774e-ff13-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-secrets-bglmh" to be "success or failure"
Dec 13 20:14:59.802: INFO: Pod "pod-secrets-c451774e-ff13-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325515ms
Dec 13 20:15:01.806: INFO: Pod "pod-secrets-c451774e-ff13-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006018068s
STEP: Saw pod success
Dec 13 20:15:01.806: INFO: Pod "pod-secrets-c451774e-ff13-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:15:01.808: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-secrets-c451774e-ff13-11e8-b666-eabb5e37e61c container secret-env-test: <nil>
STEP: delete the pod
Dec 13 20:15:01.827: INFO: Waiting for pod pod-secrets-c451774e-ff13-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:15:01.830: INFO: Pod pod-secrets-c451774e-ff13-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:15:01.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-bglmh" for this suite.
Dec 13 20:15:07.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:15:07.925: INFO: namespace: e2e-tests-secrets-bglmh, resource: bindings, ignored listing per whitelist
Dec 13 20:15:07.930: INFO: namespace e2e-tests-secrets-bglmh deletion completed in 6.097393734s

• [SLOW TEST:8.211 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:15:07.931: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:15:08.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c93538df-ff13-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-wxjgx" to be "success or failure"
Dec 13 20:15:08.006: INFO: Pod "downwardapi-volume-c93538df-ff13-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.754731ms
Dec 13 20:15:10.010: INFO: Pod "downwardapi-volume-c93538df-ff13-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007250413s
STEP: Saw pod success
Dec 13 20:15:10.010: INFO: Pod "downwardapi-volume-c93538df-ff13-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:15:10.014: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-c93538df-ff13-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:15:10.038: INFO: Waiting for pod downwardapi-volume-c93538df-ff13-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:15:10.041: INFO: Pod downwardapi-volume-c93538df-ff13-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:15:10.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wxjgx" for this suite.
Dec 13 20:15:16.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:15:16.136: INFO: namespace: e2e-tests-downward-api-wxjgx, resource: bindings, ignored listing per whitelist
Dec 13 20:15:16.158: INFO: namespace e2e-tests-downward-api-wxjgx deletion completed in 6.1138025s

• [SLOW TEST:8.227 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:15:16.158: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-ce1e1a69-ff13-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:15:16.248: INFO: Waiting up to 5m0s for pod "pod-secrets-ce1eb2e6-ff13-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-secrets-2z44v" to be "success or failure"
Dec 13 20:15:16.256: INFO: Pod "pod-secrets-ce1eb2e6-ff13-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.579716ms
Dec 13 20:15:18.259: INFO: Pod "pod-secrets-ce1eb2e6-ff13-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010942882s
STEP: Saw pod success
Dec 13 20:15:18.259: INFO: Pod "pod-secrets-ce1eb2e6-ff13-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:15:18.262: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-secrets-ce1eb2e6-ff13-11e8-b666-eabb5e37e61c container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:15:18.279: INFO: Waiting for pod pod-secrets-ce1eb2e6-ff13-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:15:18.281: INFO: Pod pod-secrets-ce1eb2e6-ff13-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:15:18.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2z44v" for this suite.
Dec 13 20:15:24.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:15:24.388: INFO: namespace: e2e-tests-secrets-2z44v, resource: bindings, ignored listing per whitelist
Dec 13 20:15:24.388: INFO: namespace e2e-tests-secrets-2z44v deletion completed in 6.104039797s

• [SLOW TEST:8.230 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:15:24.389: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:15:24.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d30442a4-ff13-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-wkn7h" to be "success or failure"
Dec 13 20:15:24.462: INFO: Pod "downwardapi-volume-d30442a4-ff13-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.889783ms
Dec 13 20:15:26.465: INFO: Pod "downwardapi-volume-d30442a4-ff13-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006418944s
STEP: Saw pod success
Dec 13 20:15:26.465: INFO: Pod "downwardapi-volume-d30442a4-ff13-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:15:26.468: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-d30442a4-ff13-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:15:26.486: INFO: Waiting for pod downwardapi-volume-d30442a4-ff13-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:15:26.488: INFO: Pod downwardapi-volume-d30442a4-ff13-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:15:26.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-wkn7h" for this suite.
Dec 13 20:15:32.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:15:32.550: INFO: namespace: e2e-tests-downward-api-wkn7h, resource: bindings, ignored listing per whitelist
Dec 13 20:15:32.591: INFO: namespace e2e-tests-downward-api-wkn7h deletion completed in 6.09976378s

• [SLOW TEST:8.203 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:15:32.591: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 13 20:15:32.688: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:32.690: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:32.690: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:32.693: INFO: Number of nodes with available pods: 0
Dec 13 20:15:32.693: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:15:33.698: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:33.698: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:33.698: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:33.701: INFO: Number of nodes with available pods: 2
Dec 13 20:15:33.701: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 13 20:15:33.721: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:33.721: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:33.721: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:33.725: INFO: Number of nodes with available pods: 1
Dec 13 20:15:33.725: INFO: Node ip-10-0-3-214.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:15:34.733: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:34.733: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:34.733: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:15:34.736: INFO: Number of nodes with available pods: 2
Dec 13 20:15:34.736: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-shp8p, will wait for the garbage collector to delete the pods
Dec 13 20:15:34.802: INFO: Deleting DaemonSet.extensions daemon-set took: 7.79641ms
Dec 13 20:15:34.902: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.166095ms
Dec 13 20:16:08.609: INFO: Number of nodes with available pods: 0
Dec 13 20:16:08.609: INFO: Number of running nodes: 0, number of available pods: 0
Dec 13 20:16:08.613: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-shp8p/daemonsets","resourceVersion":"50056"},"items":null}

Dec 13 20:16:08.615: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-shp8p/pods","resourceVersion":"50056"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:16:08.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-shp8p" for this suite.
Dec 13 20:16:14.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:16:14.663: INFO: namespace: e2e-tests-daemonsets-shp8p, resource: bindings, ignored listing per whitelist
Dec 13 20:16:14.725: INFO: namespace e2e-tests-daemonsets-shp8p deletion completed in 6.097690295s

• [SLOW TEST:42.133 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:16:14.725: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-f10622ed-ff13-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:16:14.807: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f106ab5a-ff13-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-tdrth" to be "success or failure"
Dec 13 20:16:14.813: INFO: Pod "pod-projected-configmaps-f106ab5a-ff13-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.688466ms
Dec 13 20:16:16.816: INFO: Pod "pod-projected-configmaps-f106ab5a-ff13-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009282366s
STEP: Saw pod success
Dec 13 20:16:16.816: INFO: Pod "pod-projected-configmaps-f106ab5a-ff13-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:16:16.819: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-configmaps-f106ab5a-ff13-11e8-b666-eabb5e37e61c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:16:16.839: INFO: Waiting for pod pod-projected-configmaps-f106ab5a-ff13-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:16:16.842: INFO: Pod pod-projected-configmaps-f106ab5a-ff13-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:16:16.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tdrth" for this suite.
Dec 13 20:16:22.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:16:22.931: INFO: namespace: e2e-tests-projected-tdrth, resource: bindings, ignored listing per whitelist
Dec 13 20:16:22.945: INFO: namespace e2e-tests-projected-tdrth deletion completed in 6.099024698s

• [SLOW TEST:8.220 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:16:22.945: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Dec 13 20:16:23.018: INFO: Waiting up to 5m0s for pod "var-expansion-f5eb839f-ff13-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-var-expansion-7gm5f" to be "success or failure"
Dec 13 20:16:23.023: INFO: Pod "var-expansion-f5eb839f-ff13-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.859954ms
Dec 13 20:16:25.027: INFO: Pod "var-expansion-f5eb839f-ff13-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009329891s
STEP: Saw pod success
Dec 13 20:16:25.027: INFO: Pod "var-expansion-f5eb839f-ff13-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:16:25.030: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod var-expansion-f5eb839f-ff13-11e8-b666-eabb5e37e61c container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:16:25.048: INFO: Waiting for pod var-expansion-f5eb839f-ff13-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:16:25.050: INFO: Pod var-expansion-f5eb839f-ff13-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:16:25.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-7gm5f" for this suite.
Dec 13 20:16:31.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:16:31.112: INFO: namespace: e2e-tests-var-expansion-7gm5f, resource: bindings, ignored listing per whitelist
Dec 13 20:16:31.149: INFO: namespace e2e-tests-var-expansion-7gm5f deletion completed in 6.095800555s

• [SLOW TEST:8.204 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:16:31.149: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-fad198f2-ff13-11e8-b666-eabb5e37e61c
STEP: Creating configMap with name cm-test-opt-upd-fad19930-ff13-11e8-b666-eabb5e37e61c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fad198f2-ff13-11e8-b666-eabb5e37e61c
STEP: Updating configmap cm-test-opt-upd-fad19930-ff13-11e8-b666-eabb5e37e61c
STEP: Creating configMap with name cm-test-opt-create-fad19949-ff13-11e8-b666-eabb5e37e61c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:16:35.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-x8s6m" for this suite.
Dec 13 20:16:57.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:16:57.328: INFO: namespace: e2e-tests-projected-x8s6m, resource: bindings, ignored listing per whitelist
Dec 13 20:16:57.406: INFO: namespace e2e-tests-projected-x8s6m deletion completed in 22.095744776s

• [SLOW TEST:26.256 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:16:57.406: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 13 20:17:01.530: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:01.532: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:03.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:03.537: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:05.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:05.536: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:07.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:07.536: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:09.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:09.536: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:11.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:11.540: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:13.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:13.537: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:15.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:15.537: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:17.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:17.537: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:19.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:19.536: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:21.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:21.536: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:23.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:23.540: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:25.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:25.537: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:27.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:27.536: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 13 20:17:29.533: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 13 20:17:29.536: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:17:29.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-pz4vs" for this suite.
Dec 13 20:17:51.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:17:51.590: INFO: namespace: e2e-tests-container-lifecycle-hook-pz4vs, resource: bindings, ignored listing per whitelist
Dec 13 20:17:51.643: INFO: namespace e2e-tests-container-lifecycle-hook-pz4vs deletion completed in 22.097300355s

• [SLOW TEST:54.237 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:17:51.643: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-smx4
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 20:17:51.729: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-smx4" in namespace "e2e-tests-subpath-c725p" to be "success or failure"
Dec 13 20:17:51.731: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439723ms
Dec 13 20:17:53.735: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006109717s
Dec 13 20:17:55.743: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 4.013966566s
Dec 13 20:17:57.756: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 6.027247056s
Dec 13 20:17:59.760: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 8.030712037s
Dec 13 20:18:01.763: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 10.034528116s
Dec 13 20:18:03.767: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 12.038191379s
Dec 13 20:18:05.775: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 14.045910327s
Dec 13 20:18:07.778: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 16.049489405s
Dec 13 20:18:09.782: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 18.052863989s
Dec 13 20:18:11.785: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 20.056379424s
Dec 13 20:18:13.789: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Running", Reason="", readiness=false. Elapsed: 22.060082773s
Dec 13 20:18:15.796: INFO: Pod "pod-subpath-test-secret-smx4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.067608341s
STEP: Saw pod success
Dec 13 20:18:15.796: INFO: Pod "pod-subpath-test-secret-smx4" satisfied condition "success or failure"
Dec 13 20:18:15.799: INFO: Trying to get logs from node ip-10-0-2-167.eu-west-1.compute.internal pod pod-subpath-test-secret-smx4 container test-container-subpath-secret-smx4: <nil>
STEP: delete the pod
Dec 13 20:18:15.820: INFO: Waiting for pod pod-subpath-test-secret-smx4 to disappear
Dec 13 20:18:15.826: INFO: Pod pod-subpath-test-secret-smx4 no longer exists
STEP: Deleting pod pod-subpath-test-secret-smx4
Dec 13 20:18:15.826: INFO: Deleting pod "pod-subpath-test-secret-smx4" in namespace "e2e-tests-subpath-c725p"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:18:15.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-c725p" for this suite.
Dec 13 20:18:21.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:18:21.919: INFO: namespace: e2e-tests-subpath-c725p, resource: bindings, ignored listing per whitelist
Dec 13 20:18:21.928: INFO: namespace e2e-tests-subpath-c725p deletion completed in 6.095732514s

• [SLOW TEST:30.284 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:18:21.928: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-drb9g
Dec 13 20:18:24.013: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-drb9g
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 20:18:24.016: INFO: Initial restart count of pod liveness-exec is 0
Dec 13 20:19:16.139: INFO: Restart count of pod e2e-tests-container-probe-drb9g/liveness-exec is now 1 (52.123004816s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:19:16.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-drb9g" for this suite.
Dec 13 20:19:22.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:19:22.187: INFO: namespace: e2e-tests-container-probe-drb9g, resource: bindings, ignored listing per whitelist
Dec 13 20:19:22.257: INFO: namespace e2e-tests-container-probe-drb9g deletion completed in 6.102364747s

• [SLOW TEST:60.329 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:19:22.257: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-60cda110-ff14-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:19:22.342: INFO: Waiting up to 5m0s for pod "pod-configmaps-60ce32df-ff14-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-configmap-lwcld" to be "success or failure"
Dec 13 20:19:22.349: INFO: Pod "pod-configmaps-60ce32df-ff14-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.653137ms
Dec 13 20:19:24.352: INFO: Pod "pod-configmaps-60ce32df-ff14-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010305633s
STEP: Saw pod success
Dec 13 20:19:24.352: INFO: Pod "pod-configmaps-60ce32df-ff14-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:19:24.355: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-60ce32df-ff14-11e8-b666-eabb5e37e61c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:19:24.375: INFO: Waiting for pod pod-configmaps-60ce32df-ff14-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:19:24.377: INFO: Pod pod-configmaps-60ce32df-ff14-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:19:24.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-lwcld" for this suite.
Dec 13 20:19:30.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:19:30.437: INFO: namespace: e2e-tests-configmap-lwcld, resource: bindings, ignored listing per whitelist
Dec 13 20:19:30.491: INFO: namespace e2e-tests-configmap-lwcld deletion completed in 6.110446365s

• [SLOW TEST:8.234 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:19:30.492: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:19:30.572: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 13 20:19:30.580: INFO: Number of nodes with available pods: 0
Dec 13 20:19:30.580: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 13 20:19:30.599: INFO: Number of nodes with available pods: 0
Dec 13 20:19:30.599: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:31.603: INFO: Number of nodes with available pods: 0
Dec 13 20:19:31.603: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:32.605: INFO: Number of nodes with available pods: 1
Dec 13 20:19:32.605: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 13 20:19:32.624: INFO: Number of nodes with available pods: 1
Dec 13 20:19:32.643: INFO: Number of running nodes: 0, number of available pods: 1
Dec 13 20:19:33.648: INFO: Number of nodes with available pods: 0
Dec 13 20:19:33.648: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 13 20:19:33.661: INFO: Number of nodes with available pods: 0
Dec 13 20:19:33.661: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:34.665: INFO: Number of nodes with available pods: 0
Dec 13 20:19:34.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:35.665: INFO: Number of nodes with available pods: 0
Dec 13 20:19:35.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:36.669: INFO: Number of nodes with available pods: 0
Dec 13 20:19:36.669: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:37.665: INFO: Number of nodes with available pods: 0
Dec 13 20:19:37.666: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:38.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:38.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:39.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:39.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:40.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:40.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:41.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:41.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:42.665: INFO: Number of nodes with available pods: 0
Dec 13 20:19:42.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:43.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:43.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:44.665: INFO: Number of nodes with available pods: 0
Dec 13 20:19:44.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:45.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:45.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:46.665: INFO: Number of nodes with available pods: 0
Dec 13 20:19:46.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:47.673: INFO: Number of nodes with available pods: 0
Dec 13 20:19:47.673: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:48.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:48.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:49.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:49.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:50.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:50.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:51.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:51.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:52.665: INFO: Number of nodes with available pods: 0
Dec 13 20:19:52.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:53.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:53.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:54.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:54.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:55.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:55.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:56.665: INFO: Number of nodes with available pods: 0
Dec 13 20:19:56.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:57.665: INFO: Number of nodes with available pods: 0
Dec 13 20:19:57.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:58.668: INFO: Number of nodes with available pods: 0
Dec 13 20:19:58.668: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:19:59.664: INFO: Number of nodes with available pods: 0
Dec 13 20:19:59.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:20:00.664: INFO: Number of nodes with available pods: 0
Dec 13 20:20:00.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:20:01.664: INFO: Number of nodes with available pods: 0
Dec 13 20:20:01.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:20:02.665: INFO: Number of nodes with available pods: 0
Dec 13 20:20:02.665: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:20:03.664: INFO: Number of nodes with available pods: 0
Dec 13 20:20:03.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:20:04.664: INFO: Number of nodes with available pods: 0
Dec 13 20:20:04.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:20:05.664: INFO: Number of nodes with available pods: 0
Dec 13 20:20:05.664: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:20:06.670: INFO: Number of nodes with available pods: 0
Dec 13 20:20:06.670: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:20:07.670: INFO: Number of nodes with available pods: 1
Dec 13 20:20:07.670: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-9ddpg, will wait for the garbage collector to delete the pods
Dec 13 20:20:07.756: INFO: Deleting DaemonSet.extensions daemon-set took: 26.07115ms
Dec 13 20:20:07.857: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.167744ms
Dec 13 20:20:47.666: INFO: Number of nodes with available pods: 0
Dec 13 20:20:47.666: INFO: Number of running nodes: 0, number of available pods: 0
Dec 13 20:20:47.670: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-9ddpg/daemonsets","resourceVersion":"50881"},"items":null}

Dec 13 20:20:47.673: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-9ddpg/pods","resourceVersion":"50881"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:20:47.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-9ddpg" for this suite.
Dec 13 20:20:53.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:20:53.787: INFO: namespace: e2e-tests-daemonsets-9ddpg, resource: bindings, ignored listing per whitelist
Dec 13 20:20:53.817: INFO: namespace e2e-tests-daemonsets-9ddpg deletion completed in 6.107785489s

• [SLOW TEST:83.326 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:20:53.818: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1213 20:20:59.923952      14 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 20:20:59.924: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:20:59.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-6q9xg" for this suite.
Dec 13 20:21:05.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:21:05.959: INFO: namespace: e2e-tests-gc-6q9xg, resource: bindings, ignored listing per whitelist
Dec 13 20:21:06.024: INFO: namespace e2e-tests-gc-6q9xg deletion completed in 6.096432764s

• [SLOW TEST:12.206 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:21:06.024: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:21:06.101: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ea69f64-ff14-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-g2678" to be "success or failure"
Dec 13 20:21:06.104: INFO: Pod "downwardapi-volume-9ea69f64-ff14-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.075772ms
Dec 13 20:21:08.107: INFO: Pod "downwardapi-volume-9ea69f64-ff14-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006610944s
STEP: Saw pod success
Dec 13 20:21:08.107: INFO: Pod "downwardapi-volume-9ea69f64-ff14-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:21:08.110: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-9ea69f64-ff14-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:21:08.127: INFO: Waiting for pod downwardapi-volume-9ea69f64-ff14-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:21:08.130: INFO: Pod downwardapi-volume-9ea69f64-ff14-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:21:08.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-g2678" for this suite.
Dec 13 20:21:14.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:21:14.156: INFO: namespace: e2e-tests-projected-g2678, resource: bindings, ignored listing per whitelist
Dec 13 20:21:14.227: INFO: namespace e2e-tests-projected-g2678 deletion completed in 6.093527696s

• [SLOW TEST:8.203 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:21:14.227: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:21:38.309: INFO: Container started at 2018-12-13 20:21:15 +0000 UTC, pod became ready at 2018-12-13 20:21:37 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:21:38.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-rlj75" for this suite.
Dec 13 20:22:00.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:22:00.394: INFO: namespace: e2e-tests-container-probe-rlj75, resource: bindings, ignored listing per whitelist
Dec 13 20:22:00.423: INFO: namespace e2e-tests-container-probe-rlj75 deletion completed in 22.110748242s

• [SLOW TEST:46.196 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:22:00.423: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec 13 20:22:00.496: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:22:02.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-79bgh" for this suite.
Dec 13 20:22:08.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:22:08.797: INFO: namespace: e2e-tests-init-container-79bgh, resource: bindings, ignored listing per whitelist
Dec 13 20:22:08.839: INFO: namespace e2e-tests-init-container-79bgh deletion completed in 6.095603314s

• [SLOW TEST:8.416 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:22:08.839: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Dec 13 20:22:08.909: INFO: Waiting up to 5m0s for pod "var-expansion-c4169f91-ff14-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-var-expansion-vkd47" to be "success or failure"
Dec 13 20:22:08.912: INFO: Pod "var-expansion-c4169f91-ff14-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924698ms
Dec 13 20:22:10.916: INFO: Pod "var-expansion-c4169f91-ff14-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006666733s
STEP: Saw pod success
Dec 13 20:22:10.916: INFO: Pod "var-expansion-c4169f91-ff14-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:22:10.919: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod var-expansion-c4169f91-ff14-11e8-b666-eabb5e37e61c container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:22:10.937: INFO: Waiting for pod var-expansion-c4169f91-ff14-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:22:10.940: INFO: Pod var-expansion-c4169f91-ff14-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:22:10.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-vkd47" for this suite.
Dec 13 20:22:16.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:22:16.979: INFO: namespace: e2e-tests-var-expansion-vkd47, resource: bindings, ignored listing per whitelist
Dec 13 20:22:17.041: INFO: namespace e2e-tests-var-expansion-vkd47 deletion completed in 6.096099431s

• [SLOW TEST:8.202 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:22:17.041: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec 13 20:22:17.119: INFO: Waiting up to 5m0s for pod "downward-api-c8fb3a51-ff14-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-cfhjt" to be "success or failure"
Dec 13 20:22:17.123: INFO: Pod "downward-api-c8fb3a51-ff14-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111943ms
Dec 13 20:22:19.126: INFO: Pod "downward-api-c8fb3a51-ff14-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006352865s
STEP: Saw pod success
Dec 13 20:22:19.126: INFO: Pod "downward-api-c8fb3a51-ff14-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:22:19.128: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downward-api-c8fb3a51-ff14-11e8-b666-eabb5e37e61c container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:22:19.149: INFO: Waiting for pod downward-api-c8fb3a51-ff14-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:22:19.151: INFO: Pod downward-api-c8fb3a51-ff14-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:22:19.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-cfhjt" for this suite.
Dec 13 20:22:25.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:22:25.207: INFO: namespace: e2e-tests-downward-api-cfhjt, resource: bindings, ignored listing per whitelist
Dec 13 20:22:25.254: INFO: namespace e2e-tests-downward-api-cfhjt deletion completed in 6.099189989s

• [SLOW TEST:8.213 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:22:25.254: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Dec 13 20:22:25.326: INFO: Waiting up to 5m0s for pod "client-containers-cddf94f1-ff14-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-containers-rnzvq" to be "success or failure"
Dec 13 20:22:25.329: INFO: Pod "client-containers-cddf94f1-ff14-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.469097ms
Dec 13 20:22:27.332: INFO: Pod "client-containers-cddf94f1-ff14-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006077902s
Dec 13 20:22:29.336: INFO: Pod "client-containers-cddf94f1-ff14-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009709161s
STEP: Saw pod success
Dec 13 20:22:29.336: INFO: Pod "client-containers-cddf94f1-ff14-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:22:29.339: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod client-containers-cddf94f1-ff14-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:22:29.355: INFO: Waiting for pod client-containers-cddf94f1-ff14-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:22:29.358: INFO: Pod client-containers-cddf94f1-ff14-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:22:29.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-rnzvq" for this suite.
Dec 13 20:22:35.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:22:35.414: INFO: namespace: e2e-tests-containers-rnzvq, resource: bindings, ignored listing per whitelist
Dec 13 20:22:35.460: INFO: namespace e2e-tests-containers-rnzvq deletion completed in 6.098691075s

• [SLOW TEST:10.206 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:22:35.460: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-d3f5c2a3-ff14-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:22:35.544: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3f69b6f-ff14-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-972mh" to be "success or failure"
Dec 13 20:22:35.546: INFO: Pod "pod-projected-secrets-d3f69b6f-ff14-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.58194ms
Dec 13 20:22:37.550: INFO: Pod "pod-projected-secrets-d3f69b6f-ff14-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006007542s
STEP: Saw pod success
Dec 13 20:22:37.550: INFO: Pod "pod-projected-secrets-d3f69b6f-ff14-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:22:37.553: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-secrets-d3f69b6f-ff14-11e8-b666-eabb5e37e61c container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:22:37.570: INFO: Waiting for pod pod-projected-secrets-d3f69b6f-ff14-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:22:37.572: INFO: Pod pod-projected-secrets-d3f69b6f-ff14-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:22:37.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-972mh" for this suite.
Dec 13 20:22:43.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:22:43.607: INFO: namespace: e2e-tests-projected-972mh, resource: bindings, ignored listing per whitelist
Dec 13 20:22:43.678: INFO: namespace e2e-tests-projected-972mh deletion completed in 6.102495236s

• [SLOW TEST:8.218 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:22:43.678: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Dec 13 20:22:45.765: INFO: Pod pod-hostip-d8da9f6c-ff14-11e8-b666-eabb5e37e61c has hostIP: 10.0.3.214
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:22:45.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-jrm47" for this suite.
Dec 13 20:23:07.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:23:07.823: INFO: namespace: e2e-tests-pods-jrm47, resource: bindings, ignored listing per whitelist
Dec 13 20:23:07.869: INFO: namespace e2e-tests-pods-jrm47 deletion completed in 22.100864901s

• [SLOW TEST:24.191 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:23:07.870: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-q9pr6
Dec 13 20:23:09.956: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-q9pr6
STEP: checking the pod's current state and verifying that restartCount is present
Dec 13 20:23:09.959: INFO: Initial restart count of pod liveness-http is 0
Dec 13 20:23:25.995: INFO: Restart count of pod e2e-tests-container-probe-q9pr6/liveness-http is now 1 (16.036277674s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:23:26.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-q9pr6" for this suite.
Dec 13 20:23:32.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:23:32.112: INFO: namespace: e2e-tests-container-probe-q9pr6, resource: bindings, ignored listing per whitelist
Dec 13 20:23:32.115: INFO: namespace e2e-tests-container-probe-q9pr6 deletion completed in 6.101815714s

• [SLOW TEST:24.245 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:23:32.115: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 13 20:23:32.206: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6v2cx,SelfLink:/api/v1/namespaces/e2e-tests-watch-6v2cx/configmaps/e2e-watch-test-label-changed,UID:f5ba8a95-ff14-11e8-b982-02b9355b966e,ResourceVersion:51676,Generation:0,CreationTimestamp:2018-12-13 20:23:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 20:23:32.206: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6v2cx,SelfLink:/api/v1/namespaces/e2e-tests-watch-6v2cx/configmaps/e2e-watch-test-label-changed,UID:f5ba8a95-ff14-11e8-b982-02b9355b966e,ResourceVersion:51677,Generation:0,CreationTimestamp:2018-12-13 20:23:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 13 20:23:32.207: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6v2cx,SelfLink:/api/v1/namespaces/e2e-tests-watch-6v2cx/configmaps/e2e-watch-test-label-changed,UID:f5ba8a95-ff14-11e8-b982-02b9355b966e,ResourceVersion:51678,Generation:0,CreationTimestamp:2018-12-13 20:23:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 13 20:23:42.236: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6v2cx,SelfLink:/api/v1/namespaces/e2e-tests-watch-6v2cx/configmaps/e2e-watch-test-label-changed,UID:f5ba8a95-ff14-11e8-b982-02b9355b966e,ResourceVersion:51698,Generation:0,CreationTimestamp:2018-12-13 20:23:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 20:23:42.237: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6v2cx,SelfLink:/api/v1/namespaces/e2e-tests-watch-6v2cx/configmaps/e2e-watch-test-label-changed,UID:f5ba8a95-ff14-11e8-b982-02b9355b966e,ResourceVersion:51699,Generation:0,CreationTimestamp:2018-12-13 20:23:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 13 20:23:42.237: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-6v2cx,SelfLink:/api/v1/namespaces/e2e-tests-watch-6v2cx/configmaps/e2e-watch-test-label-changed,UID:f5ba8a95-ff14-11e8-b982-02b9355b966e,ResourceVersion:51700,Generation:0,CreationTimestamp:2018-12-13 20:23:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:23:42.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-6v2cx" for this suite.
Dec 13 20:23:48.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:23:48.302: INFO: namespace: e2e-tests-watch-6v2cx, resource: bindings, ignored listing per whitelist
Dec 13 20:23:48.337: INFO: namespace e2e-tests-watch-6v2cx deletion completed in 6.097103374s

• [SLOW TEST:16.222 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:23:48.338: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-ff660711-ff14-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:23:48.421: INFO: Waiting up to 5m0s for pod "pod-secrets-ff669bdd-ff14-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-secrets-nsfjs" to be "success or failure"
Dec 13 20:23:48.431: INFO: Pod "pod-secrets-ff669bdd-ff14-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.724422ms
Dec 13 20:23:50.434: INFO: Pod "pod-secrets-ff669bdd-ff14-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013325168s
STEP: Saw pod success
Dec 13 20:23:50.434: INFO: Pod "pod-secrets-ff669bdd-ff14-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:23:50.437: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-secrets-ff669bdd-ff14-11e8-b666-eabb5e37e61c container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:23:50.455: INFO: Waiting for pod pod-secrets-ff669bdd-ff14-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:23:50.458: INFO: Pod pod-secrets-ff669bdd-ff14-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:23:50.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nsfjs" for this suite.
Dec 13 20:23:56.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:23:56.535: INFO: namespace: e2e-tests-secrets-nsfjs, resource: bindings, ignored listing per whitelist
Dec 13 20:23:56.557: INFO: namespace e2e-tests-secrets-nsfjs deletion completed in 6.09509357s

• [SLOW TEST:8.219 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:23:56.557: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-044bdca5-ff15-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:23:56.638: INFO: Waiting up to 5m0s for pod "pod-configmaps-044c9080-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-configmap-cjmj2" to be "success or failure"
Dec 13 20:23:56.641: INFO: Pod "pod-configmaps-044c9080-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.360223ms
Dec 13 20:23:58.645: INFO: Pod "pod-configmaps-044c9080-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007111508s
STEP: Saw pod success
Dec 13 20:23:58.645: INFO: Pod "pod-configmaps-044c9080-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:23:58.648: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-044c9080-ff15-11e8-b666-eabb5e37e61c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:23:58.666: INFO: Waiting for pod pod-configmaps-044c9080-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:23:58.669: INFO: Pod pod-configmaps-044c9080-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:23:58.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cjmj2" for this suite.
Dec 13 20:24:04.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:04.717: INFO: namespace: e2e-tests-configmap-cjmj2, resource: bindings, ignored listing per whitelist
Dec 13 20:24:04.767: INFO: namespace e2e-tests-configmap-cjmj2 deletion completed in 6.094666215s

• [SLOW TEST:8.210 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:24:04.767: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-09331d9c-ff15-11e8-b666-eabb5e37e61c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-09331d9c-ff15-11e8-b666-eabb5e37e61c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:24:08.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cz6t6" for this suite.
Dec 13 20:24:30.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:30.985: INFO: namespace: e2e-tests-configmap-cz6t6, resource: bindings, ignored listing per whitelist
Dec 13 20:24:31.019: INFO: namespace e2e-tests-configmap-cz6t6 deletion completed in 22.117960333s

• [SLOW TEST:26.251 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:24:31.019: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-18d662ff-ff15-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:24:31.106: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18d708da-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-v46g6" to be "success or failure"
Dec 13 20:24:31.110: INFO: Pod "pod-projected-secrets-18d708da-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.285805ms
Dec 13 20:24:33.113: INFO: Pod "pod-projected-secrets-18d708da-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0069382s
STEP: Saw pod success
Dec 13 20:24:33.113: INFO: Pod "pod-projected-secrets-18d708da-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:24:33.116: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-secrets-18d708da-ff15-11e8-b666-eabb5e37e61c container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:24:33.138: INFO: Waiting for pod pod-projected-secrets-18d708da-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:24:33.142: INFO: Pod pod-projected-secrets-18d708da-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:24:33.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v46g6" for this suite.
Dec 13 20:24:39.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:39.230: INFO: namespace: e2e-tests-projected-v46g6, resource: bindings, ignored listing per whitelist
Dec 13 20:24:39.243: INFO: namespace e2e-tests-projected-v46g6 deletion completed in 6.097759608s

• [SLOW TEST:8.224 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:24:39.243: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1213 20:24:49.383168      14 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 20:24:49.383: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:24:49.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-hr7v6" for this suite.
Dec 13 20:24:55.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:24:55.466: INFO: namespace: e2e-tests-gc-hr7v6, resource: bindings, ignored listing per whitelist
Dec 13 20:24:55.483: INFO: namespace e2e-tests-gc-hr7v6 deletion completed in 6.097196615s

• [SLOW TEST:16.241 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:24:55.484: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-276c5ffc-ff15-11e8-b666-eabb5e37e61c
STEP: Creating configMap with name cm-test-opt-upd-276c604a-ff15-11e8-b666-eabb5e37e61c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-276c5ffc-ff15-11e8-b666-eabb5e37e61c
STEP: Updating configmap cm-test-opt-upd-276c604a-ff15-11e8-b666-eabb5e37e61c
STEP: Creating configMap with name cm-test-opt-create-276c6067-ff15-11e8-b666-eabb5e37e61c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:24:59.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8c5ln" for this suite.
Dec 13 20:25:21.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:25:21.713: INFO: namespace: e2e-tests-configmap-8c5ln, resource: bindings, ignored listing per whitelist
Dec 13 20:25:21.749: INFO: namespace e2e-tests-configmap-8c5ln deletion completed in 22.098050452s

• [SLOW TEST:26.265 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:25:21.749: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1213 20:25:31.852708      14 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 20:25:31.852: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:25:31.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-vtwg6" for this suite.
Dec 13 20:25:37.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:25:37.925: INFO: namespace: e2e-tests-gc-vtwg6, resource: bindings, ignored listing per whitelist
Dec 13 20:25:37.960: INFO: namespace e2e-tests-gc-vtwg6 deletion completed in 6.103050338s

• [SLOW TEST:16.210 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:25:37.960: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 13 20:25:38.039: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-a,UID:40bcf44b-ff15-11e8-b982-02b9355b966e,ResourceVersion:52369,Generation:0,CreationTimestamp:2018-12-13 20:25:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 20:25:38.040: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-a,UID:40bcf44b-ff15-11e8-b982-02b9355b966e,ResourceVersion:52369,Generation:0,CreationTimestamp:2018-12-13 20:25:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 13 20:25:48.052: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-a,UID:40bcf44b-ff15-11e8-b982-02b9355b966e,ResourceVersion:52387,Generation:0,CreationTimestamp:2018-12-13 20:25:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 13 20:25:48.052: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-a,UID:40bcf44b-ff15-11e8-b982-02b9355b966e,ResourceVersion:52387,Generation:0,CreationTimestamp:2018-12-13 20:25:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 13 20:25:58.064: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-a,UID:40bcf44b-ff15-11e8-b982-02b9355b966e,ResourceVersion:52406,Generation:0,CreationTimestamp:2018-12-13 20:25:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 20:25:58.064: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-a,UID:40bcf44b-ff15-11e8-b982-02b9355b966e,ResourceVersion:52406,Generation:0,CreationTimestamp:2018-12-13 20:25:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 13 20:26:08.075: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-a,UID:40bcf44b-ff15-11e8-b982-02b9355b966e,ResourceVersion:52425,Generation:0,CreationTimestamp:2018-12-13 20:25:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 20:26:08.075: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-a,UID:40bcf44b-ff15-11e8-b982-02b9355b966e,ResourceVersion:52425,Generation:0,CreationTimestamp:2018-12-13 20:25:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 13 20:26:18.085: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-b,UID:589b303c-ff15-11e8-b982-02b9355b966e,ResourceVersion:52444,Generation:0,CreationTimestamp:2018-12-13 20:26:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 20:26:18.086: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-b,UID:589b303c-ff15-11e8-b982-02b9355b966e,ResourceVersion:52444,Generation:0,CreationTimestamp:2018-12-13 20:26:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 13 20:26:28.097: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-b,UID:589b303c-ff15-11e8-b982-02b9355b966e,ResourceVersion:52463,Generation:0,CreationTimestamp:2018-12-13 20:26:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 13 20:26:28.097: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-s9mtb,SelfLink:/api/v1/namespaces/e2e-tests-watch-s9mtb/configmaps/e2e-watch-test-configmap-b,UID:589b303c-ff15-11e8-b982-02b9355b966e,ResourceVersion:52463,Generation:0,CreationTimestamp:2018-12-13 20:26:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:26:38.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-s9mtb" for this suite.
Dec 13 20:26:44.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:26:44.156: INFO: namespace: e2e-tests-watch-s9mtb, resource: bindings, ignored listing per whitelist
Dec 13 20:26:44.207: INFO: namespace e2e-tests-watch-s9mtb deletion completed in 6.100889045s

• [SLOW TEST:66.247 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:26:44.207: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:26:44.285: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68394d2f-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-5qcqs" to be "success or failure"
Dec 13 20:26:44.291: INFO: Pod "downwardapi-volume-68394d2f-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.42197ms
Dec 13 20:26:46.294: INFO: Pod "downwardapi-volume-68394d2f-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008904325s
STEP: Saw pod success
Dec 13 20:26:46.294: INFO: Pod "downwardapi-volume-68394d2f-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:26:46.297: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-68394d2f-ff15-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:26:46.320: INFO: Waiting for pod downwardapi-volume-68394d2f-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:26:46.322: INFO: Pod downwardapi-volume-68394d2f-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:26:46.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5qcqs" for this suite.
Dec 13 20:26:52.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:26:52.393: INFO: namespace: e2e-tests-downward-api-5qcqs, resource: bindings, ignored listing per whitelist
Dec 13 20:26:52.435: INFO: namespace e2e-tests-downward-api-5qcqs deletion completed in 6.109302233s

• [SLOW TEST:8.228 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:26:52.435: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:26:52.513: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d203d2b-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-mzkph" to be "success or failure"
Dec 13 20:26:52.516: INFO: Pod "downwardapi-volume-6d203d2b-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.957362ms
Dec 13 20:26:54.520: INFO: Pod "downwardapi-volume-6d203d2b-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006604447s
STEP: Saw pod success
Dec 13 20:26:54.520: INFO: Pod "downwardapi-volume-6d203d2b-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:26:54.522: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-6d203d2b-ff15-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:26:54.539: INFO: Waiting for pod downwardapi-volume-6d203d2b-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:26:54.545: INFO: Pod downwardapi-volume-6d203d2b-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:26:54.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mzkph" for this suite.
Dec 13 20:27:00.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:27:00.568: INFO: namespace: e2e-tests-downward-api-mzkph, resource: bindings, ignored listing per whitelist
Dec 13 20:27:00.644: INFO: namespace e2e-tests-downward-api-mzkph deletion completed in 6.095718297s

• [SLOW TEST:8.209 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:27:00.645: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec 13 20:27:00.722: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 13 20:27:00.728: INFO: Waiting for terminating namespaces to be deleted...
Dec 13 20:27:00.731: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-167.eu-west-1.compute.internal before test
Dec 13 20:27:00.738: INFO: kube-proxy-246r7 from kube-system started at 2018-12-13 14:51:13 +0000 UTC (1 container statuses recorded)
Dec 13 20:27:00.738: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:27:00.738: INFO: coredns-86c58d9df4-jdbxq from kube-system started at 2018-12-13 19:48:04 +0000 UTC (1 container statuses recorded)
Dec 13 20:27:00.738: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:27:00.738: INFO: sonobuoy-e2e-job-87b901a1b8cd4728 from heptio-sonobuoy started at 2018-12-13 19:50:51 +0000 UTC (2 container statuses recorded)
Dec 13 20:27:00.738: INFO: 	Container e2e ready: true, restart count 0
Dec 13 20:27:00.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 20:27:00.738: INFO: calico-node-p8l9p from kube-system started at 2018-12-13 15:23:12 +0000 UTC (1 container statuses recorded)
Dec 13 20:27:00.738: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:27:00.738: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-13 19:50:49 +0000 UTC (1 container statuses recorded)
Dec 13 20:27:00.738: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 13 20:27:00.738: INFO: sonobuoy-systemd-logs-daemon-set-e5a404e9fcc945c3-zrx9c from heptio-sonobuoy started at 2018-12-13 19:50:51 +0000 UTC (2 container statuses recorded)
Dec 13 20:27:00.738: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec 13 20:27:00.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 20:27:00.738: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-214.eu-west-1.compute.internal before test
Dec 13 20:27:00.744: INFO: kube-proxy-kqrgn from kube-system started at 2018-12-13 14:51:08 +0000 UTC (1 container statuses recorded)
Dec 13 20:27:00.744: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:27:00.744: INFO: calico-node-h9tzd from kube-system started at 2018-12-13 15:23:05 +0000 UTC (1 container statuses recorded)
Dec 13 20:27:00.744: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:27:00.744: INFO: nginx-7cdbd8cdc9-qwxrq from default started at 2018-12-13 19:48:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:27:00.744: INFO: 	Container nginx ready: true, restart count 0
Dec 13 20:27:00.744: INFO: calicoctl from kube-system started at 2018-12-13 15:22:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:27:00.744: INFO: 	Container calicoctl ready: true, restart count 0
Dec 13 20:27:00.744: INFO: coredns-86c58d9df4-8x9zn from kube-system started at 2018-12-13 19:48:04 +0000 UTC (1 container statuses recorded)
Dec 13 20:27:00.744: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:27:00.744: INFO: sonobuoy-systemd-logs-daemon-set-e5a404e9fcc945c3-c4ztz from heptio-sonobuoy started at 2018-12-13 19:50:51 +0000 UTC (2 container statuses recorded)
Dec 13 20:27:00.744: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec 13 20:27:00.744: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.156ffe50ee854396], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:27:01.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-t6x8n" for this suite.
Dec 13 20:27:07.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:27:07.801: INFO: namespace: e2e-tests-sched-pred-t6x8n, resource: bindings, ignored listing per whitelist
Dec 13 20:27:07.870: INFO: namespace e2e-tests-sched-pred-t6x8n deletion completed in 6.101511338s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.226 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:27:07.870: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec 13 20:27:10.471: INFO: Successfully updated pod "labelsupdate765350b0-ff15-11e8-b666-eabb5e37e61c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:27:14.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fxts6" for this suite.
Dec 13 20:27:36.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:27:36.552: INFO: namespace: e2e-tests-downward-api-fxts6, resource: bindings, ignored listing per whitelist
Dec 13 20:27:36.594: INFO: namespace e2e-tests-downward-api-fxts6 deletion completed in 22.09848837s

• [SLOW TEST:28.723 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:27:36.594: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Dec 13 20:27:37.231: INFO: created pod pod-service-account-defaultsa
Dec 13 20:27:37.231: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 13 20:27:37.236: INFO: created pod pod-service-account-mountsa
Dec 13 20:27:37.236: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 13 20:27:37.245: INFO: created pod pod-service-account-nomountsa
Dec 13 20:27:37.245: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 13 20:27:37.251: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 13 20:27:37.251: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 13 20:27:37.264: INFO: created pod pod-service-account-mountsa-mountspec
Dec 13 20:27:37.271: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 13 20:27:37.278: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 13 20:27:37.278: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 13 20:27:37.284: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 13 20:27:37.284: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 13 20:27:37.292: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 13 20:27:37.292: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 13 20:27:37.300: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 13 20:27:37.300: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:27:37.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-cjc8v" for this suite.
Dec 13 20:27:43.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:27:43.349: INFO: namespace: e2e-tests-svcaccounts-cjc8v, resource: bindings, ignored listing per whitelist
Dec 13 20:27:43.405: INFO: namespace e2e-tests-svcaccounts-cjc8v deletion completed in 6.099911248s

• [SLOW TEST:6.812 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:27:43.406: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 13 20:27:43.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-p4nzl'
Dec 13 20:27:43.723: INFO: stderr: ""
Dec 13 20:27:43.723: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec 13 20:27:48.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-p4nzl -o json'
Dec 13 20:27:48.833: INFO: stderr: ""
Dec 13 20:27:48.833: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.3.161/32\"\n        },\n        \"creationTimestamp\": \"2018-12-13T20:27:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-p4nzl\",\n        \"resourceVersion\": \"52821\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-p4nzl/pods/e2e-test-nginx-pod\",\n        \"uid\": \"8ba59660-ff15-11e8-b982-02b9355b966e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-kzmjx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-3-214.eu-west-1.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-kzmjx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-kzmjx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-13T20:27:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-13T20:27:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-13T20:27:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2018-12-13T20:27:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://39bb358cd6b3240245a6057648507804cbcc6da9a0e07d2a98d85ddf29b7e9ea\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2018-12-13T20:27:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.3.214\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.3.161\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2018-12-13T20:27:43Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 13 20:27:48.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 replace -f - --namespace=e2e-tests-kubectl-p4nzl'
Dec 13 20:27:48.975: INFO: stderr: ""
Dec 13 20:27:48.975: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Dec 13 20:27:48.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-p4nzl'
Dec 13 20:27:57.962: INFO: stderr: ""
Dec 13 20:27:57.962: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:27:57.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-p4nzl" for this suite.
Dec 13 20:28:03.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:28:03.993: INFO: namespace: e2e-tests-kubectl-p4nzl, resource: bindings, ignored listing per whitelist
Dec 13 20:28:04.066: INFO: namespace e2e-tests-kubectl-p4nzl deletion completed in 6.096367348s

• [SLOW TEST:20.660 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:28:04.066: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:28:04.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97d340d1-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-544qz" to be "success or failure"
Dec 13 20:28:04.149: INFO: Pod "downwardapi-volume-97d340d1-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765105ms
Dec 13 20:28:06.154: INFO: Pod "downwardapi-volume-97d340d1-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00760563s
STEP: Saw pod success
Dec 13 20:28:06.154: INFO: Pod "downwardapi-volume-97d340d1-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:28:06.156: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-97d340d1-ff15-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:28:06.174: INFO: Waiting for pod downwardapi-volume-97d340d1-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:28:06.177: INFO: Pod downwardapi-volume-97d340d1-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:28:06.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-544qz" for this suite.
Dec 13 20:28:12.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:28:12.270: INFO: namespace: e2e-tests-projected-544qz, resource: bindings, ignored listing per whitelist
Dec 13 20:28:12.279: INFO: namespace e2e-tests-projected-544qz deletion completed in 6.098513103s

• [SLOW TEST:8.213 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:28:12.279: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec 13 20:28:12.346: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:28:16.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-27nkw" for this suite.
Dec 13 20:28:22.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:28:22.141: INFO: namespace: e2e-tests-init-container-27nkw, resource: bindings, ignored listing per whitelist
Dec 13 20:28:22.177: INFO: namespace e2e-tests-init-container-27nkw deletion completed in 6.100437066s

• [SLOW TEST:9.898 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:28:22.177: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 13 20:28:22.263: INFO: Waiting up to 5m0s for pod "pod-a29f5cdc-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-rlvvw" to be "success or failure"
Dec 13 20:28:22.267: INFO: Pod "pod-a29f5cdc-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135912ms
Dec 13 20:28:24.270: INFO: Pod "pod-a29f5cdc-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006682705s
STEP: Saw pod success
Dec 13 20:28:24.270: INFO: Pod "pod-a29f5cdc-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:28:24.273: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-a29f5cdc-ff15-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:28:24.295: INFO: Waiting for pod pod-a29f5cdc-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:28:24.297: INFO: Pod pod-a29f5cdc-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:28:24.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rlvvw" for this suite.
Dec 13 20:28:30.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:28:30.349: INFO: namespace: e2e-tests-emptydir-rlvvw, resource: bindings, ignored listing per whitelist
Dec 13 20:28:30.403: INFO: namespace e2e-tests-emptydir-rlvvw deletion completed in 6.102343045s

• [SLOW TEST:8.226 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:28:30.403: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:28:30.527: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 13 20:28:35.530: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 13 20:28:35.530: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 13 20:28:37.534: INFO: Creating deployment "test-rollover-deployment"
Dec 13 20:28:37.541: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 13 20:28:39.547: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 13 20:28:39.552: INFO: Ensure that both replica sets have 1 created replica
Dec 13 20:28:39.557: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 13 20:28:39.564: INFO: Updating deployment test-rollover-deployment
Dec 13 20:28:39.564: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 13 20:28:41.574: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 13 20:28:41.579: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 13 20:28:41.584: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 20:28:41.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329720, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 20:28:43.591: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 20:28:43.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329720, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 20:28:45.590: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 20:28:45.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329720, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 20:28:47.591: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 20:28:47.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329720, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 20:28:49.591: INFO: all replica sets need to contain the pod-template-hash label
Dec 13 20:28:49.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329720, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329717, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 13 20:28:51.595: INFO: 
Dec 13 20:28:51.595: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec 13 20:28:51.602: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-x9hvz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x9hvz/deployments/test-rollover-deployment,UID:abba34be-ff15-11e8-b982-02b9355b966e,ResourceVersion:53126,Generation:2,CreationTimestamp:2018-12-13 20:28:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2018-12-13 20:28:37 +0000 UTC 2018-12-13 20:28:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2018-12-13 20:28:50 +0000 UTC 2018-12-13 20:28:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 13 20:28:51.605: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-x9hvz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x9hvz/replicasets/test-rollover-deployment-6b7f9d6597,UID:acf111b8-ff15-11e8-b31c-0a2b404fde34,ResourceVersion:53117,Generation:2,CreationTimestamp:2018-12-13 20:28:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment abba34be-ff15-11e8-b982-02b9355b966e 0xc001d43997 0xc001d43998}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 13 20:28:51.605: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 13 20:28:51.605: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-x9hvz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x9hvz/replicasets/test-rollover-controller,UID:a78ba6d1-ff15-11e8-b982-02b9355b966e,ResourceVersion:53125,Generation:2,CreationTimestamp:2018-12-13 20:28:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment abba34be-ff15-11e8-b982-02b9355b966e 0xc001d43807 0xc001d43808}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 13 20:28:51.605: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-x9hvz,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x9hvz/replicasets/test-rollover-deployment-6586df867b,UID:abbe57f6-ff15-11e8-b31c-0a2b404fde34,ResourceVersion:53083,Generation:2,CreationTimestamp:2018-12-13 20:28:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment abba34be-ff15-11e8-b982-02b9355b966e 0xc001d438c7 0xc001d438c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 13 20:28:51.608: INFO: Pod "test-rollover-deployment-6b7f9d6597-p45c4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-p45c4,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-x9hvz,SelfLink:/api/v1/namespaces/e2e-tests-deployment-x9hvz/pods/test-rollover-deployment-6b7f9d6597-p45c4,UID:acf61c0a-ff15-11e8-b31c-0a2b404fde34,ResourceVersion:53096,Generation:0,CreationTimestamp:2018-12-13 20:28:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.47/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 acf111b8-ff15-11e8-b31c-0a2b404fde34 0xc001d35ae7 0xc001d35ae8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fd4qk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fd4qk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-fd4qk true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d35b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d35b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:28:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:28:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:28:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:28:39 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:192.168.4.47,StartTime:2018-12-13 20:28:39 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2018-12-13 20:28:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://ae39149cc9dddbdf365d15b4f330a1417706cada8689f2f5650815ebbb5edef4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:28:51.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-x9hvz" for this suite.
Dec 13 20:28:57.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:28:57.722: INFO: namespace: e2e-tests-deployment-x9hvz, resource: bindings, ignored listing per whitelist
Dec 13 20:28:57.807: INFO: namespace e2e-tests-deployment-x9hvz deletion completed in 6.194951004s

• [SLOW TEST:27.404 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:28:57.807: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-b7dbb15c-ff15-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:28:57.893: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7dc557c-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-configmap-7ngfx" to be "success or failure"
Dec 13 20:28:57.896: INFO: Pod "pod-configmaps-b7dc557c-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.85587ms
Dec 13 20:28:59.899: INFO: Pod "pod-configmaps-b7dc557c-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006631784s
STEP: Saw pod success
Dec 13 20:28:59.900: INFO: Pod "pod-configmaps-b7dc557c-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:28:59.902: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-b7dc557c-ff15-11e8-b666-eabb5e37e61c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:28:59.919: INFO: Waiting for pod pod-configmaps-b7dc557c-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:28:59.922: INFO: Pod pod-configmaps-b7dc557c-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:28:59.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7ngfx" for this suite.
Dec 13 20:29:05.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:29:06.018: INFO: namespace: e2e-tests-configmap-7ngfx, resource: bindings, ignored listing per whitelist
Dec 13 20:29:06.019: INFO: namespace e2e-tests-configmap-7ngfx deletion completed in 6.094363472s

• [SLOW TEST:8.212 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:29:06.021: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 13 20:29:06.096: INFO: Waiting up to 5m0s for pod "pod-bcc02b7a-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-5h29w" to be "success or failure"
Dec 13 20:29:06.099: INFO: Pod "pod-bcc02b7a-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.823668ms
Dec 13 20:29:08.102: INFO: Pod "pod-bcc02b7a-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006233635s
Dec 13 20:29:10.106: INFO: Pod "pod-bcc02b7a-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009973245s
STEP: Saw pod success
Dec 13 20:29:10.106: INFO: Pod "pod-bcc02b7a-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:29:10.108: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-bcc02b7a-ff15-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:29:10.125: INFO: Waiting for pod pod-bcc02b7a-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:29:10.128: INFO: Pod pod-bcc02b7a-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:29:10.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5h29w" for this suite.
Dec 13 20:29:16.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:29:16.153: INFO: namespace: e2e-tests-emptydir-5h29w, resource: bindings, ignored listing per whitelist
Dec 13 20:29:16.227: INFO: namespace e2e-tests-emptydir-5h29w deletion completed in 6.095636856s

• [SLOW TEST:10.207 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:29:16.227: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:29:16.298: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:29:18.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-66wck" for this suite.
Dec 13 20:29:58.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:29:58.367: INFO: namespace: e2e-tests-pods-66wck, resource: bindings, ignored listing per whitelist
Dec 13 20:29:58.437: INFO: namespace e2e-tests-pods-66wck deletion completed in 40.109237497s

• [SLOW TEST:42.210 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:29:58.438: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-dbfea31e-ff15-11e8-b666-eabb5e37e61c
STEP: Creating secret with name secret-projected-all-test-volume-dbfea307-ff15-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 13 20:29:58.523: INFO: Waiting up to 5m0s for pod "projected-volume-dbfea2d4-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-5vx85" to be "success or failure"
Dec 13 20:29:58.526: INFO: Pod "projected-volume-dbfea2d4-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863147ms
Dec 13 20:30:00.529: INFO: Pod "projected-volume-dbfea2d4-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006082711s
STEP: Saw pod success
Dec 13 20:30:00.529: INFO: Pod "projected-volume-dbfea2d4-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:30:00.532: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod projected-volume-dbfea2d4-ff15-11e8-b666-eabb5e37e61c container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 13 20:30:00.549: INFO: Waiting for pod projected-volume-dbfea2d4-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:30:00.552: INFO: Pod projected-volume-dbfea2d4-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:30:00.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5vx85" for this suite.
Dec 13 20:30:06.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:30:06.634: INFO: namespace: e2e-tests-projected-5vx85, resource: bindings, ignored listing per whitelist
Dec 13 20:30:06.675: INFO: namespace e2e-tests-projected-5vx85 deletion completed in 6.119694914s

• [SLOW TEST:8.237 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:30:06.675: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1213 20:30:07.802508      14 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 20:30:07.802: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:30:07.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kfhzr" for this suite.
Dec 13 20:30:13.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:30:13.902: INFO: namespace: e2e-tests-gc-kfhzr, resource: bindings, ignored listing per whitelist
Dec 13 20:30:13.911: INFO: namespace e2e-tests-gc-kfhzr deletion completed in 6.106186722s

• [SLOW TEST:7.236 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:30:13.912: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:30:14.001: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Dec 13 20:30:14.007: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-q7lm7/daemonsets","resourceVersion":"53471"},"items":null}

Dec 13 20:30:14.010: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-q7lm7/pods","resourceVersion":"53471"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:30:14.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-q7lm7" for this suite.
Dec 13 20:30:20.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:30:20.143: INFO: namespace: e2e-tests-daemonsets-q7lm7, resource: bindings, ignored listing per whitelist
Dec 13 20:30:20.157: INFO: namespace e2e-tests-daemonsets-q7lm7 deletion completed in 6.136043452s

S [SKIPPING] [6.246 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Dec 13 20:30:14.001: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:30:20.157: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:30:20.232: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8f02242-ff15-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-882ll" to be "success or failure"
Dec 13 20:30:20.235: INFO: Pod "downwardapi-volume-e8f02242-ff15-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.920648ms
Dec 13 20:30:22.238: INFO: Pod "downwardapi-volume-e8f02242-ff15-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006360832s
STEP: Saw pod success
Dec 13 20:30:22.238: INFO: Pod "downwardapi-volume-e8f02242-ff15-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:30:22.241: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-e8f02242-ff15-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:30:22.261: INFO: Waiting for pod downwardapi-volume-e8f02242-ff15-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:30:22.264: INFO: Pod downwardapi-volume-e8f02242-ff15-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:30:22.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-882ll" for this suite.
Dec 13 20:30:28.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:30:28.285: INFO: namespace: e2e-tests-downward-api-882ll, resource: bindings, ignored listing per whitelist
Dec 13 20:30:28.367: INFO: namespace e2e-tests-downward-api-882ll deletion completed in 6.100007694s

• [SLOW TEST:8.209 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:30:28.367: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-tnqtb
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-tnqtb
STEP: Deleting pre-stop pod
Dec 13 20:30:39.493: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:30:39.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-tnqtb" for this suite.
Dec 13 20:31:17.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:31:17.540: INFO: namespace: e2e-tests-prestop-tnqtb, resource: bindings, ignored listing per whitelist
Dec 13 20:31:17.607: INFO: namespace e2e-tests-prestop-tnqtb deletion completed in 38.102304495s

• [SLOW TEST:49.240 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:31:17.610: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:31:17.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b317b79-ff16-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-4psqx" to be "success or failure"
Dec 13 20:31:17.725: INFO: Pod "downwardapi-volume-0b317b79-ff16-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.920583ms
Dec 13 20:31:19.729: INFO: Pod "downwardapi-volume-0b317b79-ff16-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020565749s
STEP: Saw pod success
Dec 13 20:31:19.729: INFO: Pod "downwardapi-volume-0b317b79-ff16-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:31:19.731: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-0b317b79-ff16-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:31:19.750: INFO: Waiting for pod downwardapi-volume-0b317b79-ff16-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:31:19.752: INFO: Pod downwardapi-volume-0b317b79-ff16-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:31:19.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4psqx" for this suite.
Dec 13 20:31:25.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:31:25.792: INFO: namespace: e2e-tests-projected-4psqx, resource: bindings, ignored listing per whitelist
Dec 13 20:31:25.851: INFO: namespace e2e-tests-projected-4psqx deletion completed in 6.095791917s

• [SLOW TEST:8.242 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:31:25.852: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Dec 13 20:31:25.927: INFO: Waiting up to 5m0s for pod "client-containers-10182db9-ff16-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-containers-kgk49" to be "success or failure"
Dec 13 20:31:25.933: INFO: Pod "client-containers-10182db9-ff16-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.528246ms
Dec 13 20:31:27.941: INFO: Pod "client-containers-10182db9-ff16-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01432226s
STEP: Saw pod success
Dec 13 20:31:27.941: INFO: Pod "client-containers-10182db9-ff16-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:31:27.943: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod client-containers-10182db9-ff16-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:31:27.962: INFO: Waiting for pod client-containers-10182db9-ff16-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:31:27.965: INFO: Pod client-containers-10182db9-ff16-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:31:27.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-kgk49" for this suite.
Dec 13 20:31:33.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:31:34.035: INFO: namespace: e2e-tests-containers-kgk49, resource: bindings, ignored listing per whitelist
Dec 13 20:31:34.071: INFO: namespace e2e-tests-containers-kgk49 deletion completed in 6.10350417s

• [SLOW TEST:8.220 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:31:34.072: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-tmznd/configmap-test-14fee62c-ff16-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:31:34.151: INFO: Waiting up to 5m0s for pod "pod-configmaps-14ff7466-ff16-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-configmap-tmznd" to be "success or failure"
Dec 13 20:31:34.154: INFO: Pod "pod-configmaps-14ff7466-ff16-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084418ms
Dec 13 20:31:36.157: INFO: Pod "pod-configmaps-14ff7466-ff16-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006568321s
STEP: Saw pod success
Dec 13 20:31:36.157: INFO: Pod "pod-configmaps-14ff7466-ff16-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:31:36.160: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-14ff7466-ff16-11e8-b666-eabb5e37e61c container env-test: <nil>
STEP: delete the pod
Dec 13 20:31:36.177: INFO: Waiting for pod pod-configmaps-14ff7466-ff16-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:31:36.179: INFO: Pod pod-configmaps-14ff7466-ff16-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:31:36.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-tmznd" for this suite.
Dec 13 20:31:42.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:31:42.249: INFO: namespace: e2e-tests-configmap-tmznd, resource: bindings, ignored listing per whitelist
Dec 13 20:31:42.277: INFO: namespace e2e-tests-configmap-tmznd deletion completed in 6.094156988s

• [SLOW TEST:8.205 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:31:42.277: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-19e472c7-ff16-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:31:42.365: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-19e4fc23-ff16-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-fp678" to be "success or failure"
Dec 13 20:31:42.368: INFO: Pod "pod-projected-secrets-19e4fc23-ff16-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.555925ms
Dec 13 20:31:44.371: INFO: Pod "pod-projected-secrets-19e4fc23-ff16-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006237619s
STEP: Saw pod success
Dec 13 20:31:44.371: INFO: Pod "pod-projected-secrets-19e4fc23-ff16-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:31:44.374: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-secrets-19e4fc23-ff16-11e8-b666-eabb5e37e61c container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:31:44.391: INFO: Waiting for pod pod-projected-secrets-19e4fc23-ff16-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:31:44.394: INFO: Pod pod-projected-secrets-19e4fc23-ff16-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:31:44.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fp678" for this suite.
Dec 13 20:31:50.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:31:50.435: INFO: namespace: e2e-tests-projected-fp678, resource: bindings, ignored listing per whitelist
Dec 13 20:31:50.494: INFO: namespace e2e-tests-projected-fp678 deletion completed in 6.096467447s

• [SLOW TEST:8.217 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:31:50.495: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 13 20:31:50.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-xv2qx'
Dec 13 20:31:50.638: INFO: stderr: ""
Dec 13 20:31:50.638: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Dec 13 20:31:50.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-xv2qx'
Dec 13 20:31:57.967: INFO: stderr: ""
Dec 13 20:31:57.967: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:31:57.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xv2qx" for this suite.
Dec 13 20:32:03.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:32:04.040: INFO: namespace: e2e-tests-kubectl-xv2qx, resource: bindings, ignored listing per whitelist
Dec 13 20:32:04.073: INFO: namespace e2e-tests-kubectl-xv2qx deletion completed in 6.101447456s

• [SLOW TEST:13.579 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:32:04.074: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-26e1550e-ff16-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:32:04.157: INFO: Waiting up to 5m0s for pod "pod-secrets-26e1fefe-ff16-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-secrets-llhtg" to be "success or failure"
Dec 13 20:32:04.160: INFO: Pod "pod-secrets-26e1fefe-ff16-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.989621ms
Dec 13 20:32:06.163: INFO: Pod "pod-secrets-26e1fefe-ff16-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006397957s
STEP: Saw pod success
Dec 13 20:32:06.163: INFO: Pod "pod-secrets-26e1fefe-ff16-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:32:06.166: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-secrets-26e1fefe-ff16-11e8-b666-eabb5e37e61c container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:32:06.184: INFO: Waiting for pod pod-secrets-26e1fefe-ff16-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:32:06.187: INFO: Pod pod-secrets-26e1fefe-ff16-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:32:06.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-llhtg" for this suite.
Dec 13 20:32:12.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:32:12.264: INFO: namespace: e2e-tests-secrets-llhtg, resource: bindings, ignored listing per whitelist
Dec 13 20:32:12.285: INFO: namespace e2e-tests-secrets-llhtg deletion completed in 6.095209486s

• [SLOW TEST:8.212 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:32:12.286: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:32:14.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-7ztkg" for this suite.
Dec 13 20:33:00.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:33:00.426: INFO: namespace: e2e-tests-kubelet-test-7ztkg, resource: bindings, ignored listing per whitelist
Dec 13 20:33:00.496: INFO: namespace e2e-tests-kubelet-test-7ztkg deletion completed in 46.104827609s

• [SLOW TEST:48.210 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:33:00.496: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 13 20:33:00.575: INFO: Waiting up to 5m0s for pod "pod-4882cc4e-ff16-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-lq4sp" to be "success or failure"
Dec 13 20:33:00.579: INFO: Pod "pod-4882cc4e-ff16-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.301227ms
Dec 13 20:33:02.582: INFO: Pod "pod-4882cc4e-ff16-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006892678s
STEP: Saw pod success
Dec 13 20:33:02.582: INFO: Pod "pod-4882cc4e-ff16-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:33:02.585: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-4882cc4e-ff16-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:33:02.602: INFO: Waiting for pod pod-4882cc4e-ff16-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:33:02.605: INFO: Pod pod-4882cc4e-ff16-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:33:02.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lq4sp" for this suite.
Dec 13 20:33:08.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:33:08.678: INFO: namespace: e2e-tests-emptydir-lq4sp, resource: bindings, ignored listing per whitelist
Dec 13 20:33:08.724: INFO: namespace e2e-tests-emptydir-lq4sp deletion completed in 6.114551144s

• [SLOW TEST:8.228 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:33:08.724: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Dec 13 20:33:10.827: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-4d6aeede-ff16-11e8-b666-eabb5e37e61c", GenerateName:"", Namespace:"e2e-tests-pods-fqrpx", SelfLink:"/api/v1/namespaces/e2e-tests-pods-fqrpx/pods/pod-submit-remove-4d6aeede-ff16-11e8-b666-eabb5e37e61c", UID:"4d6b792e-ff16-11e8-b982-02b9355b966e", ResourceVersion:"54074", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63680329988, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"801716745"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.3.179/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-sdjlq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00272e640), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sdjlq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001625708), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-3-214.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0026662a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001625950)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001625970)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001625978), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00162597c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329988, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329989, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329989, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680329988, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.3.214", PodIP:"192.168.3.179", StartTime:(*v1.Time)(0xc00273c8a0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc00273c8c0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"docker.io/library/nginx:1.14-alpine", ImageID:"docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6", ContainerID:"containerd://0e4838c929d4adc9788ff7e7eb57265763f37c369a2b05fe22f2cdd0f825ee3b"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:33:17.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fqrpx" for this suite.
Dec 13 20:33:23.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:33:24.041: INFO: namespace: e2e-tests-pods-fqrpx, resource: bindings, ignored listing per whitelist
Dec 13 20:33:24.085: INFO: namespace e2e-tests-pods-fqrpx deletion completed in 6.109280252s

• [SLOW TEST:15.361 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:33:24.086: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-drtnj A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-drtnj;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-drtnj A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-drtnj;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-drtnj.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-drtnj.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-drtnj.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-drtnj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-drtnj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-drtnj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-drtnj.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-drtnj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-drtnj.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 13.158.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.158.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.158.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.158.13_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-drtnj A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-drtnj;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-drtnj A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-drtnj;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-drtnj.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-drtnj.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-drtnj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-drtnj.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-drtnj.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-drtnj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-drtnj.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 13.158.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.158.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.158.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.158.13_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 20:33:26.256: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:26.259: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:26.262: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:26.265: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:26.269: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:26.272: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:26.275: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:26.278: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:26.298: INFO: Lookups using e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-drtnj jessie_tcp@dns-test-service.e2e-tests-dns-drtnj jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc]

Dec 13 20:33:31.354: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:31.357: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:31.360: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:31.364: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:31.367: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:31.371: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:31.374: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:31.378: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:31.403: INFO: Lookups using e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-drtnj jessie_tcp@dns-test-service.e2e-tests-dns-drtnj jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc]

Dec 13 20:33:36.353: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:36.356: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:36.359: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:36.362: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:36.366: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:36.369: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:36.372: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:36.376: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:36.396: INFO: Lookups using e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-drtnj jessie_tcp@dns-test-service.e2e-tests-dns-drtnj jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc]

Dec 13 20:33:41.353: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:41.357: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:41.360: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:41.363: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:41.367: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:41.370: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:41.373: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:41.376: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:41.396: INFO: Lookups using e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-drtnj jessie_tcp@dns-test-service.e2e-tests-dns-drtnj jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc]

Dec 13 20:33:46.357: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:46.361: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:46.364: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:46.367: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:46.371: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:46.374: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:46.377: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:46.380: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:46.400: INFO: Lookups using e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-drtnj jessie_tcp@dns-test-service.e2e-tests-dns-drtnj jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc]

Dec 13 20:33:51.353: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:51.357: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:51.360: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:51.364: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:51.367: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:51.370: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:51.374: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:51.377: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc from pod e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c: the server could not find the requested resource (get pods dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c)
Dec 13 20:33:51.397: INFO: Lookups using e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-drtnj jessie_tcp@dns-test-service.e2e-tests-dns-drtnj jessie_udp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@dns-test-service.e2e-tests-dns-drtnj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-drtnj.svc]

Dec 13 20:33:56.402: INFO: DNS probes using e2e-tests-dns-drtnj/dns-test-5695946d-ff16-11e8-b666-eabb5e37e61c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:33:56.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-drtnj" for this suite.
Dec 13 20:34:02.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:34:02.660: INFO: namespace: e2e-tests-dns-drtnj, resource: bindings, ignored listing per whitelist
Dec 13 20:34:02.701: INFO: namespace e2e-tests-dns-drtnj deletion completed in 6.113736313s

• [SLOW TEST:38.615 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:34:02.701: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:34:02.773: INFO: Creating deployment "nginx-deployment"
Dec 13 20:34:02.777: INFO: Waiting for observed generation 1
Dec 13 20:34:04.783: INFO: Waiting for all required pods to come up
Dec 13 20:34:04.787: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 13 20:34:06.797: INFO: Waiting for deployment "nginx-deployment" to complete
Dec 13 20:34:06.803: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec 13 20:34:06.810: INFO: Updating deployment nginx-deployment
Dec 13 20:34:06.810: INFO: Waiting for observed generation 2
Dec 13 20:34:08.816: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 13 20:34:08.824: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 13 20:34:08.827: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 13 20:34:08.835: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 13 20:34:08.835: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 13 20:34:08.837: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 13 20:34:08.842: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec 13 20:34:08.842: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec 13 20:34:08.848: INFO: Updating deployment nginx-deployment
Dec 13 20:34:08.848: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec 13 20:34:08.853: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 13 20:34:08.856: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec 13 20:34:10.868: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-j4czp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j4czp/deployments/nginx-deployment,UID:6d962085-ff16-11e8-b982-02b9355b966e,ResourceVersion:54512,Generation:3,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2018-12-13 20:34:08 +0000 UTC 2018-12-13 20:34:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2018-12-13 20:34:08 +0000 UTC 2018-12-13 20:34:02 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec 13 20:34:10.872: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-j4czp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j4czp/replicasets/nginx-deployment-65bbdb5f8,UID:6ffdfe0b-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54510,Generation:3,CreationTimestamp:2018-12-13 20:34:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6d962085-ff16-11e8-b982-02b9355b966e 0xc0010d88e7 0xc0010d88e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 13 20:34:10.872: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec 13 20:34:10.872: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-j4czp,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-j4czp/replicasets/nginx-deployment-555b55d965,UID:6d978cd9-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54509,Generation:3,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6d962085-ff16-11e8-b982-02b9355b966e 0xc0010d87f7 0xc0010d87f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec 13 20:34:10.881: INFO: Pod "nginx-deployment-555b55d965-4flxs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-4flxs,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-4flxs,UID:713a57ec-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54584,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.194/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc001126c67 0xc001126c68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001126cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001126cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.882: INFO: Pod "nginx-deployment-555b55d965-5rxzw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5rxzw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-5rxzw,UID:713de136-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54591,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.198/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc001126dd0 0xc001126dd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001126e60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001126ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.883: INFO: Pod "nginx-deployment-555b55d965-8bf7p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-8bf7p,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-8bf7p,UID:713a4195-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54564,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.59/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc001126f40 0xc001126f41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001127120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001127190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.883: INFO: Pod "nginx-deployment-555b55d965-9gmtv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-9gmtv,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-9gmtv,UID:713dd9f5-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54590,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.62/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc0011272c0 0xc0011272c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001127350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001127370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.884: INFO: Pod "nginx-deployment-555b55d965-bc5sj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-bc5sj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-bc5sj,UID:71361424-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54572,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.191/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc001127610 0xc001127611}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001127670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001127e60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.884: INFO: Pod "nginx-deployment-555b55d965-cl8jd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-cl8jd,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-cl8jd,UID:6d9d58c1-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54365,Generation:0,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.182/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc001127f20 0xc001127f21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001127f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001127fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:192.168.3.182,StartTime:2018-12-13 20:34:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-13 20:34:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 containerd://56e1adc497edaa7d61dbce3f69a9f36d750bc9976bb1a6b231e9dab4da72cffc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.884: INFO: Pod "nginx-deployment-555b55d965-f7ndp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-f7ndp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-f7ndp,UID:713a3cdb-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54560,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.58/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b320a0 0xc000b320a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b32100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b32120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.884: INFO: Pod "nginx-deployment-555b55d965-f9n4p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-f9n4p,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-f9n4p,UID:713dc3bd-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54581,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.63/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b322c0 0xc000b322c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b32320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b32340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.884: INFO: Pod "nginx-deployment-555b55d965-gctqf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-gctqf,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-gctqf,UID:6da05e75-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54342,Generation:0,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.53/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b32400 0xc000b32401}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b32490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b32530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:192.168.4.53,StartTime:2018-12-13 20:34:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-13 20:34:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 containerd://64632a601ce155c3ab891c8db9826c043bdb9ff7dd94fde37f4473f7b633b88c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.889: INFO: Pod "nginx-deployment-555b55d965-hgrbh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-hgrbh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-hgrbh,UID:6d9d5e55-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54368,Generation:0,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.184/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b326c0 0xc000b326c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b32720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b32740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:192.168.3.184,StartTime:2018-12-13 20:34:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-13 20:34:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 containerd://71f7a18f00c6eb3607e25ad9ac854e2c973c4134707b02fb9de634dd33f4ed75}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.894: INFO: Pod "nginx-deployment-555b55d965-j5gb2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-j5gb2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-j5gb2,UID:6da0672a-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54356,Generation:0,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.54/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b32810 0xc000b32811}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b32870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b32950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:192.168.4.54,StartTime:2018-12-13 20:34:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-13 20:34:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 containerd://6723181efb05aa27040de366266e2c36d045f152758e482ff1401c4b640722a1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.896: INFO: Pod "nginx-deployment-555b55d965-j7nck" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-j7nck,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-j7nck,UID:713dbcb8-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54567,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.60/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b32a90 0xc000b32a91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b32af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b32b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.899: INFO: Pod "nginx-deployment-555b55d965-ld629" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-ld629,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-ld629,UID:6da06c2a-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54362,Generation:0,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.183/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b32bd0 0xc000b32bd1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b32c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b32c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:192.168.3.183,StartTime:2018-12-13 20:34:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-13 20:34:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 containerd://e5081e965a464126bebf791c40b7f4c402e3ee64a3d2ac7494b108b36d8bf2fe}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.908: INFO: Pod "nginx-deployment-555b55d965-mbrvt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-mbrvt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-mbrvt,UID:6d9d3add-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54346,Generation:0,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.51/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b32d20 0xc000b32d21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b32d80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b32da0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:192.168.4.51,StartTime:2018-12-13 20:34:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-13 20:34:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 containerd://10aeb9cad0e3e6aed9cb3d7bc4963a0a7c7c714604a16a41fcd64e5c4721a957}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.908: INFO: Pod "nginx-deployment-555b55d965-qqxvw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qqxvw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-qqxvw,UID:6d9b2d4c-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54352,Generation:0,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.50/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b32e70 0xc000b32e71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b32ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b32ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:192.168.4.50,StartTime:2018-12-13 20:34:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-13 20:34:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 containerd://2b0f6e086bf5c877e93bad3d952ba6b9ae33e50310e6d545a0843e9887e5dc70}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.908: INFO: Pod "nginx-deployment-555b55d965-qr7k4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qr7k4,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-qr7k4,UID:7137aced-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54558,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.188/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b32fc0 0xc000b32fc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b33020} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b33040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.908: INFO: Pod "nginx-deployment-555b55d965-t496w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-t496w,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-t496w,UID:7137def6-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54582,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.193/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b33100 0xc000b33101}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b33160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b33180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.911: INFO: Pod "nginx-deployment-555b55d965-tqb4w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-tqb4w,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-tqb4w,UID:713dea09-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54569,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.190/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b33260 0xc000b33261}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b332c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b332e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.911: INFO: Pod "nginx-deployment-555b55d965-v4rx2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-v4rx2,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-v4rx2,UID:6d9d5a0d-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54340,Generation:0,CreationTimestamp:2018-12-13 20:34:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.52/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b33360 0xc000b33361}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b333c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b333e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:02 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:192.168.4.52,StartTime:2018-12-13 20:34:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2018-12-13 20:34:04 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:2abeba7cab34eb197ff7363486a2aa590027388eafd8e740efae7aae1bed28b6 containerd://14804de5437366a0fb80febe68326b13e1291e3f91b7776802bb366787f554b1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.911: INFO: Pod "nginx-deployment-555b55d965-zwnvj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-zwnvj,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-555b55d965-zwnvj,UID:713a4e08-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54585,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.195/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 6d978cd9-ff16-11e8-b31c-0a2b404fde34 0xc000b334b0 0xc000b334b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b33510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b33530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.915: INFO: Pod "nginx-deployment-65bbdb5f8-6k7lq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6k7lq,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-6k7lq,UID:7005d7c2-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54430,Generation:0,CreationTimestamp:2018-12-13 20:34:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.56/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc000b335f0 0xc000b335f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b33660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b33680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.919: INFO: Pod "nginx-deployment-65bbdb5f8-6nppx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6nppx,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-6nppx,UID:714002a7-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54561,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.189/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc000b33750 0xc000b33751}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b337c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b337e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.919: INFO: Pod "nginx-deployment-65bbdb5f8-785vw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-785vw,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-785vw,UID:713b605a-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54595,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.196/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc000b33860 0xc000b33861}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b338d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b338f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.925: INFO: Pod "nginx-deployment-65bbdb5f8-btb9z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-btb9z,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-btb9z,UID:6fffd19c-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54428,Generation:0,CreationTimestamp:2018-12-13 20:34:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.55/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc000b339c0 0xc000b339c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b33a30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b33a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.928: INFO: Pod "nginx-deployment-65bbdb5f8-kk775" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-kk775,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-kk775,UID:7138df59-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54583,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.64/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc000b33b40 0xc000b33b41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b33bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b33bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.938: INFO: Pod "nginx-deployment-65bbdb5f8-lcg4b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-lcg4b,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-lcg4b,UID:6fffd967-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54434,Generation:0,CreationTimestamp:2018-12-13 20:34:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.186/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc000b33e20 0xc000b33e21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b33e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b33eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.939: INFO: Pod "nginx-deployment-65bbdb5f8-nrhc6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-nrhc6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-nrhc6,UID:713b3d09-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54589,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.197/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc000b33fe0 0xc000b33fe1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0013fe160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0013fe180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.941: INFO: Pod "nginx-deployment-65bbdb5f8-svxsf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-svxsf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-svxsf,UID:6ffec08d-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54429,Generation:0,CreationTimestamp:2018-12-13 20:34:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.185/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc0013fe250 0xc0013fe251}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0013fe3a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0013fe3c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.941: INFO: Pod "nginx-deployment-65bbdb5f8-t7j6l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-t7j6l,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-t7j6l,UID:713b1b80-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54587,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.65/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc0013fe490 0xc0013fe491}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0013fe590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0013fe5b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.941: INFO: Pod "nginx-deployment-65bbdb5f8-vb8hn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-vb8hn,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-vb8hn,UID:713707b9-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54549,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.57/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc0013fe680 0xc0013fe681}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0013fe6f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0013fe780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.941: INFO: Pod "nginx-deployment-65bbdb5f8-vnvf7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-vnvf7,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-vnvf7,UID:713b516c-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54576,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.4.61/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc0013fe850 0xc0013fe851}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-167.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0013fe8c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0013fe8e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.2.167,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.941: INFO: Pod "nginx-deployment-65bbdb5f8-w2nqg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-w2nqg,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-w2nqg,UID:7138f1ad-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54574,Generation:0,CreationTimestamp:2018-12-13 20:34:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.192/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc0013fed40 0xc0013fed41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0013fede0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0013fee00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:08 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 13 20:34:10.942: INFO: Pod "nginx-deployment-65bbdb5f8-zvgcc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-zvgcc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-j4czp,SelfLink:/api/v1/namespaces/e2e-tests-deployment-j4czp/pods/nginx-deployment-65bbdb5f8-zvgcc,UID:70078e90-ff16-11e8-b31c-0a2b404fde34,ResourceVersion:54435,Generation:0,CreationTimestamp:2018-12-13 20:34:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.187/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 6ffdfe0b-ff16-11e8-b31c-0a2b404fde34 0xc0013feed0 0xc0013feed1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-vfdtc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfdtc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfdtc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0013fef40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0013fef60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:34:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:34:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:34:10.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-j4czp" for this suite.
Dec 13 20:34:18.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:34:19.062: INFO: namespace: e2e-tests-deployment-j4czp, resource: bindings, ignored listing per whitelist
Dec 13 20:34:19.222: INFO: namespace e2e-tests-deployment-j4czp deletion completed in 8.274892933s

• [SLOW TEST:16.521 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:34:19.222: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:34:19.522: INFO: (0) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 144.999436ms)
Dec 13 20:34:19.530: INFO: (1) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.357945ms)
Dec 13 20:34:19.538: INFO: (2) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.050825ms)
Dec 13 20:34:19.542: INFO: (3) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.813472ms)
Dec 13 20:34:19.546: INFO: (4) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.523543ms)
Dec 13 20:34:19.553: INFO: (5) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.97906ms)
Dec 13 20:34:19.559: INFO: (6) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.735ms)
Dec 13 20:34:19.563: INFO: (7) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.081993ms)
Dec 13 20:34:19.567: INFO: (8) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.706925ms)
Dec 13 20:34:19.573: INFO: (9) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.524912ms)
Dec 13 20:34:19.577: INFO: (10) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.029431ms)
Dec 13 20:34:19.580: INFO: (11) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.536317ms)
Dec 13 20:34:19.586: INFO: (12) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.393944ms)
Dec 13 20:34:19.590: INFO: (13) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.026057ms)
Dec 13 20:34:19.594: INFO: (14) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.139782ms)
Dec 13 20:34:19.597: INFO: (15) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.738626ms)
Dec 13 20:34:19.602: INFO: (16) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.15034ms)
Dec 13 20:34:19.605: INFO: (17) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.47285ms)
Dec 13 20:34:19.609: INFO: (18) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.346208ms)
Dec 13 20:34:19.612: INFO: (19) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.583539ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:34:19.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-tp7g2" for this suite.
Dec 13 20:34:25.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:34:25.641: INFO: namespace: e2e-tests-proxy-tp7g2, resource: bindings, ignored listing per whitelist
Dec 13 20:34:25.717: INFO: namespace e2e-tests-proxy-tp7g2 deletion completed in 6.101670684s

• [SLOW TEST:6.495 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:34:25.717: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-5ftz
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 20:34:25.798: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5ftz" in namespace "e2e-tests-subpath-wgbkt" to be "success or failure"
Dec 13 20:34:25.806: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Pending", Reason="", readiness=false. Elapsed: 7.299227ms
Dec 13 20:34:27.810: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011073089s
Dec 13 20:34:29.813: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 4.014718096s
Dec 13 20:34:31.817: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 6.01833851s
Dec 13 20:34:33.824: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 8.025995846s
Dec 13 20:34:35.828: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 10.02941615s
Dec 13 20:34:37.832: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 12.033661075s
Dec 13 20:34:39.836: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 14.037664115s
Dec 13 20:34:41.840: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 16.04117796s
Dec 13 20:34:43.847: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 18.04876041s
Dec 13 20:34:45.851: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 20.052084132s
Dec 13 20:34:47.854: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Running", Reason="", readiness=false. Elapsed: 22.05574383s
Dec 13 20:34:49.858: INFO: Pod "pod-subpath-test-projected-5ftz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.059341806s
STEP: Saw pod success
Dec 13 20:34:49.858: INFO: Pod "pod-subpath-test-projected-5ftz" satisfied condition "success or failure"
Dec 13 20:34:49.861: INFO: Trying to get logs from node ip-10-0-2-167.eu-west-1.compute.internal pod pod-subpath-test-projected-5ftz container test-container-subpath-projected-5ftz: <nil>
STEP: delete the pod
Dec 13 20:34:49.888: INFO: Waiting for pod pod-subpath-test-projected-5ftz to disappear
Dec 13 20:34:49.897: INFO: Pod pod-subpath-test-projected-5ftz no longer exists
STEP: Deleting pod pod-subpath-test-projected-5ftz
Dec 13 20:34:49.897: INFO: Deleting pod "pod-subpath-test-projected-5ftz" in namespace "e2e-tests-subpath-wgbkt"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:34:49.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-wgbkt" for this suite.
Dec 13 20:34:55.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:34:55.923: INFO: namespace: e2e-tests-subpath-wgbkt, resource: bindings, ignored listing per whitelist
Dec 13 20:34:55.999: INFO: namespace e2e-tests-subpath-wgbkt deletion completed in 6.096314464s

• [SLOW TEST:30.282 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:34:55.999: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 13 20:34:56.079: INFO: Waiting up to 5m0s for pod "pod-8d5aca1e-ff16-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-gtzqf" to be "success or failure"
Dec 13 20:34:56.085: INFO: Pod "pod-8d5aca1e-ff16-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.939635ms
Dec 13 20:34:58.089: INFO: Pod "pod-8d5aca1e-ff16-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009398217s
STEP: Saw pod success
Dec 13 20:34:58.089: INFO: Pod "pod-8d5aca1e-ff16-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:34:58.091: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-8d5aca1e-ff16-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:34:58.108: INFO: Waiting for pod pod-8d5aca1e-ff16-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:34:58.111: INFO: Pod pod-8d5aca1e-ff16-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:34:58.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-gtzqf" for this suite.
Dec 13 20:35:04.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:35:04.196: INFO: namespace: e2e-tests-emptydir-gtzqf, resource: bindings, ignored listing per whitelist
Dec 13 20:35:04.213: INFO: namespace e2e-tests-emptydir-gtzqf deletion completed in 6.098376613s

• [SLOW TEST:8.213 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:35:04.213: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 13 20:35:04.477: INFO: Pod name wrapped-volume-race-925bc731-ff16-11e8-b666-eabb5e37e61c: Found 0 pods out of 5
Dec 13 20:35:09.483: INFO: Pod name wrapped-volume-race-925bc731-ff16-11e8-b666-eabb5e37e61c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-925bc731-ff16-11e8-b666-eabb5e37e61c in namespace e2e-tests-emptydir-wrapper-58p59, will wait for the garbage collector to delete the pods
Dec 13 20:35:19.563: INFO: Deleting ReplicationController wrapped-volume-race-925bc731-ff16-11e8-b666-eabb5e37e61c took: 8.841264ms
Dec 13 20:35:19.663: INFO: Terminating ReplicationController wrapped-volume-race-925bc731-ff16-11e8-b666-eabb5e37e61c pods took: 100.16259ms
STEP: Creating RC which spawns configmap-volume pods
Dec 13 20:35:58.083: INFO: Pod name wrapped-volume-race-b24e3c22-ff16-11e8-b666-eabb5e37e61c: Found 0 pods out of 5
Dec 13 20:36:03.089: INFO: Pod name wrapped-volume-race-b24e3c22-ff16-11e8-b666-eabb5e37e61c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b24e3c22-ff16-11e8-b666-eabb5e37e61c in namespace e2e-tests-emptydir-wrapper-58p59, will wait for the garbage collector to delete the pods
Dec 13 20:36:13.172: INFO: Deleting ReplicationController wrapped-volume-race-b24e3c22-ff16-11e8-b666-eabb5e37e61c took: 9.735311ms
Dec 13 20:36:13.272: INFO: Terminating ReplicationController wrapped-volume-race-b24e3c22-ff16-11e8-b666-eabb5e37e61c pods took: 100.142559ms
STEP: Creating RC which spawns configmap-volume pods
Dec 13 20:36:58.695: INFO: Pod name wrapped-volume-race-d66e7ffe-ff16-11e8-b666-eabb5e37e61c: Found 0 pods out of 5
Dec 13 20:37:03.700: INFO: Pod name wrapped-volume-race-d66e7ffe-ff16-11e8-b666-eabb5e37e61c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d66e7ffe-ff16-11e8-b666-eabb5e37e61c in namespace e2e-tests-emptydir-wrapper-58p59, will wait for the garbage collector to delete the pods
Dec 13 20:37:13.780: INFO: Deleting ReplicationController wrapped-volume-race-d66e7ffe-ff16-11e8-b666-eabb5e37e61c took: 8.276545ms
Dec 13 20:37:13.880: INFO: Terminating ReplicationController wrapped-volume-race-d66e7ffe-ff16-11e8-b666-eabb5e37e61c pods took: 100.166637ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:37:58.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-58p59" for this suite.
Dec 13 20:38:06.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:38:06.154: INFO: namespace: e2e-tests-emptydir-wrapper-58p59, resource: bindings, ignored listing per whitelist
Dec 13 20:38:06.170: INFO: namespace e2e-tests-emptydir-wrapper-58p59 deletion completed in 8.093885616s

• [SLOW TEST:181.957 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:38:06.171: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 13 20:38:06.249: INFO: Waiting up to 5m0s for pod "pod-feb510e1-ff16-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-9h6c5" to be "success or failure"
Dec 13 20:38:06.252: INFO: Pod "pod-feb510e1-ff16-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680521ms
Dec 13 20:38:08.259: INFO: Pod "pod-feb510e1-ff16-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010106238s
STEP: Saw pod success
Dec 13 20:38:08.259: INFO: Pod "pod-feb510e1-ff16-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:38:08.262: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-feb510e1-ff16-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:38:08.280: INFO: Waiting for pod pod-feb510e1-ff16-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:38:08.283: INFO: Pod pod-feb510e1-ff16-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:38:08.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9h6c5" for this suite.
Dec 13 20:38:14.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:38:14.329: INFO: namespace: e2e-tests-emptydir-9h6c5, resource: bindings, ignored listing per whitelist
Dec 13 20:38:14.391: INFO: namespace e2e-tests-emptydir-9h6c5 deletion completed in 6.104423513s

• [SLOW TEST:8.220 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:38:14.391: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Dec 13 20:38:14.462: INFO: Waiting up to 5m0s for pod "client-containers-039a3057-ff17-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-containers-qsktl" to be "success or failure"
Dec 13 20:38:14.465: INFO: Pod "client-containers-039a3057-ff17-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814153ms
Dec 13 20:38:16.468: INFO: Pod "client-containers-039a3057-ff17-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006090558s
STEP: Saw pod success
Dec 13 20:38:16.468: INFO: Pod "client-containers-039a3057-ff17-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:38:16.471: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod client-containers-039a3057-ff17-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:38:16.487: INFO: Waiting for pod client-containers-039a3057-ff17-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:38:16.492: INFO: Pod client-containers-039a3057-ff17-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:38:16.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-qsktl" for this suite.
Dec 13 20:38:22.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:38:22.565: INFO: namespace: e2e-tests-containers-qsktl, resource: bindings, ignored listing per whitelist
Dec 13 20:38:22.590: INFO: namespace e2e-tests-containers-qsktl deletion completed in 6.094842274s

• [SLOW TEST:8.199 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:38:22.591: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-087f3f98-ff17-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:38:22.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-087fd491-ff17-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-configmap-7pnt9" to be "success or failure"
Dec 13 20:38:22.685: INFO: Pod "pod-configmaps-087fd491-ff17-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.063677ms
Dec 13 20:38:24.689: INFO: Pod "pod-configmaps-087fd491-ff17-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008835954s
STEP: Saw pod success
Dec 13 20:38:24.689: INFO: Pod "pod-configmaps-087fd491-ff17-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:38:24.692: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-087fd491-ff17-11e8-b666-eabb5e37e61c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:38:24.707: INFO: Waiting for pod pod-configmaps-087fd491-ff17-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:38:24.710: INFO: Pod pod-configmaps-087fd491-ff17-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:38:24.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7pnt9" for this suite.
Dec 13 20:38:30.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:38:30.765: INFO: namespace: e2e-tests-configmap-7pnt9, resource: bindings, ignored listing per whitelist
Dec 13 20:38:30.807: INFO: namespace e2e-tests-configmap-7pnt9 deletion completed in 6.094361836s

• [SLOW TEST:8.217 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:38:30.808: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-kw6sz
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 13 20:38:30.874: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 13 20:38:52.948: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.3.204:8080/dial?request=hostName&protocol=http&host=192.168.4.82&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-kw6sz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:38:52.948: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:38:53.072: INFO: Waiting for endpoints: map[]
Dec 13 20:38:53.075: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.3.204:8080/dial?request=hostName&protocol=http&host=192.168.3.203&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-kw6sz PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:38:53.075: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:38:53.251: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:38:53.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-kw6sz" for this suite.
Dec 13 20:39:15.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:39:15.353: INFO: namespace: e2e-tests-pod-network-test-kw6sz, resource: bindings, ignored listing per whitelist
Dec 13 20:39:15.353: INFO: namespace e2e-tests-pod-network-test-kw6sz deletion completed in 22.097964389s

• [SLOW TEST:44.546 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:39:15.354: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Dec 13 20:39:15.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 api-versions'
Dec 13 20:39:15.485: INFO: stderr: ""
Dec 13 20:39:15.485: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:39:15.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5hnpt" for this suite.
Dec 13 20:39:21.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:39:21.544: INFO: namespace: e2e-tests-kubectl-5hnpt, resource: bindings, ignored listing per whitelist
Dec 13 20:39:21.586: INFO: namespace e2e-tests-kubectl-5hnpt deletion completed in 6.096835335s

• [SLOW TEST:6.232 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:39:21.586: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-bzmqc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-bzmqc to expose endpoints map[]
Dec 13 20:39:21.665: INFO: Get endpoints failed (3.350946ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 13 20:39:22.668: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-bzmqc exposes endpoints map[] (1.006600697s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-bzmqc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-bzmqc to expose endpoints map[pod1:[100]]
Dec 13 20:39:24.693: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-bzmqc exposes endpoints map[pod1:[100]] (2.017679505s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-bzmqc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-bzmqc to expose endpoints map[pod1:[100] pod2:[101]]
Dec 13 20:39:26.727: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-bzmqc exposes endpoints map[pod1:[100] pod2:[101]] (2.028582282s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-bzmqc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-bzmqc to expose endpoints map[pod2:[101]]
Dec 13 20:39:27.746: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-bzmqc exposes endpoints map[pod2:[101]] (1.013891629s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-bzmqc
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-bzmqc to expose endpoints map[]
Dec 13 20:39:27.769: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-bzmqc exposes endpoints map[] (10.283573ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:39:27.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-bzmqc" for this suite.
Dec 13 20:39:33.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:39:33.898: INFO: namespace: e2e-tests-services-bzmqc, resource: bindings, ignored listing per whitelist
Dec 13 20:39:33.903: INFO: namespace e2e-tests-services-bzmqc deletion completed in 6.0990137s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:12.317 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:39:33.903: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 13 20:39:35.999: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-330053b2-ff17-11e8-b666-eabb5e37e61c,GenerateName:,Namespace:e2e-tests-events-dbs8l,SelfLink:/api/v1/namespaces/e2e-tests-events-dbs8l/pods/send-events-330053b2-ff17-11e8-b666-eabb5e37e61c,UID:3300b65c-ff17-11e8-b982-02b9355b966e,ResourceVersion:56629,Generation:0,CreationTimestamp:2018-12-13 20:39:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 979030941,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.3.206/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-g87sj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-g87sj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-g87sj true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00210f050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00210f070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:39:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:39:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:39:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:39:33 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:192.168.3.206,StartTime:2018-12-13 20:39:33 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2018-12-13 20:39:34 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://25f28663b41b6c443fb10341e1ba68203e85c74bb01900866451d1b0f74b847d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec 13 20:39:38.003: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 13 20:39:40.008: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:39:40.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-dbs8l" for this suite.
Dec 13 20:40:26.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:40:26.098: INFO: namespace: e2e-tests-events-dbs8l, resource: bindings, ignored listing per whitelist
Dec 13 20:40:26.126: INFO: namespace e2e-tests-events-dbs8l deletion completed in 46.100871832s

• [SLOW TEST:52.223 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:40:26.126: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-5vrdk
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 13 20:40:26.212: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 13 20:40:50.283: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 192.168.3.207 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-5vrdk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:40:50.283: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:40:51.382: INFO: Found all expected endpoints: [netserver-0]
Dec 13 20:40:51.386: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 192.168.4.84 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-5vrdk PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:40:51.386: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:40:52.531: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:40:52.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-5vrdk" for this suite.
Dec 13 20:41:14.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:41:14.554: INFO: namespace: e2e-tests-pod-network-test-5vrdk, resource: bindings, ignored listing per whitelist
Dec 13 20:41:14.631: INFO: namespace e2e-tests-pod-network-test-5vrdk deletion completed in 22.095324315s

• [SLOW TEST:48.504 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:41:14.631: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec 13 20:41:14.700: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 13 20:41:14.707: INFO: Waiting for terminating namespaces to be deleted...
Dec 13 20:41:14.709: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-167.eu-west-1.compute.internal before test
Dec 13 20:41:14.716: INFO: calico-node-p8l9p from kube-system started at 2018-12-13 15:23:12 +0000 UTC (1 container statuses recorded)
Dec 13 20:41:14.716: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:41:14.716: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-13 19:50:49 +0000 UTC (1 container statuses recorded)
Dec 13 20:41:14.716: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 13 20:41:14.716: INFO: sonobuoy-systemd-logs-daemon-set-e5a404e9fcc945c3-zrx9c from heptio-sonobuoy started at 2018-12-13 19:50:51 +0000 UTC (2 container statuses recorded)
Dec 13 20:41:14.716: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec 13 20:41:14.716: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 20:41:14.716: INFO: kube-proxy-246r7 from kube-system started at 2018-12-13 14:51:13 +0000 UTC (1 container statuses recorded)
Dec 13 20:41:14.716: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:41:14.716: INFO: coredns-86c58d9df4-jdbxq from kube-system started at 2018-12-13 19:48:04 +0000 UTC (1 container statuses recorded)
Dec 13 20:41:14.716: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:41:14.716: INFO: sonobuoy-e2e-job-87b901a1b8cd4728 from heptio-sonobuoy started at 2018-12-13 19:50:51 +0000 UTC (2 container statuses recorded)
Dec 13 20:41:14.716: INFO: 	Container e2e ready: true, restart count 0
Dec 13 20:41:14.716: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 20:41:14.716: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-214.eu-west-1.compute.internal before test
Dec 13 20:41:14.722: INFO: kube-proxy-kqrgn from kube-system started at 2018-12-13 14:51:08 +0000 UTC (1 container statuses recorded)
Dec 13 20:41:14.722: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 20:41:14.722: INFO: calico-node-h9tzd from kube-system started at 2018-12-13 15:23:05 +0000 UTC (1 container statuses recorded)
Dec 13 20:41:14.722: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 20:41:14.722: INFO: nginx-7cdbd8cdc9-qwxrq from default started at 2018-12-13 19:48:29 +0000 UTC (1 container statuses recorded)
Dec 13 20:41:14.722: INFO: 	Container nginx ready: true, restart count 0
Dec 13 20:41:14.722: INFO: calicoctl from kube-system started at 2018-12-13 15:22:31 +0000 UTC (1 container statuses recorded)
Dec 13 20:41:14.722: INFO: 	Container calicoctl ready: true, restart count 0
Dec 13 20:41:14.722: INFO: coredns-86c58d9df4-8x9zn from kube-system started at 2018-12-13 19:48:04 +0000 UTC (1 container statuses recorded)
Dec 13 20:41:14.722: INFO: 	Container coredns ready: true, restart count 0
Dec 13 20:41:14.722: INFO: sonobuoy-systemd-logs-daemon-set-e5a404e9fcc945c3-c4ztz from heptio-sonobuoy started at 2018-12-13 19:50:51 +0000 UTC (2 container statuses recorded)
Dec 13 20:41:14.722: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Dec 13 20:41:14.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7041cd4e-ff17-11e8-b666-eabb5e37e61c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-7041cd4e-ff17-11e8-b666-eabb5e37e61c off the node ip-10-0-3-214.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7041cd4e-ff17-11e8-b666-eabb5e37e61c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:41:18.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-nc5pt" for this suite.
Dec 13 20:41:26.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:41:26.824: INFO: namespace: e2e-tests-sched-pred-nc5pt, resource: bindings, ignored listing per whitelist
Dec 13 20:41:26.889: INFO: namespace e2e-tests-sched-pred-nc5pt deletion completed in 8.097316254s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.258 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:41:26.890: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Dec 13 20:41:26.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 --namespace=e2e-tests-kubectl-jfh4h run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 13 20:41:29.158: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 13 20:41:29.158: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:41:31.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jfh4h" for this suite.
Dec 13 20:41:39.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:41:39.245: INFO: namespace: e2e-tests-kubectl-jfh4h, resource: bindings, ignored listing per whitelist
Dec 13 20:41:39.263: INFO: namespace e2e-tests-kubectl-jfh4h deletion completed in 8.095399278s

• [SLOW TEST:12.373 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:41:39.263: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Dec 13 20:41:39.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:39.467: INFO: stderr: ""
Dec 13 20:41:39.468: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 13 20:41:39.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:39.539: INFO: stderr: ""
Dec 13 20:41:39.539: INFO: stdout: "update-demo-nautilus-q892c update-demo-nautilus-z4dx5 "
Dec 13 20:41:39.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-q892c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:39.602: INFO: stderr: ""
Dec 13 20:41:39.602: INFO: stdout: ""
Dec 13 20:41:39.602: INFO: update-demo-nautilus-q892c is created but not running
Dec 13 20:41:44.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:44.665: INFO: stderr: ""
Dec 13 20:41:44.665: INFO: stdout: "update-demo-nautilus-q892c update-demo-nautilus-z4dx5 "
Dec 13 20:41:44.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-q892c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:44.722: INFO: stderr: ""
Dec 13 20:41:44.722: INFO: stdout: "true"
Dec 13 20:41:44.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-q892c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:44.785: INFO: stderr: ""
Dec 13 20:41:44.785: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:41:44.785: INFO: validating pod update-demo-nautilus-q892c
Dec 13 20:41:44.790: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:41:44.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:41:44.791: INFO: update-demo-nautilus-q892c is verified up and running
Dec 13 20:41:44.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-z4dx5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:44.854: INFO: stderr: ""
Dec 13 20:41:44.854: INFO: stdout: "true"
Dec 13 20:41:44.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods update-demo-nautilus-z4dx5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:44.913: INFO: stderr: ""
Dec 13 20:41:44.913: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 13 20:41:44.913: INFO: validating pod update-demo-nautilus-z4dx5
Dec 13 20:41:44.918: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 13 20:41:44.919: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 13 20:41:44.919: INFO: update-demo-nautilus-z4dx5 is verified up and running
STEP: using delete to clean up resources
Dec 13 20:41:44.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:44.985: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 20:41:44.985: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 13 20:41:44.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-r6fks'
Dec 13 20:41:45.128: INFO: stderr: "No resources found.\n"
Dec 13 20:41:45.128: INFO: stdout: ""
Dec 13 20:41:45.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -l name=update-demo --namespace=e2e-tests-kubectl-r6fks -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 13 20:41:45.244: INFO: stderr: ""
Dec 13 20:41:45.244: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:41:45.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r6fks" for this suite.
Dec 13 20:42:07.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:42:07.330: INFO: namespace: e2e-tests-kubectl-r6fks, resource: bindings, ignored listing per whitelist
Dec 13 20:42:07.347: INFO: namespace e2e-tests-kubectl-r6fks deletion completed in 22.098184419s

• [SLOW TEST:28.084 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:42:07.347: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 13 20:42:07.441: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:07.441: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:07.441: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:07.443: INFO: Number of nodes with available pods: 0
Dec 13 20:42:07.443: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:08.447: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:08.448: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:08.448: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:08.451: INFO: Number of nodes with available pods: 1
Dec 13 20:42:08.451: INFO: Node ip-10-0-3-214.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:09.447: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:09.447: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:09.447: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:09.450: INFO: Number of nodes with available pods: 2
Dec 13 20:42:09.450: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 13 20:42:09.466: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:09.466: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:09.466: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:09.469: INFO: Number of nodes with available pods: 1
Dec 13 20:42:09.469: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:10.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:10.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:10.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:10.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:10.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:11.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:11.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:11.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:11.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:11.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:12.475: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:12.475: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:12.475: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:12.479: INFO: Number of nodes with available pods: 1
Dec 13 20:42:12.479: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:13.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:13.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:13.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:13.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:13.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:14.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:14.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:14.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:14.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:14.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:15.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:15.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:15.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:15.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:15.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:16.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:16.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:16.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:16.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:16.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:17.477: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:17.477: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:17.477: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:17.480: INFO: Number of nodes with available pods: 1
Dec 13 20:42:17.480: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:18.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:18.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:18.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:18.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:18.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:19.474: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:19.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:19.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:19.477: INFO: Number of nodes with available pods: 1
Dec 13 20:42:19.477: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:20.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:20.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:20.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:20.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:20.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:21.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:21.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:21.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:21.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:21.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:22.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:22.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:22.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:22.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:22.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:23.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:23.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:23.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:23.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:23.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:24.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:24.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:24.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:24.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:24.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:25.474: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:25.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:25.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:25.477: INFO: Number of nodes with available pods: 1
Dec 13 20:42:25.477: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:26.474: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:26.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:26.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:26.477: INFO: Number of nodes with available pods: 1
Dec 13 20:42:26.477: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:27.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:27.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:27.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:27.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:27.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:28.477: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:28.477: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:28.478: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:28.480: INFO: Number of nodes with available pods: 1
Dec 13 20:42:28.480: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:29.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:29.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:29.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:29.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:29.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:30.474: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:30.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:30.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:30.477: INFO: Number of nodes with available pods: 1
Dec 13 20:42:30.477: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:31.474: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:31.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:31.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:31.477: INFO: Number of nodes with available pods: 1
Dec 13 20:42:31.477: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:32.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:32.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:32.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:32.477: INFO: Number of nodes with available pods: 1
Dec 13 20:42:32.477: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:33.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:33.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:33.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:33.477: INFO: Number of nodes with available pods: 1
Dec 13 20:42:33.477: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:34.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:34.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:34.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:34.481: INFO: Number of nodes with available pods: 1
Dec 13 20:42:34.481: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:35.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:35.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:35.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:35.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:35.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:36.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:36.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:36.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:36.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:36.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:37.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:37.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:37.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:37.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:37.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:38.474: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:38.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:38.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:38.481: INFO: Number of nodes with available pods: 1
Dec 13 20:42:38.481: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:39.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:39.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:39.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:39.476: INFO: Number of nodes with available pods: 1
Dec 13 20:42:39.476: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:40.474: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:40.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:40.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:40.477: INFO: Number of nodes with available pods: 1
Dec 13 20:42:40.477: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:41.476: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:41.476: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:41.476: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:41.479: INFO: Number of nodes with available pods: 1
Dec 13 20:42:41.479: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:42.474: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:42.474: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:42.474: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:42.479: INFO: Number of nodes with available pods: 1
Dec 13 20:42:42.479: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:42:43.473: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:43.473: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:43.473: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:42:43.476: INFO: Number of nodes with available pods: 2
Dec 13 20:42:43.476: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-jpg49, will wait for the garbage collector to delete the pods
Dec 13 20:42:43.539: INFO: Deleting DaemonSet.extensions daemon-set took: 7.321639ms
Dec 13 20:42:43.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.163981ms
Dec 13 20:43:27.650: INFO: Number of nodes with available pods: 0
Dec 13 20:43:27.650: INFO: Number of running nodes: 0, number of available pods: 0
Dec 13 20:43:27.653: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-jpg49/daemonsets","resourceVersion":"57330"},"items":null}

Dec 13 20:43:27.656: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-jpg49/pods","resourceVersion":"57330"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:43:27.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-jpg49" for this suite.
Dec 13 20:43:33.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:43:33.776: INFO: namespace: e2e-tests-daemonsets-jpg49, resource: bindings, ignored listing per whitelist
Dec 13 20:43:33.796: INFO: namespace e2e-tests-daemonsets-jpg49 deletion completed in 6.10904854s

• [SLOW TEST:86.449 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:43:33.797: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:43:33.892: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c1fe7c77-ff17-11e8-b982-02b9355b966e", Controller:(*bool)(0xc00103cebe), BlockOwnerDeletion:(*bool)(0xc00103cebf)}}
Dec 13 20:43:33.904: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c1fcd434-ff17-11e8-b982-02b9355b966e", Controller:(*bool)(0xc0014dbe6e), BlockOwnerDeletion:(*bool)(0xc0014dbe6f)}}
Dec 13 20:43:33.911: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c1fd95b5-ff17-11e8-b982-02b9355b966e", Controller:(*bool)(0xc00103d122), BlockOwnerDeletion:(*bool)(0xc00103d123)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:43:38.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4x8ff" for this suite.
Dec 13 20:43:44.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:43:44.964: INFO: namespace: e2e-tests-gc-4x8ff, resource: bindings, ignored listing per whitelist
Dec 13 20:43:45.047: INFO: namespace e2e-tests-gc-4x8ff deletion completed in 6.12036905s

• [SLOW TEST:11.250 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:43:45.047: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-c8b33360-ff17-11e8-b666-eabb5e37e61c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:43:47.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gpptg" for this suite.
Dec 13 20:44:09.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:44:09.242: INFO: namespace: e2e-tests-configmap-gpptg, resource: bindings, ignored listing per whitelist
Dec 13 20:44:09.259: INFO: namespace e2e-tests-configmap-gpptg deletion completed in 22.093918741s

• [SLOW TEST:24.212 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:44:09.260: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 13 20:44:13.379: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:13.382: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:15.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:15.386: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:17.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:17.386: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:19.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:19.385: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:21.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:21.390: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:23.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:23.386: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:25.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:25.386: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:27.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:27.386: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:29.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:29.386: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:31.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:31.386: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:33.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:33.390: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:35.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:35.386: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:37.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:37.386: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 13 20:44:39.382: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 13 20:44:39.386: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:44:39.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-dbvkd" for this suite.
Dec 13 20:45:01.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:45:01.473: INFO: namespace: e2e-tests-container-lifecycle-hook-dbvkd, resource: bindings, ignored listing per whitelist
Dec 13 20:45:01.635: INFO: namespace e2e-tests-container-lifecycle-hook-dbvkd deletion completed in 22.245797562s

• [SLOW TEST:52.375 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:45:01.638: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 13 20:45:01.737: INFO: Waiting up to 5m0s for pod "pod-f65b2aea-ff17-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-kb5c5" to be "success or failure"
Dec 13 20:45:01.742: INFO: Pod "pod-f65b2aea-ff17-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.46859ms
Dec 13 20:45:03.746: INFO: Pod "pod-f65b2aea-ff17-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009019574s
STEP: Saw pod success
Dec 13 20:45:03.746: INFO: Pod "pod-f65b2aea-ff17-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:45:03.749: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-f65b2aea-ff17-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:45:03.766: INFO: Waiting for pod pod-f65b2aea-ff17-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:45:03.769: INFO: Pod pod-f65b2aea-ff17-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:45:03.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kb5c5" for this suite.
Dec 13 20:45:09.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:45:09.792: INFO: namespace: e2e-tests-emptydir-kb5c5, resource: bindings, ignored listing per whitelist
Dec 13 20:45:09.865: INFO: namespace e2e-tests-emptydir-kb5c5 deletion completed in 6.093053854s

• [SLOW TEST:8.227 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:45:09.865: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-fb40c6d7-ff17-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:45:09.956: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb416ebe-ff17-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-9xnkw" to be "success or failure"
Dec 13 20:45:09.959: INFO: Pod "pod-projected-configmaps-fb416ebe-ff17-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.17156ms
Dec 13 20:45:11.962: INFO: Pod "pod-projected-configmaps-fb416ebe-ff17-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006625952s
STEP: Saw pod success
Dec 13 20:45:11.963: INFO: Pod "pod-projected-configmaps-fb416ebe-ff17-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:45:11.965: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-configmaps-fb416ebe-ff17-11e8-b666-eabb5e37e61c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:45:11.984: INFO: Waiting for pod pod-projected-configmaps-fb416ebe-ff17-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:45:11.986: INFO: Pod pod-projected-configmaps-fb416ebe-ff17-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:45:11.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9xnkw" for this suite.
Dec 13 20:45:18.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:45:18.007: INFO: namespace: e2e-tests-projected-9xnkw, resource: bindings, ignored listing per whitelist
Dec 13 20:45:18.084: INFO: namespace e2e-tests-projected-9xnkw deletion completed in 6.094275033s

• [SLOW TEST:8.219 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:45:18.085: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-5lcd
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 20:45:18.169: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5lcd" in namespace "e2e-tests-subpath-4vsnh" to be "success or failure"
Dec 13 20:45:18.175: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.977431ms
Dec 13 20:45:20.179: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009594065s
Dec 13 20:45:22.182: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 4.013021608s
Dec 13 20:45:24.186: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 6.016429752s
Dec 13 20:45:26.194: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 8.024248124s
Dec 13 20:45:28.197: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 10.027832692s
Dec 13 20:45:30.201: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 12.031537656s
Dec 13 20:45:32.205: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 14.035352087s
Dec 13 20:45:34.208: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 16.039145377s
Dec 13 20:45:36.217: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 18.047420648s
Dec 13 20:45:38.220: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 20.050992582s
Dec 13 20:45:40.224: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Running", Reason="", readiness=false. Elapsed: 22.054618908s
Dec 13 20:45:42.227: INFO: Pod "pod-subpath-test-configmap-5lcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.058189254s
STEP: Saw pod success
Dec 13 20:45:42.228: INFO: Pod "pod-subpath-test-configmap-5lcd" satisfied condition "success or failure"
Dec 13 20:45:42.230: INFO: Trying to get logs from node ip-10-0-2-167.eu-west-1.compute.internal pod pod-subpath-test-configmap-5lcd container test-container-subpath-configmap-5lcd: <nil>
STEP: delete the pod
Dec 13 20:45:42.251: INFO: Waiting for pod pod-subpath-test-configmap-5lcd to disappear
Dec 13 20:45:42.254: INFO: Pod pod-subpath-test-configmap-5lcd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5lcd
Dec 13 20:45:42.257: INFO: Deleting pod "pod-subpath-test-configmap-5lcd" in namespace "e2e-tests-subpath-4vsnh"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:45:42.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-4vsnh" for this suite.
Dec 13 20:45:48.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:45:48.303: INFO: namespace: e2e-tests-subpath-4vsnh, resource: bindings, ignored listing per whitelist
Dec 13 20:45:48.363: INFO: namespace e2e-tests-subpath-4vsnh deletion completed in 6.097261036s

• [SLOW TEST:30.279 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:45:48.364: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-1231d3f8-ff18-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:45:48.479: INFO: Waiting up to 5m0s for pod "pod-secrets-123796ad-ff18-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-secrets-rf254" to be "success or failure"
Dec 13 20:45:48.481: INFO: Pod "pod-secrets-123796ad-ff18-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552096ms
Dec 13 20:45:50.485: INFO: Pod "pod-secrets-123796ad-ff18-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0060956s
STEP: Saw pod success
Dec 13 20:45:50.485: INFO: Pod "pod-secrets-123796ad-ff18-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:45:50.488: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-secrets-123796ad-ff18-11e8-b666-eabb5e37e61c container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:45:50.516: INFO: Waiting for pod pod-secrets-123796ad-ff18-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:45:50.521: INFO: Pod pod-secrets-123796ad-ff18-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:45:50.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-rf254" for this suite.
Dec 13 20:45:56.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:45:56.549: INFO: namespace: e2e-tests-secrets-rf254, resource: bindings, ignored listing per whitelist
Dec 13 20:45:56.627: INFO: namespace e2e-tests-secrets-rf254 deletion completed in 6.102668367s
STEP: Destroying namespace "e2e-tests-secret-namespace-sl44v" for this suite.
Dec 13 20:46:02.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:46:02.696: INFO: namespace: e2e-tests-secret-namespace-sl44v, resource: bindings, ignored listing per whitelist
Dec 13 20:46:02.725: INFO: namespace e2e-tests-secret-namespace-sl44v deletion completed in 6.097800333s

• [SLOW TEST:14.362 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:46:02.726: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 13 20:46:02.804: INFO: Waiting up to 5m0s for pod "pod-1ac18e2e-ff18-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-xc4mt" to be "success or failure"
Dec 13 20:46:02.807: INFO: Pod "pod-1ac18e2e-ff18-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.56939ms
Dec 13 20:46:04.810: INFO: Pod "pod-1ac18e2e-ff18-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006283529s
STEP: Saw pod success
Dec 13 20:46:04.810: INFO: Pod "pod-1ac18e2e-ff18-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:46:04.813: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-1ac18e2e-ff18-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:46:04.829: INFO: Waiting for pod pod-1ac18e2e-ff18-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:46:04.832: INFO: Pod pod-1ac18e2e-ff18-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:46:04.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xc4mt" for this suite.
Dec 13 20:46:10.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:46:10.934: INFO: namespace: e2e-tests-emptydir-xc4mt, resource: bindings, ignored listing per whitelist
Dec 13 20:46:10.934: INFO: namespace e2e-tests-emptydir-xc4mt deletion completed in 6.099487083s

• [SLOW TEST:8.208 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:46:10.935: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-6w8j2
I1213 20:46:11.003275      14 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-6w8j2, replica count: 1
I1213 20:46:12.053612      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1213 20:46:13.053771      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 13 20:46:13.165: INFO: Created: latency-svc-tdvd6
Dec 13 20:46:13.179: INFO: Got endpoints: latency-svc-tdvd6 [25.605493ms]
Dec 13 20:46:13.202: INFO: Created: latency-svc-m58hn
Dec 13 20:46:13.209: INFO: Got endpoints: latency-svc-m58hn [29.649526ms]
Dec 13 20:46:13.215: INFO: Created: latency-svc-g92hh
Dec 13 20:46:13.223: INFO: Got endpoints: latency-svc-g92hh [43.418449ms]
Dec 13 20:46:13.229: INFO: Created: latency-svc-z6hhh
Dec 13 20:46:13.235: INFO: Got endpoints: latency-svc-z6hhh [55.597291ms]
Dec 13 20:46:13.241: INFO: Created: latency-svc-mdt2k
Dec 13 20:46:13.255: INFO: Got endpoints: latency-svc-mdt2k [74.782395ms]
Dec 13 20:46:13.271: INFO: Created: latency-svc-ll7tg
Dec 13 20:46:13.278: INFO: Got endpoints: latency-svc-ll7tg [98.328324ms]
Dec 13 20:46:13.285: INFO: Created: latency-svc-2kbhs
Dec 13 20:46:13.295: INFO: Got endpoints: latency-svc-2kbhs [115.001575ms]
Dec 13 20:46:13.299: INFO: Created: latency-svc-sd8dg
Dec 13 20:46:13.306: INFO: Got endpoints: latency-svc-sd8dg [125.579874ms]
Dec 13 20:46:13.312: INFO: Created: latency-svc-xj57c
Dec 13 20:46:13.316: INFO: Got endpoints: latency-svc-xj57c [135.697878ms]
Dec 13 20:46:13.332: INFO: Created: latency-svc-k7nnq
Dec 13 20:46:13.340: INFO: Got endpoints: latency-svc-k7nnq [159.694649ms]
Dec 13 20:46:13.347: INFO: Created: latency-svc-l84zv
Dec 13 20:46:13.354: INFO: Got endpoints: latency-svc-l84zv [174.105091ms]
Dec 13 20:46:13.361: INFO: Created: latency-svc-7tjr9
Dec 13 20:46:13.369: INFO: Got endpoints: latency-svc-7tjr9 [188.551163ms]
Dec 13 20:46:13.375: INFO: Created: latency-svc-btq5x
Dec 13 20:46:13.386: INFO: Got endpoints: latency-svc-btq5x [205.122592ms]
Dec 13 20:46:13.388: INFO: Created: latency-svc-fsdqk
Dec 13 20:46:13.400: INFO: Got endpoints: latency-svc-fsdqk [219.938942ms]
Dec 13 20:46:13.406: INFO: Created: latency-svc-mtjml
Dec 13 20:46:13.413: INFO: Got endpoints: latency-svc-mtjml [232.030615ms]
Dec 13 20:46:13.419: INFO: Created: latency-svc-llvnl
Dec 13 20:46:13.428: INFO: Got endpoints: latency-svc-llvnl [247.377968ms]
Dec 13 20:46:13.432: INFO: Created: latency-svc-dn8dt
Dec 13 20:46:13.443: INFO: Created: latency-svc-jkncl
Dec 13 20:46:13.446: INFO: Got endpoints: latency-svc-dn8dt [236.693868ms]
Dec 13 20:46:13.453: INFO: Got endpoints: latency-svc-jkncl [229.532992ms]
Dec 13 20:46:13.458: INFO: Created: latency-svc-lsnz9
Dec 13 20:46:13.467: INFO: Got endpoints: latency-svc-lsnz9 [231.654239ms]
Dec 13 20:46:13.472: INFO: Created: latency-svc-tc8pd
Dec 13 20:46:13.478: INFO: Got endpoints: latency-svc-tc8pd [223.56434ms]
Dec 13 20:46:13.489: INFO: Created: latency-svc-bp6g9
Dec 13 20:46:13.496: INFO: Got endpoints: latency-svc-bp6g9 [217.336107ms]
Dec 13 20:46:13.505: INFO: Created: latency-svc-lk4pt
Dec 13 20:46:13.516: INFO: Got endpoints: latency-svc-lk4pt [221.396992ms]
Dec 13 20:46:13.522: INFO: Created: latency-svc-cpfhl
Dec 13 20:46:13.530: INFO: Got endpoints: latency-svc-cpfhl [224.802854ms]
Dec 13 20:46:13.542: INFO: Created: latency-svc-b4r55
Dec 13 20:46:13.549: INFO: Got endpoints: latency-svc-b4r55 [233.124244ms]
Dec 13 20:46:13.555: INFO: Created: latency-svc-drbl4
Dec 13 20:46:13.563: INFO: Got endpoints: latency-svc-drbl4 [223.160408ms]
Dec 13 20:46:13.568: INFO: Created: latency-svc-kbdqh
Dec 13 20:46:13.574: INFO: Got endpoints: latency-svc-kbdqh [219.342019ms]
Dec 13 20:46:13.590: INFO: Created: latency-svc-24mch
Dec 13 20:46:13.597: INFO: Got endpoints: latency-svc-24mch [228.316167ms]
Dec 13 20:46:13.606: INFO: Created: latency-svc-7rxs6
Dec 13 20:46:13.615: INFO: Got endpoints: latency-svc-7rxs6 [229.266957ms]
Dec 13 20:46:13.626: INFO: Created: latency-svc-k4lhp
Dec 13 20:46:13.633: INFO: Got endpoints: latency-svc-k4lhp [232.112815ms]
Dec 13 20:46:13.643: INFO: Created: latency-svc-nkhqs
Dec 13 20:46:13.666: INFO: Got endpoints: latency-svc-nkhqs [252.88466ms]
Dec 13 20:46:13.674: INFO: Created: latency-svc-8dr8l
Dec 13 20:46:13.685: INFO: Got endpoints: latency-svc-8dr8l [257.495075ms]
Dec 13 20:46:13.694: INFO: Created: latency-svc-b2m7l
Dec 13 20:46:13.703: INFO: Got endpoints: latency-svc-b2m7l [257.719358ms]
Dec 13 20:46:13.706: INFO: Created: latency-svc-5lqhw
Dec 13 20:46:13.716: INFO: Got endpoints: latency-svc-5lqhw [263.055764ms]
Dec 13 20:46:13.718: INFO: Created: latency-svc-2s82n
Dec 13 20:46:13.724: INFO: Got endpoints: latency-svc-2s82n [256.547554ms]
Dec 13 20:46:13.739: INFO: Created: latency-svc-9qj4b
Dec 13 20:46:13.744: INFO: Got endpoints: latency-svc-9qj4b [265.771506ms]
Dec 13 20:46:13.753: INFO: Created: latency-svc-xtqh5
Dec 13 20:46:13.762: INFO: Got endpoints: latency-svc-xtqh5 [266.574132ms]
Dec 13 20:46:13.769: INFO: Created: latency-svc-jgzpb
Dec 13 20:46:13.777: INFO: Got endpoints: latency-svc-jgzpb [260.559699ms]
Dec 13 20:46:13.784: INFO: Created: latency-svc-8zhzc
Dec 13 20:46:13.864: INFO: Got endpoints: latency-svc-8zhzc [333.748846ms]
Dec 13 20:46:13.882: INFO: Created: latency-svc-4bn7d
Dec 13 20:46:13.890: INFO: Got endpoints: latency-svc-4bn7d [340.915858ms]
Dec 13 20:46:13.901: INFO: Created: latency-svc-nbrsm
Dec 13 20:46:13.911: INFO: Got endpoints: latency-svc-nbrsm [347.18127ms]
Dec 13 20:46:13.919: INFO: Created: latency-svc-gr5b8
Dec 13 20:46:13.929: INFO: Got endpoints: latency-svc-gr5b8 [354.838807ms]
Dec 13 20:46:13.931: INFO: Created: latency-svc-twxgb
Dec 13 20:46:13.942: INFO: Created: latency-svc-m27p7
Dec 13 20:46:13.943: INFO: Got endpoints: latency-svc-twxgb [345.568156ms]
Dec 13 20:46:13.956: INFO: Got endpoints: latency-svc-m27p7 [340.438426ms]
Dec 13 20:46:13.960: INFO: Created: latency-svc-s5s5x
Dec 13 20:46:13.972: INFO: Got endpoints: latency-svc-s5s5x [339.620632ms]
Dec 13 20:46:13.973: INFO: Created: latency-svc-94lm4
Dec 13 20:46:13.985: INFO: Got endpoints: latency-svc-94lm4 [319.613569ms]
Dec 13 20:46:13.992: INFO: Created: latency-svc-hnpbj
Dec 13 20:46:13.999: INFO: Got endpoints: latency-svc-hnpbj [314.039821ms]
Dec 13 20:46:14.006: INFO: Created: latency-svc-cbqnf
Dec 13 20:46:14.015: INFO: Created: latency-svc-t27x8
Dec 13 20:46:14.030: INFO: Got endpoints: latency-svc-cbqnf [326.364583ms]
Dec 13 20:46:14.035: INFO: Created: latency-svc-qd654
Dec 13 20:46:14.046: INFO: Created: latency-svc-nh9ts
Dec 13 20:46:14.059: INFO: Created: latency-svc-tcztt
Dec 13 20:46:14.076: INFO: Created: latency-svc-lkq2s
Dec 13 20:46:14.076: INFO: Got endpoints: latency-svc-t27x8 [360.44509ms]
Dec 13 20:46:14.098: INFO: Created: latency-svc-s87gz
Dec 13 20:46:14.113: INFO: Created: latency-svc-qzkbm
Dec 13 20:46:14.127: INFO: Got endpoints: latency-svc-qd654 [403.274969ms]
Dec 13 20:46:14.133: INFO: Created: latency-svc-vh2lw
Dec 13 20:46:14.153: INFO: Created: latency-svc-lr5df
Dec 13 20:46:14.172: INFO: Created: latency-svc-vwznx
Dec 13 20:46:14.181: INFO: Got endpoints: latency-svc-nh9ts [437.177073ms]
Dec 13 20:46:14.213: INFO: Created: latency-svc-dtx4g
Dec 13 20:46:14.228: INFO: Got endpoints: latency-svc-tcztt [465.658346ms]
Dec 13 20:46:14.231: INFO: Created: latency-svc-wk7nt
Dec 13 20:46:14.249: INFO: Created: latency-svc-nw758
Dec 13 20:46:14.265: INFO: Created: latency-svc-ghck4
Dec 13 20:46:14.279: INFO: Got endpoints: latency-svc-lkq2s [501.560406ms]
Dec 13 20:46:14.287: INFO: Created: latency-svc-vn6vq
Dec 13 20:46:14.301: INFO: Created: latency-svc-znf8d
Dec 13 20:46:14.320: INFO: Created: latency-svc-qw4dg
Dec 13 20:46:14.328: INFO: Got endpoints: latency-svc-s87gz [463.377377ms]
Dec 13 20:46:14.331: INFO: Created: latency-svc-sgzhc
Dec 13 20:46:14.349: INFO: Created: latency-svc-prt9j
Dec 13 20:46:14.361: INFO: Created: latency-svc-pt96z
Dec 13 20:46:14.374: INFO: Got endpoints: latency-svc-qzkbm [483.898058ms]
Dec 13 20:46:14.375: INFO: Created: latency-svc-p2lpk
Dec 13 20:46:14.391: INFO: Created: latency-svc-qcg6s
Dec 13 20:46:14.423: INFO: Got endpoints: latency-svc-vh2lw [512.165242ms]
Dec 13 20:46:14.442: INFO: Created: latency-svc-s7jv8
Dec 13 20:46:14.473: INFO: Got endpoints: latency-svc-lr5df [543.723772ms]
Dec 13 20:46:14.489: INFO: Created: latency-svc-lwt62
Dec 13 20:46:14.528: INFO: Got endpoints: latency-svc-vwznx [584.812961ms]
Dec 13 20:46:14.544: INFO: Created: latency-svc-n8rbr
Dec 13 20:46:14.574: INFO: Got endpoints: latency-svc-dtx4g [618.269877ms]
Dec 13 20:46:14.594: INFO: Created: latency-svc-8bdjl
Dec 13 20:46:14.623: INFO: Got endpoints: latency-svc-wk7nt [650.618485ms]
Dec 13 20:46:14.640: INFO: Created: latency-svc-6xsz5
Dec 13 20:46:14.673: INFO: Got endpoints: latency-svc-nw758 [687.299162ms]
Dec 13 20:46:14.689: INFO: Created: latency-svc-jf69z
Dec 13 20:46:14.723: INFO: Got endpoints: latency-svc-ghck4 [723.983242ms]
Dec 13 20:46:14.743: INFO: Created: latency-svc-77dt6
Dec 13 20:46:14.774: INFO: Got endpoints: latency-svc-vn6vq [744.162467ms]
Dec 13 20:46:14.796: INFO: Created: latency-svc-6nvld
Dec 13 20:46:14.823: INFO: Got endpoints: latency-svc-znf8d [746.87446ms]
Dec 13 20:46:14.846: INFO: Created: latency-svc-ftnh8
Dec 13 20:46:14.874: INFO: Got endpoints: latency-svc-qw4dg [746.593039ms]
Dec 13 20:46:14.896: INFO: Created: latency-svc-c9rhj
Dec 13 20:46:14.924: INFO: Got endpoints: latency-svc-sgzhc [742.289893ms]
Dec 13 20:46:14.942: INFO: Created: latency-svc-s29c5
Dec 13 20:46:14.973: INFO: Got endpoints: latency-svc-prt9j [745.254717ms]
Dec 13 20:46:14.996: INFO: Created: latency-svc-979kv
Dec 13 20:46:15.024: INFO: Got endpoints: latency-svc-pt96z [744.762582ms]
Dec 13 20:46:15.044: INFO: Created: latency-svc-258dh
Dec 13 20:46:15.078: INFO: Got endpoints: latency-svc-p2lpk [750.279388ms]
Dec 13 20:46:15.113: INFO: Created: latency-svc-fksff
Dec 13 20:46:15.124: INFO: Got endpoints: latency-svc-qcg6s [749.602147ms]
Dec 13 20:46:15.143: INFO: Created: latency-svc-d79z6
Dec 13 20:46:15.176: INFO: Got endpoints: latency-svc-s7jv8 [753.114097ms]
Dec 13 20:46:15.195: INFO: Created: latency-svc-fh74s
Dec 13 20:46:15.224: INFO: Got endpoints: latency-svc-lwt62 [751.215485ms]
Dec 13 20:46:15.242: INFO: Created: latency-svc-7c92n
Dec 13 20:46:15.276: INFO: Got endpoints: latency-svc-n8rbr [748.095498ms]
Dec 13 20:46:15.295: INFO: Created: latency-svc-9d8xd
Dec 13 20:46:15.325: INFO: Got endpoints: latency-svc-8bdjl [750.77005ms]
Dec 13 20:46:15.346: INFO: Created: latency-svc-zxqf9
Dec 13 20:46:15.373: INFO: Got endpoints: latency-svc-6xsz5 [750.024378ms]
Dec 13 20:46:15.393: INFO: Created: latency-svc-d54fz
Dec 13 20:46:15.423: INFO: Got endpoints: latency-svc-jf69z [749.991906ms]
Dec 13 20:46:15.443: INFO: Created: latency-svc-lgkfx
Dec 13 20:46:15.474: INFO: Got endpoints: latency-svc-77dt6 [750.756695ms]
Dec 13 20:46:15.492: INFO: Created: latency-svc-mpk2b
Dec 13 20:46:15.523: INFO: Got endpoints: latency-svc-6nvld [749.119562ms]
Dec 13 20:46:15.542: INFO: Created: latency-svc-vfwql
Dec 13 20:46:15.577: INFO: Got endpoints: latency-svc-ftnh8 [753.644899ms]
Dec 13 20:46:15.593: INFO: Created: latency-svc-zwr7k
Dec 13 20:46:15.624: INFO: Got endpoints: latency-svc-c9rhj [749.708451ms]
Dec 13 20:46:15.645: INFO: Created: latency-svc-mwsrb
Dec 13 20:46:15.674: INFO: Got endpoints: latency-svc-s29c5 [750.296308ms]
Dec 13 20:46:15.702: INFO: Created: latency-svc-lp5pg
Dec 13 20:46:15.724: INFO: Got endpoints: latency-svc-979kv [750.994057ms]
Dec 13 20:46:15.746: INFO: Created: latency-svc-dzb9g
Dec 13 20:46:15.930: INFO: Got endpoints: latency-svc-258dh [906.514356ms]
Dec 13 20:46:15.931: INFO: Got endpoints: latency-svc-fksff [853.105626ms]
Dec 13 20:46:15.934: INFO: Got endpoints: latency-svc-d79z6 [810.129147ms]
Dec 13 20:46:15.935: INFO: Got endpoints: latency-svc-fh74s [758.818652ms]
Dec 13 20:46:15.953: INFO: Created: latency-svc-frqq2
Dec 13 20:46:15.964: INFO: Created: latency-svc-sh8tb
Dec 13 20:46:15.979: INFO: Got endpoints: latency-svc-7c92n [754.677756ms]
Dec 13 20:46:15.989: INFO: Created: latency-svc-sq88k
Dec 13 20:46:16.000: INFO: Created: latency-svc-qfc9q
Dec 13 20:46:16.011: INFO: Created: latency-svc-ss4vt
Dec 13 20:46:16.022: INFO: Got endpoints: latency-svc-9d8xd [746.248786ms]
Dec 13 20:46:16.042: INFO: Created: latency-svc-8fqzs
Dec 13 20:46:16.075: INFO: Got endpoints: latency-svc-zxqf9 [750.089435ms]
Dec 13 20:46:16.095: INFO: Created: latency-svc-fhn64
Dec 13 20:46:16.123: INFO: Got endpoints: latency-svc-d54fz [750.324541ms]
Dec 13 20:46:16.146: INFO: Created: latency-svc-smlt7
Dec 13 20:46:16.178: INFO: Got endpoints: latency-svc-lgkfx [754.754098ms]
Dec 13 20:46:16.207: INFO: Created: latency-svc-djkl6
Dec 13 20:46:16.223: INFO: Got endpoints: latency-svc-mpk2b [749.098604ms]
Dec 13 20:46:16.247: INFO: Created: latency-svc-k74j7
Dec 13 20:46:16.274: INFO: Got endpoints: latency-svc-vfwql [750.136874ms]
Dec 13 20:46:16.295: INFO: Created: latency-svc-jk5bx
Dec 13 20:46:16.326: INFO: Got endpoints: latency-svc-zwr7k [748.831901ms]
Dec 13 20:46:16.356: INFO: Created: latency-svc-hd56r
Dec 13 20:46:16.373: INFO: Got endpoints: latency-svc-mwsrb [749.556258ms]
Dec 13 20:46:16.391: INFO: Created: latency-svc-95zvm
Dec 13 20:46:16.424: INFO: Got endpoints: latency-svc-lp5pg [749.271132ms]
Dec 13 20:46:16.452: INFO: Created: latency-svc-wvd25
Dec 13 20:46:16.474: INFO: Got endpoints: latency-svc-dzb9g [750.010341ms]
Dec 13 20:46:16.495: INFO: Created: latency-svc-5bf6k
Dec 13 20:46:16.523: INFO: Got endpoints: latency-svc-frqq2 [592.769305ms]
Dec 13 20:46:16.544: INFO: Created: latency-svc-fl9kh
Dec 13 20:46:16.576: INFO: Got endpoints: latency-svc-sh8tb [644.891256ms]
Dec 13 20:46:16.598: INFO: Created: latency-svc-nrdrx
Dec 13 20:46:16.624: INFO: Got endpoints: latency-svc-sq88k [689.176655ms]
Dec 13 20:46:16.646: INFO: Created: latency-svc-9w26v
Dec 13 20:46:16.674: INFO: Got endpoints: latency-svc-qfc9q [739.308175ms]
Dec 13 20:46:16.704: INFO: Created: latency-svc-h77zp
Dec 13 20:46:16.723: INFO: Got endpoints: latency-svc-ss4vt [744.067113ms]
Dec 13 20:46:16.745: INFO: Created: latency-svc-xvhh5
Dec 13 20:46:16.775: INFO: Got endpoints: latency-svc-8fqzs [751.913619ms]
Dec 13 20:46:16.796: INFO: Created: latency-svc-c8v5q
Dec 13 20:46:16.823: INFO: Got endpoints: latency-svc-fhn64 [748.024386ms]
Dec 13 20:46:16.842: INFO: Created: latency-svc-sqbdh
Dec 13 20:46:16.878: INFO: Got endpoints: latency-svc-smlt7 [754.096558ms]
Dec 13 20:46:16.901: INFO: Created: latency-svc-zhtmn
Dec 13 20:46:16.923: INFO: Got endpoints: latency-svc-djkl6 [745.302204ms]
Dec 13 20:46:16.943: INFO: Created: latency-svc-bwwmc
Dec 13 20:46:16.973: INFO: Got endpoints: latency-svc-k74j7 [748.304969ms]
Dec 13 20:46:17.093: INFO: Got endpoints: latency-svc-hd56r [766.553932ms]
Dec 13 20:46:17.093: INFO: Got endpoints: latency-svc-jk5bx [819.026279ms]
Dec 13 20:46:17.104: INFO: Created: latency-svc-d6hzb
Dec 13 20:46:17.233: INFO: Got endpoints: latency-svc-95zvm [859.438348ms]
Dec 13 20:46:17.233: INFO: Got endpoints: latency-svc-wvd25 [809.437924ms]
Dec 13 20:46:17.234: INFO: Got endpoints: latency-svc-5bf6k [759.895833ms]
Dec 13 20:46:17.236: INFO: Created: latency-svc-h9pdz
Dec 13 20:46:17.254: INFO: Created: latency-svc-rz7bf
Dec 13 20:46:17.267: INFO: Created: latency-svc-xvcqk
Dec 13 20:46:17.278: INFO: Got endpoints: latency-svc-fl9kh [754.276453ms]
Dec 13 20:46:17.284: INFO: Created: latency-svc-cg7bg
Dec 13 20:46:17.298: INFO: Created: latency-svc-8n9qg
Dec 13 20:46:17.309: INFO: Created: latency-svc-x6t9l
Dec 13 20:46:17.324: INFO: Got endpoints: latency-svc-nrdrx [747.471823ms]
Dec 13 20:46:17.344: INFO: Created: latency-svc-shs9t
Dec 13 20:46:17.377: INFO: Got endpoints: latency-svc-9w26v [752.883923ms]
Dec 13 20:46:17.397: INFO: Created: latency-svc-dqcp7
Dec 13 20:46:17.423: INFO: Got endpoints: latency-svc-h77zp [748.699657ms]
Dec 13 20:46:17.439: INFO: Created: latency-svc-p595s
Dec 13 20:46:17.475: INFO: Got endpoints: latency-svc-xvhh5 [751.843485ms]
Dec 13 20:46:17.493: INFO: Created: latency-svc-wkljc
Dec 13 20:46:17.523: INFO: Got endpoints: latency-svc-c8v5q [748.595053ms]
Dec 13 20:46:17.538: INFO: Created: latency-svc-4mm4v
Dec 13 20:46:17.574: INFO: Got endpoints: latency-svc-sqbdh [750.40478ms]
Dec 13 20:46:17.592: INFO: Created: latency-svc-jsqgp
Dec 13 20:46:17.624: INFO: Got endpoints: latency-svc-zhtmn [746.164536ms]
Dec 13 20:46:17.653: INFO: Created: latency-svc-4qxcf
Dec 13 20:46:17.677: INFO: Got endpoints: latency-svc-bwwmc [753.886202ms]
Dec 13 20:46:17.703: INFO: Created: latency-svc-27tht
Dec 13 20:46:17.725: INFO: Got endpoints: latency-svc-d6hzb [751.597123ms]
Dec 13 20:46:17.776: INFO: Created: latency-svc-pnxr7
Dec 13 20:46:17.781: INFO: Got endpoints: latency-svc-h9pdz [688.21365ms]
Dec 13 20:46:17.811: INFO: Created: latency-svc-rzcmv
Dec 13 20:46:17.824: INFO: Got endpoints: latency-svc-rz7bf [731.765558ms]
Dec 13 20:46:17.843: INFO: Created: latency-svc-l6hnk
Dec 13 20:46:17.873: INFO: Got endpoints: latency-svc-xvcqk [639.986507ms]
Dec 13 20:46:17.903: INFO: Created: latency-svc-tfw9l
Dec 13 20:46:17.925: INFO: Got endpoints: latency-svc-cg7bg [691.454758ms]
Dec 13 20:46:17.943: INFO: Created: latency-svc-sdxln
Dec 13 20:46:17.975: INFO: Got endpoints: latency-svc-8n9qg [741.26629ms]
Dec 13 20:46:17.997: INFO: Created: latency-svc-8g642
Dec 13 20:46:18.024: INFO: Got endpoints: latency-svc-x6t9l [746.002945ms]
Dec 13 20:46:18.044: INFO: Created: latency-svc-9ws7n
Dec 13 20:46:18.074: INFO: Got endpoints: latency-svc-shs9t [749.817447ms]
Dec 13 20:46:18.096: INFO: Created: latency-svc-mhm4t
Dec 13 20:46:18.123: INFO: Got endpoints: latency-svc-dqcp7 [746.406715ms]
Dec 13 20:46:18.142: INFO: Created: latency-svc-zk6wp
Dec 13 20:46:18.173: INFO: Got endpoints: latency-svc-p595s [750.035182ms]
Dec 13 20:46:18.196: INFO: Created: latency-svc-sdwlb
Dec 13 20:46:18.223: INFO: Got endpoints: latency-svc-wkljc [748.206616ms]
Dec 13 20:46:18.242: INFO: Created: latency-svc-gsb76
Dec 13 20:46:18.276: INFO: Got endpoints: latency-svc-4mm4v [753.001884ms]
Dec 13 20:46:18.295: INFO: Created: latency-svc-5w6ws
Dec 13 20:46:18.323: INFO: Got endpoints: latency-svc-jsqgp [749.62134ms]
Dec 13 20:46:18.349: INFO: Created: latency-svc-h6xg2
Dec 13 20:46:18.374: INFO: Got endpoints: latency-svc-4qxcf [749.680036ms]
Dec 13 20:46:18.390: INFO: Created: latency-svc-h5f6x
Dec 13 20:46:18.424: INFO: Got endpoints: latency-svc-27tht [746.294262ms]
Dec 13 20:46:18.441: INFO: Created: latency-svc-t4kb9
Dec 13 20:46:18.475: INFO: Got endpoints: latency-svc-pnxr7 [750.018495ms]
Dec 13 20:46:18.491: INFO: Created: latency-svc-zxnz4
Dec 13 20:46:18.525: INFO: Got endpoints: latency-svc-rzcmv [744.176105ms]
Dec 13 20:46:18.551: INFO: Created: latency-svc-5ms2d
Dec 13 20:46:18.575: INFO: Got endpoints: latency-svc-l6hnk [749.953372ms]
Dec 13 20:46:18.600: INFO: Created: latency-svc-rz2pd
Dec 13 20:46:18.623: INFO: Got endpoints: latency-svc-tfw9l [749.821679ms]
Dec 13 20:46:18.642: INFO: Created: latency-svc-9sjng
Dec 13 20:46:18.674: INFO: Got endpoints: latency-svc-sdxln [748.456137ms]
Dec 13 20:46:18.696: INFO: Created: latency-svc-w9977
Dec 13 20:46:18.724: INFO: Got endpoints: latency-svc-8g642 [748.234958ms]
Dec 13 20:46:18.744: INFO: Created: latency-svc-4glz6
Dec 13 20:46:18.773: INFO: Got endpoints: latency-svc-9ws7n [749.514856ms]
Dec 13 20:46:18.792: INFO: Created: latency-svc-rscl7
Dec 13 20:46:18.826: INFO: Got endpoints: latency-svc-mhm4t [752.055854ms]
Dec 13 20:46:18.844: INFO: Created: latency-svc-shnjz
Dec 13 20:46:18.873: INFO: Got endpoints: latency-svc-zk6wp [749.623981ms]
Dec 13 20:46:18.892: INFO: Created: latency-svc-bs5ks
Dec 13 20:46:18.923: INFO: Got endpoints: latency-svc-sdwlb [749.530241ms]
Dec 13 20:46:18.942: INFO: Created: latency-svc-zgxkb
Dec 13 20:46:18.974: INFO: Got endpoints: latency-svc-gsb76 [751.20088ms]
Dec 13 20:46:18.992: INFO: Created: latency-svc-wftwx
Dec 13 20:46:19.025: INFO: Got endpoints: latency-svc-5w6ws [748.687427ms]
Dec 13 20:46:19.043: INFO: Created: latency-svc-kb46q
Dec 13 20:46:19.074: INFO: Got endpoints: latency-svc-h6xg2 [750.401032ms]
Dec 13 20:46:19.094: INFO: Created: latency-svc-kx4pc
Dec 13 20:46:19.123: INFO: Got endpoints: latency-svc-h5f6x [749.25957ms]
Dec 13 20:46:19.146: INFO: Created: latency-svc-n67ln
Dec 13 20:46:19.173: INFO: Got endpoints: latency-svc-t4kb9 [749.488208ms]
Dec 13 20:46:19.196: INFO: Created: latency-svc-zdl6k
Dec 13 20:46:19.223: INFO: Got endpoints: latency-svc-zxnz4 [748.444347ms]
Dec 13 20:46:19.245: INFO: Created: latency-svc-fdcjk
Dec 13 20:46:19.275: INFO: Got endpoints: latency-svc-5ms2d [749.575044ms]
Dec 13 20:46:19.294: INFO: Created: latency-svc-fx2cj
Dec 13 20:46:19.323: INFO: Got endpoints: latency-svc-rz2pd [748.252364ms]
Dec 13 20:46:19.345: INFO: Created: latency-svc-cwvxg
Dec 13 20:46:19.373: INFO: Got endpoints: latency-svc-9sjng [750.177762ms]
Dec 13 20:46:19.408: INFO: Created: latency-svc-tm2zc
Dec 13 20:46:19.423: INFO: Got endpoints: latency-svc-w9977 [749.068213ms]
Dec 13 20:46:19.446: INFO: Created: latency-svc-nkpk8
Dec 13 20:46:19.474: INFO: Got endpoints: latency-svc-4glz6 [750.675427ms]
Dec 13 20:46:19.492: INFO: Created: latency-svc-z8ljt
Dec 13 20:46:19.523: INFO: Got endpoints: latency-svc-rscl7 [749.827208ms]
Dec 13 20:46:19.543: INFO: Created: latency-svc-pt6h6
Dec 13 20:46:19.574: INFO: Got endpoints: latency-svc-shnjz [747.826756ms]
Dec 13 20:46:19.594: INFO: Created: latency-svc-5qxtm
Dec 13 20:46:19.623: INFO: Got endpoints: latency-svc-bs5ks [750.160946ms]
Dec 13 20:46:19.643: INFO: Created: latency-svc-bxjkh
Dec 13 20:46:19.673: INFO: Got endpoints: latency-svc-zgxkb [750.560036ms]
Dec 13 20:46:19.694: INFO: Created: latency-svc-lmjl2
Dec 13 20:46:19.723: INFO: Got endpoints: latency-svc-wftwx [748.875184ms]
Dec 13 20:46:19.740: INFO: Created: latency-svc-fn48q
Dec 13 20:46:19.773: INFO: Got endpoints: latency-svc-kb46q [747.410288ms]
Dec 13 20:46:19.790: INFO: Created: latency-svc-l2f62
Dec 13 20:46:19.824: INFO: Got endpoints: latency-svc-kx4pc [749.95924ms]
Dec 13 20:46:19.848: INFO: Created: latency-svc-hnkhz
Dec 13 20:46:19.873: INFO: Got endpoints: latency-svc-n67ln [750.037387ms]
Dec 13 20:46:19.892: INFO: Created: latency-svc-rz6wr
Dec 13 20:46:19.923: INFO: Got endpoints: latency-svc-zdl6k [749.362433ms]
Dec 13 20:46:19.941: INFO: Created: latency-svc-jk6jf
Dec 13 20:46:19.974: INFO: Got endpoints: latency-svc-fdcjk [750.833146ms]
Dec 13 20:46:20.014: INFO: Created: latency-svc-hgkjs
Dec 13 20:46:20.033: INFO: Got endpoints: latency-svc-fx2cj [757.79302ms]
Dec 13 20:46:20.055: INFO: Created: latency-svc-dxgjj
Dec 13 20:46:20.077: INFO: Got endpoints: latency-svc-cwvxg [753.925672ms]
Dec 13 20:46:20.117: INFO: Created: latency-svc-rbj9m
Dec 13 20:46:20.124: INFO: Got endpoints: latency-svc-tm2zc [750.18371ms]
Dec 13 20:46:20.142: INFO: Created: latency-svc-szkkq
Dec 13 20:46:20.173: INFO: Got endpoints: latency-svc-nkpk8 [749.911827ms]
Dec 13 20:46:20.192: INFO: Created: latency-svc-zpw4k
Dec 13 20:46:20.224: INFO: Got endpoints: latency-svc-z8ljt [749.596142ms]
Dec 13 20:46:20.247: INFO: Created: latency-svc-n9cm9
Dec 13 20:46:20.273: INFO: Got endpoints: latency-svc-pt6h6 [749.978796ms]
Dec 13 20:46:20.294: INFO: Created: latency-svc-q2mqm
Dec 13 20:46:20.324: INFO: Got endpoints: latency-svc-5qxtm [749.626127ms]
Dec 13 20:46:20.345: INFO: Created: latency-svc-vw6r5
Dec 13 20:46:20.376: INFO: Got endpoints: latency-svc-bxjkh [752.286867ms]
Dec 13 20:46:20.395: INFO: Created: latency-svc-b8bxt
Dec 13 20:46:20.423: INFO: Got endpoints: latency-svc-lmjl2 [749.570462ms]
Dec 13 20:46:20.442: INFO: Created: latency-svc-jmdvw
Dec 13 20:46:20.475: INFO: Got endpoints: latency-svc-fn48q [751.356253ms]
Dec 13 20:46:20.493: INFO: Created: latency-svc-6cr8v
Dec 13 20:46:20.524: INFO: Got endpoints: latency-svc-l2f62 [751.311696ms]
Dec 13 20:46:20.542: INFO: Created: latency-svc-9qgrr
Dec 13 20:46:20.573: INFO: Got endpoints: latency-svc-hnkhz [749.250852ms]
Dec 13 20:46:20.591: INFO: Created: latency-svc-kbcfc
Dec 13 20:46:20.623: INFO: Got endpoints: latency-svc-rz6wr [749.273423ms]
Dec 13 20:46:20.640: INFO: Created: latency-svc-468k9
Dec 13 20:46:20.674: INFO: Got endpoints: latency-svc-jk6jf [750.409331ms]
Dec 13 20:46:20.694: INFO: Created: latency-svc-j2pvb
Dec 13 20:46:20.724: INFO: Got endpoints: latency-svc-hgkjs [749.405397ms]
Dec 13 20:46:20.742: INFO: Created: latency-svc-kx8q5
Dec 13 20:46:20.792: INFO: Got endpoints: latency-svc-dxgjj [758.914229ms]
Dec 13 20:46:20.812: INFO: Created: latency-svc-44mnx
Dec 13 20:46:20.825: INFO: Got endpoints: latency-svc-rbj9m [747.921622ms]
Dec 13 20:46:20.842: INFO: Created: latency-svc-tmb4k
Dec 13 20:46:20.873: INFO: Got endpoints: latency-svc-szkkq [749.593841ms]
Dec 13 20:46:20.893: INFO: Created: latency-svc-gxqkn
Dec 13 20:46:20.923: INFO: Got endpoints: latency-svc-zpw4k [750.2086ms]
Dec 13 20:46:21.003: INFO: Got endpoints: latency-svc-n9cm9 [778.875414ms]
Dec 13 20:46:21.070: INFO: Got endpoints: latency-svc-q2mqm [797.117291ms]
Dec 13 20:46:21.075: INFO: Got endpoints: latency-svc-vw6r5 [751.687476ms]
Dec 13 20:46:21.080: INFO: Created: latency-svc-jkkpg
Dec 13 20:46:21.096: INFO: Created: latency-svc-f528w
Dec 13 20:46:21.124: INFO: Got endpoints: latency-svc-b8bxt [747.787202ms]
Dec 13 20:46:21.173: INFO: Got endpoints: latency-svc-jmdvw [749.74505ms]
Dec 13 20:46:21.225: INFO: Got endpoints: latency-svc-6cr8v [749.792161ms]
Dec 13 20:46:21.275: INFO: Got endpoints: latency-svc-9qgrr [750.77787ms]
Dec 13 20:46:21.338: INFO: Got endpoints: latency-svc-kbcfc [764.644388ms]
Dec 13 20:46:21.374: INFO: Got endpoints: latency-svc-468k9 [750.236347ms]
Dec 13 20:46:21.423: INFO: Got endpoints: latency-svc-j2pvb [748.796142ms]
Dec 13 20:46:21.473: INFO: Got endpoints: latency-svc-kx8q5 [749.020815ms]
Dec 13 20:46:21.523: INFO: Got endpoints: latency-svc-44mnx [730.581074ms]
Dec 13 20:46:21.574: INFO: Got endpoints: latency-svc-tmb4k [748.969502ms]
Dec 13 20:46:21.623: INFO: Got endpoints: latency-svc-gxqkn [749.869731ms]
Dec 13 20:46:21.674: INFO: Got endpoints: latency-svc-jkkpg [751.136783ms]
Dec 13 20:46:21.723: INFO: Got endpoints: latency-svc-f528w [719.677779ms]
Dec 13 20:46:21.723: INFO: Latencies: [29.649526ms 43.418449ms 55.597291ms 74.782395ms 98.328324ms 115.001575ms 125.579874ms 135.697878ms 159.694649ms 174.105091ms 188.551163ms 205.122592ms 217.336107ms 219.342019ms 219.938942ms 221.396992ms 223.160408ms 223.56434ms 224.802854ms 228.316167ms 229.266957ms 229.532992ms 231.654239ms 232.030615ms 232.112815ms 233.124244ms 236.693868ms 247.377968ms 252.88466ms 256.547554ms 257.495075ms 257.719358ms 260.559699ms 263.055764ms 265.771506ms 266.574132ms 314.039821ms 319.613569ms 326.364583ms 333.748846ms 339.620632ms 340.438426ms 340.915858ms 345.568156ms 347.18127ms 354.838807ms 360.44509ms 403.274969ms 437.177073ms 463.377377ms 465.658346ms 483.898058ms 501.560406ms 512.165242ms 543.723772ms 584.812961ms 592.769305ms 618.269877ms 639.986507ms 644.891256ms 650.618485ms 687.299162ms 688.21365ms 689.176655ms 691.454758ms 719.677779ms 723.983242ms 730.581074ms 731.765558ms 739.308175ms 741.26629ms 742.289893ms 744.067113ms 744.162467ms 744.176105ms 744.762582ms 745.254717ms 745.302204ms 746.002945ms 746.164536ms 746.248786ms 746.294262ms 746.406715ms 746.593039ms 746.87446ms 747.410288ms 747.471823ms 747.787202ms 747.826756ms 747.921622ms 748.024386ms 748.095498ms 748.206616ms 748.234958ms 748.252364ms 748.304969ms 748.444347ms 748.456137ms 748.595053ms 748.687427ms 748.699657ms 748.796142ms 748.831901ms 748.875184ms 748.969502ms 749.020815ms 749.068213ms 749.098604ms 749.119562ms 749.250852ms 749.25957ms 749.271132ms 749.273423ms 749.362433ms 749.405397ms 749.488208ms 749.514856ms 749.530241ms 749.556258ms 749.570462ms 749.575044ms 749.593841ms 749.596142ms 749.602147ms 749.62134ms 749.623981ms 749.626127ms 749.680036ms 749.708451ms 749.74505ms 749.792161ms 749.817447ms 749.821679ms 749.827208ms 749.869731ms 749.911827ms 749.953372ms 749.95924ms 749.978796ms 749.991906ms 750.010341ms 750.018495ms 750.024378ms 750.035182ms 750.037387ms 750.089435ms 750.136874ms 750.160946ms 750.177762ms 750.18371ms 750.2086ms 750.236347ms 750.279388ms 750.296308ms 750.324541ms 750.401032ms 750.40478ms 750.409331ms 750.560036ms 750.675427ms 750.756695ms 750.77005ms 750.77787ms 750.833146ms 750.994057ms 751.136783ms 751.20088ms 751.215485ms 751.311696ms 751.356253ms 751.597123ms 751.687476ms 751.843485ms 751.913619ms 752.055854ms 752.286867ms 752.883923ms 753.001884ms 753.114097ms 753.644899ms 753.886202ms 753.925672ms 754.096558ms 754.276453ms 754.677756ms 754.754098ms 757.79302ms 758.818652ms 758.914229ms 759.895833ms 764.644388ms 766.553932ms 778.875414ms 797.117291ms 809.437924ms 810.129147ms 819.026279ms 853.105626ms 859.438348ms 906.514356ms]
Dec 13 20:46:21.723: INFO: 50 %ile: 748.699657ms
Dec 13 20:46:21.723: INFO: 90 %ile: 753.886202ms
Dec 13 20:46:21.723: INFO: 99 %ile: 859.438348ms
Dec 13 20:46:21.723: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:46:21.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-6w8j2" for this suite.
Dec 13 20:46:37.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:46:37.836: INFO: namespace: e2e-tests-svc-latency-6w8j2, resource: bindings, ignored listing per whitelist
Dec 13 20:46:37.851: INFO: namespace e2e-tests-svc-latency-6w8j2 deletion completed in 16.123072543s

• [SLOW TEST:26.917 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:46:37.851: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:46:37.926: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fb0ce4a-ff18-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-fbksk" to be "success or failure"
Dec 13 20:46:37.930: INFO: Pod "downwardapi-volume-2fb0ce4a-ff18-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.822452ms
Dec 13 20:46:39.934: INFO: Pod "downwardapi-volume-2fb0ce4a-ff18-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007160846s
STEP: Saw pod success
Dec 13 20:46:39.934: INFO: Pod "downwardapi-volume-2fb0ce4a-ff18-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:46:39.936: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-2fb0ce4a-ff18-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:46:39.954: INFO: Waiting for pod downwardapi-volume-2fb0ce4a-ff18-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:46:39.956: INFO: Pod downwardapi-volume-2fb0ce4a-ff18-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:46:39.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fbksk" for this suite.
Dec 13 20:46:45.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:46:45.979: INFO: namespace: e2e-tests-downward-api-fbksk, resource: bindings, ignored listing per whitelist
Dec 13 20:46:46.054: INFO: namespace e2e-tests-downward-api-fbksk deletion completed in 6.094153355s

• [SLOW TEST:8.203 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:46:46.055: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-349434ee-ff18-11e8-b666-eabb5e37e61c
STEP: Creating secret with name s-test-opt-upd-3494352a-ff18-11e8-b666-eabb5e37e61c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-349434ee-ff18-11e8-b666-eabb5e37e61c
STEP: Updating secret s-test-opt-upd-3494352a-ff18-11e8-b666-eabb5e37e61c
STEP: Creating secret with name s-test-opt-create-34943547-ff18-11e8-b666-eabb5e37e61c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:46:50.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pl78s" for this suite.
Dec 13 20:47:12.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:47:12.276: INFO: namespace: e2e-tests-secrets-pl78s, resource: bindings, ignored listing per whitelist
Dec 13 20:47:12.297: INFO: namespace e2e-tests-secrets-pl78s deletion completed in 22.092201311s

• [SLOW TEST:26.242 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:47:12.297: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:47:12.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4439098b-ff18-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-79j4f" to be "success or failure"
Dec 13 20:47:12.382: INFO: Pod "downwardapi-volume-4439098b-ff18-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.342818ms
Dec 13 20:47:14.385: INFO: Pod "downwardapi-volume-4439098b-ff18-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009921891s
STEP: Saw pod success
Dec 13 20:47:14.385: INFO: Pod "downwardapi-volume-4439098b-ff18-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:47:14.388: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-4439098b-ff18-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:47:14.404: INFO: Waiting for pod downwardapi-volume-4439098b-ff18-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:47:14.407: INFO: Pod downwardapi-volume-4439098b-ff18-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:47:14.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-79j4f" for this suite.
Dec 13 20:47:20.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:47:20.445: INFO: namespace: e2e-tests-downward-api-79j4f, resource: bindings, ignored listing per whitelist
Dec 13 20:47:20.518: INFO: namespace e2e-tests-downward-api-79j4f deletion completed in 6.108137868s

• [SLOW TEST:8.221 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:47:20.519: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 13 20:47:20.641: INFO: Waiting up to 5m0s for pod "pod-492663cc-ff18-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-649vs" to be "success or failure"
Dec 13 20:47:20.643: INFO: Pod "pod-492663cc-ff18-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.70763ms
Dec 13 20:47:22.646: INFO: Pod "pod-492663cc-ff18-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005774716s
STEP: Saw pod success
Dec 13 20:47:22.646: INFO: Pod "pod-492663cc-ff18-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:47:22.649: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-492663cc-ff18-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:47:22.666: INFO: Waiting for pod pod-492663cc-ff18-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:47:22.668: INFO: Pod pod-492663cc-ff18-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:47:22.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-649vs" for this suite.
Dec 13 20:47:28.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:47:28.743: INFO: namespace: e2e-tests-emptydir-649vs, resource: bindings, ignored listing per whitelist
Dec 13 20:47:28.775: INFO: namespace e2e-tests-emptydir-649vs deletion completed in 6.103307435s

• [SLOW TEST:8.256 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:47:28.775: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:47:28.854: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e0b6797-ff18-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-sm8jt" to be "success or failure"
Dec 13 20:47:28.857: INFO: Pod "downwardapi-volume-4e0b6797-ff18-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603231ms
Dec 13 20:47:30.865: INFO: Pod "downwardapi-volume-4e0b6797-ff18-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010966898s
STEP: Saw pod success
Dec 13 20:47:30.865: INFO: Pod "downwardapi-volume-4e0b6797-ff18-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:47:30.867: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-4e0b6797-ff18-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:47:30.884: INFO: Waiting for pod downwardapi-volume-4e0b6797-ff18-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:47:30.886: INFO: Pod downwardapi-volume-4e0b6797-ff18-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:47:30.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sm8jt" for this suite.
Dec 13 20:47:36.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:47:36.956: INFO: namespace: e2e-tests-projected-sm8jt, resource: bindings, ignored listing per whitelist
Dec 13 20:47:36.982: INFO: namespace e2e-tests-projected-sm8jt deletion completed in 6.091765422s

• [SLOW TEST:8.207 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:47:36.982: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 13 20:47:37.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-tdkzs'
Dec 13 20:47:37.119: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 20:47:37.119: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Dec 13 20:47:39.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-tdkzs'
Dec 13 20:47:39.199: INFO: stderr: ""
Dec 13 20:47:39.199: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:47:39.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tdkzs" for this suite.
Dec 13 20:47:45.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:47:45.266: INFO: namespace: e2e-tests-kubectl-tdkzs, resource: bindings, ignored listing per whitelist
Dec 13 20:47:45.295: INFO: namespace e2e-tests-kubectl-tdkzs deletion completed in 6.093067951s

• [SLOW TEST:8.314 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:47:45.296: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Dec 13 20:47:45.375: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-gkkx2" to be "success or failure"
Dec 13 20:47:45.377: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473978ms
Dec 13 20:47:47.381: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005930293s
STEP: Saw pod success
Dec 13 20:47:47.381: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 13 20:47:47.383: INFO: Trying to get logs from node ip-10-0-2-167.eu-west-1.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 13 20:47:47.402: INFO: Waiting for pod pod-host-path-test to disappear
Dec 13 20:47:47.407: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:47:47.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-gkkx2" for this suite.
Dec 13 20:47:53.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:47:53.467: INFO: namespace: e2e-tests-hostpath-gkkx2, resource: bindings, ignored listing per whitelist
Dec 13 20:47:53.507: INFO: namespace e2e-tests-hostpath-gkkx2 deletion completed in 6.095252744s

• [SLOW TEST:8.212 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:47:53.507: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 13 20:47:56.102: INFO: Successfully updated pod "pod-update-5cc899dc-ff18-11e8-b666-eabb5e37e61c"
STEP: verifying the updated pod is in kubernetes
Dec 13 20:47:56.107: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:47:56.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5vbb5" for this suite.
Dec 13 20:48:18.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:48:18.149: INFO: namespace: e2e-tests-pods-5vbb5, resource: bindings, ignored listing per whitelist
Dec 13 20:48:18.206: INFO: namespace e2e-tests-pods-5vbb5 deletion completed in 22.095376166s

• [SLOW TEST:24.698 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:48:18.206: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-6b82745a-ff18-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:48:18.291: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b82ffcc-ff18-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-28skk" to be "success or failure"
Dec 13 20:48:18.296: INFO: Pod "pod-projected-secrets-6b82ffcc-ff18-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.384878ms
Dec 13 20:48:20.300: INFO: Pod "pod-projected-secrets-6b82ffcc-ff18-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009060358s
STEP: Saw pod success
Dec 13 20:48:20.300: INFO: Pod "pod-projected-secrets-6b82ffcc-ff18-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:48:20.303: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-secrets-6b82ffcc-ff18-11e8-b666-eabb5e37e61c container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:48:20.320: INFO: Waiting for pod pod-projected-secrets-6b82ffcc-ff18-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:48:20.322: INFO: Pod pod-projected-secrets-6b82ffcc-ff18-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:48:20.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-28skk" for this suite.
Dec 13 20:48:26.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:48:26.386: INFO: namespace: e2e-tests-projected-28skk, resource: bindings, ignored listing per whitelist
Dec 13 20:48:26.431: INFO: namespace e2e-tests-projected-28skk deletion completed in 6.105581681s

• [SLOW TEST:8.225 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:48:26.431: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:48:26.498: INFO: Creating ReplicaSet my-hostname-basic-70685054-ff18-11e8-b666-eabb5e37e61c
Dec 13 20:48:26.505: INFO: Pod name my-hostname-basic-70685054-ff18-11e8-b666-eabb5e37e61c: Found 0 pods out of 1
Dec 13 20:48:31.509: INFO: Pod name my-hostname-basic-70685054-ff18-11e8-b666-eabb5e37e61c: Found 1 pods out of 1
Dec 13 20:48:31.509: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-70685054-ff18-11e8-b666-eabb5e37e61c" is running
Dec 13 20:48:31.511: INFO: Pod "my-hostname-basic-70685054-ff18-11e8-b666-eabb5e37e61c-m477k" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-13 20:48:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-13 20:48:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-13 20:48:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-13 20:48:26 +0000 UTC Reason: Message:}])
Dec 13 20:48:31.511: INFO: Trying to dial the pod
Dec 13 20:48:36.528: INFO: Controller my-hostname-basic-70685054-ff18-11e8-b666-eabb5e37e61c: Got expected result from replica 1 [my-hostname-basic-70685054-ff18-11e8-b666-eabb5e37e61c-m477k]: "my-hostname-basic-70685054-ff18-11e8-b666-eabb5e37e61c-m477k", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:48:36.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-rvwxc" for this suite.
Dec 13 20:48:42.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:48:42.575: INFO: namespace: e2e-tests-replicaset-rvwxc, resource: bindings, ignored listing per whitelist
Dec 13 20:48:42.630: INFO: namespace e2e-tests-replicaset-rvwxc deletion completed in 6.098405523s

• [SLOW TEST:16.199 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:48:42.631: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 13 20:48:42.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-bzl7k'
Dec 13 20:48:42.782: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 20:48:42.782: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Dec 13 20:48:46.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-bzl7k'
Dec 13 20:48:46.870: INFO: stderr: ""
Dec 13 20:48:46.870: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:48:46.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bzl7k" for this suite.
Dec 13 20:48:52.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:48:52.967: INFO: namespace: e2e-tests-kubectl-bzl7k, resource: bindings, ignored listing per whitelist
Dec 13 20:48:52.978: INFO: namespace e2e-tests-kubectl-bzl7k deletion completed in 6.099685957s

• [SLOW TEST:10.347 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:48:52.979: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 13 20:48:53.056: INFO: Waiting up to 5m0s for pod "pod-803bbe57-ff18-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-bq8bn" to be "success or failure"
Dec 13 20:48:53.061: INFO: Pod "pod-803bbe57-ff18-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.509155ms
Dec 13 20:48:55.064: INFO: Pod "pod-803bbe57-ff18-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008229285s
STEP: Saw pod success
Dec 13 20:48:55.064: INFO: Pod "pod-803bbe57-ff18-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:48:55.067: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-803bbe57-ff18-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:48:55.084: INFO: Waiting for pod pod-803bbe57-ff18-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:48:55.086: INFO: Pod pod-803bbe57-ff18-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:48:55.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bq8bn" for this suite.
Dec 13 20:49:01.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:49:01.180: INFO: namespace: e2e-tests-emptydir-bq8bn, resource: bindings, ignored listing per whitelist
Dec 13 20:49:01.184: INFO: namespace e2e-tests-emptydir-bq8bn deletion completed in 6.095165057s

• [SLOW TEST:8.206 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:49:01.185: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 13 20:49:05.292: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:49:05.295: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:49:07.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:49:07.303: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:49:09.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:49:09.299: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:49:11.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:49:11.299: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:49:13.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:49:13.299: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:49:15.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:49:15.298: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:49:17.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:49:17.298: INFO: Pod pod-with-poststart-http-hook still exists
Dec 13 20:49:19.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 13 20:49:19.302: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:49:19.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-bxztj" for this suite.
Dec 13 20:49:41.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:49:41.347: INFO: namespace: e2e-tests-container-lifecycle-hook-bxztj, resource: bindings, ignored listing per whitelist
Dec 13 20:49:41.406: INFO: namespace e2e-tests-container-lifecycle-hook-bxztj deletion completed in 22.099941302s

• [SLOW TEST:40.221 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:49:41.406: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-6qxwd
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6qxwd to expose endpoints map[]
Dec 13 20:49:41.493: INFO: Get endpoints failed (3.1502ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 13 20:49:42.496: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6qxwd exposes endpoints map[] (1.006590636s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-6qxwd
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6qxwd to expose endpoints map[pod1:[80]]
Dec 13 20:49:44.522: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6qxwd exposes endpoints map[pod1:[80]] (2.019203305s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-6qxwd
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6qxwd to expose endpoints map[pod1:[80] pod2:[80]]
Dec 13 20:49:46.554: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6qxwd exposes endpoints map[pod1:[80] pod2:[80]] (2.027523474s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-6qxwd
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6qxwd to expose endpoints map[pod2:[80]]
Dec 13 20:49:47.576: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6qxwd exposes endpoints map[pod2:[80]] (1.014581523s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-6qxwd
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-6qxwd to expose endpoints map[]
Dec 13 20:49:48.588: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-6qxwd exposes endpoints map[] (1.006596206s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:49:48.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-6qxwd" for this suite.
Dec 13 20:49:54.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:49:54.662: INFO: namespace: e2e-tests-services-6qxwd, resource: bindings, ignored listing per whitelist
Dec 13 20:49:54.713: INFO: namespace e2e-tests-services-6qxwd deletion completed in 6.097668546s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:13.307 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:49:54.713: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:49:54.804: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 13 20:49:54.811: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:54.812: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:54.812: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:54.814: INFO: Number of nodes with available pods: 0
Dec 13 20:49:54.814: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:49:55.818: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:55.818: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:55.818: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:55.821: INFO: Number of nodes with available pods: 0
Dec 13 20:49:55.821: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:49:56.818: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:56.818: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:56.818: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:56.821: INFO: Number of nodes with available pods: 2
Dec 13 20:49:56.821: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 13 20:49:56.839: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:49:56.839: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:49:56.843: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:56.843: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:56.843: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:57.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:49:57.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:49:57.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:57.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:57.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:58.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:49:58.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:49:58.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:58.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:58.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:59.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:49:59.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:49:59.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:59.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:49:59.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:00.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:00.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:00.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:00.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:00.851: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:01.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:01.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:01.851: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:01.851: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:01.851: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:02.857: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:02.857: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:02.861: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:02.861: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:02.861: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:03.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:03.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:03.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:03.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:03.851: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:04.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:04.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:04.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:04.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:04.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:05.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:05.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:05.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:05.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:05.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:06.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:06.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:06.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:06.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:06.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:07.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:07.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:07.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:07.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:07.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:08.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:08.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:08.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:08.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:08.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:09.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:09.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:09.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:09.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:09.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:10.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:10.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:10.849: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:10.849: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:10.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:11.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:11.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:11.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:11.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:11.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:12.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:12.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:12.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:12.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:12.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:13.850: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:13.850: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:13.854: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:13.854: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:13.854: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:14.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:14.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:14.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:14.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:14.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:15.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:15.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:15.851: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:15.865: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:15.865: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:16.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:16.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:16.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:16.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:16.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:17.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:17.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:17.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:17.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:17.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:18.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:18.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:18.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:18.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:18.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:19.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:19.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:19.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:19.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:19.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:20.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:20.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:20.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:20.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:20.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:21.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:21.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:21.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:21.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:21.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:22.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:22.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:22.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:22.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:22.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:23.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:23.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:23.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:23.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:23.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:24.851: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:24.851: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:24.854: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:24.854: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:24.854: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:25.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:25.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:25.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:25.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:25.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:26.846: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:26.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:26.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:26.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:26.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:27.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:27.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:27.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:27.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:27.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:28.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:28.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:28.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:28.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:28.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:29.847: INFO: Wrong image for pod: daemon-set-f95qz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:29.847: INFO: Pod daemon-set-f95qz is not available
Dec 13 20:50:29.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:29.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:29.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:29.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:30.846: INFO: Pod daemon-set-hw2p5 is not available
Dec 13 20:50:30.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:30.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:30.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:30.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:31.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:31.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:31.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:31.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:32.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:32.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:32.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:32.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:33.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:33.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:33.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:33.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:34.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:34.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:34.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:34.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:35.850: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:35.854: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:35.854: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:35.854: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:36.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:36.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:36.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:36.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:37.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:37.849: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:37.849: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:37.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:38.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:38.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:38.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:38.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:39.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:39.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:39.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:39.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:40.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:40.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:40.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:40.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:41.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:41.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:41.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:41.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:42.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:42.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:42.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:42.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:43.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:43.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:43.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:43.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:44.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:44.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:44.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:44.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:45.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:45.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:45.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:45.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:46.850: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:46.854: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:46.854: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:46.854: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:47.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:47.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:47.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:47.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:48.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:48.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:48.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:48.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:49.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:49.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:49.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:49.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:50.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:50.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:50.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:50.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:51.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:51.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:51.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:51.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:52.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:52.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:52.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:52.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:53.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:53.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:53.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:53.851: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:54.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:54.849: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:54.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:54.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:55.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:55.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:55.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:55.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:56.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:56.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:56.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:56.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:57.851: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:57.854: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:57.854: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:57.854: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:58.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:58.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:58.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:58.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:59.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:50:59.849: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:59.849: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:50:59.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:00.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:51:00.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:00.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:00.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:01.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:51:01.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:01.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:01.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:02.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:51:02.847: INFO: Pod daemon-set-tvklf is not available
Dec 13 20:51:02.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:02.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:02.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:03.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:51:03.847: INFO: Pod daemon-set-tvklf is not available
Dec 13 20:51:03.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:03.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:03.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:04.848: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:51:04.848: INFO: Pod daemon-set-tvklf is not available
Dec 13 20:51:04.851: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:04.851: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:04.851: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:05.846: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:51:05.846: INFO: Pod daemon-set-tvklf is not available
Dec 13 20:51:05.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:05.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:05.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:06.847: INFO: Wrong image for pod: daemon-set-tvklf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Dec 13 20:51:06.847: INFO: Pod daemon-set-tvklf is not available
Dec 13 20:51:06.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:06.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:06.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:07.847: INFO: Pod daemon-set-hbtrt is not available
Dec 13 20:51:07.850: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:07.850: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:07.850: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 13 20:51:07.858: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:07.858: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:07.858: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:07.861: INFO: Number of nodes with available pods: 1
Dec 13 20:51:07.861: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:51:08.865: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:08.865: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:08.865: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:08.868: INFO: Number of nodes with available pods: 1
Dec 13 20:51:08.868: INFO: Node ip-10-0-2-167.eu-west-1.compute.internal is running more than one daemon pod
Dec 13 20:51:09.865: INFO: DaemonSet pods can't tolerate node ip-10-0-1-195.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:09.865: INFO: DaemonSet pods can't tolerate node ip-10-0-2-203.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:09.865: INFO: DaemonSet pods can't tolerate node ip-10-0-3-47.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 13 20:51:09.868: INFO: Number of nodes with available pods: 2
Dec 13 20:51:09.868: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-m5tnb, will wait for the garbage collector to delete the pods
Dec 13 20:51:09.941: INFO: Deleting DaemonSet.extensions daemon-set took: 6.940235ms
Dec 13 20:51:10.041: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.144959ms
Dec 13 20:51:17.645: INFO: Number of nodes with available pods: 0
Dec 13 20:51:17.645: INFO: Number of running nodes: 0, number of available pods: 0
Dec 13 20:51:17.649: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-m5tnb/daemonsets","resourceVersion":"60460"},"items":null}

Dec 13 20:51:17.651: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-m5tnb/pods","resourceVersion":"60460"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:51:17.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-m5tnb" for this suite.
Dec 13 20:51:23.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:51:23.750: INFO: namespace: e2e-tests-daemonsets-m5tnb, resource: bindings, ignored listing per whitelist
Dec 13 20:51:23.775: INFO: namespace e2e-tests-daemonsets-m5tnb deletion completed in 6.101880225s

• [SLOW TEST:89.062 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:51:23.775: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 13 20:51:28.881: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:51:29.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-s849n" for this suite.
Dec 13 20:51:51.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:51:51.976: INFO: namespace: e2e-tests-replicaset-s849n, resource: bindings, ignored listing per whitelist
Dec 13 20:51:52.008: INFO: namespace e2e-tests-replicaset-s849n deletion completed in 22.105731679s

• [SLOW TEST:28.233 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:51:52.009: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-ndf4g
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 13 20:51:52.081: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 13 20:52:08.148: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.4.95:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-ndf4g PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:52:08.148: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:52:08.269: INFO: Found all expected endpoints: [netserver-0]
Dec 13 20:52:08.272: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.3.238:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-ndf4g PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:52:08.272: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:52:08.409: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:52:08.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-ndf4g" for this suite.
Dec 13 20:52:30.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:52:30.443: INFO: namespace: e2e-tests-pod-network-test-ndf4g, resource: bindings, ignored listing per whitelist
Dec 13 20:52:30.512: INFO: namespace e2e-tests-pod-network-test-ndf4g deletion completed in 22.097881369s

• [SLOW TEST:38.502 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:52:30.512: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:52:30.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 version --client'
Dec 13 20:52:30.632: INFO: stderr: ""
Dec 13 20:52:30.632: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Dec 13 20:52:30.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-rcv9r'
Dec 13 20:52:30.921: INFO: stderr: ""
Dec 13 20:52:30.921: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 13 20:52:30.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-rcv9r'
Dec 13 20:52:31.065: INFO: stderr: ""
Dec 13 20:52:31.065: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 13 20:52:32.069: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:52:32.069: INFO: Found 0 / 1
Dec 13 20:52:33.069: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:52:33.069: INFO: Found 1 / 1
Dec 13 20:52:33.069: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 13 20:52:33.072: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:52:33.072: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 13 20:52:33.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 describe pod redis-master-qmzls --namespace=e2e-tests-kubectl-rcv9r'
Dec 13 20:52:33.148: INFO: stderr: ""
Dec 13 20:52:33.148: INFO: stdout: "Name:               redis-master-qmzls\nNamespace:          e2e-tests-kubectl-rcv9r\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-10-0-3-214.eu-west-1.compute.internal/10.0.3.214\nStart Time:         Thu, 13 Dec 2018 20:52:30 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 192.168.3.239/32\nStatus:             Running\nIP:                 192.168.3.239\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://50b13bf8a3bcd545ae2e9ddc6432a571db4c7b7a9279a239c9c2dfd8fb67d843\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 13 Dec 2018 20:52:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-b94lq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-b94lq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-b94lq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                               Message\n  ----    ------     ----  ----                                               -------\n  Normal  Scheduled  3s    default-scheduler                                  Successfully assigned e2e-tests-kubectl-rcv9r/redis-master-qmzls to ip-10-0-3-214.eu-west-1.compute.internal\n  Normal  Pulled     2s    kubelet, ip-10-0-3-214.eu-west-1.compute.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, ip-10-0-3-214.eu-west-1.compute.internal  Created container\n  Normal  Started    2s    kubelet, ip-10-0-3-214.eu-west-1.compute.internal  Started container\n"
Dec 13 20:52:33.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 describe rc redis-master --namespace=e2e-tests-kubectl-rcv9r'
Dec 13 20:52:33.242: INFO: stderr: ""
Dec 13 20:52:33.242: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-rcv9r\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-qmzls\n"
Dec 13 20:52:33.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 describe service redis-master --namespace=e2e-tests-kubectl-rcv9r'
Dec 13 20:52:33.317: INFO: stderr: ""
Dec 13 20:52:33.317: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-rcv9r\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.110.184.89\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.3.239:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 13 20:52:33.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 describe node ip-10-0-1-195.eu-west-1.compute.internal'
Dec 13 20:52:33.409: INFO: stderr: ""
Dec 13 20:52:33.409: INFO: stdout: "Name:               ip-10-0-1-195.eu-west-1.compute.internal\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-1\n                    failure-domain.beta.kubernetes.io/zone=eu-west-1a\n                    kubernetes.io/hostname=ip-10-0-1-195.eu-west-1.compute.internal\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.1.195/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 13 Dec 2018 14:49:34 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 13 Dec 2018 20:52:24 +0000   Thu, 13 Dec 2018 14:49:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 13 Dec 2018 20:52:24 +0000   Thu, 13 Dec 2018 14:49:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 13 Dec 2018 20:52:24 +0000   Thu, 13 Dec 2018 14:49:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 13 Dec 2018 20:52:24 +0000   Thu, 13 Dec 2018 14:52:35 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.1.195\n  InternalDNS:  ip-10-0-1-195.eu-west-1.compute.internal\n  Hostname:     ip-10-0-1-195.eu-west-1.compute.internal\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         2\n ephemeral-storage:           48375392Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      8066796Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         2\n ephemeral-storage:           44582761194\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      7964396Ki\n pods:                        110\nSystem Info:\n Machine ID:                 ec27b0c9a508aeba297049f2c2cc71cd\n System UUID:                EC27B0C9-A508-AEBA-2970-49F2C2CC71CD\n Boot ID:                    cfde8c5f-b5ce-4252-8ec2-fa29e0a23d57\n Kernel Version:             4.14.81-coreos\n OS Image:                   Container Linux by CoreOS 1911.4.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.0\n Kubelet Version:            v1.13.1\n Kube-Proxy Version:         v1.13.1\nPodCIDR:                     192.168.0.0/24\nProviderID:                  aws:///eu-west-1a/i-09334fa767039e3d7\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                                ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-e5a404e9fcc945c3-sf6q6             0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\n  kube-system                calico-node-lsz7g                                                   250m (12%)    0 (0%)      0 (0%)           0 (0%)         5h29m\n  kube-system                etcd-ip-10-0-1-195.eu-west-1.compute.internal                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h2m\n  kube-system                kube-apiserver-ip-10-0-1-195.eu-west-1.compute.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         6h2m\n  kube-system                kube-controller-manager-ip-10-0-1-195.eu-west-1.compute.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         6h1m\n  kube-system                kube-proxy-tt9xv                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h2m\n  kube-system                kube-scheduler-ip-10-0-1-195.eu-west-1.compute.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         6h1m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         800m (40%)  0 (0%)\n  memory                      0 (0%)      0 (0%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Dec 13 20:52:33.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 describe namespace e2e-tests-kubectl-rcv9r'
Dec 13 20:52:33.484: INFO: stderr: ""
Dec 13 20:52:33.484: INFO: stdout: "Name:         e2e-tests-kubectl-rcv9r\nLabels:       e2e-framework=kubectl\n              e2e-run=6ef568b8-ff10-11e8-b666-eabb5e37e61c\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:52:33.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rcv9r" for this suite.
Dec 13 20:52:55.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:52:55.527: INFO: namespace: e2e-tests-kubectl-rcv9r, resource: bindings, ignored listing per whitelist
Dec 13 20:52:55.585: INFO: namespace e2e-tests-kubectl-rcv9r deletion completed in 22.096862052s

• [SLOW TEST:25.073 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:52:55.585: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 13 20:52:59.746: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:52:59.746: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:52:59.884: INFO: Exec stderr: ""
Dec 13 20:52:59.884: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:52:59.884: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:53:00.021: INFO: Exec stderr: ""
Dec 13 20:53:00.021: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:53:00.021: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:53:00.149: INFO: Exec stderr: ""
Dec 13 20:53:00.149: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:53:00.149: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:53:00.293: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 13 20:53:00.293: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:53:00.293: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:53:00.418: INFO: Exec stderr: ""
Dec 13 20:53:00.418: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:53:00.418: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:53:00.549: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 13 20:53:00.549: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:53:00.549: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:53:00.679: INFO: Exec stderr: ""
Dec 13 20:53:00.679: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:53:00.679: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:53:00.802: INFO: Exec stderr: ""
Dec 13 20:53:00.802: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:53:00.803: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:53:00.921: INFO: Exec stderr: ""
Dec 13 20:53:00.921: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-6cp4q PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 20:53:00.921: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 20:53:01.039: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:53:01.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-6cp4q" for this suite.
Dec 13 20:53:39.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:53:39.132: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-6cp4q, resource: bindings, ignored listing per whitelist
Dec 13 20:53:39.144: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-6cp4q deletion completed in 38.100835424s

• [SLOW TEST:43.559 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:53:39.144: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:53:39.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2acd1390-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-rr5tm" to be "success or failure"
Dec 13 20:53:39.224: INFO: Pod "downwardapi-volume-2acd1390-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.175154ms
Dec 13 20:53:41.227: INFO: Pod "downwardapi-volume-2acd1390-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006787143s
STEP: Saw pod success
Dec 13 20:53:41.227: INFO: Pod "downwardapi-volume-2acd1390-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:53:41.230: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-2acd1390-ff19-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:53:41.249: INFO: Waiting for pod downwardapi-volume-2acd1390-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:53:41.251: INFO: Pod downwardapi-volume-2acd1390-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:53:41.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rr5tm" for this suite.
Dec 13 20:53:47.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:53:47.298: INFO: namespace: e2e-tests-downward-api-rr5tm, resource: bindings, ignored listing per whitelist
Dec 13 20:53:47.352: INFO: namespace e2e-tests-downward-api-rr5tm deletion completed in 6.097050788s

• [SLOW TEST:8.208 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:53:47.352: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-n87vj/configmap-test-2fb0a47b-ff19-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:53:47.427: INFO: Waiting up to 5m0s for pod "pod-configmaps-2fb12bf7-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-configmap-n87vj" to be "success or failure"
Dec 13 20:53:47.430: INFO: Pod "pod-configmaps-2fb12bf7-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.528864ms
Dec 13 20:53:49.434: INFO: Pod "pod-configmaps-2fb12bf7-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007148201s
STEP: Saw pod success
Dec 13 20:53:49.434: INFO: Pod "pod-configmaps-2fb12bf7-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:53:49.437: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-2fb12bf7-ff19-11e8-b666-eabb5e37e61c container env-test: <nil>
STEP: delete the pod
Dec 13 20:53:49.452: INFO: Waiting for pod pod-configmaps-2fb12bf7-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:53:49.455: INFO: Pod pod-configmaps-2fb12bf7-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:53:49.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-n87vj" for this suite.
Dec 13 20:53:55.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:53:55.524: INFO: namespace: e2e-tests-configmap-n87vj, resource: bindings, ignored listing per whitelist
Dec 13 20:53:55.552: INFO: namespace e2e-tests-configmap-n87vj deletion completed in 6.094436095s

• [SLOW TEST:8.200 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:53:55.553: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:53:55.631: INFO: (0) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.103743ms)
Dec 13 20:53:55.634: INFO: (1) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.519087ms)
Dec 13 20:53:55.638: INFO: (2) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.511465ms)
Dec 13 20:53:55.641: INFO: (3) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.435387ms)
Dec 13 20:53:55.645: INFO: (4) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.291594ms)
Dec 13 20:53:55.648: INFO: (5) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.339566ms)
Dec 13 20:53:55.651: INFO: (6) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.352077ms)
Dec 13 20:53:55.655: INFO: (7) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.171898ms)
Dec 13 20:53:55.658: INFO: (8) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.33593ms)
Dec 13 20:53:55.661: INFO: (9) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.251116ms)
Dec 13 20:53:55.665: INFO: (10) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.322435ms)
Dec 13 20:53:55.668: INFO: (11) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.364569ms)
Dec 13 20:53:55.671: INFO: (12) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.424528ms)
Dec 13 20:53:55.675: INFO: (13) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.185161ms)
Dec 13 20:53:55.678: INFO: (14) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.261583ms)
Dec 13 20:53:55.681: INFO: (15) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.361355ms)
Dec 13 20:53:55.685: INFO: (16) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.2344ms)
Dec 13 20:53:55.688: INFO: (17) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.419179ms)
Dec 13 20:53:55.691: INFO: (18) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.190504ms)
Dec 13 20:53:55.695: INFO: (19) /api/v1/nodes/ip-10-0-2-167.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.441212ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:53:55.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-jzqhf" for this suite.
Dec 13 20:54:01.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:54:01.762: INFO: namespace: e2e-tests-proxy-jzqhf, resource: bindings, ignored listing per whitelist
Dec 13 20:54:01.792: INFO: namespace e2e-tests-proxy-jzqhf deletion completed in 6.093742642s

• [SLOW TEST:6.239 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:54:01.792: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec 13 20:54:04.413: INFO: Successfully updated pod "annotationupdate384d96d5-ff19-11e8-b666-eabb5e37e61c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:54:06.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-k2zjz" for this suite.
Dec 13 20:54:28.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:54:28.508: INFO: namespace: e2e-tests-projected-k2zjz, resource: bindings, ignored listing per whitelist
Dec 13 20:54:28.525: INFO: namespace e2e-tests-projected-k2zjz deletion completed in 22.096151224s

• [SLOW TEST:26.733 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:54:28.525: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:54:28.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-6nvmd" for this suite.
Dec 13 20:54:34.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:54:34.654: INFO: namespace: e2e-tests-services-6nvmd, resource: bindings, ignored listing per whitelist
Dec 13 20:54:34.706: INFO: namespace e2e-tests-services-6nvmd deletion completed in 6.09884734s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

• [SLOW TEST:6.182 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:54:34.707: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-4beae072-ff19-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:54:34.786: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4beb7658-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-pz4zm" to be "success or failure"
Dec 13 20:54:34.789: INFO: Pod "pod-projected-secrets-4beb7658-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.022654ms
Dec 13 20:54:36.796: INFO: Pod "pod-projected-secrets-4beb7658-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010505048s
STEP: Saw pod success
Dec 13 20:54:36.796: INFO: Pod "pod-projected-secrets-4beb7658-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:54:36.799: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-secrets-4beb7658-ff19-11e8-b666-eabb5e37e61c container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:54:36.815: INFO: Waiting for pod pod-projected-secrets-4beb7658-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:54:36.818: INFO: Pod pod-projected-secrets-4beb7658-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:54:36.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pz4zm" for this suite.
Dec 13 20:54:42.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:54:42.872: INFO: namespace: e2e-tests-projected-pz4zm, resource: bindings, ignored listing per whitelist
Dec 13 20:54:42.919: INFO: namespace e2e-tests-projected-pz4zm deletion completed in 6.097424327s

• [SLOW TEST:8.212 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:54:42.919: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 13 20:54:42.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-9x9mq'
Dec 13 20:54:43.059: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 20:54:43.059: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec 13 20:54:45.069: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-tdhht]
Dec 13 20:54:45.069: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-tdhht" in namespace "e2e-tests-kubectl-9x9mq" to be "running and ready"
Dec 13 20:54:45.071: INFO: Pod "e2e-test-nginx-rc-tdhht": Phase="Running", Reason="", readiness=true. Elapsed: 2.614478ms
Dec 13 20:54:45.071: INFO: Pod "e2e-test-nginx-rc-tdhht" satisfied condition "running and ready"
Dec 13 20:54:45.071: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-tdhht]
Dec 13 20:54:45.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9x9mq'
Dec 13 20:54:45.151: INFO: stderr: ""
Dec 13 20:54:45.151: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Dec 13 20:54:45.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-9x9mq'
Dec 13 20:54:45.229: INFO: stderr: ""
Dec 13 20:54:45.229: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:54:45.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9x9mq" for this suite.
Dec 13 20:54:51.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:54:51.286: INFO: namespace: e2e-tests-kubectl-9x9mq, resource: bindings, ignored listing per whitelist
Dec 13 20:54:51.332: INFO: namespace e2e-tests-kubectl-9x9mq deletion completed in 6.09630997s

• [SLOW TEST:8.413 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:54:51.332: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:54:51.405: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 13 20:54:56.408: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 13 20:54:56.409: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec 13 20:54:56.425: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-ttkr4,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-ttkr4/deployments/test-cleanup-deployment,UID:58d0cf06-ff19-11e8-b982-02b9355b966e,ResourceVersion:61293,Generation:1,CreationTimestamp:2018-12-13 20:54:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Dec 13 20:54:56.428: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:54:56.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-ttkr4" for this suite.
Dec 13 20:55:02.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:02.492: INFO: namespace: e2e-tests-deployment-ttkr4, resource: bindings, ignored listing per whitelist
Dec 13 20:55:02.537: INFO: namespace e2e-tests-deployment-ttkr4 deletion completed in 6.09978196s

• [SLOW TEST:11.205 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:55:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 20:55:02.607: INFO: Creating deployment "test-recreate-deployment"
Dec 13 20:55:02.612: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 13 20:55:02.618: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 13 20:55:04.625: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 13 20:55:04.627: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 13 20:55:04.635: INFO: Updating deployment test-recreate-deployment
Dec 13 20:55:04.635: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec 13 20:55:04.706: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-rxgcd,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rxgcd/deployments/test-recreate-deployment,UID:5c8213e3-ff19-11e8-b982-02b9355b966e,ResourceVersion:61442,Generation:2,CreationTimestamp:2018-12-13 20:55:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2018-12-13 20:55:04 +0000 UTC 2018-12-13 20:55:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2018-12-13 20:55:04 +0000 UTC 2018-12-13 20:55:02 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec 13 20:55:04.709: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-rxgcd,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rxgcd/replicasets/test-recreate-deployment-697fbf54bf,UID:5dbac56c-ff19-11e8-b31c-0a2b404fde34,ResourceVersion:61440,Generation:1,CreationTimestamp:2018-12-13 20:55:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5c8213e3-ff19-11e8-b982-02b9355b966e 0xc00213e9e7 0xc00213e9e8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 13 20:55:04.709: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 13 20:55:04.709: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-rxgcd,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rxgcd/replicasets/test-recreate-deployment-5dfdcc846d,UID:5c80c712-ff19-11e8-b31c-0a2b404fde34,ResourceVersion:61431,Generation:2,CreationTimestamp:2018-12-13 20:55:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5c8213e3-ff19-11e8-b982-02b9355b966e 0xc00213e927 0xc00213e928}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 13 20:55:04.712: INFO: Pod "test-recreate-deployment-697fbf54bf-jnsqb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-jnsqb,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-rxgcd,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rxgcd/pods/test-recreate-deployment-697fbf54bf-jnsqb,UID:5dbb689b-ff19-11e8-b31c-0a2b404fde34,ResourceVersion:61443,Generation:0,CreationTimestamp:2018-12-13 20:55:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 5dbac56c-ff19-11e8-b31c-0a2b404fde34 0xc001ea5c27 0xc001ea5c28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-7h8cr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7h8cr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7h8cr true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-214.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea5cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:55:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:55:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:55:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 20:55:04 +0000 UTC  }],Message:,Reason:,HostIP:10.0.3.214,PodIP:,StartTime:2018-12-13 20:55:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:55:04.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-rxgcd" for this suite.
Dec 13 20:55:10.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:10.785: INFO: namespace: e2e-tests-deployment-rxgcd, resource: bindings, ignored listing per whitelist
Dec 13 20:55:10.814: INFO: namespace e2e-tests-deployment-rxgcd deletion completed in 6.098884673s

• [SLOW TEST:8.277 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:55:10.814: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 13 20:55:10.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-cbgm9'
Dec 13 20:55:10.951: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 20:55:10.951: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Dec 13 20:55:10.956: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Dec 13 20:55:10.967: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec 13 20:55:10.974: INFO: scanned /root for discovery docs: <nil>
Dec 13 20:55:10.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-cbgm9'
Dec 13 20:55:26.742: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 13 20:55:26.742: INFO: stdout: "Created e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341\nScaling up e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec 13 20:55:26.742: INFO: stdout: "Created e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341\nScaling up e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec 13 20:55:26.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-cbgm9'
Dec 13 20:55:26.822: INFO: stderr: ""
Dec 13 20:55:26.822: INFO: stdout: "e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341-nq2q4 e2e-test-nginx-rc-nq25s "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Dec 13 20:55:31.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-cbgm9'
Dec 13 20:55:31.889: INFO: stderr: ""
Dec 13 20:55:31.889: INFO: stdout: "e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341-nq2q4 "
Dec 13 20:55:31.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341-nq2q4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cbgm9'
Dec 13 20:55:31.950: INFO: stderr: ""
Dec 13 20:55:31.950: INFO: stdout: "true"
Dec 13 20:55:31.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341-nq2q4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cbgm9'
Dec 13 20:55:32.012: INFO: stderr: ""
Dec 13 20:55:32.012: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec 13 20:55:32.012: INFO: e2e-test-nginx-rc-5138a819328e739b0d42e116210e5341-nq2q4 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Dec 13 20:55:32.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-cbgm9'
Dec 13 20:55:32.079: INFO: stderr: ""
Dec 13 20:55:32.079: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:55:32.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cbgm9" for this suite.
Dec 13 20:55:38.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:38.111: INFO: namespace: e2e-tests-kubectl-cbgm9, resource: bindings, ignored listing per whitelist
Dec 13 20:55:38.177: INFO: namespace e2e-tests-kubectl-cbgm9 deletion completed in 6.094095213s

• [SLOW TEST:27.363 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:55:38.178: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:55:40.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-65lpr" for this suite.
Dec 13 20:55:46.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:46.333: INFO: namespace: e2e-tests-emptydir-wrapper-65lpr, resource: bindings, ignored listing per whitelist
Dec 13 20:55:46.412: INFO: namespace e2e-tests-emptydir-wrapper-65lpr deletion completed in 6.104734059s

• [SLOW TEST:8.235 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:55:46.412: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 13 20:55:46.565: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-8w2vw,SelfLink:/api/v1/namespaces/e2e-tests-watch-8w2vw/configmaps/e2e-watch-test-resource-version,UID:76b1d110-ff19-11e8-b982-02b9355b966e,ResourceVersion:61671,Generation:0,CreationTimestamp:2018-12-13 20:55:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 13 20:55:46.565: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-8w2vw,SelfLink:/api/v1/namespaces/e2e-tests-watch-8w2vw/configmaps/e2e-watch-test-resource-version,UID:76b1d110-ff19-11e8-b982-02b9355b966e,ResourceVersion:61672,Generation:0,CreationTimestamp:2018-12-13 20:55:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:55:46.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-8w2vw" for this suite.
Dec 13 20:55:52.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:55:52.602: INFO: namespace: e2e-tests-watch-8w2vw, resource: bindings, ignored listing per whitelist
Dec 13 20:55:52.666: INFO: namespace e2e-tests-watch-8w2vw deletion completed in 6.096533661s

• [SLOW TEST:6.253 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:55:52.666: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:55:52.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a6335fc-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-z64k4" to be "success or failure"
Dec 13 20:55:52.752: INFO: Pod "downwardapi-volume-7a6335fc-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055448ms
Dec 13 20:55:54.755: INFO: Pod "downwardapi-volume-7a6335fc-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009166693s
STEP: Saw pod success
Dec 13 20:55:54.755: INFO: Pod "downwardapi-volume-7a6335fc-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:55:54.758: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-7a6335fc-ff19-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:55:54.775: INFO: Waiting for pod downwardapi-volume-7a6335fc-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:55:54.778: INFO: Pod downwardapi-volume-7a6335fc-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:55:54.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-z64k4" for this suite.
Dec 13 20:56:00.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:00.859: INFO: namespace: e2e-tests-projected-z64k4, resource: bindings, ignored listing per whitelist
Dec 13 20:56:00.877: INFO: namespace e2e-tests-projected-z64k4 deletion completed in 6.09504691s

• [SLOW TEST:8.211 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:56:00.877: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Dec 13 20:56:00.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 create -f - --namespace=e2e-tests-kubectl-h87rs'
Dec 13 20:56:01.090: INFO: stderr: ""
Dec 13 20:56:01.090: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Dec 13 20:56:02.094: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:56:02.094: INFO: Found 0 / 1
Dec 13 20:56:03.098: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:56:03.098: INFO: Found 1 / 1
Dec 13 20:56:03.098: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 13 20:56:03.101: INFO: Selector matched 1 pods for map[app:redis]
Dec 13 20:56:03.101: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec 13 20:56:03.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 logs redis-master-bsfgp redis-master --namespace=e2e-tests-kubectl-h87rs'
Dec 13 20:56:03.170: INFO: stderr: ""
Dec 13 20:56:03.170: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Dec 20:56:01.940 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Dec 20:56:01.940 # Server started, Redis version 3.2.12\n1:M 13 Dec 20:56:01.940 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Dec 20:56:01.940 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec 13 20:56:03.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 log redis-master-bsfgp redis-master --namespace=e2e-tests-kubectl-h87rs --tail=1'
Dec 13 20:56:03.242: INFO: stderr: ""
Dec 13 20:56:03.242: INFO: stdout: "1:M 13 Dec 20:56:01.940 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec 13 20:56:03.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 log redis-master-bsfgp redis-master --namespace=e2e-tests-kubectl-h87rs --limit-bytes=1'
Dec 13 20:56:03.317: INFO: stderr: ""
Dec 13 20:56:03.318: INFO: stdout: " "
STEP: exposing timestamps
Dec 13 20:56:03.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 log redis-master-bsfgp redis-master --namespace=e2e-tests-kubectl-h87rs --tail=1 --timestamps'
Dec 13 20:56:03.394: INFO: stderr: ""
Dec 13 20:56:03.394: INFO: stdout: "2018-12-13T20:56:01.941127738Z 1:M 13 Dec 20:56:01.940 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec 13 20:56:05.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 log redis-master-bsfgp redis-master --namespace=e2e-tests-kubectl-h87rs --since=1s'
Dec 13 20:56:05.969: INFO: stderr: ""
Dec 13 20:56:05.969: INFO: stdout: ""
Dec 13 20:56:05.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 log redis-master-bsfgp redis-master --namespace=e2e-tests-kubectl-h87rs --since=24h'
Dec 13 20:56:06.047: INFO: stderr: ""
Dec 13 20:56:06.047: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Dec 20:56:01.940 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Dec 20:56:01.940 # Server started, Redis version 3.2.12\n1:M 13 Dec 20:56:01.940 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Dec 20:56:01.940 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Dec 13 20:56:06.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-h87rs'
Dec 13 20:56:06.112: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 13 20:56:06.112: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec 13 20:56:06.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-h87rs'
Dec 13 20:56:06.180: INFO: stderr: "No resources found.\n"
Dec 13 20:56:06.180: INFO: stdout: ""
Dec 13 20:56:06.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 get pods -l name=nginx --namespace=e2e-tests-kubectl-h87rs -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 13 20:56:06.240: INFO: stderr: ""
Dec 13 20:56:06.240: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:56:06.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-h87rs" for this suite.
Dec 13 20:56:28.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:28.317: INFO: namespace: e2e-tests-kubectl-h87rs, resource: bindings, ignored listing per whitelist
Dec 13 20:56:28.340: INFO: namespace e2e-tests-kubectl-h87rs deletion completed in 22.096203217s

• [SLOW TEST:27.463 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:56:28.340: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:56:28.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-fhhfp" for this suite.
Dec 13 20:56:34.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:34.479: INFO: namespace: e2e-tests-kubelet-test-fhhfp, resource: bindings, ignored listing per whitelist
Dec 13 20:56:34.537: INFO: namespace e2e-tests-kubelet-test-fhhfp deletion completed in 6.096283149s

• [SLOW TEST:6.197 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:56:34.537: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec 13 20:56:34.609: INFO: Waiting up to 5m0s for pod "downward-api-93572dc6-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-nscjm" to be "success or failure"
Dec 13 20:56:34.612: INFO: Pod "downward-api-93572dc6-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.800286ms
Dec 13 20:56:36.620: INFO: Pod "downward-api-93572dc6-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010086312s
STEP: Saw pod success
Dec 13 20:56:36.620: INFO: Pod "downward-api-93572dc6-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:56:36.624: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downward-api-93572dc6-ff19-11e8-b666-eabb5e37e61c container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:56:36.643: INFO: Waiting for pod downward-api-93572dc6-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:56:36.648: INFO: Pod downward-api-93572dc6-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:56:36.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nscjm" for this suite.
Dec 13 20:56:42.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:42.723: INFO: namespace: e2e-tests-downward-api-nscjm, resource: bindings, ignored listing per whitelist
Dec 13 20:56:42.764: INFO: namespace e2e-tests-downward-api-nscjm deletion completed in 6.110868997s

• [SLOW TEST:8.227 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:56:42.765: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-983f8604-ff19-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 20:56:42.846: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-984013cf-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-9c848" to be "success or failure"
Dec 13 20:56:42.849: INFO: Pod "pod-projected-configmaps-984013cf-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91506ms
Dec 13 20:56:44.852: INFO: Pod "pod-projected-configmaps-984013cf-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006473358s
STEP: Saw pod success
Dec 13 20:56:44.852: INFO: Pod "pod-projected-configmaps-984013cf-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:56:44.855: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-configmaps-984013cf-ff19-11e8-b666-eabb5e37e61c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 20:56:44.873: INFO: Waiting for pod pod-projected-configmaps-984013cf-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:56:44.875: INFO: Pod pod-projected-configmaps-984013cf-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:56:44.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9c848" for this suite.
Dec 13 20:56:50.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:56:50.953: INFO: namespace: e2e-tests-projected-9c848, resource: bindings, ignored listing per whitelist
Dec 13 20:56:50.972: INFO: namespace e2e-tests-projected-9c848 deletion completed in 6.093859196s

• [SLOW TEST:8.208 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:56:50.973: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:56:53.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-g629z" for this suite.
Dec 13 20:57:39.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:57:39.089: INFO: namespace: e2e-tests-kubelet-test-g629z, resource: bindings, ignored listing per whitelist
Dec 13 20:57:39.166: INFO: namespace e2e-tests-kubelet-test-g629z deletion completed in 46.103130601s

• [SLOW TEST:48.193 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:57:39.166: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:57:39.244: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9ddd8d0-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-znsnf" to be "success or failure"
Dec 13 20:57:39.247: INFO: Pod "downwardapi-volume-b9ddd8d0-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.926759ms
Dec 13 20:57:41.250: INFO: Pod "downwardapi-volume-b9ddd8d0-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006453971s
STEP: Saw pod success
Dec 13 20:57:41.250: INFO: Pod "downwardapi-volume-b9ddd8d0-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:57:41.253: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-b9ddd8d0-ff19-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:57:41.276: INFO: Waiting for pod downwardapi-volume-b9ddd8d0-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:57:41.279: INFO: Pod downwardapi-volume-b9ddd8d0-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:57:41.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-znsnf" for this suite.
Dec 13 20:57:47.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:57:47.310: INFO: namespace: e2e-tests-projected-znsnf, resource: bindings, ignored listing per whitelist
Dec 13 20:57:47.375: INFO: namespace e2e-tests-projected-znsnf deletion completed in 6.092860317s

• [SLOW TEST:8.208 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:57:47.375: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-bec29de0-ff19-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:57:47.459: INFO: Waiting up to 5m0s for pod "pod-secrets-bec32c06-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-secrets-dn7lz" to be "success or failure"
Dec 13 20:57:47.462: INFO: Pod "pod-secrets-bec32c06-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.221798ms
Dec 13 20:57:49.465: INFO: Pod "pod-secrets-bec32c06-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006508703s
STEP: Saw pod success
Dec 13 20:57:49.465: INFO: Pod "pod-secrets-bec32c06-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:57:49.468: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-secrets-bec32c06-ff19-11e8-b666-eabb5e37e61c container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:57:49.484: INFO: Waiting for pod pod-secrets-bec32c06-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:57:49.486: INFO: Pod pod-secrets-bec32c06-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:57:49.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-dn7lz" for this suite.
Dec 13 20:57:55.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:57:55.540: INFO: namespace: e2e-tests-secrets-dn7lz, resource: bindings, ignored listing per whitelist
Dec 13 20:57:55.583: INFO: namespace e2e-tests-secrets-dn7lz deletion completed in 6.094022867s

• [SLOW TEST:8.209 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:57:55.584: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c3a891a0-ff19-11e8-b666-eabb5e37e61c
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-c3a891a0-ff19-11e8-b666-eabb5e37e61c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:57:59.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2mjrk" for this suite.
Dec 13 20:58:21.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:58:21.754: INFO: namespace: e2e-tests-projected-2mjrk, resource: bindings, ignored listing per whitelist
Dec 13 20:58:21.828: INFO: namespace e2e-tests-projected-2mjrk deletion completed in 22.097279165s

• [SLOW TEST:26.244 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:58:21.828: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 20:58:21.905: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d34b1dab-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-5rzgm" to be "success or failure"
Dec 13 20:58:21.909: INFO: Pod "downwardapi-volume-d34b1dab-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598647ms
Dec 13 20:58:23.912: INFO: Pod "downwardapi-volume-d34b1dab-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00704842s
STEP: Saw pod success
Dec 13 20:58:23.912: INFO: Pod "downwardapi-volume-d34b1dab-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:58:23.915: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-d34b1dab-ff19-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 20:58:23.932: INFO: Waiting for pod downwardapi-volume-d34b1dab-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:58:23.934: INFO: Pod downwardapi-volume-d34b1dab-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:58:23.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5rzgm" for this suite.
Dec 13 20:58:29.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:58:30.053: INFO: namespace: e2e-tests-projected-5rzgm, resource: bindings, ignored listing per whitelist
Dec 13 20:58:30.092: INFO: namespace e2e-tests-projected-5rzgm deletion completed in 6.154060086s

• [SLOW TEST:8.264 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:58:30.092: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-d837e7b2-ff19-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 20:58:30.170: INFO: Waiting up to 5m0s for pod "pod-secrets-d838827e-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-secrets-q5gd5" to be "success or failure"
Dec 13 20:58:30.172: INFO: Pod "pod-secrets-d838827e-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.360444ms
Dec 13 20:58:32.176: INFO: Pod "pod-secrets-d838827e-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006092738s
STEP: Saw pod success
Dec 13 20:58:32.176: INFO: Pod "pod-secrets-d838827e-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:58:32.179: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-secrets-d838827e-ff19-11e8-b666-eabb5e37e61c container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 20:58:32.197: INFO: Waiting for pod pod-secrets-d838827e-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:58:32.200: INFO: Pod pod-secrets-d838827e-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:58:32.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-q5gd5" for this suite.
Dec 13 20:58:38.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:58:38.241: INFO: namespace: e2e-tests-secrets-q5gd5, resource: bindings, ignored listing per whitelist
Dec 13 20:58:38.301: INFO: namespace e2e-tests-secrets-q5gd5 deletion completed in 6.097664306s

• [SLOW TEST:8.208 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:58:38.301: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec 13 20:58:38.381: INFO: Waiting up to 5m0s for pod "downward-api-dd1d3b4a-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-qn6gg" to be "success or failure"
Dec 13 20:58:38.384: INFO: Pod "downward-api-dd1d3b4a-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.049832ms
Dec 13 20:58:40.387: INFO: Pod "downward-api-dd1d3b4a-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006690385s
STEP: Saw pod success
Dec 13 20:58:40.387: INFO: Pod "downward-api-dd1d3b4a-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:58:40.390: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downward-api-dd1d3b4a-ff19-11e8-b666-eabb5e37e61c container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:58:40.406: INFO: Waiting for pod downward-api-dd1d3b4a-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:58:40.409: INFO: Pod downward-api-dd1d3b4a-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:58:40.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qn6gg" for this suite.
Dec 13 20:58:46.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:58:46.461: INFO: namespace: e2e-tests-downward-api-qn6gg, resource: bindings, ignored listing per whitelist
Dec 13 20:58:46.511: INFO: namespace e2e-tests-downward-api-qn6gg deletion completed in 6.098939181s

• [SLOW TEST:8.210 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:58:46.511: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 13 20:58:46.594: INFO: Waiting up to 5m0s for pod "pod-e2027c5b-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-z62fx" to be "success or failure"
Dec 13 20:58:46.598: INFO: Pod "pod-e2027c5b-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305911ms
Dec 13 20:58:48.601: INFO: Pod "pod-e2027c5b-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006932362s
STEP: Saw pod success
Dec 13 20:58:48.601: INFO: Pod "pod-e2027c5b-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:58:48.604: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-e2027c5b-ff19-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 20:58:48.624: INFO: Waiting for pod pod-e2027c5b-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:58:48.626: INFO: Pod pod-e2027c5b-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:58:48.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z62fx" for this suite.
Dec 13 20:58:54.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:58:54.717: INFO: namespace: e2e-tests-emptydir-z62fx, resource: bindings, ignored listing per whitelist
Dec 13 20:58:54.724: INFO: namespace e2e-tests-emptydir-z62fx deletion completed in 6.094382476s

• [SLOW TEST:8.213 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:58:54.724: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:58:58.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-rzwht" for this suite.
Dec 13 20:59:04.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:59:04.892: INFO: namespace: e2e-tests-kubelet-test-rzwht, resource: bindings, ignored listing per whitelist
Dec 13 20:59:04.914: INFO: namespace e2e-tests-kubelet-test-rzwht deletion completed in 6.094516974s

• [SLOW TEST:10.190 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:59:04.915: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Dec 13 20:59:04.991: INFO: Waiting up to 5m0s for pod "var-expansion-ecf9def4-ff19-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-var-expansion-92fzt" to be "success or failure"
Dec 13 20:59:04.994: INFO: Pod "var-expansion-ecf9def4-ff19-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.060912ms
Dec 13 20:59:06.998: INFO: Pod "var-expansion-ecf9def4-ff19-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006640559s
STEP: Saw pod success
Dec 13 20:59:06.998: INFO: Pod "var-expansion-ecf9def4-ff19-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 20:59:07.001: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod var-expansion-ecf9def4-ff19-11e8-b666-eabb5e37e61c container dapi-container: <nil>
STEP: delete the pod
Dec 13 20:59:07.018: INFO: Waiting for pod var-expansion-ecf9def4-ff19-11e8-b666-eabb5e37e61c to disappear
Dec 13 20:59:07.020: INFO: Pod var-expansion-ecf9def4-ff19-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 20:59:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-92fzt" for this suite.
Dec 13 20:59:13.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 20:59:13.048: INFO: namespace: e2e-tests-var-expansion-92fzt, resource: bindings, ignored listing per whitelist
Dec 13 20:59:13.121: INFO: namespace e2e-tests-var-expansion-92fzt deletion completed in 6.097699122s

• [SLOW TEST:8.206 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 20:59:13.121: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-gg4vc
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-gg4vc
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-gg4vc
Dec 13 20:59:13.203: INFO: Found 0 stateful pods, waiting for 1
Dec 13 20:59:23.211: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 13 20:59:23.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-gg4vc ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 20:59:23.409: INFO: stderr: ""
Dec 13 20:59:23.409: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 20:59:23.409: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 13 20:59:23.413: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 13 20:59:33.420: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 20:59:33.420: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 20:59:33.432: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999799s
Dec 13 20:59:34.436: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997201508s
Dec 13 20:59:35.440: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993582758s
Dec 13 20:59:36.444: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98962865s
Dec 13 20:59:37.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.986018944s
Dec 13 20:59:38.451: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982478615s
Dec 13 20:59:39.456: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.978699824s
Dec 13 20:59:40.459: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974116817s
Dec 13 20:59:41.463: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.970115427s
Dec 13 20:59:42.467: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.333577ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-gg4vc
Dec 13 20:59:43.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-gg4vc ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 13 20:59:43.663: INFO: stderr: ""
Dec 13 20:59:43.663: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 13 20:59:43.663: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 13 20:59:43.666: INFO: Found 1 stateful pods, waiting for 3
Dec 13 20:59:53.674: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 20:59:53.674: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 20:59:53.674: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 13 20:59:53.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-gg4vc ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 20:59:53.866: INFO: stderr: ""
Dec 13 20:59:53.867: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 20:59:53.867: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 13 20:59:53.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-gg4vc ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 20:59:54.047: INFO: stderr: ""
Dec 13 20:59:54.047: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 20:59:54.047: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 13 20:59:54.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-gg4vc ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 20:59:54.230: INFO: stderr: ""
Dec 13 20:59:54.230: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 20:59:54.230: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 13 20:59:54.230: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 20:59:54.234: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 13 21:00:04.246: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 21:00:04.246: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 21:00:04.246: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 21:00:04.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999763s
Dec 13 21:00:05.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997012987s
Dec 13 21:00:06.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992900215s
Dec 13 21:00:07.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988998269s
Dec 13 21:00:08.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985048851s
Dec 13 21:00:09.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981112696s
Dec 13 21:00:10.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977497142s
Dec 13 21:00:11.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973598514s
Dec 13 21:00:12.288: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969702073s
Dec 13 21:00:13.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.810943ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-gg4vc
Dec 13 21:00:14.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-gg4vc ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 13 21:00:14.482: INFO: stderr: ""
Dec 13 21:00:14.482: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 13 21:00:14.482: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 13 21:00:14.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-gg4vc ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 13 21:00:14.669: INFO: stderr: ""
Dec 13 21:00:14.669: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 13 21:00:14.669: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 13 21:00:14.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-gg4vc ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 13 21:00:14.867: INFO: stderr: ""
Dec 13 21:00:14.867: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 13 21:00:14.867: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 13 21:00:14.867: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec 13 21:00:34.882: INFO: Deleting all statefulset in ns e2e-tests-statefulset-gg4vc
Dec 13 21:00:34.885: INFO: Scaling statefulset ss to 0
Dec 13 21:00:34.897: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 21:00:34.899: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:00:34.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-gg4vc" for this suite.
Dec 13 21:00:40.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:00:41.008: INFO: namespace: e2e-tests-statefulset-gg4vc, resource: bindings, ignored listing per whitelist
Dec 13 21:00:41.014: INFO: namespace e2e-tests-statefulset-gg4vc deletion completed in 6.098124095s

• [SLOW TEST:87.893 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:00:41.014: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-2642e691-ff1a-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 21:00:41.105: INFO: Waiting up to 5m0s for pod "pod-configmaps-26436fdb-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-configmap-x7z8q" to be "success or failure"
Dec 13 21:00:41.108: INFO: Pod "pod-configmaps-26436fdb-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.889193ms
Dec 13 21:00:43.112: INFO: Pod "pod-configmaps-26436fdb-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006559832s
STEP: Saw pod success
Dec 13 21:00:43.112: INFO: Pod "pod-configmaps-26436fdb-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:00:43.115: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-26436fdb-ff1a-11e8-b666-eabb5e37e61c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 21:00:43.132: INFO: Waiting for pod pod-configmaps-26436fdb-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:00:43.134: INFO: Pod pod-configmaps-26436fdb-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:00:43.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-x7z8q" for this suite.
Dec 13 21:00:49.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:00:49.233: INFO: namespace: e2e-tests-configmap-x7z8q, resource: bindings, ignored listing per whitelist
Dec 13 21:00:49.242: INFO: namespace e2e-tests-configmap-x7z8q deletion completed in 6.10400291s

• [SLOW TEST:8.227 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:00:49.242: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 13 21:00:49.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-fm97q'
Dec 13 21:00:49.386: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 13 21:00:49.386: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Dec 13 21:00:49.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-fm97q'
Dec 13 21:00:49.461: INFO: stderr: ""
Dec 13 21:00:49.461: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:00:49.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fm97q" for this suite.
Dec 13 21:01:11.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:01:11.505: INFO: namespace: e2e-tests-kubectl-fm97q, resource: bindings, ignored listing per whitelist
Dec 13 21:01:11.562: INFO: namespace e2e-tests-kubectl-fm97q deletion completed in 22.096418514s

• [SLOW TEST:22.320 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:01:11.563: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-mg649/secret-test-387724ce-ff1a-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 21:01:11.648: INFO: Waiting up to 5m0s for pod "pod-configmaps-3877c2fa-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-secrets-mg649" to be "success or failure"
Dec 13 21:01:11.653: INFO: Pod "pod-configmaps-3877c2fa-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.050936ms
Dec 13 21:01:13.657: INFO: Pod "pod-configmaps-3877c2fa-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008722525s
STEP: Saw pod success
Dec 13 21:01:13.657: INFO: Pod "pod-configmaps-3877c2fa-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:01:13.660: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-3877c2fa-ff1a-11e8-b666-eabb5e37e61c container env-test: <nil>
STEP: delete the pod
Dec 13 21:01:13.676: INFO: Waiting for pod pod-configmaps-3877c2fa-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:01:13.679: INFO: Pod pod-configmaps-3877c2fa-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:01:13.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-mg649" for this suite.
Dec 13 21:01:19.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:01:19.757: INFO: namespace: e2e-tests-secrets-mg649, resource: bindings, ignored listing per whitelist
Dec 13 21:01:19.778: INFO: namespace e2e-tests-secrets-mg649 deletion completed in 6.095713446s

• [SLOW TEST:8.215 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:01:19.778: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 21:01:19.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d5b9940-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-kfrxl" to be "success or failure"
Dec 13 21:01:19.855: INFO: Pod "downwardapi-volume-3d5b9940-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.232631ms
Dec 13 21:01:21.859: INFO: Pod "downwardapi-volume-3d5b9940-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007818035s
STEP: Saw pod success
Dec 13 21:01:21.859: INFO: Pod "downwardapi-volume-3d5b9940-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:01:21.861: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-3d5b9940-ff1a-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 21:01:21.877: INFO: Waiting for pod downwardapi-volume-3d5b9940-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:01:21.880: INFO: Pod downwardapi-volume-3d5b9940-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:01:21.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kfrxl" for this suite.
Dec 13 21:01:27.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:01:27.922: INFO: namespace: e2e-tests-projected-kfrxl, resource: bindings, ignored listing per whitelist
Dec 13 21:01:27.985: INFO: namespace e2e-tests-projected-kfrxl deletion completed in 6.101681744s

• [SLOW TEST:8.207 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:01:27.985: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Dec 13 21:01:28.062: INFO: Waiting up to 5m0s for pod "client-containers-42408800-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-containers-5w287" to be "success or failure"
Dec 13 21:01:28.067: INFO: Pod "client-containers-42408800-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.440387ms
Dec 13 21:01:30.071: INFO: Pod "client-containers-42408800-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00974442s
STEP: Saw pod success
Dec 13 21:01:30.071: INFO: Pod "client-containers-42408800-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:01:30.076: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod client-containers-42408800-ff1a-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 21:01:30.102: INFO: Waiting for pod client-containers-42408800-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:01:30.105: INFO: Pod client-containers-42408800-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:01:30.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-5w287" for this suite.
Dec 13 21:01:36.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:01:36.148: INFO: namespace: e2e-tests-containers-5w287, resource: bindings, ignored listing per whitelist
Dec 13 21:01:36.205: INFO: namespace e2e-tests-containers-5w287 deletion completed in 6.096884217s

• [SLOW TEST:8.220 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:01:36.205: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-wwlbt in namespace e2e-tests-proxy-26dw7
I1213 21:01:36.290072      14 runners.go:184] Created replication controller with name: proxy-service-wwlbt, namespace: e2e-tests-proxy-26dw7, replica count: 1
I1213 21:01:37.340538      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1213 21:01:38.340695      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1213 21:01:39.340841      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1213 21:01:40.340991      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 21:01:41.341139      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 21:01:42.341326      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 21:01:43.341487      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 21:01:44.341639      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 21:01:45.341789      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1213 21:01:46.341947      14 runners.go:184] proxy-service-wwlbt Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 13 21:01:46.350: INFO: setup took 10.079032612s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 13 21:01:46.357: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 6.984076ms)
Dec 13 21:01:46.363: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 12.791497ms)
Dec 13 21:01:46.363: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 12.578836ms)
Dec 13 21:01:46.363: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 13.185595ms)
Dec 13 21:01:46.369: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 18.06984ms)
Dec 13 21:01:46.369: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 18.775883ms)
Dec 13 21:01:46.369: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 18.617328ms)
Dec 13 21:01:46.369: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 18.773074ms)
Dec 13 21:01:46.369: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 18.692823ms)
Dec 13 21:01:46.379: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 28.235771ms)
Dec 13 21:01:46.379: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 28.095912ms)
Dec 13 21:01:46.379: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 28.064805ms)
Dec 13 21:01:46.379: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 29.16299ms)
Dec 13 21:01:46.380: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 29.09786ms)
Dec 13 21:01:46.380: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 29.7795ms)
Dec 13 21:01:46.380: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 30.008165ms)
Dec 13 21:01:46.390: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 8.671763ms)
Dec 13 21:01:46.390: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 9.264136ms)
Dec 13 21:01:46.391: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 10.579115ms)
Dec 13 21:01:46.392: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 11.308331ms)
Dec 13 21:01:46.392: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 11.29482ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 11.983281ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 11.85519ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 11.982651ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 12.103904ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 12.482896ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 12.118137ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 12.41281ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 12.244783ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 12.404756ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 12.496302ms)
Dec 13 21:01:46.393: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 12.482413ms)
Dec 13 21:01:46.401: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 7.467017ms)
Dec 13 21:01:46.402: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 7.699648ms)
Dec 13 21:01:46.402: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 8.091185ms)
Dec 13 21:01:46.402: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 8.175855ms)
Dec 13 21:01:46.403: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 8.745681ms)
Dec 13 21:01:46.403: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 8.971234ms)
Dec 13 21:01:46.403: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 8.969435ms)
Dec 13 21:01:46.403: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 8.88095ms)
Dec 13 21:01:46.403: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 9.428861ms)
Dec 13 21:01:46.403: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 9.344353ms)
Dec 13 21:01:46.404: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 10.185765ms)
Dec 13 21:01:46.405: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 11.48242ms)
Dec 13 21:01:46.406: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 11.984333ms)
Dec 13 21:01:46.406: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 11.932945ms)
Dec 13 21:01:46.406: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 12.423484ms)
Dec 13 21:01:46.407: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 13.031374ms)
Dec 13 21:01:46.416: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 9.415228ms)
Dec 13 21:01:46.422: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 14.453332ms)
Dec 13 21:01:46.422: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 14.73498ms)
Dec 13 21:01:46.422: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 15.164422ms)
Dec 13 21:01:46.422: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 15.370901ms)
Dec 13 21:01:46.423: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 15.446147ms)
Dec 13 21:01:46.423: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 15.665941ms)
Dec 13 21:01:46.423: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 15.579355ms)
Dec 13 21:01:46.423: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 15.740253ms)
Dec 13 21:01:46.423: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 15.531027ms)
Dec 13 21:01:46.427: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 20.26263ms)
Dec 13 21:01:46.428: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 20.358451ms)
Dec 13 21:01:46.428: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 20.782092ms)
Dec 13 21:01:46.428: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 20.464831ms)
Dec 13 21:01:46.428: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 20.563304ms)
Dec 13 21:01:46.428: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 20.457816ms)
Dec 13 21:01:46.436: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 7.215239ms)
Dec 13 21:01:46.436: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 7.756664ms)
Dec 13 21:01:46.436: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 7.895021ms)
Dec 13 21:01:46.436: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 8.344168ms)
Dec 13 21:01:46.437: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 7.801805ms)
Dec 13 21:01:46.437: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 8.339943ms)
Dec 13 21:01:46.437: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 8.34729ms)
Dec 13 21:01:46.438: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 9.967026ms)
Dec 13 21:01:46.438: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 9.814058ms)
Dec 13 21:01:46.438: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 10.506379ms)
Dec 13 21:01:46.439: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 10.380566ms)
Dec 13 21:01:46.440: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 11.842211ms)
Dec 13 21:01:46.441: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 12.384354ms)
Dec 13 21:01:46.441: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 12.727812ms)
Dec 13 21:01:46.441: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 12.759389ms)
Dec 13 21:01:46.441: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 13.45388ms)
Dec 13 21:01:46.448: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 6.491631ms)
Dec 13 21:01:46.450: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 7.458164ms)
Dec 13 21:01:46.450: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 8.339406ms)
Dec 13 21:01:46.450: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 8.326262ms)
Dec 13 21:01:46.450: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 8.270099ms)
Dec 13 21:01:46.451: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 9.061594ms)
Dec 13 21:01:46.451: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 9.072469ms)
Dec 13 21:01:46.452: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 10.26041ms)
Dec 13 21:01:46.452: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 10.410927ms)
Dec 13 21:01:46.453: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 10.922979ms)
Dec 13 21:01:46.453: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 11.18463ms)
Dec 13 21:01:46.453: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 11.235572ms)
Dec 13 21:01:46.454: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 11.463572ms)
Dec 13 21:01:46.454: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 12.042188ms)
Dec 13 21:01:46.454: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 11.95683ms)
Dec 13 21:01:46.454: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 12.598371ms)
Dec 13 21:01:46.460: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 6.142474ms)
Dec 13 21:01:46.461: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 6.280473ms)
Dec 13 21:01:46.461: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 6.349963ms)
Dec 13 21:01:46.461: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 6.693139ms)
Dec 13 21:01:46.462: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 7.602025ms)
Dec 13 21:01:46.466: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 10.87972ms)
Dec 13 21:01:46.466: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 11.479369ms)
Dec 13 21:01:46.466: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 11.455554ms)
Dec 13 21:01:46.466: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 11.537692ms)
Dec 13 21:01:46.467: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 12.316773ms)
Dec 13 21:01:46.467: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 12.512953ms)
Dec 13 21:01:46.467: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 12.448681ms)
Dec 13 21:01:46.467: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 12.412929ms)
Dec 13 21:01:46.467: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 12.473933ms)
Dec 13 21:01:46.467: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 12.782778ms)
Dec 13 21:01:46.468: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 12.903586ms)
Dec 13 21:01:46.473: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 5.020394ms)
Dec 13 21:01:46.474: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 5.847108ms)
Dec 13 21:01:46.474: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 6.406279ms)
Dec 13 21:01:46.475: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 6.809742ms)
Dec 13 21:01:46.475: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 7.133471ms)
Dec 13 21:01:46.475: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 7.171545ms)
Dec 13 21:01:46.475: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 7.296995ms)
Dec 13 21:01:46.476: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 7.726658ms)
Dec 13 21:01:46.476: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 7.827127ms)
Dec 13 21:01:46.477: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 8.720528ms)
Dec 13 21:01:46.478: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 9.602456ms)
Dec 13 21:01:46.478: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 10.732915ms)
Dec 13 21:01:46.479: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 10.857091ms)
Dec 13 21:01:46.479: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 11.124177ms)
Dec 13 21:01:46.479: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 11.475701ms)
Dec 13 21:01:46.480: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 12.363743ms)
Dec 13 21:01:46.488: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 7.232354ms)
Dec 13 21:01:46.488: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 7.359868ms)
Dec 13 21:01:46.488: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 7.88668ms)
Dec 13 21:01:46.488: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 7.694018ms)
Dec 13 21:01:46.488: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 7.964643ms)
Dec 13 21:01:46.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 7.980111ms)
Dec 13 21:01:46.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 8.194684ms)
Dec 13 21:01:46.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 8.122042ms)
Dec 13 21:01:46.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 8.511473ms)
Dec 13 21:01:46.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 8.480705ms)
Dec 13 21:01:46.489: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 8.827602ms)
Dec 13 21:01:46.490: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 9.766464ms)
Dec 13 21:01:46.491: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 10.91908ms)
Dec 13 21:01:46.491: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 11.145865ms)
Dec 13 21:01:46.492: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 10.87089ms)
Dec 13 21:01:46.492: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 10.998821ms)
Dec 13 21:01:46.496: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 4.745473ms)
Dec 13 21:01:46.498: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 6.222505ms)
Dec 13 21:01:46.500: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 7.696007ms)
Dec 13 21:01:46.500: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 7.823682ms)
Dec 13 21:01:46.500: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 8.359153ms)
Dec 13 21:01:46.500: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 8.221548ms)
Dec 13 21:01:46.501: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 8.570153ms)
Dec 13 21:01:46.501: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 8.725773ms)
Dec 13 21:01:46.501: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 8.982989ms)
Dec 13 21:01:46.501: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 8.883087ms)
Dec 13 21:01:46.503: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 10.578136ms)
Dec 13 21:01:46.504: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 11.8538ms)
Dec 13 21:01:46.504: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 12.537885ms)
Dec 13 21:01:46.505: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 12.856793ms)
Dec 13 21:01:46.505: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 12.970899ms)
Dec 13 21:01:46.505: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 13.383106ms)
Dec 13 21:01:46.510: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 4.620088ms)
Dec 13 21:01:46.512: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 6.848377ms)
Dec 13 21:01:46.513: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 7.294189ms)
Dec 13 21:01:46.513: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 7.244896ms)
Dec 13 21:01:46.514: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 8.143909ms)
Dec 13 21:01:46.514: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 8.68885ms)
Dec 13 21:01:46.515: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 9.08794ms)
Dec 13 21:01:46.515: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 9.171421ms)
Dec 13 21:01:46.515: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 8.94971ms)
Dec 13 21:01:46.515: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 9.125156ms)
Dec 13 21:01:46.515: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 9.363309ms)
Dec 13 21:01:46.515: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 9.618734ms)
Dec 13 21:01:46.516: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 9.791236ms)
Dec 13 21:01:46.516: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 10.07132ms)
Dec 13 21:01:46.516: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 9.970454ms)
Dec 13 21:01:46.517: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 11.349115ms)
Dec 13 21:01:46.526: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 8.176371ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 10.216941ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 10.416222ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 10.57601ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 10.510382ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 10.628669ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 10.600195ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 10.549972ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 10.482477ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 10.482646ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 10.964195ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 10.980344ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 10.728458ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 10.815865ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 11.247261ms)
Dec 13 21:01:46.528: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 10.91188ms)
Dec 13 21:01:46.534: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 5.763468ms)
Dec 13 21:01:46.535: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 6.035565ms)
Dec 13 21:01:46.536: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 7.624959ms)
Dec 13 21:01:46.536: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 7.457311ms)
Dec 13 21:01:46.537: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 7.534022ms)
Dec 13 21:01:46.537: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 7.828589ms)
Dec 13 21:01:46.538: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 9.413078ms)
Dec 13 21:01:46.539: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 9.658356ms)
Dec 13 21:01:46.539: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 10.62928ms)
Dec 13 21:01:46.539: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 10.077126ms)
Dec 13 21:01:46.540: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 10.654447ms)
Dec 13 21:01:46.540: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 11.343947ms)
Dec 13 21:01:46.540: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 11.365868ms)
Dec 13 21:01:46.540: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 11.319357ms)
Dec 13 21:01:46.541: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 11.578814ms)
Dec 13 21:01:46.541: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 11.40353ms)
Dec 13 21:01:46.549: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 7.761046ms)
Dec 13 21:01:46.549: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 8.114095ms)
Dec 13 21:01:46.550: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 8.556377ms)
Dec 13 21:01:46.550: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 8.733469ms)
Dec 13 21:01:46.550: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 8.951753ms)
Dec 13 21:01:46.550: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 9.233832ms)
Dec 13 21:01:46.550: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 9.431735ms)
Dec 13 21:01:46.550: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 9.357731ms)
Dec 13 21:01:46.550: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 9.30193ms)
Dec 13 21:01:46.551: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 9.590318ms)
Dec 13 21:01:46.552: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 11.408409ms)
Dec 13 21:01:46.554: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 12.682272ms)
Dec 13 21:01:46.554: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 12.662215ms)
Dec 13 21:01:46.554: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 12.64886ms)
Dec 13 21:01:46.554: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 13.146279ms)
Dec 13 21:01:46.554: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 13.132506ms)
Dec 13 21:01:46.560: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 5.575006ms)
Dec 13 21:01:46.564: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 9.123315ms)
Dec 13 21:01:46.564: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 8.995677ms)
Dec 13 21:01:46.564: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 9.156921ms)
Dec 13 21:01:46.564: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 9.00975ms)
Dec 13 21:01:46.564: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 9.626909ms)
Dec 13 21:01:46.564: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 9.626448ms)
Dec 13 21:01:46.565: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 9.803222ms)
Dec 13 21:01:46.565: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 10.331675ms)
Dec 13 21:01:46.565: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 9.980761ms)
Dec 13 21:01:46.565: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 10.174085ms)
Dec 13 21:01:46.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 10.625248ms)
Dec 13 21:01:46.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 11.282779ms)
Dec 13 21:01:46.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 11.199491ms)
Dec 13 21:01:46.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 11.21248ms)
Dec 13 21:01:46.566: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 11.168268ms)
Dec 13 21:01:46.575: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 8.757563ms)
Dec 13 21:01:46.575: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 8.574029ms)
Dec 13 21:01:46.575: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 8.713599ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 13.433966ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 13.598501ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 13.753083ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 13.943728ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 13.651602ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 13.993043ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 14.149839ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 14.188357ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 14.07549ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 14.091485ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 14.212298ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 14.249319ms)
Dec 13 21:01:46.580: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 14.195019ms)
Dec 13 21:01:46.589: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 8.107214ms)
Dec 13 21:01:46.589: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 7.938708ms)
Dec 13 21:01:46.589: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 8.369773ms)
Dec 13 21:01:46.589: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 8.212474ms)
Dec 13 21:01:46.589: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 8.42046ms)
Dec 13 21:01:46.592: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 10.750407ms)
Dec 13 21:01:46.592: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 10.904294ms)
Dec 13 21:01:46.592: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 11.173357ms)
Dec 13 21:01:46.592: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 11.397288ms)
Dec 13 21:01:46.592: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 11.39602ms)
Dec 13 21:01:46.592: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 11.077908ms)
Dec 13 21:01:46.594: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 13.109073ms)
Dec 13 21:01:46.594: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 12.748169ms)
Dec 13 21:01:46.594: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 12.871374ms)
Dec 13 21:01:46.594: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 12.945848ms)
Dec 13 21:01:46.594: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 13.435361ms)
Dec 13 21:01:46.600: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 5.728153ms)
Dec 13 21:01:46.602: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 7.753136ms)
Dec 13 21:01:46.604: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 10.013539ms)
Dec 13 21:01:46.604: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 9.763965ms)
Dec 13 21:01:46.604: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 9.811732ms)
Dec 13 21:01:46.604: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 9.6494ms)
Dec 13 21:01:46.604: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 9.890576ms)
Dec 13 21:01:46.604: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 9.782466ms)
Dec 13 21:01:46.604: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 9.617919ms)
Dec 13 21:01:46.605: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 10.668922ms)
Dec 13 21:01:46.605: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 11.260645ms)
Dec 13 21:01:46.606: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 10.82132ms)
Dec 13 21:01:46.609: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 13.868169ms)
Dec 13 21:01:46.609: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 14.075146ms)
Dec 13 21:01:46.609: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 14.346033ms)
Dec 13 21:01:46.609: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 14.67896ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 10.78702ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 10.434499ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 10.289377ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 11.459533ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 10.967593ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 11.343782ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 10.624428ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 10.603196ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 11.453476ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 11.220828ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 11.676108ms)
Dec 13 21:01:46.622: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 11.202729ms)
Dec 13 21:01:46.625: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 13.732527ms)
Dec 13 21:01:46.625: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 13.502946ms)
Dec 13 21:01:46.625: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 13.856542ms)
Dec 13 21:01:46.625: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 13.72113ms)
Dec 13 21:01:46.632: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:462/proxy/: tls qux (200; 6.593854ms)
Dec 13 21:01:46.632: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 6.695703ms)
Dec 13 21:01:46.632: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:1080/proxy/... (200; 6.844555ms)
Dec 13 21:01:46.632: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 7.221101ms)
Dec 13 21:01:46.633: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:1080/proxy/rewri... (200; 7.948579ms)
Dec 13 21:01:46.633: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/http:proxy-service-wwlbt-z9n8m:162/proxy/: bar (200; 8.034961ms)
Dec 13 21:01:46.635: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname2/proxy/: tls qux (200; 9.61582ms)
Dec 13 21:01:46.635: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m:160/proxy/: foo (200; 9.868036ms)
Dec 13 21:01:46.635: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname2/proxy/: bar (200; 10.275596ms)
Dec 13 21:01:46.638: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/proxy-service-wwlbt-z9n8m/proxy/rewriteme"... (200; 12.405208ms)
Dec 13 21:01:46.638: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/https:proxy-service-wwlbt:tlsportname1/proxy/: tls baz (200; 12.892688ms)
Dec 13 21:01:46.638: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/proxy-service-wwlbt:portname1/proxy/: foo (200; 12.783544ms)
Dec 13 21:01:46.638: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:443/proxy/... (200; 12.655513ms)
Dec 13 21:01:46.638: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname2/proxy/: bar (200; 12.561121ms)
Dec 13 21:01:46.638: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/services/http:proxy-service-wwlbt:portname1/proxy/: foo (200; 13.015583ms)
Dec 13 21:01:46.638: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-26dw7/pods/https:proxy-service-wwlbt-z9n8m:460/proxy/: tls baz (200; 12.66197ms)
STEP: deleting ReplicationController proxy-service-wwlbt in namespace e2e-tests-proxy-26dw7, will wait for the garbage collector to delete the pods
Dec 13 21:01:46.700: INFO: Deleting ReplicationController proxy-service-wwlbt took: 8.093364ms
Dec 13 21:01:46.800: INFO: Terminating ReplicationController proxy-service-wwlbt pods took: 100.175163ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:01:48.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-26dw7" for this suite.
Dec 13 21:01:54.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:01:54.740: INFO: namespace: e2e-tests-proxy-26dw7, resource: bindings, ignored listing per whitelist
Dec 13 21:01:54.810: INFO: namespace e2e-tests-proxy-26dw7 deletion completed in 6.105922312s

• [SLOW TEST:18.605 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:01:54.810: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Dec 13 21:01:54.891: INFO: Waiting up to 5m0s for pod "downward-api-523e4a18-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-kg8mp" to be "success or failure"
Dec 13 21:01:54.893: INFO: Pod "downward-api-523e4a18-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.654462ms
Dec 13 21:01:56.901: INFO: Pod "downward-api-523e4a18-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010037103s
STEP: Saw pod success
Dec 13 21:01:56.901: INFO: Pod "downward-api-523e4a18-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:01:56.903: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downward-api-523e4a18-ff1a-11e8-b666-eabb5e37e61c container dapi-container: <nil>
STEP: delete the pod
Dec 13 21:01:56.922: INFO: Waiting for pod downward-api-523e4a18-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:01:56.925: INFO: Pod downward-api-523e4a18-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:01:56.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kg8mp" for this suite.
Dec 13 21:02:02.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:02:02.970: INFO: namespace: e2e-tests-downward-api-kg8mp, resource: bindings, ignored listing per whitelist
Dec 13 21:02:03.025: INFO: namespace e2e-tests-downward-api-kg8mp deletion completed in 6.096477467s

• [SLOW TEST:8.215 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:02:03.025: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-5722bb18-ff1a-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 21:02:03.101: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-57234887-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-957wj" to be "success or failure"
Dec 13 21:02:03.104: INFO: Pod "pod-projected-configmaps-57234887-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876211ms
Dec 13 21:02:05.108: INFO: Pod "pod-projected-configmaps-57234887-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006322702s
STEP: Saw pod success
Dec 13 21:02:05.108: INFO: Pod "pod-projected-configmaps-57234887-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:02:05.110: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-configmaps-57234887-ff1a-11e8-b666-eabb5e37e61c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 21:02:05.130: INFO: Waiting for pod pod-projected-configmaps-57234887-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:02:05.134: INFO: Pod pod-projected-configmaps-57234887-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:02:05.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-957wj" for this suite.
Dec 13 21:02:11.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:02:11.212: INFO: namespace: e2e-tests-projected-957wj, resource: bindings, ignored listing per whitelist
Dec 13 21:02:11.246: INFO: namespace e2e-tests-projected-957wj deletion completed in 6.109430032s

• [SLOW TEST:8.221 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:02:11.247: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-5c0a0198-ff1a-11e8-b666-eabb5e37e61c
Dec 13 21:02:11.326: INFO: Pod name my-hostname-basic-5c0a0198-ff1a-11e8-b666-eabb5e37e61c: Found 0 pods out of 1
Dec 13 21:02:16.330: INFO: Pod name my-hostname-basic-5c0a0198-ff1a-11e8-b666-eabb5e37e61c: Found 1 pods out of 1
Dec 13 21:02:16.330: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5c0a0198-ff1a-11e8-b666-eabb5e37e61c" are running
Dec 13 21:02:16.333: INFO: Pod "my-hostname-basic-5c0a0198-ff1a-11e8-b666-eabb5e37e61c-mdzqw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-13 21:02:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-13 21:02:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-13 21:02:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-12-13 21:02:11 +0000 UTC Reason: Message:}])
Dec 13 21:02:16.333: INFO: Trying to dial the pod
Dec 13 21:02:21.348: INFO: Controller my-hostname-basic-5c0a0198-ff1a-11e8-b666-eabb5e37e61c: Got expected result from replica 1 [my-hostname-basic-5c0a0198-ff1a-11e8-b666-eabb5e37e61c-mdzqw]: "my-hostname-basic-5c0a0198-ff1a-11e8-b666-eabb5e37e61c-mdzqw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:02:21.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-mkc6c" for this suite.
Dec 13 21:02:27.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:02:27.402: INFO: namespace: e2e-tests-replication-controller-mkc6c, resource: bindings, ignored listing per whitelist
Dec 13 21:02:27.454: INFO: namespace e2e-tests-replication-controller-mkc6c deletion completed in 6.103039262s

• [SLOW TEST:16.207 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:02:27.454: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Dec 13 21:02:27.526: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-413709859 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:02:27.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-c4jz8" for this suite.
Dec 13 21:02:33.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:02:33.658: INFO: namespace: e2e-tests-kubectl-c4jz8, resource: bindings, ignored listing per whitelist
Dec 13 21:02:33.683: INFO: namespace e2e-tests-kubectl-c4jz8 deletion completed in 6.100643481s

• [SLOW TEST:6.229 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:02:33.684: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 21:02:35.795: INFO: Waiting up to 5m0s for pod "client-envvars-6a9fd824-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-pods-7nd7h" to be "success or failure"
Dec 13 21:02:35.799: INFO: Pod "client-envvars-6a9fd824-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074409ms
Dec 13 21:02:37.802: INFO: Pod "client-envvars-6a9fd824-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007390589s
STEP: Saw pod success
Dec 13 21:02:37.802: INFO: Pod "client-envvars-6a9fd824-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:02:37.805: INFO: Trying to get logs from node ip-10-0-2-167.eu-west-1.compute.internal pod client-envvars-6a9fd824-ff1a-11e8-b666-eabb5e37e61c container env3cont: <nil>
STEP: delete the pod
Dec 13 21:02:37.823: INFO: Waiting for pod client-envvars-6a9fd824-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:02:37.827: INFO: Pod client-envvars-6a9fd824-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:02:37.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-7nd7h" for this suite.
Dec 13 21:03:15.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:03:15.941: INFO: namespace: e2e-tests-pods-7nd7h, resource: bindings, ignored listing per whitelist
Dec 13 21:03:15.955: INFO: namespace e2e-tests-pods-7nd7h deletion completed in 38.122258573s

• [SLOW TEST:42.271 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:03:15.956: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 13 21:03:16.033: INFO: Waiting up to 5m0s for pod "pod-829b9fb4-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-58q96" to be "success or failure"
Dec 13 21:03:16.038: INFO: Pod "pod-829b9fb4-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826489ms
Dec 13 21:03:18.042: INFO: Pod "pod-829b9fb4-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008259728s
STEP: Saw pod success
Dec 13 21:03:18.042: INFO: Pod "pod-829b9fb4-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:03:18.044: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-829b9fb4-ff1a-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 21:03:18.064: INFO: Waiting for pod pod-829b9fb4-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:03:18.066: INFO: Pod pod-829b9fb4-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:03:18.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-58q96" for this suite.
Dec 13 21:03:24.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:03:24.107: INFO: namespace: e2e-tests-emptydir-58q96, resource: bindings, ignored listing per whitelist
Dec 13 21:03:24.164: INFO: namespace e2e-tests-emptydir-58q96 deletion completed in 6.095091881s

• [SLOW TEST:8.209 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:03:24.164: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 13 21:03:24.236: INFO: Waiting up to 5m0s for pod "pod-877f3045-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-z76xw" to be "success or failure"
Dec 13 21:03:24.239: INFO: Pod "pod-877f3045-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.937192ms
Dec 13 21:03:26.246: INFO: Pod "pod-877f3045-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010448114s
STEP: Saw pod success
Dec 13 21:03:26.246: INFO: Pod "pod-877f3045-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:03:26.249: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-877f3045-ff1a-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 21:03:26.267: INFO: Waiting for pod pod-877f3045-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:03:26.270: INFO: Pod pod-877f3045-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:03:26.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-z76xw" for this suite.
Dec 13 21:03:32.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:03:32.306: INFO: namespace: e2e-tests-emptydir-z76xw, resource: bindings, ignored listing per whitelist
Dec 13 21:03:32.381: INFO: namespace e2e-tests-emptydir-z76xw deletion completed in 6.105292497s

• [SLOW TEST:8.216 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:03:32.381: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-8c65c6df-ff1a-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 21:03:32.461: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8c666df5-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-tgjdc" to be "success or failure"
Dec 13 21:03:32.465: INFO: Pod "pod-projected-configmaps-8c666df5-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43752ms
Dec 13 21:03:34.468: INFO: Pod "pod-projected-configmaps-8c666df5-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007073596s
STEP: Saw pod success
Dec 13 21:03:34.468: INFO: Pod "pod-projected-configmaps-8c666df5-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:03:34.471: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-configmaps-8c666df5-ff1a-11e8-b666-eabb5e37e61c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 21:03:34.487: INFO: Waiting for pod pod-projected-configmaps-8c666df5-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:03:34.489: INFO: Pod pod-projected-configmaps-8c666df5-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:03:34.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tgjdc" for this suite.
Dec 13 21:03:40.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:03:40.549: INFO: namespace: e2e-tests-projected-tgjdc, resource: bindings, ignored listing per whitelist
Dec 13 21:03:40.593: INFO: namespace e2e-tests-projected-tgjdc deletion completed in 6.100153996s

• [SLOW TEST:8.212 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:03:40.593: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 13 21:03:43.188: INFO: Successfully updated pod "pod-update-activedeadlineseconds-914a04b5-ff1a-11e8-b666-eabb5e37e61c"
Dec 13 21:03:43.188: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-914a04b5-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-pods-lwwct" to be "terminated due to deadline exceeded"
Dec 13 21:03:43.190: INFO: Pod "pod-update-activedeadlineseconds-914a04b5-ff1a-11e8-b666-eabb5e37e61c": Phase="Running", Reason="", readiness=true. Elapsed: 2.522957ms
Dec 13 21:03:45.194: INFO: Pod "pod-update-activedeadlineseconds-914a04b5-ff1a-11e8-b666-eabb5e37e61c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00625077s
Dec 13 21:03:47.201: INFO: Pod "pod-update-activedeadlineseconds-914a04b5-ff1a-11e8-b666-eabb5e37e61c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.013094639s
Dec 13 21:03:47.201: INFO: Pod "pod-update-activedeadlineseconds-914a04b5-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:03:47.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lwwct" for this suite.
Dec 13 21:03:53.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:03:53.287: INFO: namespace: e2e-tests-pods-lwwct, resource: bindings, ignored listing per whitelist
Dec 13 21:03:53.299: INFO: namespace e2e-tests-pods-lwwct deletion completed in 6.09462408s

• [SLOW TEST:12.706 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:03:53.299: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:04:15.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-rlqxl" for this suite.
Dec 13 21:04:21.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:04:21.620: INFO: namespace: e2e-tests-container-runtime-rlqxl, resource: bindings, ignored listing per whitelist
Dec 13 21:04:21.662: INFO: namespace e2e-tests-container-runtime-rlqxl deletion completed in 6.096991156s

• [SLOW TEST:28.363 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:04:21.662: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 21:04:21.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a9c5d1c6-ff1a-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-l6zgf" to be "success or failure"
Dec 13 21:04:21.744: INFO: Pod "downwardapi-volume-a9c5d1c6-ff1a-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.257286ms
Dec 13 21:04:23.748: INFO: Pod "downwardapi-volume-a9c5d1c6-ff1a-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006752909s
STEP: Saw pod success
Dec 13 21:04:23.748: INFO: Pod "downwardapi-volume-a9c5d1c6-ff1a-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:04:23.750: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-a9c5d1c6-ff1a-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 21:04:23.769: INFO: Waiting for pod downwardapi-volume-a9c5d1c6-ff1a-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:04:23.771: INFO: Pod downwardapi-volume-a9c5d1c6-ff1a-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:04:23.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l6zgf" for this suite.
Dec 13 21:04:29.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:04:29.819: INFO: namespace: e2e-tests-projected-l6zgf, resource: bindings, ignored listing per whitelist
Dec 13 21:04:29.869: INFO: namespace e2e-tests-projected-l6zgf deletion completed in 6.094002998s

• [SLOW TEST:8.207 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:04:29.869: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec 13 21:04:29.935: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:04:33.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-ldfkx" for this suite.
Dec 13 21:04:55.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:04:55.088: INFO: namespace: e2e-tests-init-container-ldfkx, resource: bindings, ignored listing per whitelist
Dec 13 21:04:55.148: INFO: namespace e2e-tests-init-container-ldfkx deletion completed in 22.097669953s

• [SLOW TEST:25.279 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:04:55.149: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Dec 13 21:04:55.223: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-413709859 proxy --unix-socket=/tmp/kubectl-proxy-unix504553254/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:04:55.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hvjct" for this suite.
Dec 13 21:05:01.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:05:01.297: INFO: namespace: e2e-tests-kubectl-hvjct, resource: bindings, ignored listing per whitelist
Dec 13 21:05:01.375: INFO: namespace e2e-tests-kubectl-hvjct deletion completed in 6.103094586s

• [SLOW TEST:6.227 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:05:01.376: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:05:01.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-k2hch" for this suite.
Dec 13 21:05:23.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:05:23.489: INFO: namespace: e2e-tests-pods-k2hch, resource: bindings, ignored listing per whitelist
Dec 13 21:05:23.558: INFO: namespace e2e-tests-pods-k2hch deletion completed in 22.099415275s

• [SLOW TEST:22.182 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:05:23.559: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec 13 21:05:23.629: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 13 21:05:23.636: INFO: Waiting for terminating namespaces to be deleted...
Dec 13 21:05:23.638: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-167.eu-west-1.compute.internal before test
Dec 13 21:05:23.644: INFO: calico-node-p8l9p from kube-system started at 2018-12-13 15:23:12 +0000 UTC (1 container statuses recorded)
Dec 13 21:05:23.644: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 21:05:23.644: INFO: sonobuoy from heptio-sonobuoy started at 2018-12-13 19:50:49 +0000 UTC (1 container statuses recorded)
Dec 13 21:05:23.644: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 13 21:05:23.644: INFO: sonobuoy-systemd-logs-daemon-set-e5a404e9fcc945c3-zrx9c from heptio-sonobuoy started at 2018-12-13 19:50:51 +0000 UTC (2 container statuses recorded)
Dec 13 21:05:23.644: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec 13 21:05:23.644: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 13 21:05:23.644: INFO: kube-proxy-246r7 from kube-system started at 2018-12-13 14:51:13 +0000 UTC (1 container statuses recorded)
Dec 13 21:05:23.644: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 21:05:23.644: INFO: coredns-86c58d9df4-jdbxq from kube-system started at 2018-12-13 19:48:04 +0000 UTC (1 container statuses recorded)
Dec 13 21:05:23.644: INFO: 	Container coredns ready: true, restart count 0
Dec 13 21:05:23.644: INFO: sonobuoy-e2e-job-87b901a1b8cd4728 from heptio-sonobuoy started at 2018-12-13 19:50:51 +0000 UTC (2 container statuses recorded)
Dec 13 21:05:23.644: INFO: 	Container e2e ready: true, restart count 0
Dec 13 21:05:23.644: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 13 21:05:23.644: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-214.eu-west-1.compute.internal before test
Dec 13 21:05:23.650: INFO: kube-proxy-kqrgn from kube-system started at 2018-12-13 14:51:08 +0000 UTC (1 container statuses recorded)
Dec 13 21:05:23.650: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 13 21:05:23.650: INFO: calico-node-h9tzd from kube-system started at 2018-12-13 15:23:05 +0000 UTC (1 container statuses recorded)
Dec 13 21:05:23.650: INFO: 	Container calico-node ready: true, restart count 0
Dec 13 21:05:23.650: INFO: nginx-7cdbd8cdc9-qwxrq from default started at 2018-12-13 19:48:29 +0000 UTC (1 container statuses recorded)
Dec 13 21:05:23.650: INFO: 	Container nginx ready: true, restart count 0
Dec 13 21:05:23.650: INFO: calicoctl from kube-system started at 2018-12-13 15:22:31 +0000 UTC (1 container statuses recorded)
Dec 13 21:05:23.650: INFO: 	Container calicoctl ready: true, restart count 0
Dec 13 21:05:23.650: INFO: coredns-86c58d9df4-8x9zn from kube-system started at 2018-12-13 19:48:04 +0000 UTC (1 container statuses recorded)
Dec 13 21:05:23.650: INFO: 	Container coredns ready: true, restart count 0
Dec 13 21:05:23.650: INFO: sonobuoy-systemd-logs-daemon-set-e5a404e9fcc945c3-c4ztz from heptio-sonobuoy started at 2018-12-13 19:50:51 +0000 UTC (2 container statuses recorded)
Dec 13 21:05:23.650: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Dec 13 21:05:23.650: INFO: 	Container sonobuoy-worker ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node ip-10-0-2-167.eu-west-1.compute.internal
STEP: verifying the node has the label node ip-10-0-3-214.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod nginx-7cdbd8cdc9-qwxrq requesting resource cpu=0m on Node ip-10-0-3-214.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-2-167.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod sonobuoy-e2e-job-87b901a1b8cd4728 requesting resource cpu=0m on Node ip-10-0-2-167.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod sonobuoy-systemd-logs-daemon-set-e5a404e9fcc945c3-c4ztz requesting resource cpu=0m on Node ip-10-0-3-214.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod sonobuoy-systemd-logs-daemon-set-e5a404e9fcc945c3-zrx9c requesting resource cpu=0m on Node ip-10-0-2-167.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod calico-node-h9tzd requesting resource cpu=250m on Node ip-10-0-3-214.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod calico-node-p8l9p requesting resource cpu=250m on Node ip-10-0-2-167.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod calicoctl requesting resource cpu=0m on Node ip-10-0-3-214.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod coredns-86c58d9df4-8x9zn requesting resource cpu=100m on Node ip-10-0-3-214.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod coredns-86c58d9df4-jdbxq requesting resource cpu=100m on Node ip-10-0-2-167.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod kube-proxy-246r7 requesting resource cpu=0m on Node ip-10-0-2-167.eu-west-1.compute.internal
Dec 13 21:05:23.688: INFO: Pod kube-proxy-kqrgn requesting resource cpu=0m on Node ip-10-0-3-214.eu-west-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ceb348b1-ff1a-11e8-b666-eabb5e37e61c.157000691f7d57bc], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-ktvvs/filler-pod-ceb348b1-ff1a-11e8-b666-eabb5e37e61c to ip-10-0-2-167.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ceb348b1-ff1a-11e8-b666-eabb5e37e61c.1570006948baa745], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ceb348b1-ff1a-11e8-b666-eabb5e37e61c.157000694d108876], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ceb348b1-ff1a-11e8-b666-eabb5e37e61c.1570006952945d65], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ceb4291e-ff1a-11e8-b666-eabb5e37e61c.157000691fce2830], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-ktvvs/filler-pod-ceb4291e-ff1a-11e8-b666-eabb5e37e61c to ip-10-0-3-214.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ceb4291e-ff1a-11e8-b666-eabb5e37e61c.157000694f7ac401], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ceb4291e-ff1a-11e8-b666-eabb5e37e61c.1570006953995453], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ceb4291e-ff1a-11e8-b666-eabb5e37e61c.157000695e384925], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1570006997d86fe4], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node ip-10-0-2-167.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-3-214.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:05:26.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-ktvvs" for this suite.
Dec 13 21:05:32.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:05:32.845: INFO: namespace: e2e-tests-sched-pred-ktvvs, resource: bindings, ignored listing per whitelist
Dec 13 21:05:32.868: INFO: namespace e2e-tests-sched-pred-ktvvs deletion completed in 6.102931411s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.309 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:05:32.868: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-n65g4
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-n65g4
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-n65g4
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-n65g4
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-n65g4
Dec 13 21:05:36.967: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-n65g4, name: ss-0, uid: d64081df-ff1a-11e8-b31c-0a2b404fde34, status phase: Failed. Waiting for statefulset controller to delete.
Dec 13 21:05:36.969: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-n65g4
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-n65g4
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-n65g4 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec 13 21:05:41.009: INFO: Deleting all statefulset in ns e2e-tests-statefulset-n65g4
Dec 13 21:05:41.012: INFO: Scaling statefulset ss to 0
Dec 13 21:05:51.030: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 21:05:51.033: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:05:51.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-n65g4" for this suite.
Dec 13 21:05:57.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:05:57.089: INFO: namespace: e2e-tests-statefulset-n65g4, resource: bindings, ignored listing per whitelist
Dec 13 21:05:57.156: INFO: namespace e2e-tests-statefulset-n65g4 deletion completed in 6.099653901s

• [SLOW TEST:24.289 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:05:57.157: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Dec 13 21:05:57.229: INFO: PodSpec: initContainers in spec.initContainers
Dec 13 21:06:40.350: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e2b13009-ff1a-11e8-b666-eabb5e37e61c", GenerateName:"", Namespace:"e2e-tests-init-container-4rh4n", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-4rh4n/pods/pod-init-e2b13009-ff1a-11e8-b666-eabb5e37e61c", UID:"e2b1fa41-ff1a-11e8-b982-02b9355b966e", ResourceVersion:"64267", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63680331957, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"time":"229573413", "name":"foo"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.3.34/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-57dvh", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001e5fc80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-57dvh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-57dvh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-57dvh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001bf1ff8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-3-214.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002d3f620), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00218a070)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00218a090)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00218a098), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00218a09c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680331957, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680331957, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680331957, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63680331957, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.3.214", PodIP:"192.168.3.34", StartTime:(*v1.Time)(0xc0008b9400), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000612070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006120e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:2a03a6059f21e150ae84b0973863609494aad70f0a80eaeb64bddd8d92465812", ContainerID:"containerd://b203c1a174992296a263fdb15cc56a8c8259564e54fb8b8072780ab44d62a1f3"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0008b9460), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0008b9420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:06:40.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-4rh4n" for this suite.
Dec 13 21:07:02.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:07:02.467: INFO: namespace: e2e-tests-init-container-4rh4n, resource: bindings, ignored listing per whitelist
Dec 13 21:07:02.472: INFO: namespace e2e-tests-init-container-4rh4n deletion completed in 22.109297469s

• [SLOW TEST:65.315 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:07:02.472: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-099fb8ef-ff1b-11e8-b666-eabb5e37e61c
STEP: Creating secret with name s-test-opt-upd-099fb93c-ff1b-11e8-b666-eabb5e37e61c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-099fb8ef-ff1b-11e8-b666-eabb5e37e61c
STEP: Updating secret s-test-opt-upd-099fb93c-ff1b-11e8-b666-eabb5e37e61c
STEP: Creating secret with name s-test-opt-create-099fb95a-ff1b-11e8-b666-eabb5e37e61c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:07:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9x7fh" for this suite.
Dec 13 21:07:30.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:07:30.729: INFO: namespace: e2e-tests-projected-9x7fh, resource: bindings, ignored listing per whitelist
Dec 13 21:07:30.729: INFO: namespace e2e-tests-projected-9x7fh deletion completed in 22.096567908s

• [SLOW TEST:28.257 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:07:30.729: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1213 21:08:01.340888      14 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 13 21:08:01.341: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:08:01.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-txsr5" for this suite.
Dec 13 21:08:07.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:08:07.410: INFO: namespace: e2e-tests-gc-txsr5, resource: bindings, ignored listing per whitelist
Dec 13 21:08:07.443: INFO: namespace e2e-tests-gc-txsr5 deletion completed in 6.099424452s

• [SLOW TEST:36.714 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:08:07.444: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 13 21:08:07.524: INFO: Waiting up to 5m0s for pod "pod-305966dd-ff1b-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-tcp8q" to be "success or failure"
Dec 13 21:08:07.530: INFO: Pod "pod-305966dd-ff1b-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.11983ms
Dec 13 21:08:09.533: INFO: Pod "pod-305966dd-ff1b-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009658285s
STEP: Saw pod success
Dec 13 21:08:09.533: INFO: Pod "pod-305966dd-ff1b-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:08:09.537: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-305966dd-ff1b-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 21:08:09.554: INFO: Waiting for pod pod-305966dd-ff1b-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:08:09.556: INFO: Pod pod-305966dd-ff1b-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:08:09.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tcp8q" for this suite.
Dec 13 21:08:15.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:08:15.584: INFO: namespace: e2e-tests-emptydir-tcp8q, resource: bindings, ignored listing per whitelist
Dec 13 21:08:15.660: INFO: namespace e2e-tests-emptydir-tcp8q deletion completed in 6.100627836s

• [SLOW TEST:8.216 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:08:15.661: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:08:21.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-s9b2w" for this suite.
Dec 13 21:08:27.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:08:27.902: INFO: namespace: e2e-tests-namespaces-s9b2w, resource: bindings, ignored listing per whitelist
Dec 13 21:08:27.970: INFO: namespace e2e-tests-namespaces-s9b2w deletion completed in 6.096324344s
STEP: Destroying namespace "e2e-tests-nsdeletetest-v7f4x" for this suite.
Dec 13 21:08:27.973: INFO: Namespace e2e-tests-nsdeletetest-v7f4x was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-s7bw2" for this suite.
Dec 13 21:08:33.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:08:34.038: INFO: namespace: e2e-tests-nsdeletetest-s7bw2, resource: bindings, ignored listing per whitelist
Dec 13 21:08:34.066: INFO: namespace e2e-tests-nsdeletetest-s7bw2 deletion completed in 6.09363911s

• [SLOW TEST:18.406 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:08:34.067: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 13 21:08:34.144: INFO: Waiting up to 5m0s for pod "pod-403790ea-ff1b-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-emptydir-qr6cc" to be "success or failure"
Dec 13 21:08:34.148: INFO: Pod "pod-403790ea-ff1b-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.761065ms
Dec 13 21:08:36.151: INFO: Pod "pod-403790ea-ff1b-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007163999s
STEP: Saw pod success
Dec 13 21:08:36.151: INFO: Pod "pod-403790ea-ff1b-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:08:36.154: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-403790ea-ff1b-11e8-b666-eabb5e37e61c container test-container: <nil>
STEP: delete the pod
Dec 13 21:08:36.169: INFO: Waiting for pod pod-403790ea-ff1b-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:08:36.172: INFO: Pod pod-403790ea-ff1b-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:08:36.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qr6cc" for this suite.
Dec 13 21:08:42.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:08:42.258: INFO: namespace: e2e-tests-emptydir-qr6cc, resource: bindings, ignored listing per whitelist
Dec 13 21:08:42.277: INFO: namespace e2e-tests-emptydir-qr6cc deletion completed in 6.100150571s

• [SLOW TEST:8.210 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:08:42.277: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-451be6ee-ff1b-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 21:08:42.357: INFO: Waiting up to 5m0s for pod "pod-configmaps-451ca670-ff1b-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-configmap-5pv54" to be "success or failure"
Dec 13 21:08:42.360: INFO: Pod "pod-configmaps-451ca670-ff1b-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.175821ms
Dec 13 21:08:44.364: INFO: Pod "pod-configmaps-451ca670-ff1b-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006778639s
STEP: Saw pod success
Dec 13 21:08:44.364: INFO: Pod "pod-configmaps-451ca670-ff1b-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:08:44.367: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-configmaps-451ca670-ff1b-11e8-b666-eabb5e37e61c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 21:08:44.384: INFO: Waiting for pod pod-configmaps-451ca670-ff1b-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:08:44.386: INFO: Pod pod-configmaps-451ca670-ff1b-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:08:44.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5pv54" for this suite.
Dec 13 21:08:50.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:08:50.428: INFO: namespace: e2e-tests-configmap-5pv54, resource: bindings, ignored listing per whitelist
Dec 13 21:08:50.502: INFO: namespace e2e-tests-configmap-5pv54 deletion completed in 6.111339235s

• [SLOW TEST:8.225 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:08:50.503: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Dec 13 21:08:51.087: INFO: Waiting up to 5m0s for pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-vhw7k" in namespace "e2e-tests-svcaccounts-p42hn" to be "success or failure"
Dec 13 21:08:51.091: INFO: Pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-vhw7k": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039167ms
Dec 13 21:08:53.098: INFO: Pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-vhw7k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010419329s
STEP: Saw pod success
Dec 13 21:08:53.098: INFO: Pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-vhw7k" satisfied condition "success or failure"
Dec 13 21:08:53.101: INFO: Trying to get logs from node ip-10-0-2-167.eu-west-1.compute.internal pod pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-vhw7k container token-test: <nil>
STEP: delete the pod
Dec 13 21:08:53.120: INFO: Waiting for pod pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-vhw7k to disappear
Dec 13 21:08:53.122: INFO: Pod pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-vhw7k no longer exists
STEP: Creating a pod to test consume service account root CA
Dec 13 21:08:53.130: INFO: Waiting up to 5m0s for pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-rmmfd" in namespace "e2e-tests-svcaccounts-p42hn" to be "success or failure"
Dec 13 21:08:53.135: INFO: Pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-rmmfd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277204ms
Dec 13 21:08:55.143: INFO: Pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-rmmfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012691434s
STEP: Saw pod success
Dec 13 21:08:55.143: INFO: Pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-rmmfd" satisfied condition "success or failure"
Dec 13 21:08:55.146: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-rmmfd container root-ca-test: <nil>
STEP: delete the pod
Dec 13 21:08:55.166: INFO: Waiting for pod pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-rmmfd to disappear
Dec 13 21:08:55.169: INFO: Pod pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-rmmfd no longer exists
STEP: Creating a pod to test consume service account namespace
Dec 13 21:08:55.174: INFO: Waiting up to 5m0s for pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-98rbg" in namespace "e2e-tests-svcaccounts-p42hn" to be "success or failure"
Dec 13 21:08:55.177: INFO: Pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-98rbg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.411488ms
Dec 13 21:08:57.181: INFO: Pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-98rbg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007049418s
STEP: Saw pod success
Dec 13 21:08:57.181: INFO: Pod "pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-98rbg" satisfied condition "success or failure"
Dec 13 21:08:57.184: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-98rbg container namespace-test: <nil>
STEP: delete the pod
Dec 13 21:08:57.201: INFO: Waiting for pod pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-98rbg to disappear
Dec 13 21:08:57.204: INFO: Pod pod-service-account-4a50959a-ff1b-11e8-b666-eabb5e37e61c-98rbg no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:08:57.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-p42hn" for this suite.
Dec 13 21:09:03.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:09:03.288: INFO: namespace: e2e-tests-svcaccounts-p42hn, resource: bindings, ignored listing per whitelist
Dec 13 21:09:03.309: INFO: namespace e2e-tests-svcaccounts-p42hn deletion completed in 6.101711486s

• [SLOW TEST:12.806 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:09:03.309: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-spmm4
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Dec 13 21:09:03.389: INFO: Found 0 stateful pods, waiting for 3
Dec 13 21:09:13.397: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 21:09:13.397: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 21:09:13.397: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 13 21:09:13.424: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 13 21:09:23.457: INFO: Updating stateful set ss2
Dec 13 21:09:23.463: INFO: Waiting for Pod e2e-tests-statefulset-spmm4/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Dec 13 21:09:33.508: INFO: Found 1 stateful pods, waiting for 3
Dec 13 21:09:43.517: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 21:09:43.517: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 21:09:43.517: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 13 21:09:43.541: INFO: Updating stateful set ss2
Dec 13 21:09:43.546: INFO: Waiting for Pod e2e-tests-statefulset-spmm4/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Dec 13 21:09:53.574: INFO: Updating stateful set ss2
Dec 13 21:09:53.580: INFO: Waiting for StatefulSet e2e-tests-statefulset-spmm4/ss2 to complete update
Dec 13 21:09:53.580: INFO: Waiting for Pod e2e-tests-statefulset-spmm4/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec 13 21:10:03.590: INFO: Deleting all statefulset in ns e2e-tests-statefulset-spmm4
Dec 13 21:10:03.593: INFO: Scaling statefulset ss2 to 0
Dec 13 21:10:23.607: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 21:10:23.610: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:10:23.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-spmm4" for this suite.
Dec 13 21:10:29.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:10:29.677: INFO: namespace: e2e-tests-statefulset-spmm4, resource: bindings, ignored listing per whitelist
Dec 13 21:10:29.725: INFO: namespace e2e-tests-statefulset-spmm4 deletion completed in 6.096325081s

• [SLOW TEST:86.416 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:10:29.725: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-d8x5b
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 13 21:10:29.796: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 13 21:10:47.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.3.46:8080/dial?request=hostName&protocol=udp&host=192.168.4.110&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-d8x5b PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:10:47.862: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 21:10:47.997: INFO: Waiting for endpoints: map[]
Dec 13 21:10:48.001: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.3.46:8080/dial?request=hostName&protocol=udp&host=192.168.3.45&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-d8x5b PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 13 21:10:48.001: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
Dec 13 21:10:48.122: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:10:48.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-d8x5b" for this suite.
Dec 13 21:11:10.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:11:10.219: INFO: namespace: e2e-tests-pod-network-test-d8x5b, resource: bindings, ignored listing per whitelist
Dec 13 21:11:10.222: INFO: namespace e2e-tests-pod-network-test-d8x5b deletion completed in 22.096003912s

• [SLOW TEST:40.497 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:11:10.222: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9d4b1502-ff1b-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 21:11:10.303: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9d4b9b84-ff1b-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-mf9kz" to be "success or failure"
Dec 13 21:11:10.306: INFO: Pod "pod-projected-configmaps-9d4b9b84-ff1b-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.957916ms
Dec 13 21:11:12.310: INFO: Pod "pod-projected-configmaps-9d4b9b84-ff1b-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006498726s
STEP: Saw pod success
Dec 13 21:11:12.310: INFO: Pod "pod-projected-configmaps-9d4b9b84-ff1b-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:11:12.313: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-configmaps-9d4b9b84-ff1b-11e8-b666-eabb5e37e61c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 21:11:12.335: INFO: Waiting for pod pod-projected-configmaps-9d4b9b84-ff1b-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:11:12.352: INFO: Pod pod-projected-configmaps-9d4b9b84-ff1b-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:11:12.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mf9kz" for this suite.
Dec 13 21:11:18.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:11:18.395: INFO: namespace: e2e-tests-projected-mf9kz, resource: bindings, ignored listing per whitelist
Dec 13 21:11:18.455: INFO: namespace e2e-tests-projected-mf9kz deletion completed in 6.099394243s

• [SLOW TEST:8.233 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:11:18.455: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-nfl9
STEP: Creating a pod to test atomic-volume-subpath
Dec 13 21:11:18.535: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nfl9" in namespace "e2e-tests-subpath-298jz" to be "success or failure"
Dec 13 21:11:18.537: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.818703ms
Dec 13 21:11:20.541: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006196134s
Dec 13 21:11:22.544: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 4.009686926s
Dec 13 21:11:24.548: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 6.013187497s
Dec 13 21:11:26.552: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 8.016884398s
Dec 13 21:11:28.559: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 10.024423264s
Dec 13 21:11:30.563: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 12.028137499s
Dec 13 21:11:32.566: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 14.031712886s
Dec 13 21:11:34.570: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 16.035210475s
Dec 13 21:11:36.574: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 18.038890979s
Dec 13 21:11:38.593: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 20.058349583s
Dec 13 21:11:40.597: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Running", Reason="", readiness=false. Elapsed: 22.062342578s
Dec 13 21:11:42.601: INFO: Pod "pod-subpath-test-configmap-nfl9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.065904267s
STEP: Saw pod success
Dec 13 21:11:42.601: INFO: Pod "pod-subpath-test-configmap-nfl9" satisfied condition "success or failure"
Dec 13 21:11:42.603: INFO: Trying to get logs from node ip-10-0-2-167.eu-west-1.compute.internal pod pod-subpath-test-configmap-nfl9 container test-container-subpath-configmap-nfl9: <nil>
STEP: delete the pod
Dec 13 21:11:42.623: INFO: Waiting for pod pod-subpath-test-configmap-nfl9 to disappear
Dec 13 21:11:42.632: INFO: Pod pod-subpath-test-configmap-nfl9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nfl9
Dec 13 21:11:42.632: INFO: Deleting pod "pod-subpath-test-configmap-nfl9" in namespace "e2e-tests-subpath-298jz"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:11:42.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-298jz" for this suite.
Dec 13 21:11:48.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:11:48.702: INFO: namespace: e2e-tests-subpath-298jz, resource: bindings, ignored listing per whitelist
Dec 13 21:11:48.738: INFO: namespace e2e-tests-subpath-298jz deletion completed in 6.099755891s

• [SLOW TEST:30.283 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:11:48.738: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 13 21:11:52.852: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 13 21:11:52.856: INFO: Pod pod-with-prestop-http-hook still exists
Dec 13 21:11:54.856: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 13 21:11:54.860: INFO: Pod pod-with-prestop-http-hook still exists
Dec 13 21:11:56.857: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 13 21:11:56.860: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:11:56.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-k4pfn" for this suite.
Dec 13 21:12:18.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:12:18.918: INFO: namespace: e2e-tests-container-lifecycle-hook-k4pfn, resource: bindings, ignored listing per whitelist
Dec 13 21:12:18.969: INFO: namespace e2e-tests-container-lifecycle-hook-k4pfn deletion completed in 22.100218113s

• [SLOW TEST:30.231 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:12:18.970: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-c645ac4e-ff1b-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 21:12:19.054: INFO: Waiting up to 5m0s for pod "pod-secrets-c6463513-ff1b-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-secrets-j2hdf" to be "success or failure"
Dec 13 21:12:19.057: INFO: Pod "pod-secrets-c6463513-ff1b-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.545954ms
Dec 13 21:12:21.064: INFO: Pod "pod-secrets-c6463513-ff1b-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010145416s
STEP: Saw pod success
Dec 13 21:12:21.064: INFO: Pod "pod-secrets-c6463513-ff1b-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:12:21.067: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-secrets-c6463513-ff1b-11e8-b666-eabb5e37e61c container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 21:12:21.084: INFO: Waiting for pod pod-secrets-c6463513-ff1b-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:12:21.087: INFO: Pod pod-secrets-c6463513-ff1b-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:12:21.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-j2hdf" for this suite.
Dec 13 21:12:27.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:12:27.127: INFO: namespace: e2e-tests-secrets-j2hdf, resource: bindings, ignored listing per whitelist
Dec 13 21:12:27.185: INFO: namespace e2e-tests-secrets-j2hdf deletion completed in 6.094756344s

• [SLOW TEST:8.215 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:12:27.185: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:12:29.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-pctj9" for this suite.
Dec 13 21:13:07.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:13:07.344: INFO: namespace: e2e-tests-kubelet-test-pctj9, resource: bindings, ignored listing per whitelist
Dec 13 21:13:07.384: INFO: namespace e2e-tests-kubelet-test-pctj9 deletion completed in 38.102275938s

• [SLOW TEST:40.199 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:13:07.384: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-e320283d-ff1b-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume configMaps
Dec 13 21:13:07.463: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e320af2d-ff1b-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-6w747" to be "success or failure"
Dec 13 21:13:07.469: INFO: Pod "pod-projected-configmaps-e320af2d-ff1b-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.763582ms
Dec 13 21:13:09.472: INFO: Pod "pod-projected-configmaps-e320af2d-ff1b-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009089463s
STEP: Saw pod success
Dec 13 21:13:09.472: INFO: Pod "pod-projected-configmaps-e320af2d-ff1b-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:13:09.475: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-configmaps-e320af2d-ff1b-11e8-b666-eabb5e37e61c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 13 21:13:09.491: INFO: Waiting for pod pod-projected-configmaps-e320af2d-ff1b-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:13:09.493: INFO: Pod pod-projected-configmaps-e320af2d-ff1b-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:13:09.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6w747" for this suite.
Dec 13 21:13:15.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:13:15.517: INFO: namespace: e2e-tests-projected-6w747, resource: bindings, ignored listing per whitelist
Dec 13 21:13:15.590: INFO: namespace e2e-tests-projected-6w747 deletion completed in 6.093940999s

• [SLOW TEST:8.206 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:13:15.591: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-79md2.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-79md2.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-79md2.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-79md2.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-79md2.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-79md2.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 13 21:13:27.822: INFO: DNS probes using e2e-tests-dns-79md2/dns-test-e805b4bc-ff1b-11e8-b666-eabb5e37e61c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:13:27.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-79md2" for this suite.
Dec 13 21:13:33.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:13:33.927: INFO: namespace: e2e-tests-dns-79md2, resource: bindings, ignored listing per whitelist
Dec 13 21:13:33.934: INFO: namespace e2e-tests-dns-79md2 deletion completed in 6.096229115s

• [SLOW TEST:18.344 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:13:33.935: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-f2f3c556-ff1b-11e8-b666-eabb5e37e61c
STEP: Creating a pod to test consume secrets
Dec 13 21:13:34.016: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f2f47ea2-ff1b-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-projected-5tg2q" to be "success or failure"
Dec 13 21:13:34.019: INFO: Pod "pod-projected-secrets-f2f47ea2-ff1b-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641689ms
Dec 13 21:13:36.022: INFO: Pod "pod-projected-secrets-f2f47ea2-ff1b-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00633962s
STEP: Saw pod success
Dec 13 21:13:36.022: INFO: Pod "pod-projected-secrets-f2f47ea2-ff1b-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:13:36.025: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod pod-projected-secrets-f2f47ea2-ff1b-11e8-b666-eabb5e37e61c container secret-volume-test: <nil>
STEP: delete the pod
Dec 13 21:13:36.041: INFO: Waiting for pod pod-projected-secrets-f2f47ea2-ff1b-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:13:36.043: INFO: Pod pod-projected-secrets-f2f47ea2-ff1b-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:13:36.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5tg2q" for this suite.
Dec 13 21:13:42.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:13:42.066: INFO: namespace: e2e-tests-projected-5tg2q, resource: bindings, ignored listing per whitelist
Dec 13 21:13:42.143: INFO: namespace e2e-tests-projected-5tg2q deletion completed in 6.096595994s

• [SLOW TEST:8.208 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:13:42.144: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Dec 13 21:13:42.216: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7d7686c-ff1b-11e8-b666-eabb5e37e61c" in namespace "e2e-tests-downward-api-zh7wl" to be "success or failure"
Dec 13 21:13:42.221: INFO: Pod "downwardapi-volume-f7d7686c-ff1b-11e8-b666-eabb5e37e61c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.891008ms
Dec 13 21:13:44.224: INFO: Pod "downwardapi-volume-f7d7686c-ff1b-11e8-b666-eabb5e37e61c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008372778s
STEP: Saw pod success
Dec 13 21:13:44.224: INFO: Pod "downwardapi-volume-f7d7686c-ff1b-11e8-b666-eabb5e37e61c" satisfied condition "success or failure"
Dec 13 21:13:44.227: INFO: Trying to get logs from node ip-10-0-3-214.eu-west-1.compute.internal pod downwardapi-volume-f7d7686c-ff1b-11e8-b666-eabb5e37e61c container client-container: <nil>
STEP: delete the pod
Dec 13 21:13:44.243: INFO: Waiting for pod downwardapi-volume-f7d7686c-ff1b-11e8-b666-eabb5e37e61c to disappear
Dec 13 21:13:44.249: INFO: Pod downwardapi-volume-f7d7686c-ff1b-11e8-b666-eabb5e37e61c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:13:44.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-zh7wl" for this suite.
Dec 13 21:13:50.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:13:50.329: INFO: namespace: e2e-tests-downward-api-zh7wl, resource: bindings, ignored listing per whitelist
Dec 13 21:13:50.356: INFO: namespace e2e-tests-downward-api-zh7wl deletion completed in 6.103214971s

• [SLOW TEST:8.212 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:13:50.356: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-97pm5
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-97pm5
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-97pm5
Dec 13 21:13:50.440: INFO: Found 0 stateful pods, waiting for 1
Dec 13 21:14:00.448: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 13 21:14:00.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-97pm5 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 21:14:00.633: INFO: stderr: ""
Dec 13 21:14:00.633: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 21:14:00.633: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 13 21:14:00.636: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 13 21:14:10.644: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 21:14:10.644: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 21:14:10.656: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Dec 13 21:14:10.657: INFO: ss-0  ip-10-0-3-214.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  }]
Dec 13 21:14:10.657: INFO: 
Dec 13 21:14:10.657: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 13 21:14:11.660: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996809436s
Dec 13 21:14:12.664: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992954586s
Dec 13 21:14:13.668: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989055888s
Dec 13 21:14:14.672: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985092402s
Dec 13 21:14:15.676: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981373899s
Dec 13 21:14:16.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975042632s
Dec 13 21:14:17.694: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971277118s
Dec 13 21:14:18.698: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959414786s
Dec 13 21:14:19.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.521649ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-97pm5
Dec 13 21:14:20.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-97pm5 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 13 21:14:20.894: INFO: stderr: ""
Dec 13 21:14:20.894: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 13 21:14:20.894: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 13 21:14:20.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-97pm5 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 13 21:14:21.080: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Dec 13 21:14:21.080: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 13 21:14:21.080: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 13 21:14:21.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-97pm5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 13 21:14:21.259: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Dec 13 21:14:21.259: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 13 21:14:21.259: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 13 21:14:21.263: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 21:14:21.263: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 13 21:14:21.263: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 13 21:14:21.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-97pm5 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 21:14:21.442: INFO: stderr: ""
Dec 13 21:14:21.442: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 21:14:21.442: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 13 21:14:21.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-97pm5 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 21:14:21.656: INFO: stderr: ""
Dec 13 21:14:21.656: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 21:14:21.656: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 13 21:14:21.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 exec --namespace=e2e-tests-statefulset-97pm5 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 13 21:14:21.843: INFO: stderr: ""
Dec 13 21:14:21.843: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 13 21:14:21.843: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 13 21:14:21.843: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 21:14:21.847: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 13 21:14:31.858: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 21:14:31.858: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 21:14:31.858: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 13 21:14:31.867: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Dec 13 21:14:31.867: INFO: ss-0  ip-10-0-3-214.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  }]
Dec 13 21:14:31.867: INFO: ss-1  ip-10-0-2-167.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  }]
Dec 13 21:14:31.867: INFO: ss-2  ip-10-0-3-214.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  }]
Dec 13 21:14:31.867: INFO: 
Dec 13 21:14:31.867: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 13 21:14:32.871: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Dec 13 21:14:32.871: INFO: ss-0  ip-10-0-3-214.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  }]
Dec 13 21:14:32.871: INFO: ss-1  ip-10-0-2-167.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  }]
Dec 13 21:14:32.871: INFO: ss-2  ip-10-0-3-214.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  }]
Dec 13 21:14:32.871: INFO: 
Dec 13 21:14:32.871: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 13 21:14:33.875: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Dec 13 21:14:33.875: INFO: ss-0  ip-10-0-3-214.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  }]
Dec 13 21:14:33.875: INFO: ss-1  ip-10-0-2-167.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  }]
Dec 13 21:14:33.875: INFO: ss-2  ip-10-0-3-214.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  }]
Dec 13 21:14:33.875: INFO: 
Dec 13 21:14:33.875: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 13 21:14:34.881: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Dec 13 21:14:34.881: INFO: ss-0  ip-10-0-3-214.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  }]
Dec 13 21:14:34.881: INFO: ss-1  ip-10-0-2-167.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  }]
Dec 13 21:14:34.881: INFO: 
Dec 13 21:14:34.881: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 13 21:14:35.885: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Dec 13 21:14:35.885: INFO: ss-0  ip-10-0-3-214.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  }]
Dec 13 21:14:35.885: INFO: ss-1  ip-10-0-2-167.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  }]
Dec 13 21:14:35.885: INFO: 
Dec 13 21:14:35.885: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 13 21:14:36.889: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Dec 13 21:14:36.889: INFO: ss-0  ip-10-0-3-214.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  }]
Dec 13 21:14:36.889: INFO: ss-1  ip-10-0-2-167.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:10 +0000 UTC  }]
Dec 13 21:14:36.889: INFO: 
Dec 13 21:14:36.889: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 13 21:14:38.015: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Dec 13 21:14:38.015: INFO: ss-0  ip-10-0-3-214.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:14:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-12-13 21:13:50 +0000 UTC  }]
Dec 13 21:14:38.015: INFO: 
Dec 13 21:14:38.015: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 13 21:14:39.019: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.848966975s
Dec 13 21:14:40.023: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.845403714s
Dec 13 21:14:41.027: INFO: Verifying statefulset ss doesn't scale past 0 for another 841.087896ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-97pm5
Dec 13 21:14:42.034: INFO: Scaling statefulset ss to 0
Dec 13 21:14:42.042: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec 13 21:14:42.045: INFO: Deleting all statefulset in ns e2e-tests-statefulset-97pm5
Dec 13 21:14:42.047: INFO: Scaling statefulset ss to 0
Dec 13 21:14:42.055: INFO: Waiting for statefulset status.replicas updated to 0
Dec 13 21:14:42.058: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:14:42.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-97pm5" for this suite.
Dec 13 21:14:48.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:14:48.166: INFO: namespace: e2e-tests-statefulset-97pm5, resource: bindings, ignored listing per whitelist
Dec 13 21:14:48.172: INFO: namespace e2e-tests-statefulset-97pm5 deletion completed in 6.099715667s

• [SLOW TEST:57.816 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:14:48.173: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Dec 13 21:14:50.784: INFO: Successfully updated pod "labelsupdate1f341b67-ff1c-11e8-b666-eabb5e37e61c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:14:54.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fsmjm" for this suite.
Dec 13 21:15:08.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:15:08.892: INFO: namespace: e2e-tests-projected-fsmjm, resource: bindings, ignored listing per whitelist
Dec 13 21:15:08.919: INFO: namespace e2e-tests-projected-fsmjm deletion completed in 14.104212403s

• [SLOW TEST:20.747 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Dec 13 21:15:08.920: INFO: >>> kubeConfig: /tmp/kubeconfig-413709859
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Dec 13 21:15:08.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-413709859 version'
Dec 13 21:15:09.051: INFO: stderr: ""
Dec 13 21:15:09.051: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.1\", GitCommit:\"eec55b9ba98609a46fee712359c7b5b365bdd920\", GitTreeState:\"clean\", BuildDate:\"2018-12-13T10:31:33Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Dec 13 21:15:09.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xqwvp" for this suite.
Dec 13 21:15:15.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 13 21:15:15.090: INFO: namespace: e2e-tests-kubectl-xqwvp, resource: bindings, ignored listing per whitelist
Dec 13 21:15:15.154: INFO: namespace e2e-tests-kubectl-xqwvp deletion completed in 6.098421455s

• [SLOW TEST:6.234 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SDec 13 21:15:15.154: INFO: Running AfterSuite actions on all nodes
Dec 13 21:15:15.154: INFO: Running AfterSuite actions on node 1
Dec 13 21:15:15.154: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5046.285 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h24m7.141048637s
Test Suite Passed
