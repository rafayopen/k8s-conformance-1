I0121 04:56:56.801554      15 test_context.go:358] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-453531067
I0121 04:56:56.805138      15 e2e.go:224] Starting e2e run "f9cd8265-1d38-11e9-b032-0a580af40356" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1548046615 - Will randomize all specs
Will run 201 of 1946 specs

Jan 21 04:56:57.100: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 04:56:57.102: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 21 04:56:57.133: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 21 04:56:57.209: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 21 04:56:57.209: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 21 04:56:57.209: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 21 04:56:57.244: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Jan 21 04:56:57.245: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 21 04:56:57.245: INFO: e2e test version: v1.13.0
Jan 21 04:56:57.247: INFO: kube-apiserver version: v1.13.1
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:56:57.247: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
Jan 21 04:56:57.378: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 04:56:57.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fac07f90-1d38-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-bc7rf" to be "success or failure"
Jan 21 04:56:57.408: INFO: Pod "downwardapi-volume-fac07f90-1d38-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 10.154139ms
Jan 21 04:56:59.418: INFO: Pod "downwardapi-volume-fac07f90-1d38-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020081251s
STEP: Saw pod success
Jan 21 04:56:59.418: INFO: Pod "downwardapi-volume-fac07f90-1d38-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 04:56:59.424: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-fac07f90-1d38-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 04:56:59.460: INFO: Waiting for pod downwardapi-volume-fac07f90-1d38-11e9-b032-0a580af40356 to disappear
Jan 21 04:56:59.465: INFO: Pod downwardapi-volume-fac07f90-1d38-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 04:56:59.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bc7rf" for this suite.
Jan 21 04:57:05.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 04:57:05.506: INFO: namespace: e2e-tests-projected-bc7rf, resource: bindings, ignored listing per whitelist
Jan 21 04:57:05.657: INFO: namespace e2e-tests-projected-bc7rf deletion completed in 6.184808849s

â€¢ [SLOW TEST:8.410 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:57:05.657: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4k2r4 in namespace e2e-tests-proxy-m4mtk
I0121 04:57:05.824846      15 runners.go:184] Created replication controller with name: proxy-service-4k2r4, namespace: e2e-tests-proxy-m4mtk, replica count: 1
I0121 04:57:06.875638      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 04:57:07.875906      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0121 04:57:08.876150      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0121 04:57:09.876456      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0121 04:57:10.876713      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0121 04:57:11.877037      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0121 04:57:12.877302      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0121 04:57:13.877835      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0121 04:57:14.878116      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0121 04:57:15.878314      15 runners.go:184] proxy-service-4k2r4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 04:57:15.894: INFO: setup took 10.13317671s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 21 04:57:15.924: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 29.385383ms)
Jan 21 04:57:15.929: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 34.877843ms)
Jan 21 04:57:15.929: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 34.500684ms)
Jan 21 04:57:15.930: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 34.486264ms)
Jan 21 04:57:15.930: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 34.818246ms)
Jan 21 04:57:15.935: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 40.492062ms)
Jan 21 04:57:15.935: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 40.780401ms)
Jan 21 04:57:15.935: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 40.357533ms)
Jan 21 04:57:15.935: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 40.577761ms)
Jan 21 04:57:15.936: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 40.437635ms)
Jan 21 04:57:15.936: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 40.398964ms)
Jan 21 04:57:15.936: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 40.388897ms)
Jan 21 04:57:15.938: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 42.977896ms)
Jan 21 04:57:15.938: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 42.869724ms)
Jan 21 04:57:15.939: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 44.839686ms)
Jan 21 04:57:15.943: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 48.476372ms)
Jan 21 04:57:15.951: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 7.329917ms)
Jan 21 04:57:15.953: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 9.222699ms)
Jan 21 04:57:15.953: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 9.678979ms)
Jan 21 04:57:15.957: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 13.013648ms)
Jan 21 04:57:15.959: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 15.083253ms)
Jan 21 04:57:15.960: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 15.662995ms)
Jan 21 04:57:15.960: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 15.947794ms)
Jan 21 04:57:15.963: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 19.187641ms)
Jan 21 04:57:15.964: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 19.635164ms)
Jan 21 04:57:15.967: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 22.309943ms)
Jan 21 04:57:15.967: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 22.552657ms)
Jan 21 04:57:15.967: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 22.989729ms)
Jan 21 04:57:15.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 23.508569ms)
Jan 21 04:57:15.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 23.439729ms)
Jan 21 04:57:15.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 24.073016ms)
Jan 21 04:57:15.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 24.277773ms)
Jan 21 04:57:15.985: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 15.111741ms)
Jan 21 04:57:15.985: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 16.221218ms)
Jan 21 04:57:15.986: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 17.407556ms)
Jan 21 04:57:15.988: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 19.263455ms)
Jan 21 04:57:15.989: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 18.95836ms)
Jan 21 04:57:15.989: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 19.38264ms)
Jan 21 04:57:15.989: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 19.662958ms)
Jan 21 04:57:15.990: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 20.257936ms)
Jan 21 04:57:15.990: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 19.626577ms)
Jan 21 04:57:15.990: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 21.867422ms)
Jan 21 04:57:15.993: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 24.098998ms)
Jan 21 04:57:15.994: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 23.831773ms)
Jan 21 04:57:15.994: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 24.086437ms)
Jan 21 04:57:15.995: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 25.364877ms)
Jan 21 04:57:15.996: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 26.061715ms)
Jan 21 04:57:15.996: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 27.352225ms)
Jan 21 04:57:16.008: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 10.383371ms)
Jan 21 04:57:16.012: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 14.857749ms)
Jan 21 04:57:16.012: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 15.397034ms)
Jan 21 04:57:16.012: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 15.998351ms)
Jan 21 04:57:16.012: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 15.225059ms)
Jan 21 04:57:16.013: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 16.433684ms)
Jan 21 04:57:16.013: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 15.854365ms)
Jan 21 04:57:16.013: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 16.124986ms)
Jan 21 04:57:16.014: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 17.304368ms)
Jan 21 04:57:16.014: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 16.589785ms)
Jan 21 04:57:16.017: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 20.051064ms)
Jan 21 04:57:16.019: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 21.676027ms)
Jan 21 04:57:16.019: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 22.334705ms)
Jan 21 04:57:16.020: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 22.404522ms)
Jan 21 04:57:16.020: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 23.174356ms)
Jan 21 04:57:16.020: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 23.503743ms)
Jan 21 04:57:16.027: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 5.991389ms)
Jan 21 04:57:16.032: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 10.590353ms)
Jan 21 04:57:16.033: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 11.766076ms)
Jan 21 04:57:16.034: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 11.769312ms)
Jan 21 04:57:16.035: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 13.003525ms)
Jan 21 04:57:16.035: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 12.585847ms)
Jan 21 04:57:16.035: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 14.416403ms)
Jan 21 04:57:16.036: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 15.133829ms)
Jan 21 04:57:16.037: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 14.723825ms)
Jan 21 04:57:16.037: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 16.042946ms)
Jan 21 04:57:16.038: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 16.459555ms)
Jan 21 04:57:16.040: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 19.080034ms)
Jan 21 04:57:16.040: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 19.53201ms)
Jan 21 04:57:16.042: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 20.417862ms)
Jan 21 04:57:16.042: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 19.765138ms)
Jan 21 04:57:16.042: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 20.313732ms)
Jan 21 04:57:16.049: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 6.104039ms)
Jan 21 04:57:16.052: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 8.433813ms)
Jan 21 04:57:16.052: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 8.900895ms)
Jan 21 04:57:16.054: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 11.538165ms)
Jan 21 04:57:16.054: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 11.015817ms)
Jan 21 04:57:16.058: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 14.902352ms)
Jan 21 04:57:16.061: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 17.132294ms)
Jan 21 04:57:16.061: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 17.248297ms)
Jan 21 04:57:16.061: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 17.698534ms)
Jan 21 04:57:16.061: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 17.349035ms)
Jan 21 04:57:16.061: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 18.393387ms)
Jan 21 04:57:16.061: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 17.731712ms)
Jan 21 04:57:16.061: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 18.104551ms)
Jan 21 04:57:16.062: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 19.159571ms)
Jan 21 04:57:16.063: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 19.722907ms)
Jan 21 04:57:16.063: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 19.563272ms)
Jan 21 04:57:16.077: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 13.286183ms)
Jan 21 04:57:16.077: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 13.108701ms)
Jan 21 04:57:16.077: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 13.019865ms)
Jan 21 04:57:16.077: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 13.307057ms)
Jan 21 04:57:16.078: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 13.377508ms)
Jan 21 04:57:16.078: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 14.320109ms)
Jan 21 04:57:16.078: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 14.207492ms)
Jan 21 04:57:16.079: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 16.014847ms)
Jan 21 04:57:16.080: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 15.160483ms)
Jan 21 04:57:16.080: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 14.943332ms)
Jan 21 04:57:16.086: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 21.834342ms)
Jan 21 04:57:16.087: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 22.523523ms)
Jan 21 04:57:16.088: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 24.173744ms)
Jan 21 04:57:16.088: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 23.518192ms)
Jan 21 04:57:16.088: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 23.928296ms)
Jan 21 04:57:16.088: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 25.361911ms)
Jan 21 04:57:16.097: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 7.944673ms)
Jan 21 04:57:16.098: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 8.9392ms)
Jan 21 04:57:16.098: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 8.859197ms)
Jan 21 04:57:16.099: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 9.954636ms)
Jan 21 04:57:16.099: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 10.59472ms)
Jan 21 04:57:16.101: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 11.268019ms)
Jan 21 04:57:16.102: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 12.660057ms)
Jan 21 04:57:16.102: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 12.453199ms)
Jan 21 04:57:16.104: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 15.325675ms)
Jan 21 04:57:16.104: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 15.5704ms)
Jan 21 04:57:16.107: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 16.937016ms)
Jan 21 04:57:16.107: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 17.035688ms)
Jan 21 04:57:16.109: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 18.924416ms)
Jan 21 04:57:16.109: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 18.914907ms)
Jan 21 04:57:16.110: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 19.251833ms)
Jan 21 04:57:16.110: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 19.399869ms)
Jan 21 04:57:16.131: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 20.381419ms)
Jan 21 04:57:16.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 21.063142ms)
Jan 21 04:57:16.132: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 21.793566ms)
Jan 21 04:57:16.133: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 22.082845ms)
Jan 21 04:57:16.133: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 21.990548ms)
Jan 21 04:57:16.135: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 25.028339ms)
Jan 21 04:57:16.136: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 25.038717ms)
Jan 21 04:57:16.136: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 25.805358ms)
Jan 21 04:57:16.136: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 24.729305ms)
Jan 21 04:57:16.138: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 26.845489ms)
Jan 21 04:57:16.138: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 28.534926ms)
Jan 21 04:57:16.139: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 27.139893ms)
Jan 21 04:57:16.140: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 28.354912ms)
Jan 21 04:57:16.140: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 29.396446ms)
Jan 21 04:57:16.140: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 28.593023ms)
Jan 21 04:57:16.140: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 29.390098ms)
Jan 21 04:57:16.151: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 10.323604ms)
Jan 21 04:57:16.152: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 11.256638ms)
Jan 21 04:57:16.153: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 12.185601ms)
Jan 21 04:57:16.153: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 12.884424ms)
Jan 21 04:57:16.154: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 13.972199ms)
Jan 21 04:57:16.154: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 14.324248ms)
Jan 21 04:57:16.154: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 14.386471ms)
Jan 21 04:57:16.155: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 14.390209ms)
Jan 21 04:57:16.155: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 14.688316ms)
Jan 21 04:57:16.155: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 15.015458ms)
Jan 21 04:57:16.156: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 15.252289ms)
Jan 21 04:57:16.157: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 17.107509ms)
Jan 21 04:57:16.158: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 17.369032ms)
Jan 21 04:57:16.158: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 17.666376ms)
Jan 21 04:57:16.158: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 18.346685ms)
Jan 21 04:57:16.159: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 18.68192ms)
Jan 21 04:57:16.167: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 7.735498ms)
Jan 21 04:57:16.169: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 9.8566ms)
Jan 21 04:57:16.172: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 12.817702ms)
Jan 21 04:57:16.172: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 13.280529ms)
Jan 21 04:57:16.173: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 13.393842ms)
Jan 21 04:57:16.173: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 13.782725ms)
Jan 21 04:57:16.173: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 14.037078ms)
Jan 21 04:57:16.176: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 15.776294ms)
Jan 21 04:57:16.176: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 16.061886ms)
Jan 21 04:57:16.177: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 16.636438ms)
Jan 21 04:57:16.178: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 18.281191ms)
Jan 21 04:57:16.180: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 19.998654ms)
Jan 21 04:57:16.180: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 20.074934ms)
Jan 21 04:57:16.181: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 21.996828ms)
Jan 21 04:57:16.181: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 21.067448ms)
Jan 21 04:57:16.182: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 22.241928ms)
Jan 21 04:57:16.191: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 9.388704ms)
Jan 21 04:57:16.194: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 10.611737ms)
Jan 21 04:57:16.195: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 12.76304ms)
Jan 21 04:57:16.195: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 12.53836ms)
Jan 21 04:57:16.196: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 12.616854ms)
Jan 21 04:57:16.196: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 12.431958ms)
Jan 21 04:57:16.196: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 12.433936ms)
Jan 21 04:57:16.196: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 13.019008ms)
Jan 21 04:57:16.196: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 12.837044ms)
Jan 21 04:57:16.196: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 12.482674ms)
Jan 21 04:57:16.201: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 18.723292ms)
Jan 21 04:57:16.203: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 20.265388ms)
Jan 21 04:57:16.203: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 20.165262ms)
Jan 21 04:57:16.204: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 21.234089ms)
Jan 21 04:57:16.205: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 21.602128ms)
Jan 21 04:57:16.205: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 22.202045ms)
Jan 21 04:57:16.215: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 8.99482ms)
Jan 21 04:57:16.220: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 12.425224ms)
Jan 21 04:57:16.221: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 14.580147ms)
Jan 21 04:57:16.221: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 16.273822ms)
Jan 21 04:57:16.222: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 14.43706ms)
Jan 21 04:57:16.222: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 15.092652ms)
Jan 21 04:57:16.222: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 15.676329ms)
Jan 21 04:57:16.222: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 15.641753ms)
Jan 21 04:57:16.223: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 17.798197ms)
Jan 21 04:57:16.224: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 18.971492ms)
Jan 21 04:57:16.226: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 20.803092ms)
Jan 21 04:57:16.228: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 22.325056ms)
Jan 21 04:57:16.228: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 20.997157ms)
Jan 21 04:57:16.229: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 21.440931ms)
Jan 21 04:57:16.229: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 23.487339ms)
Jan 21 04:57:16.230: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 22.056667ms)
Jan 21 04:57:16.241: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 11.293148ms)
Jan 21 04:57:16.242: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 11.013012ms)
Jan 21 04:57:16.244: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 14.104378ms)
Jan 21 04:57:16.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 14.438545ms)
Jan 21 04:57:16.245: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 14.828547ms)
Jan 21 04:57:16.246: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 14.900459ms)
Jan 21 04:57:16.246: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 14.984569ms)
Jan 21 04:57:16.246: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 15.470035ms)
Jan 21 04:57:16.247: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 16.638507ms)
Jan 21 04:57:16.247: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 17.560125ms)
Jan 21 04:57:16.248: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 17.470085ms)
Jan 21 04:57:16.250: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 19.0176ms)
Jan 21 04:57:16.250: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 19.69134ms)
Jan 21 04:57:16.251: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 20.287194ms)
Jan 21 04:57:16.251: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 20.763407ms)
Jan 21 04:57:16.252: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 21.403885ms)
Jan 21 04:57:16.265: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 12.584374ms)
Jan 21 04:57:16.267: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 14.687915ms)
Jan 21 04:57:16.267: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 14.17729ms)
Jan 21 04:57:16.268: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 15.178085ms)
Jan 21 04:57:16.268: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 15.757102ms)
Jan 21 04:57:16.270: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 16.452524ms)
Jan 21 04:57:16.270: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 17.489785ms)
Jan 21 04:57:16.272: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 19.064908ms)
Jan 21 04:57:16.272: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 19.872152ms)
Jan 21 04:57:16.273: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 20.222402ms)
Jan 21 04:57:16.274: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 21.637258ms)
Jan 21 04:57:16.276: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 23.281772ms)
Jan 21 04:57:16.277: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 24.034341ms)
Jan 21 04:57:16.277: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 24.319123ms)
Jan 21 04:57:16.278: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 25.072936ms)
Jan 21 04:57:16.278: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 25.304628ms)
Jan 21 04:57:16.286: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 7.914191ms)
Jan 21 04:57:16.295: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 15.586939ms)
Jan 21 04:57:16.296: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 16.630081ms)
Jan 21 04:57:16.297: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 18.575076ms)
Jan 21 04:57:16.297: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 18.465168ms)
Jan 21 04:57:16.298: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 19.382699ms)
Jan 21 04:57:16.298: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 18.813037ms)
Jan 21 04:57:16.298: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 19.823656ms)
Jan 21 04:57:16.299: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 20.539606ms)
Jan 21 04:57:16.299: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 20.707579ms)
Jan 21 04:57:16.301: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 21.261328ms)
Jan 21 04:57:16.302: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 23.493116ms)
Jan 21 04:57:16.303: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 24.191934ms)
Jan 21 04:57:16.303: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 24.396491ms)
Jan 21 04:57:16.304: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 24.311234ms)
Jan 21 04:57:16.304: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 24.878238ms)
Jan 21 04:57:16.314: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 9.240143ms)
Jan 21 04:57:16.316: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 11.634945ms)
Jan 21 04:57:16.320: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 14.685438ms)
Jan 21 04:57:16.320: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 16.199094ms)
Jan 21 04:57:16.321: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 16.177769ms)
Jan 21 04:57:16.321: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 15.679045ms)
Jan 21 04:57:16.321: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 16.501303ms)
Jan 21 04:57:16.322: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 17.00038ms)
Jan 21 04:57:16.322: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 18.118238ms)
Jan 21 04:57:16.322: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 17.834596ms)
Jan 21 04:57:16.322: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 17.123127ms)
Jan 21 04:57:16.324: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 19.495101ms)
Jan 21 04:57:16.325: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 20.331719ms)
Jan 21 04:57:16.326: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 20.566317ms)
Jan 21 04:57:16.327: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 21.910367ms)
Jan 21 04:57:16.327: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 21.911216ms)
Jan 21 04:57:16.342: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 13.315599ms)
Jan 21 04:57:16.343: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 16.049038ms)
Jan 21 04:57:16.343: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 15.207766ms)
Jan 21 04:57:16.344: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 16.136802ms)
Jan 21 04:57:16.344: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 15.289862ms)
Jan 21 04:57:16.344: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 16.586765ms)
Jan 21 04:57:16.344: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 14.850347ms)
Jan 21 04:57:16.344: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 14.716593ms)
Jan 21 04:57:16.345: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 16.032509ms)
Jan 21 04:57:16.345: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 14.891748ms)
Jan 21 04:57:16.347: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 16.92956ms)
Jan 21 04:57:16.348: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 20.485155ms)
Jan 21 04:57:16.348: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 19.598568ms)
Jan 21 04:57:16.349: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 20.815329ms)
Jan 21 04:57:16.349: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 20.151349ms)
Jan 21 04:57:16.350: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 20.311564ms)
Jan 21 04:57:16.359: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 9.28126ms)
Jan 21 04:57:16.362: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 11.884734ms)
Jan 21 04:57:16.363: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 12.441242ms)
Jan 21 04:57:16.363: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 12.616367ms)
Jan 21 04:57:16.364: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 13.633234ms)
Jan 21 04:57:16.364: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 14.118064ms)
Jan 21 04:57:16.365: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 14.38953ms)
Jan 21 04:57:16.365: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 14.553011ms)
Jan 21 04:57:16.365: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 14.606742ms)
Jan 21 04:57:16.365: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 14.352558ms)
Jan 21 04:57:16.366: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 15.32317ms)
Jan 21 04:57:16.367: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 17.011526ms)
Jan 21 04:57:16.368: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 17.081371ms)
Jan 21 04:57:16.368: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 18.157882ms)
Jan 21 04:57:16.368: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 17.979028ms)
Jan 21 04:57:16.369: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 18.888086ms)
Jan 21 04:57:16.379: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:1080/proxy/... (200; 8.760582ms)
Jan 21 04:57:16.381: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 11.618909ms)
Jan 21 04:57:16.384: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:460/proxy/: tls baz (200; 14.19056ms)
Jan 21 04:57:16.384: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:160/proxy/: foo (200; 14.91505ms)
Jan 21 04:57:16.385: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2/proxy/rewriteme"... (200; 15.65119ms)
Jan 21 04:57:16.385: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:1080/proxy/rewri... (200; 15.132097ms)
Jan 21 04:57:16.385: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 15.499813ms)
Jan 21 04:57:16.385: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/http:proxy-service-4k2r4-wtrr2:162/proxy/: bar (200; 15.459732ms)
Jan 21 04:57:16.386: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname2/proxy/: tls qux (200; 16.89215ms)
Jan 21 04:57:16.386: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:462/proxy/: tls qux (200; 16.687765ms)
Jan 21 04:57:16.387: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-m4mtk/pods/https:proxy-service-4k2r4-wtrr2:443/proxy/... (200; 18.238642ms)
Jan 21 04:57:16.388: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname1/proxy/: foo (200; 18.646292ms)
Jan 21 04:57:16.389: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname2/proxy/: bar (200; 19.840138ms)
Jan 21 04:57:16.389: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/http:proxy-service-4k2r4:portname2/proxy/: bar (200; 20.156454ms)
Jan 21 04:57:16.389: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/proxy-service-4k2r4:portname1/proxy/: foo (200; 19.657509ms)
Jan 21 04:57:16.390: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-m4mtk/services/https:proxy-service-4k2r4:tlsportname1/proxy/: tls baz (200; 20.452088ms)
STEP: deleting ReplicationController proxy-service-4k2r4 in namespace e2e-tests-proxy-m4mtk, will wait for the garbage collector to delete the pods
Jan 21 04:57:16.456: INFO: Deleting ReplicationController proxy-service-4k2r4 took: 12.817752ms
Jan 21 04:57:16.557: INFO: Terminating ReplicationController proxy-service-4k2r4 pods took: 100.681598ms
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 04:57:18.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-m4mtk" for this suite.
Jan 21 04:57:24.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 04:57:24.930: INFO: namespace: e2e-tests-proxy-m4mtk, resource: bindings, ignored listing per whitelist
Jan 21 04:57:25.217: INFO: namespace e2e-tests-proxy-m4mtk deletion completed in 6.45307077s

â€¢ [SLOW TEST:19.560 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:57:25.218: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 21 04:57:25.365: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 21 04:57:30.392: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 04:57:31.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-nbm94" for this suite.
Jan 21 04:57:37.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 04:57:37.621: INFO: namespace: e2e-tests-replication-controller-nbm94, resource: bindings, ignored listing per whitelist
Jan 21 04:57:37.657: INFO: namespace e2e-tests-replication-controller-nbm94 deletion completed in 6.207382839s

â€¢ [SLOW TEST:12.440 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:57:37.658: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 21 04:57:37.805: INFO: Waiting up to 5m0s for pod "pod-12d60d78-1d39-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-xznk9" to be "success or failure"
Jan 21 04:57:37.815: INFO: Pod "pod-12d60d78-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 10.177091ms
Jan 21 04:57:39.822: INFO: Pod "pod-12d60d78-1d39-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016653427s
STEP: Saw pod success
Jan 21 04:57:39.822: INFO: Pod "pod-12d60d78-1d39-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 04:57:39.828: INFO: Trying to get logs from node k8s05 pod pod-12d60d78-1d39-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 04:57:39.903: INFO: Waiting for pod pod-12d60d78-1d39-11e9-b032-0a580af40356 to disappear
Jan 21 04:57:39.909: INFO: Pod pod-12d60d78-1d39-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 04:57:39.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xznk9" for this suite.
Jan 21 04:57:45.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 04:57:45.987: INFO: namespace: e2e-tests-emptydir-xznk9, resource: bindings, ignored listing per whitelist
Jan 21 04:57:46.093: INFO: namespace e2e-tests-emptydir-xznk9 deletion completed in 6.165371376s

â€¢ [SLOW TEST:8.435 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:57:46.093: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 04:57:46.255: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17dc7d06-1d39-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-phnlj" to be "success or failure"
Jan 21 04:57:46.268: INFO: Pod "downwardapi-volume-17dc7d06-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 12.872772ms
Jan 21 04:57:48.275: INFO: Pod "downwardapi-volume-17dc7d06-1d39-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019053552s
STEP: Saw pod success
Jan 21 04:57:48.275: INFO: Pod "downwardapi-volume-17dc7d06-1d39-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 04:57:48.279: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-17dc7d06-1d39-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 04:57:48.311: INFO: Waiting for pod downwardapi-volume-17dc7d06-1d39-11e9-b032-0a580af40356 to disappear
Jan 21 04:57:48.316: INFO: Pod downwardapi-volume-17dc7d06-1d39-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 04:57:48.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-phnlj" for this suite.
Jan 21 04:57:54.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 04:57:54.444: INFO: namespace: e2e-tests-projected-phnlj, resource: bindings, ignored listing per whitelist
Jan 21 04:57:54.503: INFO: namespace e2e-tests-projected-phnlj deletion completed in 6.180989989s

â€¢ [SLOW TEST:8.410 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:57:54.503: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 04:57:54.619: INFO: Creating ReplicaSet my-hostname-basic-1cde8841-1d39-11e9-b032-0a580af40356
Jan 21 04:57:54.639: INFO: Pod name my-hostname-basic-1cde8841-1d39-11e9-b032-0a580af40356: Found 0 pods out of 1
Jan 21 04:57:59.656: INFO: Pod name my-hostname-basic-1cde8841-1d39-11e9-b032-0a580af40356: Found 1 pods out of 1
Jan 21 04:57:59.656: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1cde8841-1d39-11e9-b032-0a580af40356" is running
Jan 21 04:57:59.661: INFO: Pod "my-hostname-basic-1cde8841-1d39-11e9-b032-0a580af40356-blnx6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-21 04:57:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-21 04:57:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-21 04:57:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-21 04:57:54 +0000 UTC Reason: Message:}])
Jan 21 04:57:59.661: INFO: Trying to dial the pod
Jan 21 04:58:04.697: INFO: Controller my-hostname-basic-1cde8841-1d39-11e9-b032-0a580af40356: Got expected result from replica 1 [my-hostname-basic-1cde8841-1d39-11e9-b032-0a580af40356-blnx6]: "my-hostname-basic-1cde8841-1d39-11e9-b032-0a580af40356-blnx6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 04:58:04.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-kr8hq" for this suite.
Jan 21 04:58:10.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 04:58:10.869: INFO: namespace: e2e-tests-replicaset-kr8hq, resource: bindings, ignored listing per whitelist
Jan 21 04:58:10.870: INFO: namespace e2e-tests-replicaset-kr8hq deletion completed in 6.165439453s

â€¢ [SLOW TEST:16.367 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:58:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-z8wk
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 04:58:11.024: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-z8wk" in namespace "e2e-tests-subpath-d7lvk" to be "success or failure"
Jan 21 04:58:11.030: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.981265ms
Jan 21 04:58:13.035: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010744027s
Jan 21 04:58:15.060: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 4.035581488s
Jan 21 04:58:17.068: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 6.043584998s
Jan 21 04:58:19.074: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 8.049453979s
Jan 21 04:58:21.083: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 10.05855417s
Jan 21 04:58:23.092: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 12.067448655s
Jan 21 04:58:25.106: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 14.081329541s
Jan 21 04:58:27.113: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 16.088369904s
Jan 21 04:58:29.119: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 18.094694596s
Jan 21 04:58:31.125: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 20.100584531s
Jan 21 04:58:33.131: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Running", Reason="", readiness=false. Elapsed: 22.106988849s
Jan 21 04:58:35.150: INFO: Pod "pod-subpath-test-secret-z8wk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.126077051s
STEP: Saw pod success
Jan 21 04:58:35.150: INFO: Pod "pod-subpath-test-secret-z8wk" satisfied condition "success or failure"
Jan 21 04:58:35.155: INFO: Trying to get logs from node k8s05 pod pod-subpath-test-secret-z8wk container test-container-subpath-secret-z8wk: <nil>
STEP: delete the pod
Jan 21 04:58:35.203: INFO: Waiting for pod pod-subpath-test-secret-z8wk to disappear
Jan 21 04:58:35.207: INFO: Pod pod-subpath-test-secret-z8wk no longer exists
STEP: Deleting pod pod-subpath-test-secret-z8wk
Jan 21 04:58:35.207: INFO: Deleting pod "pod-subpath-test-secret-z8wk" in namespace "e2e-tests-subpath-d7lvk"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 04:58:35.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-d7lvk" for this suite.
Jan 21 04:58:41.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 04:58:41.391: INFO: namespace: e2e-tests-subpath-d7lvk, resource: bindings, ignored listing per whitelist
Jan 21 04:58:41.391: INFO: namespace e2e-tests-subpath-d7lvk deletion completed in 6.165158592s

â€¢ [SLOW TEST:30.520 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:58:41.391: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jan 21 04:58:41.476: INFO: namespace e2e-tests-kubectl-bnjln
Jan 21 04:58:41.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-bnjln'
Jan 21 04:58:42.244: INFO: stderr: ""
Jan 21 04:58:42.244: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 21 04:58:43.260: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 04:58:43.260: INFO: Found 0 / 1
Jan 21 04:58:44.250: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 04:58:44.250: INFO: Found 1 / 1
Jan 21 04:58:44.250: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 21 04:58:44.265: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 04:58:44.265: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 21 04:58:44.265: INFO: wait on redis-master startup in e2e-tests-kubectl-bnjln 
Jan 21 04:58:44.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 logs redis-master-4c9z4 redis-master --namespace=e2e-tests-kubectl-bnjln'
Jan 21 04:58:44.450: INFO: stderr: ""
Jan 21 04:58:44.450: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Jan 04:58:43.217 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Jan 04:58:43.217 # Server started, Redis version 3.2.12\n1:M 21 Jan 04:58:43.218 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Jan 04:58:43.218 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jan 21 04:58:44.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-bnjln'
Jan 21 04:58:44.661: INFO: stderr: ""
Jan 21 04:58:44.661: INFO: stdout: "service/rm2 exposed\n"
Jan 21 04:58:44.672: INFO: Service rm2 in namespace e2e-tests-kubectl-bnjln found.
STEP: exposing service
Jan 21 04:58:46.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-bnjln'
Jan 21 04:58:46.904: INFO: stderr: ""
Jan 21 04:58:46.904: INFO: stdout: "service/rm3 exposed\n"
Jan 21 04:58:46.914: INFO: Service rm3 in namespace e2e-tests-kubectl-bnjln found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 04:58:48.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bnjln" for this suite.
Jan 21 04:59:10.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 04:59:11.044: INFO: namespace: e2e-tests-kubectl-bnjln, resource: bindings, ignored listing per whitelist
Jan 21 04:59:11.171: INFO: namespace e2e-tests-kubectl-bnjln deletion completed in 22.237173552s

â€¢ [SLOW TEST:29.781 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:59:11.172: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 04:59:35.374: INFO: Container started at 2019-01-21 04:59:12 +0000 UTC, pod became ready at 2019-01-21 04:59:34 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 04:59:35.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-jd7hc" for this suite.
Jan 21 04:59:57.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 04:59:57.451: INFO: namespace: e2e-tests-container-probe-jd7hc, resource: bindings, ignored listing per whitelist
Jan 21 04:59:57.560: INFO: namespace e2e-tests-container-probe-jd7hc deletion completed in 22.178764169s

â€¢ [SLOW TEST:46.389 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 04:59:57.561: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 04:59:59.753: INFO: Waiting up to 5m0s for pod "client-envvars-67727661-1d39-11e9-b032-0a580af40356" in namespace "e2e-tests-pods-9q756" to be "success or failure"
Jan 21 04:59:59.760: INFO: Pod "client-envvars-67727661-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 6.855381ms
Jan 21 05:00:01.766: INFO: Pod "client-envvars-67727661-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012424511s
Jan 21 05:00:03.779: INFO: Pod "client-envvars-67727661-1d39-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025251064s
STEP: Saw pod success
Jan 21 05:00:03.779: INFO: Pod "client-envvars-67727661-1d39-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:00:03.783: INFO: Trying to get logs from node k8s05 pod client-envvars-67727661-1d39-11e9-b032-0a580af40356 container env3cont: <nil>
STEP: delete the pod
Jan 21 05:00:03.820: INFO: Waiting for pod client-envvars-67727661-1d39-11e9-b032-0a580af40356 to disappear
Jan 21 05:00:03.826: INFO: Pod client-envvars-67727661-1d39-11e9-b032-0a580af40356 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:00:03.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-9q756" for this suite.
Jan 21 05:00:49.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:00:49.994: INFO: namespace: e2e-tests-pods-9q756, resource: bindings, ignored listing per whitelist
Jan 21 05:00:50.026: INFO: namespace e2e-tests-pods-9q756 deletion completed in 46.192381975s

â€¢ [SLOW TEST:52.465 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:00:50.027: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:00:50.164: INFO: (0) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 19.397426ms)
Jan 21 05:00:50.170: INFO: (1) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.706382ms)
Jan 21 05:00:50.177: INFO: (2) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.883477ms)
Jan 21 05:00:50.182: INFO: (3) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.233071ms)
Jan 21 05:00:50.188: INFO: (4) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.628602ms)
Jan 21 05:00:50.197: INFO: (5) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 9.487552ms)
Jan 21 05:00:50.211: INFO: (6) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 13.932981ms)
Jan 21 05:00:50.219: INFO: (7) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 7.993416ms)
Jan 21 05:00:50.239: INFO: (8) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 20.29316ms)
Jan 21 05:00:50.246: INFO: (9) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.990093ms)
Jan 21 05:00:50.271: INFO: (10) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 24.92781ms)
Jan 21 05:00:50.283: INFO: (11) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 11.896799ms)
Jan 21 05:00:50.290: INFO: (12) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.177732ms)
Jan 21 05:00:50.295: INFO: (13) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.831042ms)
Jan 21 05:00:50.300: INFO: (14) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.722708ms)
Jan 21 05:00:50.306: INFO: (15) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.238225ms)
Jan 21 05:00:50.312: INFO: (16) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.604588ms)
Jan 21 05:00:50.318: INFO: (17) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.537383ms)
Jan 21 05:00:50.323: INFO: (18) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.612829ms)
Jan 21 05:00:50.330: INFO: (19) /api/v1/nodes/k8s05:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.879518ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:00:50.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-qnphg" for this suite.
Jan 21 05:00:56.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:00:56.407: INFO: namespace: e2e-tests-proxy-qnphg, resource: bindings, ignored listing per whitelist
Jan 21 05:00:56.509: INFO: namespace e2e-tests-proxy-qnphg deletion completed in 6.171863033s

â€¢ [SLOW TEST:6.482 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:00:56.509: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 21 05:00:56.637: INFO: Waiting up to 5m0s for pod "pod-8957f431-1d39-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-lr5mq" to be "success or failure"
Jan 21 05:00:56.643: INFO: Pod "pod-8957f431-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 6.109213ms
Jan 21 05:00:58.650: INFO: Pod "pod-8957f431-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013199335s
Jan 21 05:01:00.669: INFO: Pod "pod-8957f431-1d39-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031470247s
STEP: Saw pod success
Jan 21 05:01:00.669: INFO: Pod "pod-8957f431-1d39-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:01:00.675: INFO: Trying to get logs from node k8s05 pod pod-8957f431-1d39-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:01:00.734: INFO: Waiting for pod pod-8957f431-1d39-11e9-b032-0a580af40356 to disappear
Jan 21 05:01:00.740: INFO: Pod pod-8957f431-1d39-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:01:00.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lr5mq" for this suite.
Jan 21 05:01:06.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:01:06.809: INFO: namespace: e2e-tests-emptydir-lr5mq, resource: bindings, ignored listing per whitelist
Jan 21 05:01:06.958: INFO: namespace e2e-tests-emptydir-lr5mq deletion completed in 6.209822629s

â€¢ [SLOW TEST:10.449 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:01:06.958: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:01:07.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f9bed31-1d39-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-nrmc7" to be "success or failure"
Jan 21 05:01:07.184: INFO: Pod "downwardapi-volume-8f9bed31-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 33.613702ms
Jan 21 05:01:09.198: INFO: Pod "downwardapi-volume-8f9bed31-1d39-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046911642s
STEP: Saw pod success
Jan 21 05:01:09.198: INFO: Pod "downwardapi-volume-8f9bed31-1d39-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:01:09.204: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-8f9bed31-1d39-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:01:09.252: INFO: Waiting for pod downwardapi-volume-8f9bed31-1d39-11e9-b032-0a580af40356 to disappear
Jan 21 05:01:09.259: INFO: Pod downwardapi-volume-8f9bed31-1d39-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:01:09.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-nrmc7" for this suite.
Jan 21 05:01:15.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:01:15.421: INFO: namespace: e2e-tests-downward-api-nrmc7, resource: bindings, ignored listing per whitelist
Jan 21 05:01:15.431: INFO: namespace e2e-tests-downward-api-nrmc7 deletion completed in 6.164162829s

â€¢ [SLOW TEST:8.473 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:01:15.431: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 21 05:01:15.562: INFO: Waiting up to 5m0s for pod "downward-api-949f20fa-1d39-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-gk7vm" to be "success or failure"
Jan 21 05:01:15.576: INFO: Pod "downward-api-949f20fa-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 14.339728ms
Jan 21 05:01:17.583: INFO: Pod "downward-api-949f20fa-1d39-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021283773s
STEP: Saw pod success
Jan 21 05:01:17.583: INFO: Pod "downward-api-949f20fa-1d39-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:01:17.588: INFO: Trying to get logs from node k8s05 pod downward-api-949f20fa-1d39-11e9-b032-0a580af40356 container dapi-container: <nil>
STEP: delete the pod
Jan 21 05:01:17.625: INFO: Waiting for pod downward-api-949f20fa-1d39-11e9-b032-0a580af40356 to disappear
Jan 21 05:01:17.631: INFO: Pod downward-api-949f20fa-1d39-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:01:17.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gk7vm" for this suite.
Jan 21 05:01:23.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:01:23.817: INFO: namespace: e2e-tests-downward-api-gk7vm, resource: bindings, ignored listing per whitelist
Jan 21 05:01:23.817: INFO: namespace e2e-tests-downward-api-gk7vm deletion completed in 6.177595041s

â€¢ [SLOW TEST:8.386 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:01:23.817: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:01:23.945: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 21 05:01:28.953: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 21 05:01:28.953: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 21 05:01:30.961: INFO: Creating deployment "test-rollover-deployment"
Jan 21 05:01:30.984: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 21 05:01:33.009: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 21 05:01:33.022: INFO: Ensure that both replica sets have 1 created replica
Jan 21 05:01:33.034: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 21 05:01:33.051: INFO: Updating deployment test-rollover-deployment
Jan 21 05:01:33.051: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 21 05:01:35.071: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 21 05:01:35.084: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 21 05:01:35.095: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 05:01:35.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643694, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 05:01:37.114: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 05:01:37.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643694, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 05:01:39.109: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 05:01:39.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643694, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 05:01:41.111: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 05:01:41.112: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643694, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 05:01:43.114: INFO: all replica sets need to contain the pod-template-hash label
Jan 21 05:01:43.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643694, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683643691, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6b7f9d6597\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 05:01:45.118: INFO: 
Jan 21 05:01:45.118: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 21 05:01:45.138: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-bt7ms,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-bt7ms/deployments/test-rollover-deployment,UID:9dd1719b-1d39-11e9-9faa-fa163e6f5c57,ResourceVersion:29960,Generation:2,CreationTimestamp:2019-01-21 05:01:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-01-21 05:01:31 +0000 UTC 2019-01-21 05:01:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-01-21 05:01:44 +0000 UTC 2019-01-21 05:01:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-6b7f9d6597" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 21 05:01:45.145: INFO: New ReplicaSet "test-rollover-deployment-6b7f9d6597" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597,GenerateName:,Namespace:e2e-tests-deployment-bt7ms,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-bt7ms/replicasets/test-rollover-deployment-6b7f9d6597,UID:9f10193f-1d39-11e9-b27d-fa163eb7f963,ResourceVersion:29951,Generation:2,CreationTimestamp:2019-01-21 05:01:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9dd1719b-1d39-11e9-9faa-fa163e6f5c57 0xc0019aebb7 0xc0019aebb8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 21 05:01:45.145: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 21 05:01:45.145: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-bt7ms,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-bt7ms/replicasets/test-rollover-controller,UID:99a05b39-1d39-11e9-9faa-fa163e6f5c57,ResourceVersion:29959,Generation:2,CreationTimestamp:2019-01-21 05:01:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9dd1719b-1d39-11e9-9faa-fa163e6f5c57 0xc0019aea27 0xc0019aea28}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 21 05:01:45.145: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6586df867b,GenerateName:,Namespace:e2e-tests-deployment-bt7ms,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-bt7ms/replicasets/test-rollover-deployment-6586df867b,UID:9dd7f950-1d39-11e9-b27d-fa163eb7f963,ResourceVersion:29917,Generation:2,CreationTimestamp:2019-01-21 05:01:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 9dd1719b-1d39-11e9-9faa-fa163e6f5c57 0xc0019aeae7 0xc0019aeae8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6586df867b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 21 05:01:45.152: INFO: Pod "test-rollover-deployment-6b7f9d6597-k5cp8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6b7f9d6597-k5cp8,GenerateName:test-rollover-deployment-6b7f9d6597-,Namespace:e2e-tests-deployment-bt7ms,SelfLink:/api/v1/namespaces/e2e-tests-deployment-bt7ms/pods/test-rollover-deployment-6b7f9d6597-k5cp8,UID:9f18eefb-1d39-11e9-b27d-fa163eb7f963,ResourceVersion:29931,Generation:0,CreationTimestamp:2019-01-21 05:01:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6b7f9d6597,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-6b7f9d6597 9f10193f-1d39-11e9-b27d-fa163eb7f963 0xc00184e6c7 0xc00184e6c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-97zg6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-97zg6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-97zg6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00184e740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00184e760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:01:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:01:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:01:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:01:33 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.104,StartTime:2019-01-21 05:01:33 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-01-21 05:01:34 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 docker://4cd48d522a1610a4f39e8e11b665584010db4d9388c181f7783b53e01573bccf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:01:45.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-bt7ms" for this suite.
Jan 21 05:01:51.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:01:51.298: INFO: namespace: e2e-tests-deployment-bt7ms, resource: bindings, ignored listing per whitelist
Jan 21 05:01:51.323: INFO: namespace e2e-tests-deployment-bt7ms deletion completed in 6.15762684s

â€¢ [SLOW TEST:27.506 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:01:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 21 05:01:51.438: INFO: Waiting up to 5m0s for pod "pod-aa044bfd-1d39-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-h7mmv" to be "success or failure"
Jan 21 05:01:51.451: INFO: Pod "pod-aa044bfd-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 12.890612ms
Jan 21 05:01:53.464: INFO: Pod "pod-aa044bfd-1d39-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025570106s
STEP: Saw pod success
Jan 21 05:01:53.464: INFO: Pod "pod-aa044bfd-1d39-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:01:53.468: INFO: Trying to get logs from node k8s05 pod pod-aa044bfd-1d39-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:01:53.498: INFO: Waiting for pod pod-aa044bfd-1d39-11e9-b032-0a580af40356 to disappear
Jan 21 05:01:53.501: INFO: Pod pod-aa044bfd-1d39-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:01:53.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-h7mmv" for this suite.
Jan 21 05:01:59.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:01:59.558: INFO: namespace: e2e-tests-emptydir-h7mmv, resource: bindings, ignored listing per whitelist
Jan 21 05:01:59.702: INFO: namespace e2e-tests-emptydir-h7mmv deletion completed in 6.196197674s

â€¢ [SLOW TEST:8.379 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:01:59.703: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 21 05:01:59.888: INFO: Waiting up to 5m0s for pod "pod-af0c47cd-1d39-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-kgd28" to be "success or failure"
Jan 21 05:01:59.898: INFO: Pod "pod-af0c47cd-1d39-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.927494ms
Jan 21 05:02:01.905: INFO: Pod "pod-af0c47cd-1d39-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01666759s
STEP: Saw pod success
Jan 21 05:02:01.905: INFO: Pod "pod-af0c47cd-1d39-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:02:01.910: INFO: Trying to get logs from node k8s05 pod pod-af0c47cd-1d39-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:02:01.961: INFO: Waiting for pod pod-af0c47cd-1d39-11e9-b032-0a580af40356 to disappear
Jan 21 05:02:01.967: INFO: Pod pod-af0c47cd-1d39-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:02:01.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kgd28" for this suite.
Jan 21 05:02:08.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:02:08.137: INFO: namespace: e2e-tests-emptydir-kgd28, resource: bindings, ignored listing per whitelist
Jan 21 05:02:08.147: INFO: namespace e2e-tests-emptydir-kgd28 deletion completed in 6.173144979s

â€¢ [SLOW TEST:8.444 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:02:08.147: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:02:08.267: INFO: (0) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.755288ms)
Jan 21 05:02:08.278: INFO: (1) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 10.665061ms)
Jan 21 05:02:08.285: INFO: (2) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 7.587011ms)
Jan 21 05:02:08.291: INFO: (3) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.335139ms)
Jan 21 05:02:08.302: INFO: (4) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 11.745522ms)
Jan 21 05:02:08.309: INFO: (5) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.274334ms)
Jan 21 05:02:08.315: INFO: (6) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.110654ms)
Jan 21 05:02:08.320: INFO: (7) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.447964ms)
Jan 21 05:02:08.326: INFO: (8) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.408338ms)
Jan 21 05:02:08.331: INFO: (9) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.448946ms)
Jan 21 05:02:08.337: INFO: (10) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.154784ms)
Jan 21 05:02:08.343: INFO: (11) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.9944ms)
Jan 21 05:02:08.349: INFO: (12) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.71209ms)
Jan 21 05:02:08.355: INFO: (13) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 6.137244ms)
Jan 21 05:02:08.361: INFO: (14) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.758253ms)
Jan 21 05:02:08.367: INFO: (15) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.586252ms)
Jan 21 05:02:08.375: INFO: (16) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 8.068363ms)
Jan 21 05:02:08.381: INFO: (17) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.752775ms)
Jan 21 05:02:08.386: INFO: (18) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.557187ms)
Jan 21 05:02:08.392: INFO: (19) /api/v1/nodes/k8s05/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 5.465603ms)
[AfterEach] version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:02:08.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-7sqz8" for this suite.
Jan 21 05:02:14.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:02:14.523: INFO: namespace: e2e-tests-proxy-7sqz8, resource: bindings, ignored listing per whitelist
Jan 21 05:02:14.577: INFO: namespace e2e-tests-proxy-7sqz8 deletion completed in 6.179324793s

â€¢ [SLOW TEST:6.430 seconds]
[sig-network] Proxy
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:02:14.577: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:02:14.719: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 21 05:02:14.741: INFO: Number of nodes with available pods: 0
Jan 21 05:02:14.741: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 21 05:02:14.791: INFO: Number of nodes with available pods: 0
Jan 21 05:02:14.791: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:15.797: INFO: Number of nodes with available pods: 0
Jan 21 05:02:15.797: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:16.797: INFO: Number of nodes with available pods: 1
Jan 21 05:02:16.797: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 21 05:02:16.826: INFO: Number of nodes with available pods: 1
Jan 21 05:02:16.826: INFO: Number of running nodes: 0, number of available pods: 1
Jan 21 05:02:17.833: INFO: Number of nodes with available pods: 0
Jan 21 05:02:17.833: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 21 05:02:17.851: INFO: Number of nodes with available pods: 0
Jan 21 05:02:17.851: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:18.856: INFO: Number of nodes with available pods: 0
Jan 21 05:02:18.856: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:19.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:19.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:20.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:20.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:21.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:21.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:22.862: INFO: Number of nodes with available pods: 0
Jan 21 05:02:22.862: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:23.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:23.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:24.869: INFO: Number of nodes with available pods: 0
Jan 21 05:02:24.869: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:25.857: INFO: Number of nodes with available pods: 0
Jan 21 05:02:25.857: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:26.859: INFO: Number of nodes with available pods: 0
Jan 21 05:02:26.860: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:27.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:27.859: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:28.859: INFO: Number of nodes with available pods: 0
Jan 21 05:02:28.859: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:29.857: INFO: Number of nodes with available pods: 0
Jan 21 05:02:29.857: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:30.857: INFO: Number of nodes with available pods: 0
Jan 21 05:02:30.857: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:31.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:31.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:32.859: INFO: Number of nodes with available pods: 0
Jan 21 05:02:32.859: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:33.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:33.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:34.857: INFO: Number of nodes with available pods: 0
Jan 21 05:02:34.857: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:35.867: INFO: Number of nodes with available pods: 0
Jan 21 05:02:35.867: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:36.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:36.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:37.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:37.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:38.857: INFO: Number of nodes with available pods: 0
Jan 21 05:02:38.857: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:39.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:39.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:40.857: INFO: Number of nodes with available pods: 0
Jan 21 05:02:40.857: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:41.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:41.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:42.857: INFO: Number of nodes with available pods: 0
Jan 21 05:02:42.857: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:43.857: INFO: Number of nodes with available pods: 0
Jan 21 05:02:43.857: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:44.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:44.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:45.856: INFO: Number of nodes with available pods: 0
Jan 21 05:02:45.856: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:46.863: INFO: Number of nodes with available pods: 0
Jan 21 05:02:46.863: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:47.857: INFO: Number of nodes with available pods: 0
Jan 21 05:02:47.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:48.859: INFO: Number of nodes with available pods: 0
Jan 21 05:02:48.859: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:49.858: INFO: Number of nodes with available pods: 0
Jan 21 05:02:49.858: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:50.859: INFO: Number of nodes with available pods: 0
Jan 21 05:02:50.859: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:51.859: INFO: Number of nodes with available pods: 0
Jan 21 05:02:51.859: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:02:52.859: INFO: Number of nodes with available pods: 1
Jan 21 05:02:52.859: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-tnq2d, will wait for the garbage collector to delete the pods
Jan 21 05:02:52.946: INFO: Deleting DaemonSet.extensions daemon-set took: 18.649899ms
Jan 21 05:02:53.046: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.302601ms
Jan 21 05:03:27.174: INFO: Number of nodes with available pods: 0
Jan 21 05:03:27.174: INFO: Number of running nodes: 0, number of available pods: 0
Jan 21 05:03:27.194: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-tnq2d/daemonsets","resourceVersion":"30282"},"items":null}

Jan 21 05:03:27.203: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-tnq2d/pods","resourceVersion":"30282"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:03:27.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-tnq2d" for this suite.
Jan 21 05:03:33.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:03:33.453: INFO: namespace: e2e-tests-daemonsets-tnq2d, resource: bindings, ignored listing per whitelist
Jan 21 05:03:33.461: INFO: namespace e2e-tests-daemonsets-tnq2d deletion completed in 6.200056973s

â€¢ [SLOW TEST:78.884 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:03:33.462: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-7tw2s
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 21 05:03:33.563: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 21 05:03:55.743: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | nc -w 1 -u 10.244.3.109 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-7tw2s PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:03:55.743: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:03:56.909: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:03:56.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-7tw2s" for this suite.
Jan 21 05:04:18.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:04:19.098: INFO: namespace: e2e-tests-pod-network-test-7tw2s, resource: bindings, ignored listing per whitelist
Jan 21 05:04:19.138: INFO: namespace e2e-tests-pod-network-test-7tw2s deletion completed in 22.220154121s

â€¢ [SLOW TEST:45.677 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:04:19.139: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-qhzdj
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-qhzdj to expose endpoints map[]
Jan 21 05:04:19.315: INFO: Get endpoints failed (8.014059ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jan 21 05:04:20.322: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-qhzdj exposes endpoints map[] (1.014856214s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-qhzdj
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-qhzdj to expose endpoints map[pod1:[100]]
Jan 21 05:04:22.387: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-qhzdj exposes endpoints map[pod1:[100]] (2.043886293s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-qhzdj
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-qhzdj to expose endpoints map[pod2:[101] pod1:[100]]
Jan 21 05:04:24.455: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-qhzdj exposes endpoints map[pod1:[100] pod2:[101]] (2.057914601s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-qhzdj
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-qhzdj to expose endpoints map[pod2:[101]]
Jan 21 05:04:24.486: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-qhzdj exposes endpoints map[pod2:[101]] (17.519751ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-qhzdj
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-qhzdj to expose endpoints map[]
Jan 21 05:04:25.509: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-qhzdj exposes endpoints map[] (1.011322254s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:04:25.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-qhzdj" for this suite.
Jan 21 05:04:31.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:04:31.631: INFO: namespace: e2e-tests-services-qhzdj, resource: bindings, ignored listing per whitelist
Jan 21 05:04:31.740: INFO: namespace e2e-tests-services-qhzdj deletion completed in 6.144466307s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:12.602 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:04:31.740: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-hpfpg/configmap-test-09a0c6ae-1d3a-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:04:31.880: INFO: Waiting up to 5m0s for pod "pod-configmaps-09a2ee79-1d3a-11e9-b032-0a580af40356" in namespace "e2e-tests-configmap-hpfpg" to be "success or failure"
Jan 21 05:04:31.889: INFO: Pod "pod-configmaps-09a2ee79-1d3a-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.274909ms
Jan 21 05:04:33.904: INFO: Pod "pod-configmaps-09a2ee79-1d3a-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02445404s
STEP: Saw pod success
Jan 21 05:04:33.904: INFO: Pod "pod-configmaps-09a2ee79-1d3a-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:04:33.908: INFO: Trying to get logs from node k8s05 pod pod-configmaps-09a2ee79-1d3a-11e9-b032-0a580af40356 container env-test: <nil>
STEP: delete the pod
Jan 21 05:04:33.945: INFO: Waiting for pod pod-configmaps-09a2ee79-1d3a-11e9-b032-0a580af40356 to disappear
Jan 21 05:04:33.952: INFO: Pod pod-configmaps-09a2ee79-1d3a-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:04:33.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-hpfpg" for this suite.
Jan 21 05:04:39.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:04:40.016: INFO: namespace: e2e-tests-configmap-hpfpg, resource: bindings, ignored listing per whitelist
Jan 21 05:04:40.122: INFO: namespace e2e-tests-configmap-hpfpg deletion completed in 6.164365993s

â€¢ [SLOW TEST:8.382 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:04:40.122: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0121 05:04:50.304589      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 21 05:04:50.304: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:04:50.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-r2zgg" for this suite.
Jan 21 05:04:56.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:04:56.463: INFO: namespace: e2e-tests-gc-r2zgg, resource: bindings, ignored listing per whitelist
Jan 21 05:04:56.557: INFO: namespace e2e-tests-gc-r2zgg deletion completed in 6.243182764s

â€¢ [SLOW TEST:16.434 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:04:56.557: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:04:56.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-186e3607-1d3a-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-v2wmp" to be "success or failure"
Jan 21 05:04:56.702: INFO: Pod "downwardapi-volume-186e3607-1d3a-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 14.987514ms
Jan 21 05:04:58.715: INFO: Pod "downwardapi-volume-186e3607-1d3a-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02794893s
STEP: Saw pod success
Jan 21 05:04:58.715: INFO: Pod "downwardapi-volume-186e3607-1d3a-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:04:58.720: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-186e3607-1d3a-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:04:58.754: INFO: Waiting for pod downwardapi-volume-186e3607-1d3a-11e9-b032-0a580af40356 to disappear
Jan 21 05:04:58.770: INFO: Pod downwardapi-volume-186e3607-1d3a-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:04:58.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v2wmp" for this suite.
Jan 21 05:05:04.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:05:04.897: INFO: namespace: e2e-tests-projected-v2wmp, resource: bindings, ignored listing per whitelist
Jan 21 05:05:04.977: INFO: namespace e2e-tests-projected-v2wmp deletion completed in 6.19996449s

â€¢ [SLOW TEST:8.420 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:05:04.977: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-wwnl
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 05:05:05.125: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-wwnl" in namespace "e2e-tests-subpath-ff4nt" to be "success or failure"
Jan 21 05:05:05.134: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.882019ms
Jan 21 05:05:07.149: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023869614s
Jan 21 05:05:09.158: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 4.032818019s
Jan 21 05:05:11.164: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 6.038220864s
Jan 21 05:05:13.171: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 8.045903606s
Jan 21 05:05:15.177: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 10.05190458s
Jan 21 05:05:17.192: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 12.066220894s
Jan 21 05:05:19.203: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 14.077574175s
Jan 21 05:05:21.209: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 16.084096133s
Jan 21 05:05:23.215: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 18.089943462s
Jan 21 05:05:25.223: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 20.097652616s
Jan 21 05:05:27.241: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Running", Reason="", readiness=false. Elapsed: 22.11539784s
Jan 21 05:05:29.254: INFO: Pod "pod-subpath-test-downwardapi-wwnl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.128979452s
STEP: Saw pod success
Jan 21 05:05:29.254: INFO: Pod "pod-subpath-test-downwardapi-wwnl" satisfied condition "success or failure"
Jan 21 05:05:29.260: INFO: Trying to get logs from node k8s05 pod pod-subpath-test-downwardapi-wwnl container test-container-subpath-downwardapi-wwnl: <nil>
STEP: delete the pod
Jan 21 05:05:29.299: INFO: Waiting for pod pod-subpath-test-downwardapi-wwnl to disappear
Jan 21 05:05:29.303: INFO: Pod pod-subpath-test-downwardapi-wwnl no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-wwnl
Jan 21 05:05:29.303: INFO: Deleting pod "pod-subpath-test-downwardapi-wwnl" in namespace "e2e-tests-subpath-ff4nt"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:05:29.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-ff4nt" for this suite.
Jan 21 05:05:35.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:05:35.473: INFO: namespace: e2e-tests-subpath-ff4nt, resource: bindings, ignored listing per whitelist
Jan 21 05:05:35.516: INFO: namespace e2e-tests-subpath-ff4nt deletion completed in 6.203666132s

â€¢ [SLOW TEST:30.539 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:05:35.516: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:05:35.650: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 21 05:05:40.669: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 21 05:05:40.669: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 21 05:05:40.712: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-x6n4z,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x6n4z/deployments/test-cleanup-deployment,UID:32a8e32d-1d3a-11e9-9faa-fa163e6f5c57,ResourceVersion:30794,Generation:1,CreationTimestamp:2019-01-21 05:05:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jan 21 05:05:40.725: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 21 05:05:40.725: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 21 05:05:40.725: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:e2e-tests-deployment-x6n4z,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-x6n4z/replicasets/test-cleanup-controller,UID:2fa6a1d3-1d3a-11e9-9faa-fa163e6f5c57,ResourceVersion:30795,Generation:1,CreationTimestamp:2019-01-21 05:05:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 32a8e32d-1d3a-11e9-9faa-fa163e6f5c57 0xc000dffbe7 0xc000dffbe8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 21 05:05:40.746: INFO: Pod "test-cleanup-controller-qp6bw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-qp6bw,GenerateName:test-cleanup-controller-,Namespace:e2e-tests-deployment-x6n4z,SelfLink:/api/v1/namespaces/e2e-tests-deployment-x6n4z/pods/test-cleanup-controller-qp6bw,UID:2fac08d2-1d3a-11e9-b27d-fa163eb7f963,ResourceVersion:30789,Generation:0,CreationTimestamp:2019-01-21 05:05:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 2fa6a1d3-1d3a-11e9-9faa-fa163e6f5c57 0xc001764547 0xc001764548}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-4fgvr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4fgvr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4fgvr true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017645c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017645e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:05:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:05:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:05:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:05:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.118,StartTime:2019-01-21 05:05:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-21 05:05:36 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://782efa54845f47fd2c528b4fbe02743caee400e98467813b3f35ab3b03b3c3fa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:05:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-x6n4z" for this suite.
Jan 21 05:05:46.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:05:46.947: INFO: namespace: e2e-tests-deployment-x6n4z, resource: bindings, ignored listing per whitelist
Jan 21 05:05:46.983: INFO: namespace e2e-tests-deployment-x6n4z deletion completed in 6.216681044s

â€¢ [SLOW TEST:11.467 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:05:46.983: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 21 05:05:49.703: INFO: Successfully updated pod "annotationupdate36806c30-1d3a-11e9-b032-0a580af40356"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:05:51.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-kldl9" for this suite.
Jan 21 05:06:13.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:06:13.901: INFO: namespace: e2e-tests-downward-api-kldl9, resource: bindings, ignored listing per whitelist
Jan 21 05:06:13.909: INFO: namespace e2e-tests-downward-api-kldl9 deletion completed in 22.162343353s

â€¢ [SLOW TEST:26.926 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:06:13.910: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 21 05:06:14.038: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2drn8,SelfLink:/api/v1/namespaces/e2e-tests-watch-2drn8/configmaps/e2e-watch-test-watch-closed,UID:4686e331-1d3a-11e9-9faa-fa163e6f5c57,ResourceVersion:30924,Generation:0,CreationTimestamp:2019-01-21 05:06:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 21 05:06:14.039: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2drn8,SelfLink:/api/v1/namespaces/e2e-tests-watch-2drn8/configmaps/e2e-watch-test-watch-closed,UID:4686e331-1d3a-11e9-9faa-fa163e6f5c57,ResourceVersion:30925,Generation:0,CreationTimestamp:2019-01-21 05:06:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 21 05:06:14.062: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2drn8,SelfLink:/api/v1/namespaces/e2e-tests-watch-2drn8/configmaps/e2e-watch-test-watch-closed,UID:4686e331-1d3a-11e9-9faa-fa163e6f5c57,ResourceVersion:30926,Generation:0,CreationTimestamp:2019-01-21 05:06:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 21 05:06:14.062: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-2drn8,SelfLink:/api/v1/namespaces/e2e-tests-watch-2drn8/configmaps/e2e-watch-test-watch-closed,UID:4686e331-1d3a-11e9-9faa-fa163e6f5c57,ResourceVersion:30927,Generation:0,CreationTimestamp:2019-01-21 05:06:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:06:14.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-2drn8" for this suite.
Jan 21 05:06:20.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:06:20.158: INFO: namespace: e2e-tests-watch-2drn8, resource: bindings, ignored listing per whitelist
Jan 21 05:06:20.256: INFO: namespace e2e-tests-watch-2drn8 deletion completed in 6.18683956s

â€¢ [SLOW TEST:6.346 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:06:20.256: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-wrklw
Jan 21 05:06:22.394: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-wrklw
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 05:06:22.397: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:10:23.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-wrklw" for this suite.
Jan 21 05:10:29.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:10:29.654: INFO: namespace: e2e-tests-container-probe-wrklw, resource: bindings, ignored listing per whitelist
Jan 21 05:10:29.711: INFO: namespace e2e-tests-container-probe-wrklw deletion completed in 6.190939601s

â€¢ [SLOW TEST:249.454 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:10:29.711: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-66hmj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-66hmj to expose endpoints map[]
Jan 21 05:10:29.875: INFO: Get endpoints failed (5.957349ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jan 21 05:10:30.885: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-66hmj exposes endpoints map[] (1.016040926s elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-66hmj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-66hmj to expose endpoints map[pod1:[80]]
Jan 21 05:10:32.946: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-66hmj exposes endpoints map[pod1:[80]] (2.041219245s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-66hmj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-66hmj to expose endpoints map[pod1:[80] pod2:[80]]
Jan 21 05:10:35.015: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-66hmj exposes endpoints map[pod1:[80] pod2:[80]] (2.057347003s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-66hmj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-66hmj to expose endpoints map[pod2:[80]]
Jan 21 05:10:36.068: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-66hmj exposes endpoints map[pod2:[80]] (1.036926725s elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-66hmj
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-66hmj to expose endpoints map[]
Jan 21 05:10:36.086: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-66hmj exposes endpoints map[] (5.579746ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:10:36.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-66hmj" for this suite.
Jan 21 05:10:42.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:10:42.274: INFO: namespace: e2e-tests-services-66hmj, resource: bindings, ignored listing per whitelist
Jan 21 05:10:42.349: INFO: namespace e2e-tests-services-66hmj deletion completed in 6.195580959s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:12.639 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:10:42.350: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Jan 21 05:10:42.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-hh2ff'
Jan 21 05:10:43.170: INFO: stderr: ""
Jan 21 05:10:43.171: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 21 05:10:44.179: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:10:44.179: INFO: Found 0 / 1
Jan 21 05:10:45.178: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:10:45.178: INFO: Found 1 / 1
Jan 21 05:10:45.178: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 21 05:10:45.182: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:10:45.182: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 21 05:10:45.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 patch pod redis-master-5r2zz --namespace=e2e-tests-kubectl-hh2ff -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 21 05:10:45.362: INFO: stderr: ""
Jan 21 05:10:45.362: INFO: stdout: "pod/redis-master-5r2zz patched\n"
STEP: checking annotations
Jan 21 05:10:45.367: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:10:45.367: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:10:45.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hh2ff" for this suite.
Jan 21 05:11:07.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:11:07.436: INFO: namespace: e2e-tests-kubectl-hh2ff, resource: bindings, ignored listing per whitelist
Jan 21 05:11:07.578: INFO: namespace e2e-tests-kubectl-hh2ff deletion completed in 22.204215985s

â€¢ [SLOW TEST:25.228 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:11:07.578: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0121 05:11:38.276946      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 21 05:11:38.277: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:11:38.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-nqc6c" for this suite.
Jan 21 05:11:44.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:11:44.321: INFO: namespace: e2e-tests-gc-nqc6c, resource: bindings, ignored listing per whitelist
Jan 21 05:11:44.461: INFO: namespace e2e-tests-gc-nqc6c deletion completed in 6.177738673s

â€¢ [SLOW TEST:36.883 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:11:44.462: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-0b934642-1d3b-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:11:44.626: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0b9524da-1d3b-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-dgpq9" to be "success or failure"
Jan 21 05:11:44.641: INFO: Pod "pod-projected-secrets-0b9524da-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 14.609011ms
Jan 21 05:11:46.650: INFO: Pod "pod-projected-secrets-0b9524da-1d3b-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023310914s
STEP: Saw pod success
Jan 21 05:11:46.650: INFO: Pod "pod-projected-secrets-0b9524da-1d3b-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:11:46.658: INFO: Trying to get logs from node k8s05 pod pod-projected-secrets-0b9524da-1d3b-11e9-b032-0a580af40356 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:11:46.706: INFO: Waiting for pod pod-projected-secrets-0b9524da-1d3b-11e9-b032-0a580af40356 to disappear
Jan 21 05:11:46.716: INFO: Pod pod-projected-secrets-0b9524da-1d3b-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:11:46.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dgpq9" for this suite.
Jan 21 05:11:52.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:11:52.796: INFO: namespace: e2e-tests-projected-dgpq9, resource: bindings, ignored listing per whitelist
Jan 21 05:11:52.913: INFO: namespace e2e-tests-projected-dgpq9 deletion completed in 6.187823912s

â€¢ [SLOW TEST:8.451 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:11:52.913: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-1098b268-1d3b-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:11:53.047: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-109a55f9-1d3b-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-9jb7z" to be "success or failure"
Jan 21 05:11:53.068: INFO: Pod "pod-projected-secrets-109a55f9-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 20.885374ms
Jan 21 05:11:55.079: INFO: Pod "pod-projected-secrets-109a55f9-1d3b-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031882251s
STEP: Saw pod success
Jan 21 05:11:55.079: INFO: Pod "pod-projected-secrets-109a55f9-1d3b-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:11:55.088: INFO: Trying to get logs from node k8s05 pod pod-projected-secrets-109a55f9-1d3b-11e9-b032-0a580af40356 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:11:55.129: INFO: Waiting for pod pod-projected-secrets-109a55f9-1d3b-11e9-b032-0a580af40356 to disappear
Jan 21 05:11:55.134: INFO: Pod pod-projected-secrets-109a55f9-1d3b-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:11:55.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9jb7z" for this suite.
Jan 21 05:12:01.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:12:01.188: INFO: namespace: e2e-tests-projected-9jb7z, resource: bindings, ignored listing per whitelist
Jan 21 05:12:01.305: INFO: namespace e2e-tests-projected-9jb7z deletion completed in 6.163827234s

â€¢ [SLOW TEST:8.393 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:12:01.306: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 21 05:12:01.433: INFO: Waiting up to 5m0s for pod "pod-15994ac4-1d3b-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-lrc5v" to be "success or failure"
Jan 21 05:12:01.443: INFO: Pod "pod-15994ac4-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 10.431237ms
Jan 21 05:12:03.452: INFO: Pod "pod-15994ac4-1d3b-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018699324s
STEP: Saw pod success
Jan 21 05:12:03.452: INFO: Pod "pod-15994ac4-1d3b-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:12:03.456: INFO: Trying to get logs from node k8s05 pod pod-15994ac4-1d3b-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:12:03.499: INFO: Waiting for pod pod-15994ac4-1d3b-11e9-b032-0a580af40356 to disappear
Jan 21 05:12:03.506: INFO: Pod pod-15994ac4-1d3b-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:12:03.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-lrc5v" for this suite.
Jan 21 05:12:09.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:12:09.699: INFO: namespace: e2e-tests-emptydir-lrc5v, resource: bindings, ignored listing per whitelist
Jan 21 05:12:09.703: INFO: namespace e2e-tests-emptydir-lrc5v deletion completed in 6.189111735s

â€¢ [SLOW TEST:8.397 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:12:09.703: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1052
STEP: creating the pod
Jan 21 05:12:09.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-zwc8q'
Jan 21 05:12:10.136: INFO: stderr: ""
Jan 21 05:12:10.136: INFO: stdout: "pod/pause created\n"
Jan 21 05:12:10.136: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 21 05:12:10.136: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-zwc8q" to be "running and ready"
Jan 21 05:12:10.154: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.206559ms
Jan 21 05:12:12.162: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.025354821s
Jan 21 05:12:12.162: INFO: Pod "pause" satisfied condition "running and ready"
Jan 21 05:12:12.162: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 21 05:12:12.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-zwc8q'
Jan 21 05:12:12.338: INFO: stderr: ""
Jan 21 05:12:12.338: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 21 05:12:12.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pod pause -L testing-label --namespace=e2e-tests-kubectl-zwc8q'
Jan 21 05:12:12.459: INFO: stderr: ""
Jan 21 05:12:12.459: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 21 05:12:12.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 label pods pause testing-label- --namespace=e2e-tests-kubectl-zwc8q'
Jan 21 05:12:12.617: INFO: stderr: ""
Jan 21 05:12:12.617: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 21 05:12:12.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pod pause -L testing-label --namespace=e2e-tests-kubectl-zwc8q'
Jan 21 05:12:12.743: INFO: stderr: ""
Jan 21 05:12:12.743: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1059
STEP: using delete to clean up resources
Jan 21 05:12:12.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-zwc8q'
Jan 21 05:12:12.926: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:12:12.926: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 21 05:12:12.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-zwc8q'
Jan 21 05:12:13.119: INFO: stderr: "No resources found.\n"
Jan 21 05:12:13.119: INFO: stdout: ""
Jan 21 05:12:13.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -l name=pause --namespace=e2e-tests-kubectl-zwc8q -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 21 05:12:13.236: INFO: stderr: ""
Jan 21 05:12:13.236: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:12:13.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-zwc8q" for this suite.
Jan 21 05:12:19.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:12:19.343: INFO: namespace: e2e-tests-kubectl-zwc8q, resource: bindings, ignored listing per whitelist
Jan 21 05:12:19.411: INFO: namespace e2e-tests-kubectl-zwc8q deletion completed in 6.164774998s

â€¢ [SLOW TEST:9.708 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:12:19.411: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:12:19.526: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Jan 21 05:12:19.535: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-9x98f/daemonsets","resourceVersion":"31838"},"items":null}

Jan 21 05:12:19.538: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-9x98f/pods","resourceVersion":"31838"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:12:19.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-9x98f" for this suite.
Jan 21 05:12:25.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:12:25.667: INFO: namespace: e2e-tests-daemonsets-9x98f, resource: bindings, ignored listing per whitelist
Jan 21 05:12:25.730: INFO: namespace e2e-tests-daemonsets-9x98f deletion completed in 6.170134262s

S [SKIPPING] [6.318 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Jan 21 05:12:19.526: Requires at least 2 nodes (not -1)

  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:12:25.730: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-g54kf
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 21 05:12:25.830: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 21 05:12:45.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.133:8080/dial?request=hostName&protocol=http&host=10.244.3.132&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-g54kf PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:12:45.955: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:12:46.141: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:12:46.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-g54kf" for this suite.
Jan 21 05:13:08.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:13:08.388: INFO: namespace: e2e-tests-pod-network-test-g54kf, resource: bindings, ignored listing per whitelist
Jan 21 05:13:08.390: INFO: namespace e2e-tests-pod-network-test-g54kf deletion completed in 22.240597678s

â€¢ [SLOW TEST:42.660 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:13:08.390: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Jan 21 05:13:08.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 --namespace=e2e-tests-kubectl-mxfsz run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 21 05:13:10.836: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 21 05:13:10.836: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:13:12.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mxfsz" for this suite.
Jan 21 05:13:18.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:13:19.038: INFO: namespace: e2e-tests-kubectl-mxfsz, resource: bindings, ignored listing per whitelist
Jan 21 05:13:19.053: INFO: namespace e2e-tests-kubectl-mxfsz deletion completed in 6.199567832s

â€¢ [SLOW TEST:10.663 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:13:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:13:19.192: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43f26329-1d3b-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-whcpc" to be "success or failure"
Jan 21 05:13:19.202: INFO: Pod "downwardapi-volume-43f26329-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.936495ms
Jan 21 05:13:21.209: INFO: Pod "downwardapi-volume-43f26329-1d3b-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016798858s
STEP: Saw pod success
Jan 21 05:13:21.209: INFO: Pod "downwardapi-volume-43f26329-1d3b-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:13:21.216: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-43f26329-1d3b-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:13:21.254: INFO: Waiting for pod downwardapi-volume-43f26329-1d3b-11e9-b032-0a580af40356 to disappear
Jan 21 05:13:21.259: INFO: Pod downwardapi-volume-43f26329-1d3b-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:13:21.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-whcpc" for this suite.
Jan 21 05:13:27.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:13:27.352: INFO: namespace: e2e-tests-projected-whcpc, resource: bindings, ignored listing per whitelist
Jan 21 05:13:27.508: INFO: namespace e2e-tests-projected-whcpc deletion completed in 6.241809292s

â€¢ [SLOW TEST:8.455 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:13:27.509: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:13:29.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-pbf5g" for this suite.
Jan 21 05:14:07.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:14:07.831: INFO: namespace: e2e-tests-kubelet-test-pbf5g, resource: bindings, ignored listing per whitelist
Jan 21 05:14:07.870: INFO: namespace e2e-tests-kubelet-test-pbf5g deletion completed in 38.160649537s

â€¢ [SLOW TEST:40.361 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:14:07.870: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 21 05:14:08.009: INFO: Waiting up to 5m0s for pod "pod-610b6039-1d3b-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-qnj7k" to be "success or failure"
Jan 21 05:14:08.017: INFO: Pod "pod-610b6039-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 7.958043ms
Jan 21 05:14:10.034: INFO: Pod "pod-610b6039-1d3b-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025269094s
STEP: Saw pod success
Jan 21 05:14:10.035: INFO: Pod "pod-610b6039-1d3b-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:14:10.041: INFO: Trying to get logs from node k8s05 pod pod-610b6039-1d3b-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:14:10.095: INFO: Waiting for pod pod-610b6039-1d3b-11e9-b032-0a580af40356 to disappear
Jan 21 05:14:10.100: INFO: Pod pod-610b6039-1d3b-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:14:10.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-qnj7k" for this suite.
Jan 21 05:14:16.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:14:16.257: INFO: namespace: e2e-tests-emptydir-qnj7k, resource: bindings, ignored listing per whitelist
Jan 21 05:14:16.311: INFO: namespace e2e-tests-emptydir-qnj7k deletion completed in 6.20092106s

â€¢ [SLOW TEST:8.441 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:14:16.311: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-t9kxs
Jan 21 05:14:18.461: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-t9kxs
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 05:14:18.466: INFO: Initial restart count of pod liveness-exec is 0
Jan 21 05:15:04.681: INFO: Restart count of pod e2e-tests-container-probe-t9kxs/liveness-exec is now 1 (46.215256905s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:15:04.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-t9kxs" for this suite.
Jan 21 05:15:10.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:15:10.860: INFO: namespace: e2e-tests-container-probe-t9kxs, resource: bindings, ignored listing per whitelist
Jan 21 05:15:10.917: INFO: namespace e2e-tests-container-probe-t9kxs deletion completed in 6.191717401s

â€¢ [SLOW TEST:54.605 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:15:10.917: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:15:11.056: INFO: Waiting up to 5m0s for pod "downwardapi-volume-869fa63a-1d3b-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-vc2d6" to be "success or failure"
Jan 21 05:15:11.060: INFO: Pod "downwardapi-volume-869fa63a-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.674628ms
Jan 21 05:15:13.067: INFO: Pod "downwardapi-volume-869fa63a-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010913238s
Jan 21 05:15:15.073: INFO: Pod "downwardapi-volume-869fa63a-1d3b-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017193595s
STEP: Saw pod success
Jan 21 05:15:15.073: INFO: Pod "downwardapi-volume-869fa63a-1d3b-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:15:15.077: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-869fa63a-1d3b-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:15:15.113: INFO: Waiting for pod downwardapi-volume-869fa63a-1d3b-11e9-b032-0a580af40356 to disappear
Jan 21 05:15:15.117: INFO: Pod downwardapi-volume-869fa63a-1d3b-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:15:15.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vc2d6" for this suite.
Jan 21 05:15:21.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:15:21.177: INFO: namespace: e2e-tests-downward-api-vc2d6, resource: bindings, ignored listing per whitelist
Jan 21 05:15:21.296: INFO: namespace e2e-tests-downward-api-vc2d6 deletion completed in 6.17283877s

â€¢ [SLOW TEST:10.379 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:15:21.297: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:15:21.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 version'
Jan 21 05:15:21.573: INFO: stderr: ""
Jan 21 05:15:21.574: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.1\", GitCommit:\"eec55b9ba98609a46fee712359c7b5b365bdd920\", GitTreeState:\"clean\", BuildDate:\"2018-12-13T10:31:33Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:15:21.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7lgk2" for this suite.
Jan 21 05:15:27.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:15:27.644: INFO: namespace: e2e-tests-kubectl-7lgk2, resource: bindings, ignored listing per whitelist
Jan 21 05:15:27.757: INFO: namespace e2e-tests-kubectl-7lgk2 deletion completed in 6.175919816s

â€¢ [SLOW TEST:6.460 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:15:27.757: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 21 05:15:27.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-hq26h,SelfLink:/api/v1/namespaces/e2e-tests-watch-hq26h/configmaps/e2e-watch-test-resource-version,UID:90a36520-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32391,Generation:0,CreationTimestamp:2019-01-21 05:15:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 21 05:15:27.931: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-hq26h,SelfLink:/api/v1/namespaces/e2e-tests-watch-hq26h/configmaps/e2e-watch-test-resource-version,UID:90a36520-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32392,Generation:0,CreationTimestamp:2019-01-21 05:15:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:15:27.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-hq26h" for this suite.
Jan 21 05:15:33.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:15:34.000: INFO: namespace: e2e-tests-watch-hq26h, resource: bindings, ignored listing per whitelist
Jan 21 05:15:34.145: INFO: namespace e2e-tests-watch-hq26h deletion completed in 6.206858359s

â€¢ [SLOW TEST:6.388 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:15:34.146: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Jan 21 05:15:34.307: INFO: Waiting up to 5m0s for pod "client-containers-947a8076-1d3b-11e9-b032-0a580af40356" in namespace "e2e-tests-containers-xl4b9" to be "success or failure"
Jan 21 05:15:34.317: INFO: Pod "client-containers-947a8076-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.573142ms
Jan 21 05:15:36.322: INFO: Pod "client-containers-947a8076-1d3b-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01495053s
STEP: Saw pod success
Jan 21 05:15:36.322: INFO: Pod "client-containers-947a8076-1d3b-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:15:36.326: INFO: Trying to get logs from node k8s05 pod client-containers-947a8076-1d3b-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:15:36.356: INFO: Waiting for pod client-containers-947a8076-1d3b-11e9-b032-0a580af40356 to disappear
Jan 21 05:15:36.359: INFO: Pod client-containers-947a8076-1d3b-11e9-b032-0a580af40356 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:15:36.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-xl4b9" for this suite.
Jan 21 05:15:42.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:15:42.445: INFO: namespace: e2e-tests-containers-xl4b9, resource: bindings, ignored listing per whitelist
Jan 21 05:15:42.535: INFO: namespace e2e-tests-containers-xl4b9 deletion completed in 6.170205625s

â€¢ [SLOW TEST:8.389 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:15:42.535: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 21 05:15:45.185: INFO: Successfully updated pod "pod-update-9972e0d6-1d3b-11e9-b032-0a580af40356"
STEP: verifying the updated pod is in kubernetes
Jan 21 05:15:45.201: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:15:45.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-xvph5" for this suite.
Jan 21 05:16:07.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:16:07.258: INFO: namespace: e2e-tests-pods-xvph5, resource: bindings, ignored listing per whitelist
Jan 21 05:16:07.403: INFO: namespace e2e-tests-pods-xvph5 deletion completed in 22.193030097s

â€¢ [SLOW TEST:24.868 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:16:07.403: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 21 05:16:07.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-a,UID:a849a06d-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32516,Generation:0,CreationTimestamp:2019-01-21 05:16:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 21 05:16:07.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-a,UID:a849a06d-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32516,Generation:0,CreationTimestamp:2019-01-21 05:16:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 21 05:16:17.565: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-a,UID:a849a06d-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32534,Generation:0,CreationTimestamp:2019-01-21 05:16:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 21 05:16:17.565: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-a,UID:a849a06d-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32534,Generation:0,CreationTimestamp:2019-01-21 05:16:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 21 05:16:27.596: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-a,UID:a849a06d-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32552,Generation:0,CreationTimestamp:2019-01-21 05:16:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 21 05:16:27.596: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-a,UID:a849a06d-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32552,Generation:0,CreationTimestamp:2019-01-21 05:16:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 21 05:16:37.623: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-a,UID:a849a06d-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32570,Generation:0,CreationTimestamp:2019-01-21 05:16:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 21 05:16:37.623: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-a,UID:a849a06d-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32570,Generation:0,CreationTimestamp:2019-01-21 05:16:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 21 05:16:47.645: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-b,UID:c0326016-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32588,Generation:0,CreationTimestamp:2019-01-21 05:16:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 21 05:16:47.646: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-b,UID:c0326016-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32588,Generation:0,CreationTimestamp:2019-01-21 05:16:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 21 05:16:57.672: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-b,UID:c0326016-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32606,Generation:0,CreationTimestamp:2019-01-21 05:16:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 21 05:16:57.672: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-nw6zs,SelfLink:/api/v1/namespaces/e2e-tests-watch-nw6zs/configmaps/e2e-watch-test-configmap-b,UID:c0326016-1d3b-11e9-9faa-fa163e6f5c57,ResourceVersion:32606,Generation:0,CreationTimestamp:2019-01-21 05:16:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:17:07.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-nw6zs" for this suite.
Jan 21 05:17:13.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:17:13.791: INFO: namespace: e2e-tests-watch-nw6zs, resource: bindings, ignored listing per whitelist
Jan 21 05:17:13.841: INFO: namespace e2e-tests-watch-nw6zs deletion completed in 6.148695282s

â€¢ [SLOW TEST:66.438 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:17:13.842: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 21 05:17:20.008: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:20.008: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:20.189: INFO: Exec stderr: ""
Jan 21 05:17:20.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:20.189: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:20.337: INFO: Exec stderr: ""
Jan 21 05:17:20.337: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:20.338: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:20.545: INFO: Exec stderr: ""
Jan 21 05:17:20.545: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:20.546: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:20.703: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 21 05:17:20.703: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:20.703: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:20.860: INFO: Exec stderr: ""
Jan 21 05:17:20.860: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:20.860: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:20.983: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 21 05:17:20.983: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:20.983: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:21.087: INFO: Exec stderr: ""
Jan 21 05:17:21.087: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:21.087: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:21.240: INFO: Exec stderr: ""
Jan 21 05:17:21.240: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:21.240: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:21.405: INFO: Exec stderr: ""
Jan 21 05:17:21.405: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-kmvz4 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:17:21.405: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:17:21.561: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:17:21.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-kmvz4" for this suite.
Jan 21 05:18:01.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:18:01.690: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-kmvz4, resource: bindings, ignored listing per whitelist
Jan 21 05:18:01.729: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-kmvz4 deletion completed in 40.159406203s

â€¢ [SLOW TEST:47.887 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:18:01.729: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-ec6e735c-1d3b-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:18:01.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec705258-1d3b-11e9-b032-0a580af40356" in namespace "e2e-tests-configmap-5sqnk" to be "success or failure"
Jan 21 05:18:01.874: INFO: Pod "pod-configmaps-ec705258-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 7.033701ms
Jan 21 05:18:03.890: INFO: Pod "pod-configmaps-ec705258-1d3b-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022314189s
Jan 21 05:18:05.895: INFO: Pod "pod-configmaps-ec705258-1d3b-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027684052s
STEP: Saw pod success
Jan 21 05:18:05.895: INFO: Pod "pod-configmaps-ec705258-1d3b-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:18:05.899: INFO: Trying to get logs from node k8s05 pod pod-configmaps-ec705258-1d3b-11e9-b032-0a580af40356 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:18:05.938: INFO: Waiting for pod pod-configmaps-ec705258-1d3b-11e9-b032-0a580af40356 to disappear
Jan 21 05:18:05.942: INFO: Pod pod-configmaps-ec705258-1d3b-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:18:05.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5sqnk" for this suite.
Jan 21 05:18:11.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:18:12.148: INFO: namespace: e2e-tests-configmap-5sqnk, resource: bindings, ignored listing per whitelist
Jan 21 05:18:12.184: INFO: namespace e2e-tests-configmap-5sqnk deletion completed in 6.235515686s

â€¢ [SLOW TEST:10.455 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:18:12.185: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-f2ae9e2e-1d3b-11e9-b032-0a580af40356
STEP: Creating configMap with name cm-test-opt-upd-f2ae9ea9-1d3b-11e9-b032-0a580af40356
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f2ae9e2e-1d3b-11e9-b032-0a580af40356
STEP: Updating configmap cm-test-opt-upd-f2ae9ea9-1d3b-11e9-b032-0a580af40356
STEP: Creating configMap with name cm-test-opt-create-f2ae9ed2-1d3b-11e9-b032-0a580af40356
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:18:16.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-c7mzz" for this suite.
Jan 21 05:18:38.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:18:38.780: INFO: namespace: e2e-tests-configmap-c7mzz, resource: bindings, ignored listing per whitelist
Jan 21 05:18:38.793: INFO: namespace e2e-tests-configmap-c7mzz deletion completed in 22.192048465s

â€¢ [SLOW TEST:26.608 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:18:38.793: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Jan 21 05:18:38.908: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jan 21 05:18:38.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:39.328: INFO: stderr: ""
Jan 21 05:18:39.328: INFO: stdout: "service/redis-slave created\n"
Jan 21 05:18:39.329: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jan 21 05:18:39.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:39.721: INFO: stderr: ""
Jan 21 05:18:39.721: INFO: stdout: "service/redis-master created\n"
Jan 21 05:18:39.721: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 21 05:18:39.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:40.070: INFO: stderr: ""
Jan 21 05:18:40.070: INFO: stdout: "service/frontend created\n"
Jan 21 05:18:40.071: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jan 21 05:18:40.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:40.384: INFO: stderr: ""
Jan 21 05:18:40.384: INFO: stdout: "deployment.extensions/frontend created\n"
Jan 21 05:18:40.384: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 21 05:18:40.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:40.748: INFO: stderr: ""
Jan 21 05:18:40.748: INFO: stdout: "deployment.extensions/redis-master created\n"
Jan 21 05:18:40.748: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jan 21 05:18:40.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:41.066: INFO: stderr: ""
Jan 21 05:18:41.066: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Jan 21 05:18:41.067: INFO: Waiting for all frontend pods to be Running.
Jan 21 05:18:46.117: INFO: Waiting for frontend to serve content.
Jan 21 05:18:46.156: INFO: Trying to add a new entry to the guestbook.
Jan 21 05:18:46.184: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 21 05:18:46.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:46.469: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:18:46.469: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 05:18:46.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:46.676: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:18:46.676: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 05:18:46.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:46.955: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:18:46.955: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 05:18:46.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:47.124: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:18:47.124: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 05:18:47.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:47.391: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:18:47.391: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 21 05:18:47.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dms46'
Jan 21 05:18:47.588: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:18:47.588: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:18:47.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dms46" for this suite.
Jan 21 05:19:29.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:19:29.680: INFO: namespace: e2e-tests-kubectl-dms46, resource: bindings, ignored listing per whitelist
Jan 21 05:19:29.844: INFO: namespace e2e-tests-kubectl-dms46 deletion completed in 42.244723991s

â€¢ [SLOW TEST:51.051 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:19:29.845: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 21 05:19:29.993: INFO: Waiting up to 5m0s for pod "pod-20f5b066-1d3c-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-mr8b8" to be "success or failure"
Jan 21 05:19:30.008: INFO: Pod "pod-20f5b066-1d3c-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 15.055658ms
Jan 21 05:19:32.024: INFO: Pod "pod-20f5b066-1d3c-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03099125s
STEP: Saw pod success
Jan 21 05:19:32.024: INFO: Pod "pod-20f5b066-1d3c-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:19:32.029: INFO: Trying to get logs from node k8s05 pod pod-20f5b066-1d3c-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:19:32.073: INFO: Waiting for pod pod-20f5b066-1d3c-11e9-b032-0a580af40356 to disappear
Jan 21 05:19:32.080: INFO: Pod pod-20f5b066-1d3c-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:19:32.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-mr8b8" for this suite.
Jan 21 05:19:38.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:19:38.230: INFO: namespace: e2e-tests-emptydir-mr8b8, resource: bindings, ignored listing per whitelist
Jan 21 05:19:38.294: INFO: namespace e2e-tests-emptydir-mr8b8 deletion completed in 6.205808645s

â€¢ [SLOW TEST:8.450 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:19:38.294: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Jan 21 05:19:38.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 cluster-info'
Jan 21 05:19:38.595: INFO: stderr: ""
Jan 21 05:19:38.595: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:19:38.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-bzsft" for this suite.
Jan 21 05:19:44.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:19:44.742: INFO: namespace: e2e-tests-kubectl-bzsft, resource: bindings, ignored listing per whitelist
Jan 21 05:19:44.792: INFO: namespace e2e-tests-kubectl-bzsft deletion completed in 6.188366196s

â€¢ [SLOW TEST:6.498 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:19:44.792: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 21 05:19:48.991: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 21 05:19:48.996: INFO: Pod pod-with-prestop-http-hook still exists
Jan 21 05:19:50.996: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 21 05:19:51.004: INFO: Pod pod-with-prestop-http-hook still exists
Jan 21 05:19:52.996: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 21 05:19:53.009: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:19:53.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-87g8x" for this suite.
Jan 21 05:20:15.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:20:15.130: INFO: namespace: e2e-tests-container-lifecycle-hook-87g8x, resource: bindings, ignored listing per whitelist
Jan 21 05:20:15.214: INFO: namespace e2e-tests-container-lifecycle-hook-87g8x deletion completed in 22.182302631s

â€¢ [SLOW TEST:30.422 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:20:15.215: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:20:15.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 version --client'
Jan 21 05:20:15.449: INFO: stderr: ""
Jan 21 05:20:15.449: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jan 21 05:20:15.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-s4zhm'
Jan 21 05:20:15.812: INFO: stderr: ""
Jan 21 05:20:15.812: INFO: stdout: "replicationcontroller/redis-master created\n"
Jan 21 05:20:15.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-s4zhm'
Jan 21 05:20:16.211: INFO: stderr: ""
Jan 21 05:20:16.211: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 21 05:20:17.217: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:20:17.217: INFO: Found 0 / 1
Jan 21 05:20:18.218: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:20:18.218: INFO: Found 1 / 1
Jan 21 05:20:18.218: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 21 05:20:18.223: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:20:18.223: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 21 05:20:18.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 describe pod redis-master-bcmpr --namespace=e2e-tests-kubectl-s4zhm'
Jan 21 05:20:18.396: INFO: stderr: ""
Jan 21 05:20:18.396: INFO: stdout: "Name:               redis-master-bcmpr\nNamespace:          e2e-tests-kubectl-s4zhm\nPriority:           0\nPriorityClassName:  <none>\nNode:               k8s05/192.168.1.21\nStart Time:         Mon, 21 Jan 2019 05:20:15 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.244.3.154\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://8770dd62f8e6d430b4ae16221a8d24452fd7d9a34f5561507baadadf2cb7e99b\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 21 Jan 2019 05:20:16 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6mdkn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-6mdkn:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-6mdkn\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned e2e-tests-kubectl-s4zhm/redis-master-bcmpr to k8s05\n  Normal  Pulled     2s    kubelet, k8s05     Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, k8s05     Created container\n  Normal  Started    2s    kubelet, k8s05     Started container\n"
Jan 21 05:20:18.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 describe rc redis-master --namespace=e2e-tests-kubectl-s4zhm'
Jan 21 05:20:18.626: INFO: stderr: ""
Jan 21 05:20:18.626: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-s4zhm\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-bcmpr\n"
Jan 21 05:20:18.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 describe service redis-master --namespace=e2e-tests-kubectl-s4zhm'
Jan 21 05:20:18.783: INFO: stderr: ""
Jan 21 05:20:18.783: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-s4zhm\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.107.209.161\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.3.154:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 21 05:20:18.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 describe node k8s01'
Jan 21 05:20:18.995: INFO: stderr: ""
Jan 21 05:20:18.995: INFO: stdout: "Name:               k8s01\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=k8s01\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"8e:76:cc:40:a5:3f\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.27\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 21 Jan 2019 02:10:38 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 21 Jan 2019 05:20:17 +0000   Mon, 21 Jan 2019 02:10:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 21 Jan 2019 05:20:17 +0000   Mon, 21 Jan 2019 02:10:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 21 Jan 2019 05:20:17 +0000   Mon, 21 Jan 2019 02:10:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 21 Jan 2019 05:20:17 +0000   Mon, 21 Jan 2019 02:11:03 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.1.27\n  Hostname:    k8s01\nCapacity:\n cpu:                2\n ephemeral-storage:  20960236Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3881676Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  19316953466\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3779276Ki\n pods:               110\nSystem Info:\n Machine ID:                 88faeb135cae489480fcf328df3734f4\n System UUID:                C5161CE8-D45D-49BD-87D7-049224F84B46\n Boot ID:                    9f4284aa-5cf4-4316-a21d-d1fe85f045e4\n Kernel Version:             3.10.0-693.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.13.1\n Kube-Proxy Version:         v1.13.1\nPodCIDR:                     10.244.0.0/24\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-713071a3b78f48ee-pbccn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  kube-system                kube-apiserver-k8s01                                       250m (12%)    0 (0%)      0 (0%)           0 (0%)         3h9m\n  kube-system                kube-controller-manager-k8s01                              200m (10%)    0 (0%)      0 (0%)           0 (0%)         3h9m\n  kube-system                kube-flannel-ds-tcv9k                                      100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      3h9m\n  kube-system                kube-proxy-tghbr                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h9m\n  kube-system                kube-scheduler-k8s01                                       100m (5%)     0 (0%)      0 (0%)           0 (0%)         3h9m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (32%)  100m (5%)\n  memory             50Mi (1%)   50Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Jan 21 05:20:18.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 describe namespace e2e-tests-kubectl-s4zhm'
Jan 21 05:20:19.190: INFO: stderr: ""
Jan 21 05:20:19.190: INFO: stdout: "Name:         e2e-tests-kubectl-s4zhm\nLabels:       e2e-framework=kubectl\n              e2e-run=f9cd8265-1d38-11e9-b032-0a580af40356\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:20:19.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-s4zhm" for this suite.
Jan 21 05:20:41.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:20:41.299: INFO: namespace: e2e-tests-kubectl-s4zhm, resource: bindings, ignored listing per whitelist
Jan 21 05:20:41.390: INFO: namespace e2e-tests-kubectl-s4zhm deletion completed in 22.190702411s

â€¢ [SLOW TEST:26.175 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:20:41.390: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:20:41.521: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b984fb7-1d3c-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-mlh27" to be "success or failure"
Jan 21 05:20:41.534: INFO: Pod "downwardapi-volume-4b984fb7-1d3c-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 13.433732ms
Jan 21 05:20:43.540: INFO: Pod "downwardapi-volume-4b984fb7-1d3c-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019728988s
STEP: Saw pod success
Jan 21 05:20:43.540: INFO: Pod "downwardapi-volume-4b984fb7-1d3c-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:20:43.546: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-4b984fb7-1d3c-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:20:43.583: INFO: Waiting for pod downwardapi-volume-4b984fb7-1d3c-11e9-b032-0a580af40356 to disappear
Jan 21 05:20:43.587: INFO: Pod downwardapi-volume-4b984fb7-1d3c-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:20:43.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mlh27" for this suite.
Jan 21 05:20:49.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:20:49.637: INFO: namespace: e2e-tests-projected-mlh27, resource: bindings, ignored listing per whitelist
Jan 21 05:20:49.801: INFO: namespace e2e-tests-projected-mlh27 deletion completed in 6.208152313s

â€¢ [SLOW TEST:8.411 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:20:49.801: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 21 05:20:49.918: INFO: Waiting up to 5m0s for pod "pod-509a015f-1d3c-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-55h2c" to be "success or failure"
Jan 21 05:20:49.926: INFO: Pod "pod-509a015f-1d3c-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 8.521586ms
Jan 21 05:20:51.933: INFO: Pod "pod-509a015f-1d3c-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015592775s
STEP: Saw pod success
Jan 21 05:20:51.933: INFO: Pod "pod-509a015f-1d3c-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:20:51.938: INFO: Trying to get logs from node k8s05 pod pod-509a015f-1d3c-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:20:51.990: INFO: Waiting for pod pod-509a015f-1d3c-11e9-b032-0a580af40356 to disappear
Jan 21 05:20:51.995: INFO: Pod pod-509a015f-1d3c-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:20:51.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-55h2c" for this suite.
Jan 21 05:20:58.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:20:58.203: INFO: namespace: e2e-tests-emptydir-55h2c, resource: bindings, ignored listing per whitelist
Jan 21 05:20:58.229: INFO: namespace e2e-tests-emptydir-55h2c deletion completed in 6.223828314s

â€¢ [SLOW TEST:8.428 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:20:58.229: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 21 05:20:58.354: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 21 05:20:58.370: INFO: Waiting for terminating namespaces to be deleted...
Jan 21 05:20:58.375: INFO: 
Logging pods the kubelet thinks is on node k8s05 before test
Jan 21 05:20:58.393: INFO: sonobuoy-e2e-job-4ff7b51a2b72420b from heptio-sonobuoy started at 2019-01-21 04:55:06 +0000 UTC (2 container statuses recorded)
Jan 21 05:20:58.393: INFO: 	Container e2e ready: true, restart count 0
Jan 21 05:20:58.393: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 05:20:58.393: INFO: sonobuoy-systemd-logs-daemon-set-713071a3b78f48ee-9xf85 from heptio-sonobuoy started at 2019-01-21 04:55:06 +0000 UTC (2 container statuses recorded)
Jan 21 05:20:58.393: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 21 05:20:58.393: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 05:20:58.393: INFO: kube-proxy-q2nqf from kube-system started at 2019-01-21 02:11:36 +0000 UTC (1 container statuses recorded)
Jan 21 05:20:58.393: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 21 05:20:58.393: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-21 04:55:03 +0000 UTC (1 container statuses recorded)
Jan 21 05:20:58.393: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 21 05:20:58.393: INFO: kube-flannel-ds-9gnm8 from kube-system started at 2019-01-21 02:11:36 +0000 UTC (1 container statuses recorded)
Jan 21 05:20:58.393: INFO: 	Container kube-flannel ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-56e4f4ad-1d3c-11e9-b032-0a580af40356 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-56e4f4ad-1d3c-11e9-b032-0a580af40356 off the node k8s05
STEP: verifying the node doesn't have the label kubernetes.io/e2e-56e4f4ad-1d3c-11e9-b032-0a580af40356
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:21:02.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-vstbt" for this suite.
Jan 21 05:21:20.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:21:20.616: INFO: namespace: e2e-tests-sched-pred-vstbt, resource: bindings, ignored listing per whitelist
Jan 21 05:21:20.727: INFO: namespace e2e-tests-sched-pred-vstbt deletion completed in 18.186134527s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:22.498 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:21:20.727: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0121 05:21:21.927151      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 21 05:21:21.927: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:21:21.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-m88ff" for this suite.
Jan 21 05:21:27.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:21:28.130: INFO: namespace: e2e-tests-gc-m88ff, resource: bindings, ignored listing per whitelist
Jan 21 05:21:28.135: INFO: namespace e2e-tests-gc-m88ff deletion completed in 6.200797644s

â€¢ [SLOW TEST:7.408 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:21:28.136: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-6775aaf4-1d3c-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:21:28.279: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6777bcd7-1d3c-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-65zkh" to be "success or failure"
Jan 21 05:21:28.283: INFO: Pod "pod-projected-configmaps-6777bcd7-1d3c-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.534797ms
Jan 21 05:21:30.291: INFO: Pod "pod-projected-configmaps-6777bcd7-1d3c-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012397763s
STEP: Saw pod success
Jan 21 05:21:30.291: INFO: Pod "pod-projected-configmaps-6777bcd7-1d3c-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:21:30.297: INFO: Trying to get logs from node k8s05 pod pod-projected-configmaps-6777bcd7-1d3c-11e9-b032-0a580af40356 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:21:30.345: INFO: Waiting for pod pod-projected-configmaps-6777bcd7-1d3c-11e9-b032-0a580af40356 to disappear
Jan 21 05:21:30.352: INFO: Pod pod-projected-configmaps-6777bcd7-1d3c-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:21:30.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-65zkh" for this suite.
Jan 21 05:21:36.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:21:36.429: INFO: namespace: e2e-tests-projected-65zkh, resource: bindings, ignored listing per whitelist
Jan 21 05:21:36.524: INFO: namespace e2e-tests-projected-65zkh deletion completed in 6.164139144s

â€¢ [SLOW TEST:8.389 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:21:36.524: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Jan 21 05:21:36.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 api-versions'
Jan 21 05:21:36.805: INFO: stderr: ""
Jan 21 05:21:36.805: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:21:36.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pb782" for this suite.
Jan 21 05:21:42.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:21:42.867: INFO: namespace: e2e-tests-kubectl-pb782, resource: bindings, ignored listing per whitelist
Jan 21 05:21:43.021: INFO: namespace e2e-tests-kubectl-pb782 deletion completed in 6.208067083s

â€¢ [SLOW TEST:6.496 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:21:43.021: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:22:05.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-runtime-ff9zp" for this suite.
Jan 21 05:22:11.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:22:11.711: INFO: namespace: e2e-tests-container-runtime-ff9zp, resource: bindings, ignored listing per whitelist
Jan 21 05:22:11.758: INFO: namespace e2e-tests-container-runtime-ff9zp deletion completed in 6.203015854s

â€¢ [SLOW TEST:28.737 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  blackbox test
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:22:11.759: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-mwctv A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-mwctv;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-mwctv A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-mwctv.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-mwctv.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-mwctv.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-mwctv.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-mwctv.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-mwctv.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 245.97.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.97.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.97.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.97.245_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-mwctv A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-mwctv;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-mwctv A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-mwctv;check="$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-mwctv.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-mwctv.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-mwctv.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-mwctv.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-mwctv.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 245.97.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.97.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.97.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.97.245_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 05:22:15.994: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.007: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.018: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.025: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.031: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.038: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.044: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.050: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.055: INFO: Unable to read wheezy_udp@PodARecord from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.062: INFO: Unable to read wheezy_tcp@PodARecord from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.068: INFO: Unable to read 10.97.97.245_udp@PTR from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.075: INFO: Unable to read 10.97.97.245_tcp@PTR from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.082: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.088: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.093: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.098: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.103: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.109: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.114: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.120: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.126: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.132: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.138: INFO: Unable to read jessie_udp@PodARecord from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.144: INFO: Unable to read jessie_tcp@PodARecord from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.151: INFO: Unable to read 10.97.97.245_udp@PTR from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.156: INFO: Unable to read 10.97.97.245_tcp@PTR from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:16.156: INFO: Lookups using e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356 failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv wheezy_udp@dns-test-service.e2e-tests-dns-mwctv.svc wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.97.97.245_udp@PTR 10.97.97.245_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-mwctv jessie_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-mwctv.svc jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-mwctv.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.97.97.245_udp@PTR 10.97.97.245_tcp@PTR]

Jan 21 05:22:21.173: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:21.187: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:21.257: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:21.263: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:21.268: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:21.273: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:21.278: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:21.284: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:21.342: INFO: Lookups using e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356 failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-mwctv jessie_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc]

Jan 21 05:22:26.176: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:26.187: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:26.245: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:26.252: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:26.258: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:26.264: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:26.269: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:26.274: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:26.322: INFO: Lookups using e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356 failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-mwctv jessie_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc]

Jan 21 05:22:31.173: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:31.185: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:31.264: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:31.271: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:31.277: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:31.284: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:31.291: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:31.298: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:31.355: INFO: Lookups using e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356 failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-mwctv jessie_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc]

Jan 21 05:22:36.169: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:36.188: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:36.251: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:36.256: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:36.261: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:36.268: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:36.272: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:36.276: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:36.316: INFO: Lookups using e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356 failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-mwctv jessie_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc]

Jan 21 05:22:41.173: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:41.185: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:41.264: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:41.271: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:41.277: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:41.283: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:41.290: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:41.296: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc from pod e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356: the server could not find the requested resource (get pods dns-test-817e1471-1d3c-11e9-b032-0a580af40356)
Jan 21 05:22:41.353: INFO: Lookups using e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356 failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-mwctv jessie_tcp@dns-test-service.e2e-tests-dns-mwctv jessie_udp@dns-test-service.e2e-tests-dns-mwctv.svc jessie_tcp@dns-test-service.e2e-tests-dns-mwctv.svc]

Jan 21 05:22:46.341: INFO: DNS probes using e2e-tests-dns-mwctv/dns-test-817e1471-1d3c-11e9-b032-0a580af40356 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:22:46.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-mwctv" for this suite.
Jan 21 05:22:52.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:22:52.662: INFO: namespace: e2e-tests-dns-mwctv, resource: bindings, ignored listing per whitelist
Jan 21 05:22:52.694: INFO: namespace e2e-tests-dns-mwctv deletion completed in 6.192735919s

â€¢ [SLOW TEST:40.936 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:22:52.695: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-99daec8c-1d3c-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:22:52.879: INFO: Waiting up to 5m0s for pod "pod-secrets-99e4a543-1d3c-11e9-b032-0a580af40356" in namespace "e2e-tests-secrets-slsmz" to be "success or failure"
Jan 21 05:22:52.887: INFO: Pod "pod-secrets-99e4a543-1d3c-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 7.214986ms
Jan 21 05:22:54.892: INFO: Pod "pod-secrets-99e4a543-1d3c-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012888106s
Jan 21 05:22:56.908: INFO: Pod "pod-secrets-99e4a543-1d3c-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02852295s
STEP: Saw pod success
Jan 21 05:22:56.908: INFO: Pod "pod-secrets-99e4a543-1d3c-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:22:56.913: INFO: Trying to get logs from node k8s05 pod pod-secrets-99e4a543-1d3c-11e9-b032-0a580af40356 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:22:56.952: INFO: Waiting for pod pod-secrets-99e4a543-1d3c-11e9-b032-0a580af40356 to disappear
Jan 21 05:22:56.955: INFO: Pod pod-secrets-99e4a543-1d3c-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:22:56.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-slsmz" for this suite.
Jan 21 05:23:02.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:23:03.072: INFO: namespace: e2e-tests-secrets-slsmz, resource: bindings, ignored listing per whitelist
Jan 21 05:23:03.129: INFO: namespace e2e-tests-secrets-slsmz deletion completed in 6.167674571s
STEP: Destroying namespace "e2e-tests-secret-namespace-7k8lm" for this suite.
Jan 21 05:23:09.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:23:09.184: INFO: namespace: e2e-tests-secret-namespace-7k8lm, resource: bindings, ignored listing per whitelist
Jan 21 05:23:09.308: INFO: namespace e2e-tests-secret-namespace-7k8lm deletion completed in 6.178524409s

â€¢ [SLOW TEST:16.613 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:23:09.308: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Jan 21 05:23:09.406: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-453531067 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:23:09.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-fhkzj" for this suite.
Jan 21 05:23:15.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:23:15.615: INFO: namespace: e2e-tests-kubectl-fhkzj, resource: bindings, ignored listing per whitelist
Jan 21 05:23:15.739: INFO: namespace e2e-tests-kubectl-fhkzj deletion completed in 6.185564547s

â€¢ [SLOW TEST:6.431 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:23:15.740: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 21 05:23:15.854: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 21 05:23:15.869: INFO: Waiting for terminating namespaces to be deleted...
Jan 21 05:23:15.874: INFO: 
Logging pods the kubelet thinks is on node k8s05 before test
Jan 21 05:23:15.890: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-21 04:55:03 +0000 UTC (1 container statuses recorded)
Jan 21 05:23:15.890: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 21 05:23:15.890: INFO: kube-flannel-ds-9gnm8 from kube-system started at 2019-01-21 02:11:36 +0000 UTC (1 container statuses recorded)
Jan 21 05:23:15.890: INFO: 	Container kube-flannel ready: true, restart count 1
Jan 21 05:23:15.890: INFO: sonobuoy-e2e-job-4ff7b51a2b72420b from heptio-sonobuoy started at 2019-01-21 04:55:06 +0000 UTC (2 container statuses recorded)
Jan 21 05:23:15.890: INFO: 	Container e2e ready: true, restart count 0
Jan 21 05:23:15.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 05:23:15.890: INFO: sonobuoy-systemd-logs-daemon-set-713071a3b78f48ee-9xf85 from heptio-sonobuoy started at 2019-01-21 04:55:06 +0000 UTC (2 container statuses recorded)
Jan 21 05:23:15.890: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Jan 21 05:23:15.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 05:23:15.890: INFO: kube-proxy-q2nqf from kube-system started at 2019-01-21 02:11:36 +0000 UTC (1 container statuses recorded)
Jan 21 05:23:15.890: INFO: 	Container kube-proxy ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.157bc5a1dd85d4cf], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:23:16.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-jx5cq" for this suite.
Jan 21 05:23:22.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:23:23.095: INFO: namespace: e2e-tests-sched-pred-jx5cq, resource: bindings, ignored listing per whitelist
Jan 21 05:23:23.144: INFO: namespace e2e-tests-sched-pred-jx5cq deletion completed in 6.190081294s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.405 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:23:23.145: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 21 05:23:23.260: INFO: Waiting up to 5m0s for pod "pod-ac000abe-1d3c-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-f2267" to be "success or failure"
Jan 21 05:23:23.270: INFO: Pod "pod-ac000abe-1d3c-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045146ms
Jan 21 05:23:25.279: INFO: Pod "pod-ac000abe-1d3c-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01926883s
STEP: Saw pod success
Jan 21 05:23:25.279: INFO: Pod "pod-ac000abe-1d3c-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:23:25.284: INFO: Trying to get logs from node k8s05 pod pod-ac000abe-1d3c-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:23:25.321: INFO: Waiting for pod pod-ac000abe-1d3c-11e9-b032-0a580af40356 to disappear
Jan 21 05:23:25.326: INFO: Pod pod-ac000abe-1d3c-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:23:25.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-f2267" for this suite.
Jan 21 05:23:31.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:23:31.453: INFO: namespace: e2e-tests-emptydir-f2267, resource: bindings, ignored listing per whitelist
Jan 21 05:23:31.479: INFO: namespace e2e-tests-emptydir-f2267 deletion completed in 6.145593979s

â€¢ [SLOW TEST:8.335 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:23:31.480: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 21 05:23:31.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-m9ndt'
Jan 21 05:23:32.175: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 21 05:23:32.175: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jan 21 05:23:32.189: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan 21 05:23:32.207: INFO: scanned /root for discovery docs: <nil>
Jan 21 05:23:32.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-m9ndt'
Jan 21 05:23:48.114: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 21 05:23:48.114: INFO: stdout: "Created e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186\nScaling up e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jan 21 05:23:48.114: INFO: stdout: "Created e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186\nScaling up e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jan 21 05:23:48.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-m9ndt'
Jan 21 05:23:48.212: INFO: stderr: ""
Jan 21 05:23:48.212: INFO: stdout: "e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186-cf6wg "
Jan 21 05:23:48.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186-cf6wg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-m9ndt'
Jan 21 05:23:48.315: INFO: stderr: ""
Jan 21 05:23:48.315: INFO: stdout: "true"
Jan 21 05:23:48.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186-cf6wg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-m9ndt'
Jan 21 05:23:48.435: INFO: stderr: ""
Jan 21 05:23:48.435: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jan 21 05:23:48.435: INFO: e2e-test-nginx-rc-68c33ad36c56323044f9b048bfcfd186-cf6wg is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Jan 21 05:23:48.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-m9ndt'
Jan 21 05:23:48.580: INFO: stderr: ""
Jan 21 05:23:48.580: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:23:48.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-m9ndt" for this suite.
Jan 21 05:23:54.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:23:54.730: INFO: namespace: e2e-tests-kubectl-m9ndt, resource: bindings, ignored listing per whitelist
Jan 21 05:23:54.782: INFO: namespace e2e-tests-kubectl-m9ndt deletion completed in 6.191543085s

â€¢ [SLOW TEST:23.302 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:23:54.782: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-bedc90ed-1d3c-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:23:54.928: INFO: Waiting up to 5m0s for pod "pod-configmaps-bedfadfe-1d3c-11e9-b032-0a580af40356" in namespace "e2e-tests-configmap-v6svh" to be "success or failure"
Jan 21 05:23:54.934: INFO: Pod "pod-configmaps-bedfadfe-1d3c-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 6.535378ms
Jan 21 05:23:56.940: INFO: Pod "pod-configmaps-bedfadfe-1d3c-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012224598s
STEP: Saw pod success
Jan 21 05:23:56.940: INFO: Pod "pod-configmaps-bedfadfe-1d3c-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:23:56.947: INFO: Trying to get logs from node k8s05 pod pod-configmaps-bedfadfe-1d3c-11e9-b032-0a580af40356 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:23:57.136: INFO: Waiting for pod pod-configmaps-bedfadfe-1d3c-11e9-b032-0a580af40356 to disappear
Jan 21 05:23:57.141: INFO: Pod pod-configmaps-bedfadfe-1d3c-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:23:57.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-v6svh" for this suite.
Jan 21 05:24:03.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:24:03.297: INFO: namespace: e2e-tests-configmap-v6svh, resource: bindings, ignored listing per whitelist
Jan 21 05:24:03.338: INFO: namespace e2e-tests-configmap-v6svh deletion completed in 6.189157195s

â€¢ [SLOW TEST:8.556 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:24:03.338: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0121 05:24:09.526781      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 21 05:24:09.527: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:24:09.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-st5qh" for this suite.
Jan 21 05:24:15.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:24:15.609: INFO: namespace: e2e-tests-gc-st5qh, resource: bindings, ignored listing per whitelist
Jan 21 05:24:15.695: INFO: namespace e2e-tests-gc-st5qh deletion completed in 6.157803825s

â€¢ [SLOW TEST:12.356 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:24:15.695: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:24:15.796: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 21 05:24:15.811: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 21 05:24:20.829: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 21 05:24:20.829: INFO: Creating deployment "test-rolling-update-deployment"
Jan 21 05:24:20.843: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 21 05:24:20.859: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 21 05:24:22.872: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 21 05:24:22.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683645060, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683645060, loc:(*time.Location)(0x7b33b80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63683645060, loc:(*time.Location)(0x7b33b80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683645060, loc:(*time.Location)(0x7b33b80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-68b55d7bc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 21 05:24:24.889: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 21 05:24:24.906: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-h4z4q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h4z4q/deployments/test-rolling-update-deployment,UID:ce52984a-1d3c-11e9-9faa-fa163e6f5c57,ResourceVersion:34527,Generation:1,CreationTimestamp:2019-01-21 05:24:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-01-21 05:24:20 +0000 UTC 2019-01-21 05:24:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-01-21 05:24:23 +0000 UTC 2019-01-21 05:24:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-68b55d7bc6" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 21 05:24:24.911: INFO: New ReplicaSet "test-rolling-update-deployment-68b55d7bc6" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6,GenerateName:,Namespace:e2e-tests-deployment-h4z4q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h4z4q/replicasets/test-rolling-update-deployment-68b55d7bc6,UID:ce569686-1d3c-11e9-b27d-fa163eb7f963,ResourceVersion:34518,Generation:1,CreationTimestamp:2019-01-21 05:24:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ce52984a-1d3c-11e9-9faa-fa163e6f5c57 0xc002fd8b07 0xc002fd8b08}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 21 05:24:24.911: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 21 05:24:24.911: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-h4z4q,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-h4z4q/replicasets/test-rolling-update-controller,UID:cb528a31-1d3c-11e9-9faa-fa163e6f5c57,ResourceVersion:34526,Generation:2,CreationTimestamp:2019-01-21 05:24:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ce52984a-1d3c-11e9-9faa-fa163e6f5c57 0xc002fd8a47 0xc002fd8a48}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 21 05:24:24.917: INFO: Pod "test-rolling-update-deployment-68b55d7bc6-tdwg5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-68b55d7bc6-tdwg5,GenerateName:test-rolling-update-deployment-68b55d7bc6-,Namespace:e2e-tests-deployment-h4z4q,SelfLink:/api/v1/namespaces/e2e-tests-deployment-h4z4q/pods/test-rolling-update-deployment-68b55d7bc6-tdwg5,UID:ce57c547-1d3c-11e9-b27d-fa163eb7f963,ResourceVersion:34517,Generation:0,CreationTimestamp:2019-01-21 05:24:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 68b55d7bc6,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-68b55d7bc6 ce569686-1d3c-11e9-b27d-fa163eb7f963 0xc002fd93c7 0xc002fd93c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-fdzcx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fdzcx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-fdzcx true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fd9440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fd9460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:24:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:24:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:24:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:24:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.184,StartTime:2019-01-21 05:24:20 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-01-21 05:24:21 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 docker://3e0f03d3357bd3f0f854cfb0b93ebcf66e1c6679b32e8352e1cd582a5d8f26a0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:24:24.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-h4z4q" for this suite.
Jan 21 05:24:30.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:24:31.130: INFO: namespace: e2e-tests-deployment-h4z4q, resource: bindings, ignored listing per whitelist
Jan 21 05:24:31.135: INFO: namespace e2e-tests-deployment-h4z4q deletion completed in 6.210660954s

â€¢ [SLOW TEST:15.440 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:24:31.135: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Jan 21 05:24:31.830: INFO: created pod pod-service-account-defaultsa
Jan 21 05:24:31.830: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 21 05:24:31.841: INFO: created pod pod-service-account-mountsa
Jan 21 05:24:31.841: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 21 05:24:31.858: INFO: created pod pod-service-account-nomountsa
Jan 21 05:24:31.858: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 21 05:24:31.879: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 21 05:24:31.879: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 21 05:24:31.890: INFO: created pod pod-service-account-mountsa-mountspec
Jan 21 05:24:31.890: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 21 05:24:31.914: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 21 05:24:31.914: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 21 05:24:31.929: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 21 05:24:31.929: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 21 05:24:31.951: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 21 05:24:31.951: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 21 05:24:31.972: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 21 05:24:31.972: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:24:31.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-q8mk5" for this suite.
Jan 21 05:24:54.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:24:54.075: INFO: namespace: e2e-tests-svcaccounts-q8mk5, resource: bindings, ignored listing per whitelist
Jan 21 05:24:54.143: INFO: namespace e2e-tests-svcaccounts-q8mk5 deletion completed in 22.153076051s

â€¢ [SLOW TEST:23.008 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:24:54.143: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-75qft
I0121 05:24:54.286124      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-75qft, replica count: 1
I0121 05:24:55.337577      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0121 05:24:56.337878      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 21 05:24:56.469: INFO: Created: latency-svc-84fd4
Jan 21 05:24:56.484: INFO: Got endpoints: latency-svc-84fd4 [46.437039ms]
Jan 21 05:24:56.531: INFO: Created: latency-svc-w28v8
Jan 21 05:24:56.539: INFO: Got endpoints: latency-svc-w28v8 [53.342308ms]
Jan 21 05:24:56.571: INFO: Created: latency-svc-mpr4p
Jan 21 05:24:56.579: INFO: Got endpoints: latency-svc-mpr4p [94.550369ms]
Jan 21 05:24:56.603: INFO: Created: latency-svc-wlxbr
Jan 21 05:24:56.611: INFO: Got endpoints: latency-svc-wlxbr [125.458539ms]
Jan 21 05:24:56.648: INFO: Created: latency-svc-kjw67
Jan 21 05:24:56.654: INFO: Got endpoints: latency-svc-kjw67 [167.468859ms]
Jan 21 05:24:56.680: INFO: Created: latency-svc-jwhsd
Jan 21 05:24:56.686: INFO: Got endpoints: latency-svc-jwhsd [199.774459ms]
Jan 21 05:24:56.711: INFO: Created: latency-svc-z4th2
Jan 21 05:24:56.724: INFO: Got endpoints: latency-svc-z4th2 [237.139004ms]
Jan 21 05:24:56.774: INFO: Created: latency-svc-g8xhl
Jan 21 05:24:56.774: INFO: Got endpoints: latency-svc-g8xhl [287.218249ms]
Jan 21 05:24:56.806: INFO: Created: latency-svc-k7m7r
Jan 21 05:24:56.816: INFO: Got endpoints: latency-svc-k7m7r [328.648614ms]
Jan 21 05:24:56.846: INFO: Created: latency-svc-7xw5x
Jan 21 05:24:56.854: INFO: Got endpoints: latency-svc-7xw5x [366.30106ms]
Jan 21 05:24:56.879: INFO: Created: latency-svc-wnz4w
Jan 21 05:24:56.892: INFO: Got endpoints: latency-svc-wnz4w [404.161597ms]
Jan 21 05:24:56.912: INFO: Created: latency-svc-md5kd
Jan 21 05:24:56.918: INFO: Got endpoints: latency-svc-md5kd [429.679042ms]
Jan 21 05:24:56.953: INFO: Created: latency-svc-rx9b8
Jan 21 05:24:56.958: INFO: Got endpoints: latency-svc-rx9b8 [469.738441ms]
Jan 21 05:24:56.997: INFO: Created: latency-svc-2846z
Jan 21 05:24:57.009: INFO: Got endpoints: latency-svc-2846z [520.773871ms]
Jan 21 05:24:57.042: INFO: Created: latency-svc-thlnv
Jan 21 05:24:57.045: INFO: Got endpoints: latency-svc-thlnv [556.373453ms]
Jan 21 05:24:57.066: INFO: Created: latency-svc-k86tx
Jan 21 05:24:57.073: INFO: Got endpoints: latency-svc-k86tx [584.271906ms]
Jan 21 05:24:57.112: INFO: Created: latency-svc-9fz9x
Jan 21 05:24:57.120: INFO: Got endpoints: latency-svc-9fz9x [580.940741ms]
Jan 21 05:24:57.149: INFO: Created: latency-svc-bp7kp
Jan 21 05:24:57.153: INFO: Got endpoints: latency-svc-bp7kp [573.357104ms]
Jan 21 05:24:57.180: INFO: Created: latency-svc-bghht
Jan 21 05:24:57.181: INFO: Got endpoints: latency-svc-bghht [570.249943ms]
Jan 21 05:24:57.235: INFO: Created: latency-svc-vn7wt
Jan 21 05:24:57.242: INFO: Got endpoints: latency-svc-vn7wt [588.811555ms]
Jan 21 05:24:57.276: INFO: Created: latency-svc-jsts6
Jan 21 05:24:57.288: INFO: Got endpoints: latency-svc-jsts6 [601.809569ms]
Jan 21 05:24:57.324: INFO: Created: latency-svc-dx9gf
Jan 21 05:24:57.335: INFO: Got endpoints: latency-svc-dx9gf [611.530063ms]
Jan 21 05:24:57.359: INFO: Created: latency-svc-tfcwm
Jan 21 05:24:57.365: INFO: Got endpoints: latency-svc-tfcwm [591.062474ms]
Jan 21 05:24:57.390: INFO: Created: latency-svc-bzrl8
Jan 21 05:24:57.409: INFO: Got endpoints: latency-svc-bzrl8 [593.248661ms]
Jan 21 05:24:57.422: INFO: Created: latency-svc-jmhjn
Jan 21 05:24:57.439: INFO: Got endpoints: latency-svc-jmhjn [585.474159ms]
Jan 21 05:24:57.453: INFO: Created: latency-svc-hst57
Jan 21 05:24:57.478: INFO: Got endpoints: latency-svc-hst57 [585.363899ms]
Jan 21 05:24:57.484: INFO: Created: latency-svc-krbtp
Jan 21 05:24:57.491: INFO: Got endpoints: latency-svc-krbtp [573.206317ms]
Jan 21 05:24:57.519: INFO: Created: latency-svc-j8s96
Jan 21 05:24:57.524: INFO: Got endpoints: latency-svc-j8s96 [566.015721ms]
Jan 21 05:24:57.573: INFO: Created: latency-svc-jlh67
Jan 21 05:24:57.580: INFO: Got endpoints: latency-svc-jlh67 [571.23177ms]
Jan 21 05:24:57.606: INFO: Created: latency-svc-drkl7
Jan 21 05:24:57.618: INFO: Got endpoints: latency-svc-drkl7 [573.469785ms]
Jan 21 05:24:57.640: INFO: Created: latency-svc-rqh7d
Jan 21 05:24:57.650: INFO: Got endpoints: latency-svc-rqh7d [577.110675ms]
Jan 21 05:24:57.693: INFO: Created: latency-svc-rht22
Jan 21 05:24:57.708: INFO: Got endpoints: latency-svc-rht22 [588.492936ms]
Jan 21 05:24:57.720: INFO: Created: latency-svc-ph42n
Jan 21 05:24:57.731: INFO: Got endpoints: latency-svc-ph42n [578.303488ms]
Jan 21 05:24:57.759: INFO: Created: latency-svc-gmfzh
Jan 21 05:24:57.767: INFO: Got endpoints: latency-svc-gmfzh [585.655912ms]
Jan 21 05:24:57.802: INFO: Created: latency-svc-6rttq
Jan 21 05:24:57.809: INFO: Got endpoints: latency-svc-6rttq [566.721164ms]
Jan 21 05:24:57.830: INFO: Created: latency-svc-swxcn
Jan 21 05:24:57.836: INFO: Got endpoints: latency-svc-swxcn [548.085799ms]
Jan 21 05:24:57.860: INFO: Created: latency-svc-mxwpx
Jan 21 05:24:57.867: INFO: Got endpoints: latency-svc-mxwpx [531.920917ms]
Jan 21 05:24:57.899: INFO: Created: latency-svc-xrj7m
Jan 21 05:24:57.903: INFO: Got endpoints: latency-svc-xrj7m [537.486734ms]
Jan 21 05:24:57.933: INFO: Created: latency-svc-f4hqh
Jan 21 05:24:57.940: INFO: Got endpoints: latency-svc-f4hqh [530.775861ms]
Jan 21 05:24:57.966: INFO: Created: latency-svc-qb2xz
Jan 21 05:24:57.971: INFO: Got endpoints: latency-svc-qb2xz [531.999092ms]
Jan 21 05:24:58.013: INFO: Created: latency-svc-gszkr
Jan 21 05:24:58.013: INFO: Got endpoints: latency-svc-gszkr [534.961326ms]
Jan 21 05:24:58.046: INFO: Created: latency-svc-cvq6d
Jan 21 05:24:58.053: INFO: Got endpoints: latency-svc-cvq6d [562.361704ms]
Jan 21 05:24:58.079: INFO: Created: latency-svc-hvlz2
Jan 21 05:24:58.082: INFO: Got endpoints: latency-svc-hvlz2 [557.445987ms]
Jan 21 05:24:58.105: INFO: Created: latency-svc-7sd6m
Jan 21 05:24:58.106: INFO: Got endpoints: latency-svc-7sd6m [525.497152ms]
Jan 21 05:24:58.129: INFO: Created: latency-svc-2rzrd
Jan 21 05:24:58.136: INFO: Got endpoints: latency-svc-2rzrd [517.849541ms]
Jan 21 05:24:58.166: INFO: Created: latency-svc-2p82w
Jan 21 05:24:58.176: INFO: Got endpoints: latency-svc-2p82w [525.672927ms]
Jan 21 05:24:58.209: INFO: Created: latency-svc-sj7w5
Jan 21 05:24:58.209: INFO: Got endpoints: latency-svc-sj7w5 [500.665646ms]
Jan 21 05:24:58.238: INFO: Created: latency-svc-m55x9
Jan 21 05:24:58.245: INFO: Got endpoints: latency-svc-m55x9 [513.382201ms]
Jan 21 05:24:58.277: INFO: Created: latency-svc-4scdh
Jan 21 05:24:58.288: INFO: Got endpoints: latency-svc-4scdh [520.725788ms]
Jan 21 05:24:58.297: INFO: Created: latency-svc-vj7kr
Jan 21 05:24:58.301: INFO: Got endpoints: latency-svc-vj7kr [491.657957ms]
Jan 21 05:24:58.325: INFO: Created: latency-svc-t9nnv
Jan 21 05:24:58.333: INFO: Got endpoints: latency-svc-t9nnv [496.912492ms]
Jan 21 05:24:58.356: INFO: Created: latency-svc-z4ghb
Jan 21 05:24:58.364: INFO: Got endpoints: latency-svc-z4ghb [496.759024ms]
Jan 21 05:24:58.387: INFO: Created: latency-svc-pkq8w
Jan 21 05:24:58.400: INFO: Got endpoints: latency-svc-pkq8w [496.886874ms]
Jan 21 05:24:58.421: INFO: Created: latency-svc-nc9vv
Jan 21 05:24:58.427: INFO: Got endpoints: latency-svc-nc9vv [487.023925ms]
Jan 21 05:24:58.466: INFO: Created: latency-svc-r4xd4
Jan 21 05:24:58.466: INFO: Got endpoints: latency-svc-r4xd4 [495.011342ms]
Jan 21 05:24:58.491: INFO: Created: latency-svc-r5c5l
Jan 21 05:24:58.499: INFO: Got endpoints: latency-svc-r5c5l [486.093469ms]
Jan 21 05:24:58.512: INFO: Created: latency-svc-v4prj
Jan 21 05:24:58.526: INFO: Got endpoints: latency-svc-v4prj [472.695664ms]
Jan 21 05:24:58.536: INFO: Created: latency-svc-n58mj
Jan 21 05:24:58.543: INFO: Got endpoints: latency-svc-n58mj [461.418733ms]
Jan 21 05:24:58.584: INFO: Created: latency-svc-fcjf8
Jan 21 05:24:58.585: INFO: Got endpoints: latency-svc-fcjf8 [479.111428ms]
Jan 21 05:24:58.617: INFO: Created: latency-svc-b86qd
Jan 21 05:24:58.617: INFO: Got endpoints: latency-svc-b86qd [481.079534ms]
Jan 21 05:24:58.638: INFO: Created: latency-svc-92r82
Jan 21 05:24:58.642: INFO: Got endpoints: latency-svc-92r82 [466.400775ms]
Jan 21 05:24:58.663: INFO: Created: latency-svc-75gxk
Jan 21 05:24:58.671: INFO: Got endpoints: latency-svc-75gxk [462.154345ms]
Jan 21 05:24:58.697: INFO: Created: latency-svc-rw6qm
Jan 21 05:24:58.712: INFO: Got endpoints: latency-svc-rw6qm [467.163444ms]
Jan 21 05:24:58.732: INFO: Created: latency-svc-6mz2f
Jan 21 05:24:58.739: INFO: Got endpoints: latency-svc-6mz2f [451.301006ms]
Jan 21 05:24:58.764: INFO: Created: latency-svc-kgqc4
Jan 21 05:24:58.771: INFO: Got endpoints: latency-svc-kgqc4 [470.41699ms]
Jan 21 05:24:58.810: INFO: Created: latency-svc-nj7sv
Jan 21 05:24:58.811: INFO: Got endpoints: latency-svc-nj7sv [477.777498ms]
Jan 21 05:24:58.848: INFO: Created: latency-svc-ptmf9
Jan 21 05:24:58.854: INFO: Got endpoints: latency-svc-ptmf9 [490.349893ms]
Jan 21 05:24:58.890: INFO: Created: latency-svc-gtdlr
Jan 21 05:24:58.890: INFO: Got endpoints: latency-svc-gtdlr [490.491864ms]
Jan 21 05:24:58.930: INFO: Created: latency-svc-h6fnz
Jan 21 05:24:58.935: INFO: Got endpoints: latency-svc-h6fnz [508.323808ms]
Jan 21 05:24:58.972: INFO: Created: latency-svc-kt4cz
Jan 21 05:24:58.981: INFO: Got endpoints: latency-svc-kt4cz [514.166424ms]
Jan 21 05:24:59.001: INFO: Created: latency-svc-mbnjb
Jan 21 05:24:59.026: INFO: Got endpoints: latency-svc-mbnjb [527.743119ms]
Jan 21 05:24:59.081: INFO: Created: latency-svc-7ksf8
Jan 21 05:24:59.087: INFO: Got endpoints: latency-svc-7ksf8 [560.89778ms]
Jan 21 05:24:59.140: INFO: Created: latency-svc-kkmgs
Jan 21 05:24:59.141: INFO: Got endpoints: latency-svc-kkmgs [597.758555ms]
Jan 21 05:24:59.186: INFO: Created: latency-svc-2jbxh
Jan 21 05:24:59.186: INFO: Got endpoints: latency-svc-2jbxh [601.089862ms]
Jan 21 05:24:59.214: INFO: Created: latency-svc-v6j2k
Jan 21 05:24:59.235: INFO: Got endpoints: latency-svc-v6j2k [617.471909ms]
Jan 21 05:24:59.243: INFO: Created: latency-svc-mp679
Jan 21 05:24:59.251: INFO: Got endpoints: latency-svc-mp679 [609.201909ms]
Jan 21 05:24:59.291: INFO: Created: latency-svc-5l48f
Jan 21 05:24:59.291: INFO: Got endpoints: latency-svc-5l48f [619.944473ms]
Jan 21 05:24:59.331: INFO: Created: latency-svc-48bbf
Jan 21 05:24:59.352: INFO: Got endpoints: latency-svc-48bbf [640.161238ms]
Jan 21 05:24:59.381: INFO: Created: latency-svc-5pjvs
Jan 21 05:24:59.383: INFO: Got endpoints: latency-svc-5pjvs [643.833132ms]
Jan 21 05:24:59.408: INFO: Created: latency-svc-zdhb9
Jan 21 05:24:59.411: INFO: Got endpoints: latency-svc-zdhb9 [639.950885ms]
Jan 21 05:24:59.435: INFO: Created: latency-svc-w2wv8
Jan 21 05:24:59.454: INFO: Got endpoints: latency-svc-w2wv8 [643.264937ms]
Jan 21 05:24:59.479: INFO: Created: latency-svc-nh6vw
Jan 21 05:24:59.518: INFO: Got endpoints: latency-svc-nh6vw [663.501804ms]
Jan 21 05:24:59.524: INFO: Created: latency-svc-xb2k6
Jan 21 05:24:59.531: INFO: Got endpoints: latency-svc-xb2k6 [640.687797ms]
Jan 21 05:24:59.552: INFO: Created: latency-svc-8gz8g
Jan 21 05:24:59.562: INFO: Got endpoints: latency-svc-8gz8g [626.341084ms]
Jan 21 05:24:59.583: INFO: Created: latency-svc-kqn8w
Jan 21 05:24:59.595: INFO: Got endpoints: latency-svc-kqn8w [614.309576ms]
Jan 21 05:24:59.602: INFO: Created: latency-svc-wq2vd
Jan 21 05:24:59.606: INFO: Got endpoints: latency-svc-wq2vd [579.610566ms]
Jan 21 05:24:59.641: INFO: Created: latency-svc-jb5xv
Jan 21 05:24:59.656: INFO: Got endpoints: latency-svc-jb5xv [568.387284ms]
Jan 21 05:24:59.670: INFO: Created: latency-svc-j2m7z
Jan 21 05:24:59.684: INFO: Got endpoints: latency-svc-j2m7z [543.153391ms]
Jan 21 05:24:59.698: INFO: Created: latency-svc-7ww6t
Jan 21 05:24:59.706: INFO: Got endpoints: latency-svc-7ww6t [519.880285ms]
Jan 21 05:24:59.729: INFO: Created: latency-svc-bcw7m
Jan 21 05:24:59.751: INFO: Got endpoints: latency-svc-bcw7m [515.84398ms]
Jan 21 05:24:59.763: INFO: Created: latency-svc-vcsm9
Jan 21 05:24:59.767: INFO: Got endpoints: latency-svc-vcsm9 [515.606793ms]
Jan 21 05:24:59.786: INFO: Created: latency-svc-r5lsq
Jan 21 05:24:59.795: INFO: Got endpoints: latency-svc-r5lsq [504.000815ms]
Jan 21 05:24:59.815: INFO: Created: latency-svc-wjnkn
Jan 21 05:24:59.820: INFO: Got endpoints: latency-svc-wjnkn [467.649444ms]
Jan 21 05:24:59.843: INFO: Created: latency-svc-b6nwv
Jan 21 05:24:59.848: INFO: Got endpoints: latency-svc-b6nwv [464.554179ms]
Jan 21 05:24:59.876: INFO: Created: latency-svc-6xcxd
Jan 21 05:24:59.883: INFO: Got endpoints: latency-svc-6xcxd [472.038412ms]
Jan 21 05:24:59.916: INFO: Created: latency-svc-nc975
Jan 21 05:24:59.932: INFO: Got endpoints: latency-svc-nc975 [477.487174ms]
Jan 21 05:24:59.946: INFO: Created: latency-svc-lmtkw
Jan 21 05:24:59.947: INFO: Got endpoints: latency-svc-lmtkw [428.817832ms]
Jan 21 05:24:59.989: INFO: Created: latency-svc-hz5n6
Jan 21 05:24:59.989: INFO: Got endpoints: latency-svc-hz5n6 [458.147554ms]
Jan 21 05:25:00.001: INFO: Created: latency-svc-q79c4
Jan 21 05:25:00.014: INFO: Got endpoints: latency-svc-q79c4 [451.880433ms]
Jan 21 05:25:00.040: INFO: Created: latency-svc-z554v
Jan 21 05:25:00.049: INFO: Got endpoints: latency-svc-z554v [453.978203ms]
Jan 21 05:25:00.066: INFO: Created: latency-svc-dh7tp
Jan 21 05:25:00.073: INFO: Got endpoints: latency-svc-dh7tp [466.755068ms]
Jan 21 05:25:00.113: INFO: Created: latency-svc-hh9d7
Jan 21 05:25:00.116: INFO: Got endpoints: latency-svc-hh9d7 [460.284733ms]
Jan 21 05:25:00.148: INFO: Created: latency-svc-ctshs
Jan 21 05:25:00.160: INFO: Got endpoints: latency-svc-ctshs [475.937452ms]
Jan 21 05:25:00.173: INFO: Created: latency-svc-dbk57
Jan 21 05:25:00.188: INFO: Got endpoints: latency-svc-dbk57 [482.286146ms]
Jan 21 05:25:00.196: INFO: Created: latency-svc-qd8hk
Jan 21 05:25:00.230: INFO: Created: latency-svc-45g8g
Jan 21 05:25:00.236: INFO: Got endpoints: latency-svc-qd8hk [485.610091ms]
Jan 21 05:25:00.263: INFO: Created: latency-svc-64z6l
Jan 21 05:25:00.285: INFO: Created: latency-svc-6zq8g
Jan 21 05:25:00.289: INFO: Got endpoints: latency-svc-45g8g [521.777494ms]
Jan 21 05:25:00.311: INFO: Created: latency-svc-pq87d
Jan 21 05:25:00.340: INFO: Got endpoints: latency-svc-64z6l [545.372318ms]
Jan 21 05:25:00.344: INFO: Created: latency-svc-94ww4
Jan 21 05:25:00.376: INFO: Created: latency-svc-j4mgv
Jan 21 05:25:00.398: INFO: Got endpoints: latency-svc-6zq8g [578.149572ms]
Jan 21 05:25:00.413: INFO: Created: latency-svc-jrcwd
Jan 21 05:25:00.436: INFO: Created: latency-svc-wj5dz
Jan 21 05:25:00.438: INFO: Got endpoints: latency-svc-pq87d [589.893674ms]
Jan 21 05:25:00.468: INFO: Created: latency-svc-vl528
Jan 21 05:25:00.509: INFO: Got endpoints: latency-svc-94ww4 [625.948405ms]
Jan 21 05:25:00.518: INFO: Created: latency-svc-67wmg
Jan 21 05:25:00.540: INFO: Got endpoints: latency-svc-j4mgv [608.775284ms]
Jan 21 05:25:00.550: INFO: Created: latency-svc-j48tj
Jan 21 05:25:00.581: INFO: Created: latency-svc-689ql
Jan 21 05:25:00.589: INFO: Got endpoints: latency-svc-jrcwd [642.191517ms]
Jan 21 05:25:00.617: INFO: Created: latency-svc-jp8gf
Jan 21 05:25:00.638: INFO: Got endpoints: latency-svc-wj5dz [649.046774ms]
Jan 21 05:25:00.644: INFO: Created: latency-svc-8rd9z
Jan 21 05:25:00.671: INFO: Created: latency-svc-psl4d
Jan 21 05:25:00.702: INFO: Got endpoints: latency-svc-vl528 [688.238299ms]
Jan 21 05:25:00.715: INFO: Created: latency-svc-2rc25
Jan 21 05:25:00.742: INFO: Got endpoints: latency-svc-67wmg [692.908004ms]
Jan 21 05:25:00.750: INFO: Created: latency-svc-k5xqb
Jan 21 05:25:00.775: INFO: Created: latency-svc-pgnvc
Jan 21 05:25:00.808: INFO: Got endpoints: latency-svc-j48tj [734.810836ms]
Jan 21 05:25:00.815: INFO: Created: latency-svc-5m5tv
Jan 21 05:25:00.847: INFO: Got endpoints: latency-svc-689ql [731.063034ms]
Jan 21 05:25:00.858: INFO: Created: latency-svc-prws8
Jan 21 05:25:00.883: INFO: Created: latency-svc-mlbgj
Jan 21 05:25:00.896: INFO: Got endpoints: latency-svc-jp8gf [736.005914ms]
Jan 21 05:25:00.911: INFO: Created: latency-svc-x6fms
Jan 21 05:25:00.940: INFO: Got endpoints: latency-svc-8rd9z [751.519165ms]
Jan 21 05:25:00.940: INFO: Created: latency-svc-4cnk7
Jan 21 05:25:00.975: INFO: Created: latency-svc-5njkn
Jan 21 05:25:00.995: INFO: Got endpoints: latency-svc-psl4d [758.893058ms]
Jan 21 05:25:01.001: INFO: Created: latency-svc-8hm9p
Jan 21 05:25:01.037: INFO: Created: latency-svc-6nsb6
Jan 21 05:25:01.041: INFO: Got endpoints: latency-svc-2rc25 [752.354858ms]
Jan 21 05:25:01.047: INFO: Created: latency-svc-skgmz
Jan 21 05:25:01.087: INFO: Got endpoints: latency-svc-k5xqb [746.666849ms]
Jan 21 05:25:01.093: INFO: Created: latency-svc-qkq4s
Jan 21 05:25:01.116: INFO: Created: latency-svc-s4wfx
Jan 21 05:25:01.153: INFO: Got endpoints: latency-svc-pgnvc [754.897684ms]
Jan 21 05:25:01.161: INFO: Created: latency-svc-8575r
Jan 21 05:25:01.183: INFO: Created: latency-svc-m7kwt
Jan 21 05:25:01.206: INFO: Got endpoints: latency-svc-5m5tv [768.541953ms]
Jan 21 05:25:01.215: INFO: Created: latency-svc-nmx6h
Jan 21 05:25:01.241: INFO: Got endpoints: latency-svc-prws8 [731.393016ms]
Jan 21 05:25:01.241: INFO: Created: latency-svc-jz4xg
Jan 21 05:25:01.273: INFO: Created: latency-svc-5fh4h
Jan 21 05:25:01.297: INFO: Got endpoints: latency-svc-mlbgj [756.7301ms]
Jan 21 05:25:01.298: INFO: Created: latency-svc-86ssv
Jan 21 05:25:01.327: INFO: Created: latency-svc-9d7ff
Jan 21 05:25:01.338: INFO: Got endpoints: latency-svc-x6fms [748.990596ms]
Jan 21 05:25:01.365: INFO: Created: latency-svc-4zwc2
Jan 21 05:25:01.387: INFO: Got endpoints: latency-svc-4cnk7 [748.902958ms]
Jan 21 05:25:01.426: INFO: Created: latency-svc-m6dgq
Jan 21 05:25:01.436: INFO: Got endpoints: latency-svc-5njkn [734.114927ms]
Jan 21 05:25:01.471: INFO: Created: latency-svc-mpk54
Jan 21 05:25:01.494: INFO: Got endpoints: latency-svc-8hm9p [752.159431ms]
Jan 21 05:25:01.538: INFO: Got endpoints: latency-svc-6nsb6 [730.591958ms]
Jan 21 05:25:01.539: INFO: Created: latency-svc-bpchw
Jan 21 05:25:01.574: INFO: Created: latency-svc-5s96q
Jan 21 05:25:01.588: INFO: Got endpoints: latency-svc-skgmz [741.046668ms]
Jan 21 05:25:01.648: INFO: Got endpoints: latency-svc-qkq4s [751.72476ms]
Jan 21 05:25:01.653: INFO: Created: latency-svc-jbtwt
Jan 21 05:25:01.680: INFO: Created: latency-svc-f2ww8
Jan 21 05:25:01.688: INFO: Got endpoints: latency-svc-s4wfx [748.110145ms]
Jan 21 05:25:01.720: INFO: Created: latency-svc-wbmvs
Jan 21 05:25:01.737: INFO: Got endpoints: latency-svc-8575r [741.77393ms]
Jan 21 05:25:01.768: INFO: Created: latency-svc-fqs7v
Jan 21 05:25:01.790: INFO: Got endpoints: latency-svc-m7kwt [748.56289ms]
Jan 21 05:25:01.850: INFO: Got endpoints: latency-svc-nmx6h [762.628751ms]
Jan 21 05:25:01.908: INFO: Got endpoints: latency-svc-jz4xg [755.580983ms]
Jan 21 05:25:01.910: INFO: Created: latency-svc-crtrh
Jan 21 05:25:01.953: INFO: Got endpoints: latency-svc-5fh4h [746.512634ms]
Jan 21 05:25:01.960: INFO: Created: latency-svc-p5h6q
Jan 21 05:25:02.018: INFO: Got endpoints: latency-svc-86ssv [776.908696ms]
Jan 21 05:25:02.026: INFO: Created: latency-svc-4zv8z
Jan 21 05:25:02.041: INFO: Got endpoints: latency-svc-9d7ff [744.129782ms]
Jan 21 05:25:02.072: INFO: Created: latency-svc-p6km9
Jan 21 05:25:02.107: INFO: Got endpoints: latency-svc-4zwc2 [769.301963ms]
Jan 21 05:25:02.125: INFO: Created: latency-svc-q77vb
Jan 21 05:25:02.161: INFO: Got endpoints: latency-svc-m6dgq [773.943879ms]
Jan 21 05:25:02.172: INFO: Created: latency-svc-jv7pl
Jan 21 05:25:02.211: INFO: Got endpoints: latency-svc-mpk54 [774.66054ms]
Jan 21 05:25:02.218: INFO: Created: latency-svc-rj7g6
Jan 21 05:25:02.252: INFO: Got endpoints: latency-svc-bpchw [757.639914ms]
Jan 21 05:25:02.253: INFO: Created: latency-svc-t8vgw
Jan 21 05:25:02.283: INFO: Created: latency-svc-lzrbc
Jan 21 05:25:02.292: INFO: Got endpoints: latency-svc-5s96q [753.602252ms]
Jan 21 05:25:02.324: INFO: Created: latency-svc-bq6z9
Jan 21 05:25:02.343: INFO: Got endpoints: latency-svc-jbtwt [754.870475ms]
Jan 21 05:25:02.344: INFO: Created: latency-svc-v4nrw
Jan 21 05:25:02.378: INFO: Created: latency-svc-bkrlb
Jan 21 05:25:02.387: INFO: Got endpoints: latency-svc-f2ww8 [739.136588ms]
Jan 21 05:25:02.428: INFO: Created: latency-svc-mjh29
Jan 21 05:25:02.438: INFO: Got endpoints: latency-svc-wbmvs [750.11168ms]
Jan 21 05:25:02.473: INFO: Created: latency-svc-4zdvm
Jan 21 05:25:02.501: INFO: Got endpoints: latency-svc-fqs7v [763.67752ms]
Jan 21 05:25:02.541: INFO: Created: latency-svc-f2krt
Jan 21 05:25:02.542: INFO: Got endpoints: latency-svc-crtrh [752.325478ms]
Jan 21 05:25:02.580: INFO: Created: latency-svc-hjqx6
Jan 21 05:25:02.588: INFO: Got endpoints: latency-svc-p5h6q [738.42324ms]
Jan 21 05:25:02.623: INFO: Created: latency-svc-spxlv
Jan 21 05:25:02.639: INFO: Got endpoints: latency-svc-4zv8z [730.55364ms]
Jan 21 05:25:02.680: INFO: Created: latency-svc-dzkzg
Jan 21 05:25:02.692: INFO: Got endpoints: latency-svc-p6km9 [739.485918ms]
Jan 21 05:25:02.720: INFO: Created: latency-svc-lfdfh
Jan 21 05:25:02.738: INFO: Got endpoints: latency-svc-q77vb [720.613548ms]
Jan 21 05:25:02.771: INFO: Created: latency-svc-9gl85
Jan 21 05:25:02.788: INFO: Got endpoints: latency-svc-jv7pl [746.303783ms]
Jan 21 05:25:02.828: INFO: Created: latency-svc-s48cz
Jan 21 05:25:02.837: INFO: Got endpoints: latency-svc-rj7g6 [729.793477ms]
Jan 21 05:25:02.868: INFO: Created: latency-svc-m9j7s
Jan 21 05:25:02.891: INFO: Got endpoints: latency-svc-t8vgw [729.922189ms]
Jan 21 05:25:02.943: INFO: Created: latency-svc-bpvsm
Jan 21 05:25:02.946: INFO: Got endpoints: latency-svc-lzrbc [735.252872ms]
Jan 21 05:25:02.983: INFO: Created: latency-svc-5lcn5
Jan 21 05:25:02.995: INFO: Got endpoints: latency-svc-bq6z9 [742.786805ms]
Jan 21 05:25:03.037: INFO: Created: latency-svc-qjd9l
Jan 21 05:25:03.039: INFO: Got endpoints: latency-svc-v4nrw [746.767779ms]
Jan 21 05:25:03.074: INFO: Created: latency-svc-n85n8
Jan 21 05:25:03.089: INFO: Got endpoints: latency-svc-bkrlb [746.233473ms]
Jan 21 05:25:03.125: INFO: Created: latency-svc-29w5v
Jan 21 05:25:03.139: INFO: Got endpoints: latency-svc-mjh29 [751.963861ms]
Jan 21 05:25:03.178: INFO: Created: latency-svc-rj5w4
Jan 21 05:25:03.187: INFO: Got endpoints: latency-svc-4zdvm [749.192605ms]
Jan 21 05:25:03.221: INFO: Created: latency-svc-cndx5
Jan 21 05:25:03.239: INFO: Got endpoints: latency-svc-f2krt [738.175126ms]
Jan 21 05:25:03.276: INFO: Created: latency-svc-d6qxb
Jan 21 05:25:03.287: INFO: Got endpoints: latency-svc-hjqx6 [744.419568ms]
Jan 21 05:25:03.325: INFO: Created: latency-svc-q6gmq
Jan 21 05:25:03.338: INFO: Got endpoints: latency-svc-spxlv [749.562234ms]
Jan 21 05:25:03.364: INFO: Created: latency-svc-6cbkk
Jan 21 05:25:03.386: INFO: Got endpoints: latency-svc-dzkzg [747.082756ms]
Jan 21 05:25:03.412: INFO: Created: latency-svc-fxbr5
Jan 21 05:25:03.438: INFO: Got endpoints: latency-svc-lfdfh [746.072963ms]
Jan 21 05:25:03.465: INFO: Created: latency-svc-xmgdt
Jan 21 05:25:03.487: INFO: Got endpoints: latency-svc-9gl85 [748.926629ms]
Jan 21 05:25:03.523: INFO: Created: latency-svc-9vlk9
Jan 21 05:25:03.537: INFO: Got endpoints: latency-svc-s48cz [749.486698ms]
Jan 21 05:25:03.572: INFO: Created: latency-svc-l7st7
Jan 21 05:25:03.590: INFO: Got endpoints: latency-svc-m9j7s [752.681991ms]
Jan 21 05:25:03.623: INFO: Created: latency-svc-4tq8w
Jan 21 05:25:03.637: INFO: Got endpoints: latency-svc-bpvsm [745.517972ms]
Jan 21 05:25:03.670: INFO: Created: latency-svc-wktlb
Jan 21 05:25:03.692: INFO: Got endpoints: latency-svc-5lcn5 [745.419398ms]
Jan 21 05:25:03.723: INFO: Created: latency-svc-n5t4j
Jan 21 05:25:03.737: INFO: Got endpoints: latency-svc-qjd9l [741.893036ms]
Jan 21 05:25:03.774: INFO: Created: latency-svc-v68jx
Jan 21 05:25:03.789: INFO: Got endpoints: latency-svc-n85n8 [749.891516ms]
Jan 21 05:25:03.815: INFO: Created: latency-svc-fgxls
Jan 21 05:25:03.837: INFO: Got endpoints: latency-svc-29w5v [747.592635ms]
Jan 21 05:25:03.862: INFO: Created: latency-svc-xj5kf
Jan 21 05:25:03.888: INFO: Got endpoints: latency-svc-rj5w4 [749.282476ms]
Jan 21 05:25:03.919: INFO: Created: latency-svc-f7zgm
Jan 21 05:25:03.939: INFO: Got endpoints: latency-svc-cndx5 [751.94945ms]
Jan 21 05:25:03.967: INFO: Created: latency-svc-mj9wh
Jan 21 05:25:03.992: INFO: Got endpoints: latency-svc-d6qxb [752.448858ms]
Jan 21 05:25:04.018: INFO: Created: latency-svc-2gh6m
Jan 21 05:25:04.040: INFO: Got endpoints: latency-svc-q6gmq [753.07447ms]
Jan 21 05:25:04.080: INFO: Created: latency-svc-2vl7p
Jan 21 05:25:04.085: INFO: Got endpoints: latency-svc-6cbkk [747.474857ms]
Jan 21 05:25:04.115: INFO: Created: latency-svc-978zb
Jan 21 05:25:04.138: INFO: Got endpoints: latency-svc-fxbr5 [751.556824ms]
Jan 21 05:25:04.168: INFO: Created: latency-svc-fsbwq
Jan 21 05:25:04.186: INFO: Got endpoints: latency-svc-xmgdt [747.649359ms]
Jan 21 05:25:04.219: INFO: Created: latency-svc-wg9kp
Jan 21 05:25:04.237: INFO: Got endpoints: latency-svc-9vlk9 [749.337561ms]
Jan 21 05:25:04.267: INFO: Created: latency-svc-qfvkb
Jan 21 05:25:04.291: INFO: Got endpoints: latency-svc-l7st7 [753.359735ms]
Jan 21 05:25:04.328: INFO: Created: latency-svc-dgdf7
Jan 21 05:25:04.336: INFO: Got endpoints: latency-svc-4tq8w [745.702772ms]
Jan 21 05:25:04.391: INFO: Got endpoints: latency-svc-wktlb [754.38426ms]
Jan 21 05:25:04.438: INFO: Got endpoints: latency-svc-n5t4j [746.068077ms]
Jan 21 05:25:04.487: INFO: Got endpoints: latency-svc-v68jx [750.610483ms]
Jan 21 05:25:04.538: INFO: Got endpoints: latency-svc-fgxls [749.07778ms]
Jan 21 05:25:04.587: INFO: Got endpoints: latency-svc-xj5kf [750.404674ms]
Jan 21 05:25:04.639: INFO: Got endpoints: latency-svc-f7zgm [750.132303ms]
Jan 21 05:25:04.687: INFO: Got endpoints: latency-svc-mj9wh [747.491856ms]
Jan 21 05:25:04.737: INFO: Got endpoints: latency-svc-2gh6m [745.530531ms]
Jan 21 05:25:04.788: INFO: Got endpoints: latency-svc-2vl7p [748.031524ms]
Jan 21 05:25:04.836: INFO: Got endpoints: latency-svc-978zb [750.755984ms]
Jan 21 05:25:04.887: INFO: Got endpoints: latency-svc-fsbwq [749.359435ms]
Jan 21 05:25:04.938: INFO: Got endpoints: latency-svc-wg9kp [751.451475ms]
Jan 21 05:25:04.987: INFO: Got endpoints: latency-svc-qfvkb [750.383672ms]
Jan 21 05:25:05.041: INFO: Got endpoints: latency-svc-dgdf7 [750.228898ms]
Jan 21 05:25:05.041: INFO: Latencies: [53.342308ms 94.550369ms 125.458539ms 167.468859ms 199.774459ms 237.139004ms 287.218249ms 328.648614ms 366.30106ms 404.161597ms 428.817832ms 429.679042ms 451.301006ms 451.880433ms 453.978203ms 458.147554ms 460.284733ms 461.418733ms 462.154345ms 464.554179ms 466.400775ms 466.755068ms 467.163444ms 467.649444ms 469.738441ms 470.41699ms 472.038412ms 472.695664ms 475.937452ms 477.487174ms 477.777498ms 479.111428ms 481.079534ms 482.286146ms 485.610091ms 486.093469ms 487.023925ms 490.349893ms 490.491864ms 491.657957ms 495.011342ms 496.759024ms 496.886874ms 496.912492ms 500.665646ms 504.000815ms 508.323808ms 513.382201ms 514.166424ms 515.606793ms 515.84398ms 517.849541ms 519.880285ms 520.725788ms 520.773871ms 521.777494ms 525.497152ms 525.672927ms 527.743119ms 530.775861ms 531.920917ms 531.999092ms 534.961326ms 537.486734ms 543.153391ms 545.372318ms 548.085799ms 556.373453ms 557.445987ms 560.89778ms 562.361704ms 566.015721ms 566.721164ms 568.387284ms 570.249943ms 571.23177ms 573.206317ms 573.357104ms 573.469785ms 577.110675ms 578.149572ms 578.303488ms 579.610566ms 580.940741ms 584.271906ms 585.363899ms 585.474159ms 585.655912ms 588.492936ms 588.811555ms 589.893674ms 591.062474ms 593.248661ms 597.758555ms 601.089862ms 601.809569ms 608.775284ms 609.201909ms 611.530063ms 614.309576ms 617.471909ms 619.944473ms 625.948405ms 626.341084ms 639.950885ms 640.161238ms 640.687797ms 642.191517ms 643.264937ms 643.833132ms 649.046774ms 663.501804ms 688.238299ms 692.908004ms 720.613548ms 729.793477ms 729.922189ms 730.55364ms 730.591958ms 731.063034ms 731.393016ms 734.114927ms 734.810836ms 735.252872ms 736.005914ms 738.175126ms 738.42324ms 739.136588ms 739.485918ms 741.046668ms 741.77393ms 741.893036ms 742.786805ms 744.129782ms 744.419568ms 745.419398ms 745.517972ms 745.530531ms 745.702772ms 746.068077ms 746.072963ms 746.233473ms 746.303783ms 746.512634ms 746.666849ms 746.767779ms 747.082756ms 747.474857ms 747.491856ms 747.592635ms 747.649359ms 748.031524ms 748.110145ms 748.56289ms 748.902958ms 748.926629ms 748.990596ms 749.07778ms 749.192605ms 749.282476ms 749.337561ms 749.359435ms 749.486698ms 749.562234ms 749.891516ms 750.11168ms 750.132303ms 750.228898ms 750.383672ms 750.404674ms 750.610483ms 750.755984ms 751.451475ms 751.519165ms 751.556824ms 751.72476ms 751.94945ms 751.963861ms 752.159431ms 752.325478ms 752.354858ms 752.448858ms 752.681991ms 753.07447ms 753.359735ms 753.602252ms 754.38426ms 754.870475ms 754.897684ms 755.580983ms 756.7301ms 757.639914ms 758.893058ms 762.628751ms 763.67752ms 768.541953ms 769.301963ms 773.943879ms 774.66054ms 776.908696ms]
Jan 21 05:25:05.041: INFO: 50 %ile: 617.471909ms
Jan 21 05:25:05.041: INFO: 90 %ile: 752.354858ms
Jan 21 05:25:05.041: INFO: 99 %ile: 774.66054ms
Jan 21 05:25:05.041: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:25:05.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-75qft" for this suite.
Jan 21 05:25:23.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:25:23.204: INFO: namespace: e2e-tests-svc-latency-75qft, resource: bindings, ignored listing per whitelist
Jan 21 05:25:23.205: INFO: namespace e2e-tests-svc-latency-75qft deletion completed in 18.157580475s

â€¢ [SLOW TEST:29.061 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:25:23.205: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:25:23.314: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:25:25.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-m2dlg" for this suite.
Jan 21 05:26:03.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:26:03.594: INFO: namespace: e2e-tests-pods-m2dlg, resource: bindings, ignored listing per whitelist
Jan 21 05:26:03.718: INFO: namespace e2e-tests-pods-m2dlg deletion completed in 38.169733519s

â€¢ [SLOW TEST:40.513 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:26:03.718: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 21 05:26:06.462: INFO: Successfully updated pod "labelsupdate0bb88de0-1d3d-11e9-b032-0a580af40356"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:26:10.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mcshx" for this suite.
Jan 21 05:26:32.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:26:32.596: INFO: namespace: e2e-tests-downward-api-mcshx, resource: bindings, ignored listing per whitelist
Jan 21 05:26:32.694: INFO: namespace e2e-tests-downward-api-mcshx deletion completed in 22.157268759s

â€¢ [SLOW TEST:28.976 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:26:32.695: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-1cfa81be-1d3d-11e9-b032-0a580af40356
STEP: Creating secret with name s-test-opt-upd-1cfa824c-1d3d-11e9-b032-0a580af40356
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-1cfa81be-1d3d-11e9-b032-0a580af40356
STEP: Updating secret s-test-opt-upd-1cfa824c-1d3d-11e9-b032-0a580af40356
STEP: Creating secret with name s-test-opt-create-1cfa8277-1d3d-11e9-b032-0a580af40356
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:26:36.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nwbhq" for this suite.
Jan 21 05:26:58.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:26:59.014: INFO: namespace: e2e-tests-secrets-nwbhq, resource: bindings, ignored listing per whitelist
Jan 21 05:26:59.139: INFO: namespace e2e-tests-secrets-nwbhq deletion completed in 22.173551073s

â€¢ [SLOW TEST:26.444 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:26:59.139: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Jan 21 05:26:59.268: INFO: Waiting up to 5m0s for pod "client-containers-2cc0695d-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-containers-677tr" to be "success or failure"
Jan 21 05:26:59.279: INFO: Pod "client-containers-2cc0695d-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.193664ms
Jan 21 05:27:01.286: INFO: Pod "client-containers-2cc0695d-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017861353s
STEP: Saw pod success
Jan 21 05:27:01.286: INFO: Pod "client-containers-2cc0695d-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:27:01.290: INFO: Trying to get logs from node k8s05 pod client-containers-2cc0695d-1d3d-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:27:01.329: INFO: Waiting for pod client-containers-2cc0695d-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:27:01.334: INFO: Pod client-containers-2cc0695d-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:27:01.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-677tr" for this suite.
Jan 21 05:27:07.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:27:07.417: INFO: namespace: e2e-tests-containers-677tr, resource: bindings, ignored listing per whitelist
Jan 21 05:27:07.493: INFO: namespace e2e-tests-containers-677tr deletion completed in 6.152107234s

â€¢ [SLOW TEST:8.353 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:27:07.493: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-31b77fd7-1d3d-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:27:07.603: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-31b9129c-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-wmvpz" to be "success or failure"
Jan 21 05:27:07.610: INFO: Pod "pod-projected-configmaps-31b9129c-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 7.205867ms
Jan 21 05:27:09.617: INFO: Pod "pod-projected-configmaps-31b9129c-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014454612s
STEP: Saw pod success
Jan 21 05:27:09.617: INFO: Pod "pod-projected-configmaps-31b9129c-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:27:09.622: INFO: Trying to get logs from node k8s05 pod pod-projected-configmaps-31b9129c-1d3d-11e9-b032-0a580af40356 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:27:09.676: INFO: Waiting for pod pod-projected-configmaps-31b9129c-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:27:09.681: INFO: Pod pod-projected-configmaps-31b9129c-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:27:09.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wmvpz" for this suite.
Jan 21 05:27:15.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:27:15.830: INFO: namespace: e2e-tests-projected-wmvpz, resource: bindings, ignored listing per whitelist
Jan 21 05:27:15.868: INFO: namespace e2e-tests-projected-wmvpz deletion completed in 6.179957823s

â€¢ [SLOW TEST:8.375 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:27:15.868: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-36b659e2-1d3d-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:27:15.986: INFO: Waiting up to 5m0s for pod "pod-configmaps-36b81158-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-configmap-pz79g" to be "success or failure"
Jan 21 05:27:15.994: INFO: Pod "pod-configmaps-36b81158-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 8.6468ms
Jan 21 05:27:18.000: INFO: Pod "pod-configmaps-36b81158-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014159056s
STEP: Saw pod success
Jan 21 05:27:18.000: INFO: Pod "pod-configmaps-36b81158-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:27:18.004: INFO: Trying to get logs from node k8s05 pod pod-configmaps-36b81158-1d3d-11e9-b032-0a580af40356 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:27:18.031: INFO: Waiting for pod pod-configmaps-36b81158-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:27:18.036: INFO: Pod pod-configmaps-36b81158-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:27:18.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-pz79g" for this suite.
Jan 21 05:27:24.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:27:24.149: INFO: namespace: e2e-tests-configmap-pz79g, resource: bindings, ignored listing per whitelist
Jan 21 05:27:24.238: INFO: namespace e2e-tests-configmap-pz79g deletion completed in 6.197105067s

â€¢ [SLOW TEST:8.370 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:27:24.238: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 21 05:27:28.506: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:28.510: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:30.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:30.516: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:32.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:32.517: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:34.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:34.522: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:36.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:36.526: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:38.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:38.517: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:40.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:40.517: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:42.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:42.520: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:44.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:44.516: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:46.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:46.516: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:48.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:48.531: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:50.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:50.516: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:52.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:52.519: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:54.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:54.534: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:56.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:56.522: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 21 05:27:58.511: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 21 05:27:58.517: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:27:58.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-vx6c8" for this suite.
Jan 21 05:28:20.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:28:20.638: INFO: namespace: e2e-tests-container-lifecycle-hook-vx6c8, resource: bindings, ignored listing per whitelist
Jan 21 05:28:20.766: INFO: namespace e2e-tests-container-lifecycle-hook-vx6c8 deletion completed in 22.217234828s

â€¢ [SLOW TEST:56.528 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:28:20.766: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-5d69373a-1d3d-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:28:20.930: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d6cee02-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-lg2gw" to be "success or failure"
Jan 21 05:28:20.941: INFO: Pod "pod-projected-configmaps-5d6cee02-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 10.701652ms
Jan 21 05:28:22.946: INFO: Pod "pod-projected-configmaps-5d6cee02-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016020182s
STEP: Saw pod success
Jan 21 05:28:22.946: INFO: Pod "pod-projected-configmaps-5d6cee02-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:28:22.950: INFO: Trying to get logs from node k8s05 pod pod-projected-configmaps-5d6cee02-1d3d-11e9-b032-0a580af40356 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:28:22.987: INFO: Waiting for pod pod-projected-configmaps-5d6cee02-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:28:22.991: INFO: Pod pod-projected-configmaps-5d6cee02-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:28:22.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lg2gw" for this suite.
Jan 21 05:28:29.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:28:29.152: INFO: namespace: e2e-tests-projected-lg2gw, resource: bindings, ignored listing per whitelist
Jan 21 05:28:29.189: INFO: namespace e2e-tests-projected-lg2gw deletion completed in 6.192046135s

â€¢ [SLOW TEST:8.423 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:28:29.190: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:28:29.377: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6271f308-1d3d-11e9-9faa-fa163e6f5c57", Controller:(*bool)(0xc002e5f41e), BlockOwnerDeletion:(*bool)(0xc002e5f41f)}}
Jan 21 05:28:29.398: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"626cf9c7-1d3d-11e9-9faa-fa163e6f5c57", Controller:(*bool)(0xc002b184e6), BlockOwnerDeletion:(*bool)(0xc002b184e7)}}
Jan 21 05:28:29.410: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"626f0a3c-1d3d-11e9-9faa-fa163e6f5c57", Controller:(*bool)(0xc002e5f65e), BlockOwnerDeletion:(*bool)(0xc002e5f65f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:28:34.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-r4fh5" for this suite.
Jan 21 05:28:40.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:28:40.626: INFO: namespace: e2e-tests-gc-r4fh5, resource: bindings, ignored listing per whitelist
Jan 21 05:28:40.641: INFO: namespace e2e-tests-gc-r4fh5 deletion completed in 6.186049485s

â€¢ [SLOW TEST:11.452 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:28:40.642: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-69448352-1d3d-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:28:40.826: INFO: Waiting up to 5m0s for pod "pod-secrets-6946f64f-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-secrets-xk62r" to be "success or failure"
Jan 21 05:28:40.835: INFO: Pod "pod-secrets-6946f64f-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.198736ms
Jan 21 05:28:42.844: INFO: Pod "pod-secrets-6946f64f-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017483057s
STEP: Saw pod success
Jan 21 05:28:42.844: INFO: Pod "pod-secrets-6946f64f-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:28:42.849: INFO: Trying to get logs from node k8s05 pod pod-secrets-6946f64f-1d3d-11e9-b032-0a580af40356 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:28:42.907: INFO: Waiting for pod pod-secrets-6946f64f-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:28:42.913: INFO: Pod pod-secrets-6946f64f-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:28:42.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xk62r" for this suite.
Jan 21 05:28:48.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:28:49.045: INFO: namespace: e2e-tests-secrets-xk62r, resource: bindings, ignored listing per whitelist
Jan 21 05:28:49.132: INFO: namespace e2e-tests-secrets-xk62r deletion completed in 6.210163639s

â€¢ [SLOW TEST:8.490 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:28:49.132: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 21 05:28:49.259: INFO: Waiting up to 5m0s for pod "downward-api-6e4fcf53-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-rm6ll" to be "success or failure"
Jan 21 05:28:49.269: INFO: Pod "downward-api-6e4fcf53-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.92042ms
Jan 21 05:28:51.278: INFO: Pod "downward-api-6e4fcf53-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018492791s
STEP: Saw pod success
Jan 21 05:28:51.278: INFO: Pod "downward-api-6e4fcf53-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:28:51.284: INFO: Trying to get logs from node k8s05 pod downward-api-6e4fcf53-1d3d-11e9-b032-0a580af40356 container dapi-container: <nil>
STEP: delete the pod
Jan 21 05:28:51.340: INFO: Waiting for pod downward-api-6e4fcf53-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:28:51.345: INFO: Pod downward-api-6e4fcf53-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:28:51.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rm6ll" for this suite.
Jan 21 05:28:57.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:28:57.443: INFO: namespace: e2e-tests-downward-api-rm6ll, resource: bindings, ignored listing per whitelist
Jan 21 05:28:57.526: INFO: namespace e2e-tests-downward-api-rm6ll deletion completed in 6.168296173s

â€¢ [SLOW TEST:8.394 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:28:57.526: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-7351086f-1d3d-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:28:57.666: INFO: Waiting up to 5m0s for pod "pod-secrets-7352d178-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-secrets-zcxl7" to be "success or failure"
Jan 21 05:28:57.677: INFO: Pod "pod-secrets-7352d178-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.232822ms
Jan 21 05:28:59.683: INFO: Pod "pod-secrets-7352d178-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017236182s
STEP: Saw pod success
Jan 21 05:28:59.683: INFO: Pod "pod-secrets-7352d178-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:28:59.688: INFO: Trying to get logs from node k8s05 pod pod-secrets-7352d178-1d3d-11e9-b032-0a580af40356 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:28:59.735: INFO: Waiting for pod pod-secrets-7352d178-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:28:59.739: INFO: Pod pod-secrets-7352d178-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:28:59.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-zcxl7" for this suite.
Jan 21 05:29:05.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:29:05.809: INFO: namespace: e2e-tests-secrets-zcxl7, resource: bindings, ignored listing per whitelist
Jan 21 05:29:05.916: INFO: namespace e2e-tests-secrets-zcxl7 deletion completed in 6.170634955s

â€¢ [SLOW TEST:8.390 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:29:05.916: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-7851c650-1d3d-11e9-b032-0a580af40356
STEP: Creating secret with name s-test-opt-upd-7851c6ae-1d3d-11e9-b032-0a580af40356
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7851c650-1d3d-11e9-b032-0a580af40356
STEP: Updating secret s-test-opt-upd-7851c6ae-1d3d-11e9-b032-0a580af40356
STEP: Creating secret with name s-test-opt-create-7851c6ce-1d3d-11e9-b032-0a580af40356
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:29:10.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-j72jw" for this suite.
Jan 21 05:29:32.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:29:32.299: INFO: namespace: e2e-tests-projected-j72jw, resource: bindings, ignored listing per whitelist
Jan 21 05:29:32.397: INFO: namespace e2e-tests-projected-j72jw deletion completed in 22.175466732s

â€¢ [SLOW TEST:26.481 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:29:32.398: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:29:32.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-kzlzr" for this suite.
Jan 21 05:29:38.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:29:38.677: INFO: namespace: e2e-tests-kubelet-test-kzlzr, resource: bindings, ignored listing per whitelist
Jan 21 05:29:38.790: INFO: namespace e2e-tests-kubelet-test-kzlzr deletion completed in 6.221983269s

â€¢ [SLOW TEST:6.393 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:29:38.791: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:29:38.921: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8be95a12-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-tj255" to be "success or failure"
Jan 21 05:29:38.935: INFO: Pod "downwardapi-volume-8be95a12-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 13.092771ms
Jan 21 05:29:40.940: INFO: Pod "downwardapi-volume-8be95a12-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018641969s
STEP: Saw pod success
Jan 21 05:29:40.940: INFO: Pod "downwardapi-volume-8be95a12-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:29:40.945: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-8be95a12-1d3d-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:29:40.988: INFO: Waiting for pod downwardapi-volume-8be95a12-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:29:40.993: INFO: Pod downwardapi-volume-8be95a12-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:29:40.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tj255" for this suite.
Jan 21 05:29:47.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:29:47.219: INFO: namespace: e2e-tests-downward-api-tj255, resource: bindings, ignored listing per whitelist
Jan 21 05:29:47.240: INFO: namespace e2e-tests-downward-api-tj255 deletion completed in 6.238333841s

â€¢ [SLOW TEST:8.449 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:29:47.240: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-90f48108-1d3d-11e9-b032-0a580af40356
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-90f48108-1d3d-11e9-b032-0a580af40356
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:29:51.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ksxvj" for this suite.
Jan 21 05:30:13.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:30:13.554: INFO: namespace: e2e-tests-projected-ksxvj, resource: bindings, ignored listing per whitelist
Jan 21 05:30:13.688: INFO: namespace e2e-tests-projected-ksxvj deletion completed in 22.186425083s

â€¢ [SLOW TEST:26.448 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:30:13.688: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:31:13.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8jfgl" for this suite.
Jan 21 05:31:35.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:31:35.921: INFO: namespace: e2e-tests-container-probe-8jfgl, resource: bindings, ignored listing per whitelist
Jan 21 05:31:36.041: INFO: namespace e2e-tests-container-probe-8jfgl deletion completed in 22.222078637s

â€¢ [SLOW TEST:82.352 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:31:36.041: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jan 21 05:31:36.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:36.601: INFO: stderr: ""
Jan 21 05:31:36.602: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 05:31:36.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:36.733: INFO: stderr: ""
Jan 21 05:31:36.733: INFO: stdout: "update-demo-nautilus-54ckh update-demo-nautilus-pk5lz "
Jan 21 05:31:36.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-54ckh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:36.889: INFO: stderr: ""
Jan 21 05:31:36.890: INFO: stdout: ""
Jan 21 05:31:36.890: INFO: update-demo-nautilus-54ckh is created but not running
Jan 21 05:31:41.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:42.052: INFO: stderr: ""
Jan 21 05:31:42.053: INFO: stdout: "update-demo-nautilus-54ckh update-demo-nautilus-pk5lz "
Jan 21 05:31:42.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-54ckh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:42.218: INFO: stderr: ""
Jan 21 05:31:42.218: INFO: stdout: "true"
Jan 21 05:31:42.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-54ckh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:42.348: INFO: stderr: ""
Jan 21 05:31:42.348: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 21 05:31:42.348: INFO: validating pod update-demo-nautilus-54ckh
Jan 21 05:31:42.360: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 05:31:42.360: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 05:31:42.360: INFO: update-demo-nautilus-54ckh is verified up and running
Jan 21 05:31:42.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-pk5lz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:42.522: INFO: stderr: ""
Jan 21 05:31:42.522: INFO: stdout: "true"
Jan 21 05:31:42.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-pk5lz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:42.661: INFO: stderr: ""
Jan 21 05:31:42.661: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 21 05:31:42.661: INFO: validating pod update-demo-nautilus-pk5lz
Jan 21 05:31:42.668: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 05:31:42.668: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 05:31:42.668: INFO: update-demo-nautilus-pk5lz is verified up and running
STEP: using delete to clean up resources
Jan 21 05:31:42.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:42.788: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:31:42.788: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 21 05:31:42.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-6trgp'
Jan 21 05:31:42.970: INFO: stderr: "No resources found.\n"
Jan 21 05:31:42.970: INFO: stdout: ""
Jan 21 05:31:42.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -l name=update-demo --namespace=e2e-tests-kubectl-6trgp -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 21 05:31:43.148: INFO: stderr: ""
Jan 21 05:31:43.148: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:31:43.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6trgp" for this suite.
Jan 21 05:31:49.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:31:49.297: INFO: namespace: e2e-tests-kubectl-6trgp, resource: bindings, ignored listing per whitelist
Jan 21 05:31:49.356: INFO: namespace e2e-tests-kubectl-6trgp deletion completed in 6.198377233s

â€¢ [SLOW TEST:13.315 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:31:49.356: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-d9bbcf1a-1d3d-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:31:49.489: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d9bd5bda-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-pzs9p" to be "success or failure"
Jan 21 05:31:49.500: INFO: Pod "pod-projected-secrets-d9bd5bda-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 10.769422ms
Jan 21 05:31:51.508: INFO: Pod "pod-projected-secrets-d9bd5bda-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01898444s
STEP: Saw pod success
Jan 21 05:31:51.508: INFO: Pod "pod-projected-secrets-d9bd5bda-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:31:51.514: INFO: Trying to get logs from node k8s05 pod pod-projected-secrets-d9bd5bda-1d3d-11e9-b032-0a580af40356 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:31:51.560: INFO: Waiting for pod pod-projected-secrets-d9bd5bda-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:31:51.565: INFO: Pod pod-projected-secrets-d9bd5bda-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:31:51.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pzs9p" for this suite.
Jan 21 05:31:57.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:31:57.737: INFO: namespace: e2e-tests-projected-pzs9p, resource: bindings, ignored listing per whitelist
Jan 21 05:31:57.786: INFO: namespace e2e-tests-projected-pzs9p deletion completed in 6.213153533s

â€¢ [SLOW TEST:8.430 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:31:57.786: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:31:57.898: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:31:59.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-nm5p2" for this suite.
Jan 21 05:32:37.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:32:38.107: INFO: namespace: e2e-tests-pods-nm5p2, resource: bindings, ignored listing per whitelist
Jan 21 05:32:38.120: INFO: namespace e2e-tests-pods-nm5p2 deletion completed in 38.153333297s

â€¢ [SLOW TEST:40.334 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:32:38.121: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Jan 21 05:32:38.227: INFO: Waiting up to 5m0s for pod "var-expansion-f6c97244-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-var-expansion-v4dr6" to be "success or failure"
Jan 21 05:32:38.231: INFO: Pod "var-expansion-f6c97244-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 3.717817ms
Jan 21 05:32:40.238: INFO: Pod "var-expansion-f6c97244-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011090707s
Jan 21 05:32:42.253: INFO: Pod "var-expansion-f6c97244-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026333077s
STEP: Saw pod success
Jan 21 05:32:42.253: INFO: Pod "var-expansion-f6c97244-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:32:42.261: INFO: Trying to get logs from node k8s05 pod var-expansion-f6c97244-1d3d-11e9-b032-0a580af40356 container dapi-container: <nil>
STEP: delete the pod
Jan 21 05:32:42.317: INFO: Waiting for pod var-expansion-f6c97244-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:32:42.322: INFO: Pod var-expansion-f6c97244-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:32:42.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-v4dr6" for this suite.
Jan 21 05:32:48.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:32:48.457: INFO: namespace: e2e-tests-var-expansion-v4dr6, resource: bindings, ignored listing per whitelist
Jan 21 05:32:48.490: INFO: namespace e2e-tests-var-expansion-v4dr6 deletion completed in 6.160255274s

â€¢ [SLOW TEST:10.369 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:32:48.490: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 21 05:32:48.615: INFO: Waiting up to 5m0s for pod "downward-api-fcf9eaf5-1d3d-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-pzpb8" to be "success or failure"
Jan 21 05:32:48.619: INFO: Pod "downward-api-fcf9eaf5-1d3d-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.414436ms
Jan 21 05:32:50.630: INFO: Pod "downward-api-fcf9eaf5-1d3d-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015336608s
STEP: Saw pod success
Jan 21 05:32:50.630: INFO: Pod "downward-api-fcf9eaf5-1d3d-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:32:50.636: INFO: Trying to get logs from node k8s05 pod downward-api-fcf9eaf5-1d3d-11e9-b032-0a580af40356 container dapi-container: <nil>
STEP: delete the pod
Jan 21 05:32:50.690: INFO: Waiting for pod downward-api-fcf9eaf5-1d3d-11e9-b032-0a580af40356 to disappear
Jan 21 05:32:50.696: INFO: Pod downward-api-fcf9eaf5-1d3d-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:32:50.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pzpb8" for this suite.
Jan 21 05:32:56.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:32:56.857: INFO: namespace: e2e-tests-downward-api-pzpb8, resource: bindings, ignored listing per whitelist
Jan 21 05:32:56.919: INFO: namespace e2e-tests-downward-api-pzpb8 deletion completed in 6.211054538s

â€¢ [SLOW TEST:8.429 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:32:56.919: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 21 05:32:57.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-w84hs'
Jan 21 05:32:57.214: INFO: stderr: ""
Jan 21 05:32:57.214: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jan 21 05:33:02.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-w84hs -o json'
Jan 21 05:33:02.400: INFO: stderr: ""
Jan 21 05:33:02.400: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-01-21T05:32:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-w84hs\",\n        \"resourceVersion\": \"37421\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-w84hs/pods/e2e-test-nginx-pod\",\n        \"uid\": \"02188e94-1d3e-11e9-b27d-fa163eb7f963\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-wrjwk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s05\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-wrjwk\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-wrjwk\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-21T05:32:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-21T05:32:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-21T05:32:58Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-01-21T05:32:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://d240478cc9b19dc102795352bf4f681a0ac09f198cc55a513f12a59a14c38675\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-01-21T05:32:58Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.21\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.217\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-01-21T05:32:57Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 21 05:33:02.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 replace -f - --namespace=e2e-tests-kubectl-w84hs'
Jan 21 05:33:02.776: INFO: stderr: ""
Jan 21 05:33:02.776: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
Jan 21 05:33:02.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-w84hs'
Jan 21 05:33:04.701: INFO: stderr: ""
Jan 21 05:33:04.701: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:33:04.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-w84hs" for this suite.
Jan 21 05:33:10.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:33:10.893: INFO: namespace: e2e-tests-kubectl-w84hs, resource: bindings, ignored listing per whitelist
Jan 21 05:33:10.925: INFO: namespace e2e-tests-kubectl-w84hs deletion completed in 6.217119503s

â€¢ [SLOW TEST:14.006 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:33:10.926: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:33:15.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-9jfch" for this suite.
Jan 21 05:33:21.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:33:21.186: INFO: namespace: e2e-tests-kubelet-test-9jfch, resource: bindings, ignored listing per whitelist
Jan 21 05:33:21.325: INFO: namespace e2e-tests-kubelet-test-9jfch deletion completed in 6.219561054s

â€¢ [SLOW TEST:10.399 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:33:21.326: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-108dae65-1d3e-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:33:21.476: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-108fd951-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-9jfc4" to be "success or failure"
Jan 21 05:33:21.481: INFO: Pod "pod-projected-configmaps-108fd951-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 5.716699ms
Jan 21 05:33:23.488: INFO: Pod "pod-projected-configmaps-108fd951-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012718367s
STEP: Saw pod success
Jan 21 05:33:23.488: INFO: Pod "pod-projected-configmaps-108fd951-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:33:23.493: INFO: Trying to get logs from node k8s05 pod pod-projected-configmaps-108fd951-1d3e-11e9-b032-0a580af40356 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:33:23.534: INFO: Waiting for pod pod-projected-configmaps-108fd951-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:33:23.540: INFO: Pod pod-projected-configmaps-108fd951-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:33:23.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9jfc4" for this suite.
Jan 21 05:33:29.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:33:29.597: INFO: namespace: e2e-tests-projected-9jfc4, resource: bindings, ignored listing per whitelist
Jan 21 05:33:29.720: INFO: namespace e2e-tests-projected-9jfc4 deletion completed in 6.173838056s

â€¢ [SLOW TEST:8.395 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:33:29.721: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Jan 21 05:33:29.823: INFO: Waiting up to 5m0s for pod "var-expansion-158aa980-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-var-expansion-g46ql" to be "success or failure"
Jan 21 05:33:29.831: INFO: Pod "var-expansion-158aa980-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 7.631857ms
Jan 21 05:33:31.836: INFO: Pod "var-expansion-158aa980-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012941279s
STEP: Saw pod success
Jan 21 05:33:31.836: INFO: Pod "var-expansion-158aa980-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:33:31.841: INFO: Trying to get logs from node k8s05 pod var-expansion-158aa980-1d3e-11e9-b032-0a580af40356 container dapi-container: <nil>
STEP: delete the pod
Jan 21 05:33:31.875: INFO: Waiting for pod var-expansion-158aa980-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:33:31.879: INFO: Pod var-expansion-158aa980-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:33:31.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-g46ql" for this suite.
Jan 21 05:33:37.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:33:38.016: INFO: namespace: e2e-tests-var-expansion-g46ql, resource: bindings, ignored listing per whitelist
Jan 21 05:33:38.058: INFO: namespace e2e-tests-var-expansion-g46ql deletion completed in 6.17142412s

â€¢ [SLOW TEST:8.338 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:33:38.058: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-1a880465-1d3e-11e9-b032-0a580af40356
Jan 21 05:33:38.203: INFO: Pod name my-hostname-basic-1a880465-1d3e-11e9-b032-0a580af40356: Found 0 pods out of 1
Jan 21 05:33:43.213: INFO: Pod name my-hostname-basic-1a880465-1d3e-11e9-b032-0a580af40356: Found 1 pods out of 1
Jan 21 05:33:43.213: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1a880465-1d3e-11e9-b032-0a580af40356" are running
Jan 21 05:33:43.218: INFO: Pod "my-hostname-basic-1a880465-1d3e-11e9-b032-0a580af40356-fksv4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-21 05:33:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-21 05:33:40 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-21 05:33:40 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-01-21 05:33:38 +0000 UTC Reason: Message:}])
Jan 21 05:33:43.218: INFO: Trying to dial the pod
Jan 21 05:33:48.251: INFO: Controller my-hostname-basic-1a880465-1d3e-11e9-b032-0a580af40356: Got expected result from replica 1 [my-hostname-basic-1a880465-1d3e-11e9-b032-0a580af40356-fksv4]: "my-hostname-basic-1a880465-1d3e-11e9-b032-0a580af40356-fksv4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:33:48.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-mdmhk" for this suite.
Jan 21 05:33:54.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:33:54.344: INFO: namespace: e2e-tests-replication-controller-mdmhk, resource: bindings, ignored listing per whitelist
Jan 21 05:33:54.409: INFO: namespace e2e-tests-replication-controller-mdmhk deletion completed in 6.149342542s

â€¢ [SLOW TEST:16.351 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:33:54.409: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 21 05:33:54.517: INFO: Waiting up to 5m0s for pod "pod-24426508-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-ftxgt" to be "success or failure"
Jan 21 05:33:54.525: INFO: Pod "pod-24426508-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091007ms
Jan 21 05:33:56.530: INFO: Pod "pod-24426508-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013519707s
STEP: Saw pod success
Jan 21 05:33:56.531: INFO: Pod "pod-24426508-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:33:56.534: INFO: Trying to get logs from node k8s05 pod pod-24426508-1d3e-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:33:56.572: INFO: Waiting for pod pod-24426508-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:33:56.578: INFO: Pod pod-24426508-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:33:56.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-ftxgt" for this suite.
Jan 21 05:34:02.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:34:02.637: INFO: namespace: e2e-tests-emptydir-ftxgt, resource: bindings, ignored listing per whitelist
Jan 21 05:34:02.734: INFO: namespace e2e-tests-emptydir-ftxgt deletion completed in 6.148637314s

â€¢ [SLOW TEST:8.325 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:34:02.735: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Jan 21 05:34:02.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:03.489: INFO: stderr: ""
Jan 21 05:34:03.489: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 05:34:03.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:03.644: INFO: stderr: ""
Jan 21 05:34:03.644: INFO: stdout: "update-demo-nautilus-9r4hk update-demo-nautilus-r26w4 "
Jan 21 05:34:03.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-9r4hk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:03.782: INFO: stderr: ""
Jan 21 05:34:03.782: INFO: stdout: ""
Jan 21 05:34:03.782: INFO: update-demo-nautilus-9r4hk is created but not running
Jan 21 05:34:08.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:08.929: INFO: stderr: ""
Jan 21 05:34:08.929: INFO: stdout: "update-demo-nautilus-9r4hk update-demo-nautilus-r26w4 "
Jan 21 05:34:08.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-9r4hk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:09.081: INFO: stderr: ""
Jan 21 05:34:09.081: INFO: stdout: "true"
Jan 21 05:34:09.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-9r4hk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:09.240: INFO: stderr: ""
Jan 21 05:34:09.240: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 21 05:34:09.240: INFO: validating pod update-demo-nautilus-9r4hk
Jan 21 05:34:09.251: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 05:34:09.251: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 05:34:09.251: INFO: update-demo-nautilus-9r4hk is verified up and running
Jan 21 05:34:09.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-r26w4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:09.412: INFO: stderr: ""
Jan 21 05:34:09.412: INFO: stdout: "true"
Jan 21 05:34:09.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-r26w4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:09.583: INFO: stderr: ""
Jan 21 05:34:09.583: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 21 05:34:09.583: INFO: validating pod update-demo-nautilus-r26w4
Jan 21 05:34:09.593: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 05:34:09.593: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 05:34:09.593: INFO: update-demo-nautilus-r26w4 is verified up and running
STEP: rolling-update to new replication controller
Jan 21 05:34:09.594: INFO: scanned /root for discovery docs: <nil>
Jan 21 05:34:09.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:32.334: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 21 05:34:32.334: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 05:34:32.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:32.471: INFO: stderr: ""
Jan 21 05:34:32.471: INFO: stdout: "update-demo-kitten-kf58n update-demo-kitten-l4zjg "
Jan 21 05:34:32.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-kitten-kf58n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:32.634: INFO: stderr: ""
Jan 21 05:34:32.634: INFO: stdout: "true"
Jan 21 05:34:32.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-kitten-kf58n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:32.797: INFO: stderr: ""
Jan 21 05:34:32.797: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 21 05:34:32.797: INFO: validating pod update-demo-kitten-kf58n
Jan 21 05:34:32.807: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 21 05:34:32.807: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 21 05:34:32.807: INFO: update-demo-kitten-kf58n is verified up and running
Jan 21 05:34:32.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-kitten-l4zjg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:32.976: INFO: stderr: ""
Jan 21 05:34:32.976: INFO: stdout: "true"
Jan 21 05:34:32.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-kitten-l4zjg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-dpjlk'
Jan 21 05:34:33.141: INFO: stderr: ""
Jan 21 05:34:33.141: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 21 05:34:33.141: INFO: validating pod update-demo-kitten-l4zjg
Jan 21 05:34:33.150: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 21 05:34:33.150: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 21 05:34:33.150: INFO: update-demo-kitten-l4zjg is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:34:33.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dpjlk" for this suite.
Jan 21 05:34:55.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:34:55.208: INFO: namespace: e2e-tests-kubectl-dpjlk, resource: bindings, ignored listing per whitelist
Jan 21 05:34:55.360: INFO: namespace e2e-tests-kubectl-dpjlk deletion completed in 22.202049119s

â€¢ [SLOW TEST:52.625 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:34:55.360: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-489e2c24-1d3e-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:34:55.541: INFO: Waiting up to 5m0s for pod "pod-configmaps-48a0cd4f-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-configmap-x2t9z" to be "success or failure"
Jan 21 05:34:55.564: INFO: Pod "pod-configmaps-48a0cd4f-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 22.998623ms
Jan 21 05:34:57.570: INFO: Pod "pod-configmaps-48a0cd4f-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028680943s
Jan 21 05:34:59.575: INFO: Pod "pod-configmaps-48a0cd4f-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033952722s
STEP: Saw pod success
Jan 21 05:34:59.575: INFO: Pod "pod-configmaps-48a0cd4f-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:34:59.579: INFO: Trying to get logs from node k8s05 pod pod-configmaps-48a0cd4f-1d3e-11e9-b032-0a580af40356 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:34:59.621: INFO: Waiting for pod pod-configmaps-48a0cd4f-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:34:59.624: INFO: Pod pod-configmaps-48a0cd4f-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:34:59.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-x2t9z" for this suite.
Jan 21 05:35:05.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:35:05.730: INFO: namespace: e2e-tests-configmap-x2t9z, resource: bindings, ignored listing per whitelist
Jan 21 05:35:05.835: INFO: namespace e2e-tests-configmap-x2t9z deletion completed in 6.204118512s

â€¢ [SLOW TEST:10.474 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:35:05.835: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:35:05.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-m87vx" for this suite.
Jan 21 05:35:27.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:35:28.013: INFO: namespace: e2e-tests-pods-m87vx, resource: bindings, ignored listing per whitelist
Jan 21 05:35:28.135: INFO: namespace e2e-tests-pods-m87vx deletion completed in 22.169086771s

â€¢ [SLOW TEST:22.300 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:35:28.135: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:35:28.251: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c211f5f-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-2v94p" to be "success or failure"
Jan 21 05:35:28.254: INFO: Pod "downwardapi-volume-5c211f5f-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 3.217712ms
Jan 21 05:35:30.263: INFO: Pod "downwardapi-volume-5c211f5f-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012129865s
STEP: Saw pod success
Jan 21 05:35:30.263: INFO: Pod "downwardapi-volume-5c211f5f-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:35:30.267: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-5c211f5f-1d3e-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:35:30.311: INFO: Waiting for pod downwardapi-volume-5c211f5f-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:35:30.317: INFO: Pod downwardapi-volume-5c211f5f-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:35:30.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2v94p" for this suite.
Jan 21 05:35:36.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:35:36.380: INFO: namespace: e2e-tests-projected-2v94p, resource: bindings, ignored listing per whitelist
Jan 21 05:35:36.534: INFO: namespace e2e-tests-projected-2v94p deletion completed in 6.209249999s

â€¢ [SLOW TEST:8.398 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:35:36.534: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:35:36.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-612572d9-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-d2jfz" to be "success or failure"
Jan 21 05:35:36.676: INFO: Pod "downwardapi-volume-612572d9-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389727ms
Jan 21 05:35:38.698: INFO: Pod "downwardapi-volume-612572d9-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026292744s
STEP: Saw pod success
Jan 21 05:35:38.698: INFO: Pod "downwardapi-volume-612572d9-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:35:38.705: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-612572d9-1d3e-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:35:38.752: INFO: Waiting for pod downwardapi-volume-612572d9-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:35:38.759: INFO: Pod downwardapi-volume-612572d9-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:35:38.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-d2jfz" for this suite.
Jan 21 05:35:44.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:35:44.838: INFO: namespace: e2e-tests-downward-api-d2jfz, resource: bindings, ignored listing per whitelist
Jan 21 05:35:44.953: INFO: namespace e2e-tests-downward-api-d2jfz deletion completed in 6.185064179s

â€¢ [SLOW TEST:8.419 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:35:44.954: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-6627d3b4-1d3e-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:35:45.083: INFO: Waiting up to 5m0s for pod "pod-configmaps-6629fbf2-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-configmap-qcfqn" to be "success or failure"
Jan 21 05:35:45.090: INFO: Pod "pod-configmaps-6629fbf2-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 6.809895ms
Jan 21 05:35:47.096: INFO: Pod "pod-configmaps-6629fbf2-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013468567s
STEP: Saw pod success
Jan 21 05:35:47.097: INFO: Pod "pod-configmaps-6629fbf2-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:35:47.101: INFO: Trying to get logs from node k8s05 pod pod-configmaps-6629fbf2-1d3e-11e9-b032-0a580af40356 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:35:47.142: INFO: Waiting for pod pod-configmaps-6629fbf2-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:35:47.149: INFO: Pod pod-configmaps-6629fbf2-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:35:47.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-qcfqn" for this suite.
Jan 21 05:35:53.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:35:53.276: INFO: namespace: e2e-tests-configmap-qcfqn, resource: bindings, ignored listing per whitelist
Jan 21 05:35:53.348: INFO: namespace e2e-tests-configmap-qcfqn deletion completed in 6.191750812s

â€¢ [SLOW TEST:8.395 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:35:53.348: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:35:53.480: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b29d8ea-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-7bq8d" to be "success or failure"
Jan 21 05:35:53.493: INFO: Pod "downwardapi-volume-6b29d8ea-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 12.605044ms
Jan 21 05:35:55.500: INFO: Pod "downwardapi-volume-6b29d8ea-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019640682s
STEP: Saw pod success
Jan 21 05:35:55.500: INFO: Pod "downwardapi-volume-6b29d8ea-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:35:55.505: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-6b29d8ea-1d3e-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:35:55.553: INFO: Waiting for pod downwardapi-volume-6b29d8ea-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:35:55.558: INFO: Pod downwardapi-volume-6b29d8ea-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:35:55.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7bq8d" for this suite.
Jan 21 05:36:01.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:36:01.697: INFO: namespace: e2e-tests-projected-7bq8d, resource: bindings, ignored listing per whitelist
Jan 21 05:36:01.793: INFO: namespace e2e-tests-projected-7bq8d deletion completed in 6.22744855s

â€¢ [SLOW TEST:8.445 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:36:01.793: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-7034ebd0-1d3e-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:36:01.948: INFO: Waiting up to 5m0s for pod "pod-secrets-7036f70a-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-secrets-4g4jw" to be "success or failure"
Jan 21 05:36:01.956: INFO: Pod "pod-secrets-7036f70a-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 7.948892ms
Jan 21 05:36:03.964: INFO: Pod "pod-secrets-7036f70a-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015311618s
STEP: Saw pod success
Jan 21 05:36:03.964: INFO: Pod "pod-secrets-7036f70a-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:36:03.968: INFO: Trying to get logs from node k8s05 pod pod-secrets-7036f70a-1d3e-11e9-b032-0a580af40356 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:36:04.008: INFO: Waiting for pod pod-secrets-7036f70a-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:36:04.014: INFO: Pod pod-secrets-7036f70a-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:36:04.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-4g4jw" for this suite.
Jan 21 05:36:10.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:36:10.206: INFO: namespace: e2e-tests-secrets-4g4jw, resource: bindings, ignored listing per whitelist
Jan 21 05:36:10.254: INFO: namespace e2e-tests-secrets-4g4jw deletion completed in 6.230844542s

â€¢ [SLOW TEST:8.461 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:36:10.254: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:36:10.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-753f4ef4-1d3e-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-r5sck" to be "success or failure"
Jan 21 05:36:10.433: INFO: Pod "downwardapi-volume-753f4ef4-1d3e-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 17.267392ms
Jan 21 05:36:12.440: INFO: Pod "downwardapi-volume-753f4ef4-1d3e-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024889125s
STEP: Saw pod success
Jan 21 05:36:12.440: INFO: Pod "downwardapi-volume-753f4ef4-1d3e-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:36:12.446: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-753f4ef4-1d3e-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:36:12.491: INFO: Waiting for pod downwardapi-volume-753f4ef4-1d3e-11e9-b032-0a580af40356 to disappear
Jan 21 05:36:12.497: INFO: Pod downwardapi-volume-753f4ef4-1d3e-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:36:12.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-r5sck" for this suite.
Jan 21 05:36:18.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:36:18.568: INFO: namespace: e2e-tests-downward-api-r5sck, resource: bindings, ignored listing per whitelist
Jan 21 05:36:18.696: INFO: namespace e2e-tests-downward-api-r5sck deletion completed in 6.181509835s

â€¢ [SLOW TEST:8.442 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:36:18.696: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-q799w
Jan 21 05:36:20.858: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-q799w
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 05:36:20.863: INFO: Initial restart count of pod liveness-http is 0
Jan 21 05:36:36.950: INFO: Restart count of pod e2e-tests-container-probe-q799w/liveness-http is now 1 (16.087014395s elapsed)
Jan 21 05:36:57.031: INFO: Restart count of pod e2e-tests-container-probe-q799w/liveness-http is now 2 (36.167852688s elapsed)
Jan 21 05:37:17.117: INFO: Restart count of pod e2e-tests-container-probe-q799w/liveness-http is now 3 (56.254738723s elapsed)
Jan 21 05:37:37.220: INFO: Restart count of pod e2e-tests-container-probe-q799w/liveness-http is now 4 (1m16.357123437s elapsed)
Jan 21 05:38:41.509: INFO: Restart count of pod e2e-tests-container-probe-q799w/liveness-http is now 5 (2m20.646079304s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:38:41.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-q799w" for this suite.
Jan 21 05:38:47.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:38:47.712: INFO: namespace: e2e-tests-container-probe-q799w, resource: bindings, ignored listing per whitelist
Jan 21 05:38:47.741: INFO: namespace e2e-tests-container-probe-q799w deletion completed in 6.186321973s

â€¢ [SLOW TEST:149.045 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:38:47.742: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:38:49.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-7k56q" for this suite.
Jan 21 05:39:41.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:39:42.072: INFO: namespace: e2e-tests-kubelet-test-7k56q, resource: bindings, ignored listing per whitelist
Jan 21 05:39:42.097: INFO: namespace e2e-tests-kubelet-test-7k56q deletion completed in 52.179998276s

â€¢ [SLOW TEST:54.356 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a read only busybox container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:186
    should not write to root filesystem [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:39:42.098: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-696ph
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-696ph
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-696ph
Jan 21 05:39:42.253: INFO: Found 0 stateful pods, waiting for 1
Jan 21 05:39:52.269: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 21 05:39:52.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 05:39:52.656: INFO: stderr: ""
Jan 21 05:39:52.656: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 05:39:52.656: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 21 05:39:52.662: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 21 05:40:02.687: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 05:40:02.687: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 05:40:02.725: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:02.726: INFO: ss-0  k8s05  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:02.726: INFO: ss-1         Pending         []
Jan 21 05:40:02.726: INFO: 
Jan 21 05:40:02.726: INFO: StatefulSet ss has not reached scale 3, at 2
Jan 21 05:40:03.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988633493s
Jan 21 05:40:04.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981225819s
Jan 21 05:40:05.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972419517s
Jan 21 05:40:06.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964556795s
Jan 21 05:40:07.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957477274s
Jan 21 05:40:08.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951583437s
Jan 21 05:40:09.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945983995s
Jan 21 05:40:10.786: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939658907s
Jan 21 05:40:11.797: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.84651ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-696ph
Jan 21 05:40:12.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:40:13.170: INFO: stderr: ""
Jan 21 05:40:13.170: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 21 05:40:13.170: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 21 05:40:13.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:40:13.523: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jan 21 05:40:13.523: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 21 05:40:13.523: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 21 05:40:13.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:40:13.827: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Jan 21 05:40:13.827: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 21 05:40:13.827: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 21 05:40:13.833: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 21 05:40:23.857: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 05:40:23.858: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 05:40:23.858: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 21 05:40:23.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 05:40:24.088: INFO: stderr: ""
Jan 21 05:40:24.088: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 05:40:24.088: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 21 05:40:24.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 05:40:24.331: INFO: stderr: ""
Jan 21 05:40:24.331: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 05:40:24.331: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 21 05:40:24.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 05:40:24.557: INFO: stderr: ""
Jan 21 05:40:24.557: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 05:40:24.557: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 21 05:40:24.557: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 05:40:24.564: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 21 05:40:34.583: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 05:40:34.583: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 05:40:34.583: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 05:40:34.611: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:34.611: INFO: ss-0  k8s05  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:34.611: INFO: ss-1  k8s05  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:34.611: INFO: ss-2  k8s05  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:34.611: INFO: 
Jan 21 05:40:34.611: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 05:40:35.617: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:35.617: INFO: ss-0  k8s05  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:35.617: INFO: ss-1  k8s05  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:35.617: INFO: ss-2  k8s05  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:35.617: INFO: 
Jan 21 05:40:35.617: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 05:40:36.623: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:36.623: INFO: ss-0  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:36.623: INFO: ss-1  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:36.623: INFO: ss-2  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:36.623: INFO: 
Jan 21 05:40:36.623: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 05:40:37.630: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:37.630: INFO: ss-0  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:37.630: INFO: ss-1  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:37.630: INFO: ss-2  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:37.631: INFO: 
Jan 21 05:40:37.631: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 05:40:38.638: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:38.638: INFO: ss-0  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:38.638: INFO: ss-1  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:38.638: INFO: ss-2  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:38.638: INFO: 
Jan 21 05:40:38.638: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 05:40:39.645: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:39.645: INFO: ss-0  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:39.645: INFO: ss-1  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:39.645: INFO: ss-2  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:39.645: INFO: 
Jan 21 05:40:39.645: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 05:40:40.653: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:40.654: INFO: ss-0  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:40.654: INFO: ss-1  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:40.654: INFO: ss-2  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:40.654: INFO: 
Jan 21 05:40:40.654: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 05:40:41.661: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:41.661: INFO: ss-0  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:41.661: INFO: ss-1  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:41.661: INFO: ss-2  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:41.661: INFO: 
Jan 21 05:40:41.661: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 05:40:42.669: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:42.669: INFO: ss-0  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:42.669: INFO: ss-1  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:42.669: INFO: ss-2  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:42.669: INFO: 
Jan 21 05:40:42.669: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 21 05:40:43.684: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Jan 21 05:40:43.684: INFO: ss-0  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:39:42 +0000 UTC  }]
Jan 21 05:40:43.684: INFO: ss-1  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:43.684: INFO: ss-2  k8s05  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 05:40:02 +0000 UTC  }]
Jan 21 05:40:43.684: INFO: 
Jan 21 05:40:43.684: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-696ph
Jan 21 05:40:44.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:40:44.920: INFO: rc: 1
Jan 21 05:40:44.920: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0019bbb90 exit status 1 <nil> <nil> true [0xc001650468 0xc001650480 0xc001650498] [0xc001650468 0xc001650480 0xc001650498] [0xc001650478 0xc001650490] [0x92f8e0 0x92f8e0] 0xc00311d1a0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Jan 21 05:40:54.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:40:55.064: INFO: rc: 1
Jan 21 05:40:55.064: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6e240 exit status 1 <nil> <nil> true [0xc00164c290 0xc00164c2a8 0xc00164c2c0] [0xc00164c290 0xc00164c2a8 0xc00164c2c0] [0xc00164c2a0 0xc00164c2b8] [0x92f8e0 0x92f8e0] 0xc002adb860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:41:05.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:41:05.252: INFO: rc: 1
Jan 21 05:41:05.252: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6e600 exit status 1 <nil> <nil> true [0xc00164c2c8 0xc00164c2e0 0xc00164c2f8] [0xc00164c2c8 0xc00164c2e0 0xc00164c2f8] [0xc00164c2d8 0xc00164c2f0] [0x92f8e0 0x92f8e0] 0xc002adbbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:41:15.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:41:15.407: INFO: rc: 1
Jan 21 05:41:15.407: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6ecf0 exit status 1 <nil> <nil> true [0xc00164c300 0xc00164c318 0xc00164c338] [0xc00164c300 0xc00164c318 0xc00164c338] [0xc00164c310 0xc00164c330] [0x92f8e0 0x92f8e0] 0xc002adbec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:41:25.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:41:25.557: INFO: rc: 1
Jan 21 05:41:25.557: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6f680 exit status 1 <nil> <nil> true [0xc00164c340 0xc00164c358 0xc00164c370] [0xc00164c340 0xc00164c358 0xc00164c370] [0xc00164c350 0xc00164c368] [0x92f8e0 0x92f8e0] 0xc0030de1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:41:35.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:41:35.662: INFO: rc: 1
Jan 21 05:41:35.662: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6fc80 exit status 1 <nil> <nil> true [0xc00164c378 0xc00164c390 0xc00164c3a8] [0xc00164c378 0xc00164c390 0xc00164c3a8] [0xc00164c388 0xc00164c3a0] [0x92f8e0 0x92f8e0] 0xc0030de4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:41:45.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:41:45.764: INFO: rc: 1
Jan 21 05:41:45.764: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014e20c0 exit status 1 <nil> <nil> true [0xc00164c3b0 0xc00164c3c8 0xc00164c3e0] [0xc00164c3b0 0xc00164c3c8 0xc00164c3e0] [0xc00164c3c0 0xc00164c3d8] [0x92f8e0 0x92f8e0] 0xc0030ded20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:41:55.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:41:55.909: INFO: rc: 1
Jan 21 05:41:55.909: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019bbf20 exit status 1 <nil> <nil> true [0xc0016504a0 0xc0016504b8 0xc0016504d0] [0xc0016504a0 0xc0016504b8 0xc0016504d0] [0xc0016504b0 0xc0016504c8] [0x92f8e0 0x92f8e0] 0xc00311d4a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:42:05.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:42:06.033: INFO: rc: 1
Jan 21 05:42:06.033: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00187a720 exit status 1 <nil> <nil> true [0xc0016504d8 0xc0016504f0 0xc001650508] [0xc0016504d8 0xc0016504f0 0xc001650508] [0xc0016504e8 0xc001650500] [0x92f8e0 0x92f8e0] 0xc00311d7a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:42:16.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:42:16.181: INFO: rc: 1
Jan 21 05:42:16.181: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6e4e0 exit status 1 <nil> <nil> true [0xc001650078 0xc0016500c8 0xc0016500f0] [0xc001650078 0xc0016500c8 0xc0016500f0] [0xc0016500b8 0xc0016500e8] [0x92f8e0 0x92f8e0] 0xc002ada480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:42:26.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:42:26.296: INFO: rc: 1
Jan 21 05:42:26.296: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6e8d0 exit status 1 <nil> <nil> true [0xc0016500f8 0xc001650140 0xc001650178] [0xc0016500f8 0xc001650140 0xc001650178] [0xc001650128 0xc001650168] [0x92f8e0 0x92f8e0] 0xc002adaa20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:42:36.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:42:36.423: INFO: rc: 1
Jan 21 05:42:36.423: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6f3e0 exit status 1 <nil> <nil> true [0xc001650198 0xc0016501b0 0xc0016501f8] [0xc001650198 0xc0016501b0 0xc0016501f8] [0xc0016501a8 0xc0016501e0] [0x92f8e0 0x92f8e0] 0xc002adb020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:42:46.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:42:46.571: INFO: rc: 1
Jan 21 05:42:46.571: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019ba420 exit status 1 <nil> <nil> true [0xc00164c000 0xc00164c018 0xc00164c030] [0xc00164c000 0xc00164c018 0xc00164c030] [0xc00164c010 0xc00164c028] [0x92f8e0 0x92f8e0] 0xc0020e82a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:42:56.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:42:56.716: INFO: rc: 1
Jan 21 05:42:56.716: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6fb90 exit status 1 <nil> <nil> true [0xc001650208 0xc001650250 0xc0016502a0] [0xc001650208 0xc001650250 0xc0016502a0] [0xc001650230 0xc001650288] [0x92f8e0 0x92f8e0] 0xc002adb5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:43:06.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:43:06.872: INFO: rc: 1
Jan 21 05:43:06.872: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019ba870 exit status 1 <nil> <nil> true [0xc00164c038 0xc00164c050 0xc00164c068] [0xc00164c038 0xc00164c050 0xc00164c068] [0xc00164c048 0xc00164c060] [0x92f8e0 0x92f8e0] 0xc0020e8840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:43:16.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:43:17.020: INFO: rc: 1
Jan 21 05:43:17.020: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbc060 exit status 1 <nil> <nil> true [0xc0016502b0 0xc0016502d8 0xc001650320] [0xc0016502b0 0xc0016502d8 0xc001650320] [0xc0016502d0 0xc001650310] [0x92f8e0 0x92f8e0] 0xc002adb9e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:43:27.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:43:27.181: INFO: rc: 1
Jan 21 05:43:27.182: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbc420 exit status 1 <nil> <nil> true [0xc001650340 0xc001650368 0xc001650380] [0xc001650340 0xc001650368 0xc001650380] [0xc001650360 0xc001650378] [0x92f8e0 0x92f8e0] 0xc002adbce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:43:37.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:43:37.348: INFO: rc: 1
Jan 21 05:43:37.348: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbc7e0 exit status 1 <nil> <nil> true [0xc001650388 0xc0016503a8 0xc0016503f0] [0xc001650388 0xc0016503a8 0xc0016503f0] [0xc001650398 0xc0016503e0] [0x92f8e0 0x92f8e0] 0xc002fd6000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:43:47.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:43:47.510: INFO: rc: 1
Jan 21 05:43:47.510: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019bac00 exit status 1 <nil> <nil> true [0xc00164c070 0xc00164c088 0xc00164c0a0] [0xc00164c070 0xc00164c088 0xc00164c0a0] [0xc00164c080 0xc00164c098] [0x92f8e0 0x92f8e0] 0xc0020e9020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:43:57.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:43:57.614: INFO: rc: 1
Jan 21 05:43:57.614: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbcba0 exit status 1 <nil> <nil> true [0xc001650408 0xc001650420 0xc001650440] [0xc001650408 0xc001650420 0xc001650440] [0xc001650418 0xc001650430] [0x92f8e0 0x92f8e0] 0xc002fd6540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:44:07.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:44:07.728: INFO: rc: 1
Jan 21 05:44:07.728: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019bb1a0 exit status 1 <nil> <nil> true [0xc00164c0a8 0xc00164c0c0 0xc00164c0d8] [0xc00164c0a8 0xc00164c0c0 0xc00164c0d8] [0xc00164c0b8 0xc00164c0d0] [0x92f8e0 0x92f8e0] 0xc0020e9bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:44:17.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:44:17.869: INFO: rc: 1
Jan 21 05:44:17.869: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6e4b0 exit status 1 <nil> <nil> true [0xc001650078 0xc0016500c8 0xc0016500f0] [0xc001650078 0xc0016500c8 0xc0016500f0] [0xc0016500b8 0xc0016500e8] [0x92f8e0 0x92f8e0] 0xc002ada480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:44:27.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:44:27.991: INFO: rc: 1
Jan 21 05:44:27.991: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbc3c0 exit status 1 <nil> <nil> true [0xc00164c000 0xc00164c018 0xc00164c030] [0xc00164c000 0xc00164c018 0xc00164c030] [0xc00164c010 0xc00164c028] [0x92f8e0 0x92f8e0] 0xc002fd6420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:44:37.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:44:38.123: INFO: rc: 1
Jan 21 05:44:38.123: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6e900 exit status 1 <nil> <nil> true [0xc0016500f8 0xc001650140 0xc001650178] [0xc0016500f8 0xc001650140 0xc001650178] [0xc001650128 0xc001650168] [0x92f8e0 0x92f8e0] 0xc002adaa20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:44:48.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:44:48.244: INFO: rc: 1
Jan 21 05:44:48.244: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbc780 exit status 1 <nil> <nil> true [0xc00164c038 0xc00164c050 0xc00164c068] [0xc00164c038 0xc00164c050 0xc00164c068] [0xc00164c048 0xc00164c060] [0x92f8e0 0x92f8e0] 0xc002fd69c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:44:58.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:44:58.363: INFO: rc: 1
Jan 21 05:44:58.363: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbcb70 exit status 1 <nil> <nil> true [0xc00164c070 0xc00164c088 0xc00164c0a0] [0xc00164c070 0xc00164c088 0xc00164c0a0] [0xc00164c080 0xc00164c098] [0x92f8e0 0x92f8e0] 0xc002fd6fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:45:08.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:45:08.518: INFO: rc: 1
Jan 21 05:45:08.518: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a6f440 exit status 1 <nil> <nil> true [0xc001650198 0xc0016501b0 0xc0016501f8] [0xc001650198 0xc0016501b0 0xc0016501f8] [0xc0016501a8 0xc0016501e0] [0x92f8e0 0x92f8e0] 0xc002adb020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:45:18.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:45:18.640: INFO: rc: 1
Jan 21 05:45:18.640: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbd1a0 exit status 1 <nil> <nil> true [0xc00164c0a8 0xc00164c0c0 0xc00164c0d8] [0xc00164c0a8 0xc00164c0c0 0xc00164c0d8] [0xc00164c0b8 0xc00164c0d0] [0x92f8e0 0x92f8e0] 0xc002fd7500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:45:28.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:45:28.802: INFO: rc: 1
Jan 21 05:45:28.803: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbd770 exit status 1 <nil> <nil> true [0xc00164c0e0 0xc00164c100 0xc00164c118] [0xc00164c0e0 0xc00164c100 0xc00164c118] [0xc00164c0f8 0xc00164c110] [0x92f8e0 0x92f8e0] 0xc002fd7920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:45:38.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:45:38.957: INFO: rc: 1
Jan 21 05:45:38.957: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bbdb30 exit status 1 <nil> <nil> true [0xc00164c120 0xc00164c138 0xc00164c150] [0xc00164c120 0xc00164c138 0xc00164c150] [0xc00164c130 0xc00164c148] [0x92f8e0 0x92f8e0] 0xc0020e8000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jan 21 05:45:48.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-696ph ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 05:45:49.111: INFO: rc: 1
Jan 21 05:45:49.111: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Jan 21 05:45:49.111: INFO: Scaling statefulset ss to 0
Jan 21 05:45:49.132: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 21 05:45:49.137: INFO: Deleting all statefulset in ns e2e-tests-statefulset-696ph
Jan 21 05:45:49.142: INFO: Scaling statefulset ss to 0
Jan 21 05:45:49.157: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 05:45:49.161: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:45:49.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-696ph" for this suite.
Jan 21 05:45:55.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:45:55.247: INFO: namespace: e2e-tests-statefulset-696ph, resource: bindings, ignored listing per whitelist
Jan 21 05:45:55.390: INFO: namespace e2e-tests-statefulset-696ph deletion completed in 6.193043964s

â€¢ [SLOW TEST:373.292 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:45:55.391: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-sszng/configmap-test-d20218f9-1d3f-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:45:55.527: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2043387-1d3f-11e9-b032-0a580af40356" in namespace "e2e-tests-configmap-sszng" to be "success or failure"
Jan 21 05:45:55.537: INFO: Pod "pod-configmaps-d2043387-1d3f-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 10.322825ms
Jan 21 05:45:57.545: INFO: Pod "pod-configmaps-d2043387-1d3f-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017582587s
STEP: Saw pod success
Jan 21 05:45:57.545: INFO: Pod "pod-configmaps-d2043387-1d3f-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:45:57.551: INFO: Trying to get logs from node k8s05 pod pod-configmaps-d2043387-1d3f-11e9-b032-0a580af40356 container env-test: <nil>
STEP: delete the pod
Jan 21 05:45:57.605: INFO: Waiting for pod pod-configmaps-d2043387-1d3f-11e9-b032-0a580af40356 to disappear
Jan 21 05:45:57.614: INFO: Pod pod-configmaps-d2043387-1d3f-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:45:57.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-sszng" for this suite.
Jan 21 05:46:03.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:46:03.770: INFO: namespace: e2e-tests-configmap-sszng, resource: bindings, ignored listing per whitelist
Jan 21 05:46:03.842: INFO: namespace e2e-tests-configmap-sszng deletion completed in 6.21903s

â€¢ [SLOW TEST:8.451 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:46:03.842: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:46:05.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubelet-test-vsg6m" for this suite.
Jan 21 05:46:52.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:46:52.129: INFO: namespace: e2e-tests-kubelet-test-vsg6m, resource: bindings, ignored listing per whitelist
Jan 21 05:46:52.191: INFO: namespace e2e-tests-kubelet-test-vsg6m deletion completed in 46.188867741s

â€¢ [SLOW TEST:48.349 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when scheduling a busybox command in a pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:46:52.192: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-f3df151f-1d3f-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:46:52.368: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3e1944c-1d3f-11e9-b032-0a580af40356" in namespace "e2e-tests-configmap-k82qn" to be "success or failure"
Jan 21 05:46:52.377: INFO: Pod "pod-configmaps-f3e1944c-1d3f-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 8.769383ms
Jan 21 05:46:54.387: INFO: Pod "pod-configmaps-f3e1944c-1d3f-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018181863s
STEP: Saw pod success
Jan 21 05:46:54.387: INFO: Pod "pod-configmaps-f3e1944c-1d3f-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:46:54.391: INFO: Trying to get logs from node k8s05 pod pod-configmaps-f3e1944c-1d3f-11e9-b032-0a580af40356 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:46:54.431: INFO: Waiting for pod pod-configmaps-f3e1944c-1d3f-11e9-b032-0a580af40356 to disappear
Jan 21 05:46:54.435: INFO: Pod pod-configmaps-f3e1944c-1d3f-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:46:54.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-k82qn" for this suite.
Jan 21 05:47:00.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:47:00.520: INFO: namespace: e2e-tests-configmap-k82qn, resource: bindings, ignored listing per whitelist
Jan 21 05:47:00.637: INFO: namespace e2e-tests-configmap-k82qn deletion completed in 6.194944358s

â€¢ [SLOW TEST:8.444 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:47:00.637: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1134
STEP: creating an rc
Jan 21 05:47:00.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-67627'
Jan 21 05:47:01.417: INFO: stderr: ""
Jan 21 05:47:01.417: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Jan 21 05:47:02.423: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:47:02.423: INFO: Found 0 / 1
Jan 21 05:47:03.423: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:47:03.423: INFO: Found 1 / 1
Jan 21 05:47:03.423: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 21 05:47:03.428: INFO: Selector matched 1 pods for map[app:redis]
Jan 21 05:47:03.428: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jan 21 05:47:03.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 logs redis-master-ww4lg redis-master --namespace=e2e-tests-kubectl-67627'
Jan 21 05:47:03.607: INFO: stderr: ""
Jan 21 05:47:03.607: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Jan 05:47:02.316 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Jan 05:47:02.316 # Server started, Redis version 3.2.12\n1:M 21 Jan 05:47:02.316 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Jan 05:47:02.316 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jan 21 05:47:03.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 log redis-master-ww4lg redis-master --namespace=e2e-tests-kubectl-67627 --tail=1'
Jan 21 05:47:03.766: INFO: stderr: ""
Jan 21 05:47:03.766: INFO: stdout: "1:M 21 Jan 05:47:02.316 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jan 21 05:47:03.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 log redis-master-ww4lg redis-master --namespace=e2e-tests-kubectl-67627 --limit-bytes=1'
Jan 21 05:47:03.907: INFO: stderr: ""
Jan 21 05:47:03.907: INFO: stdout: " "
STEP: exposing timestamps
Jan 21 05:47:03.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 log redis-master-ww4lg redis-master --namespace=e2e-tests-kubectl-67627 --tail=1 --timestamps'
Jan 21 05:47:04.051: INFO: stderr: ""
Jan 21 05:47:04.051: INFO: stdout: "2019-01-21T05:47:02.317439378Z 1:M 21 Jan 05:47:02.316 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jan 21 05:47:06.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 log redis-master-ww4lg redis-master --namespace=e2e-tests-kubectl-67627 --since=1s'
Jan 21 05:47:06.724: INFO: stderr: ""
Jan 21 05:47:06.724: INFO: stdout: ""
Jan 21 05:47:06.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 log redis-master-ww4lg redis-master --namespace=e2e-tests-kubectl-67627 --since=24h'
Jan 21 05:47:06.874: INFO: stderr: ""
Jan 21 05:47:06.874: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Jan 05:47:02.316 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Jan 05:47:02.316 # Server started, Redis version 3.2.12\n1:M 21 Jan 05:47:02.316 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Jan 05:47:02.316 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1140
STEP: using delete to clean up resources
Jan 21 05:47:06.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-67627'
Jan 21 05:47:07.013: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:47:07.013: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jan 21 05:47:07.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-67627'
Jan 21 05:47:07.141: INFO: stderr: "No resources found.\n"
Jan 21 05:47:07.141: INFO: stdout: ""
Jan 21 05:47:07.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -l name=nginx --namespace=e2e-tests-kubectl-67627 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 21 05:47:07.317: INFO: stderr: ""
Jan 21 05:47:07.317: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:47:07.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-67627" for this suite.
Jan 21 05:47:13.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:47:13.379: INFO: namespace: e2e-tests-kubectl-67627, resource: bindings, ignored listing per whitelist
Jan 21 05:47:13.486: INFO: namespace e2e-tests-kubectl-67627 deletion completed in 6.150090501s

â€¢ [SLOW TEST:12.849 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:47:13.486: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-008d1520-1d40-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:47:13.607: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-008eaf5d-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-nhg8g" to be "success or failure"
Jan 21 05:47:13.612: INFO: Pod "pod-projected-configmaps-008eaf5d-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872197ms
Jan 21 05:47:15.619: INFO: Pod "pod-projected-configmaps-008eaf5d-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012239862s
STEP: Saw pod success
Jan 21 05:47:15.619: INFO: Pod "pod-projected-configmaps-008eaf5d-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:47:15.624: INFO: Trying to get logs from node k8s05 pod pod-projected-configmaps-008eaf5d-1d40-11e9-b032-0a580af40356 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:47:15.663: INFO: Waiting for pod pod-projected-configmaps-008eaf5d-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:47:15.668: INFO: Pod pod-projected-configmaps-008eaf5d-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:47:15.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-nhg8g" for this suite.
Jan 21 05:47:21.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:47:21.734: INFO: namespace: e2e-tests-projected-nhg8g, resource: bindings, ignored listing per whitelist
Jan 21 05:47:21.863: INFO: namespace e2e-tests-projected-nhg8g deletion completed in 6.187656466s

â€¢ [SLOW TEST:8.377 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:47:21.863: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-kvzxk
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-kvzxk
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-kvzxk
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-kvzxk
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-kvzxk
Jan 21 05:47:24.049: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-kvzxk, name: ss-0, uid: 05abf8ba-1d40-11e9-b27d-fa163eb7f963, status phase: Pending. Waiting for statefulset controller to delete.
Jan 21 05:47:28.209: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-kvzxk, name: ss-0, uid: 05abf8ba-1d40-11e9-b27d-fa163eb7f963, status phase: Failed. Waiting for statefulset controller to delete.
Jan 21 05:47:28.233: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-kvzxk, name: ss-0, uid: 05abf8ba-1d40-11e9-b27d-fa163eb7f963, status phase: Failed. Waiting for statefulset controller to delete.
Jan 21 05:47:28.243: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-kvzxk
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-kvzxk
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-kvzxk and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 21 05:47:30.343: INFO: Deleting all statefulset in ns e2e-tests-statefulset-kvzxk
Jan 21 05:47:30.347: INFO: Scaling statefulset ss to 0
Jan 21 05:47:40.381: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 05:47:40.387: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:47:40.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-kvzxk" for this suite.
Jan 21 05:47:46.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:47:46.608: INFO: namespace: e2e-tests-statefulset-kvzxk, resource: bindings, ignored listing per whitelist
Jan 21 05:47:46.630: INFO: namespace e2e-tests-statefulset-kvzxk deletion completed in 6.203161254s

â€¢ [SLOW TEST:24.767 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:47:46.631: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:47:46.748: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:47:52.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-7d542" for this suite.
Jan 21 05:47:58.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:47:59.065: INFO: namespace: e2e-tests-custom-resource-definition-7d542, resource: bindings, ignored listing per whitelist
Jan 21 05:47:59.072: INFO: namespace e2e-tests-custom-resource-definition-7d542 deletion completed in 6.228875777s

â€¢ [SLOW TEST:12.441 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:47:59.072: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-1bbd304f-1d40-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:47:59.230: INFO: Waiting up to 5m0s for pod "pod-secrets-1bbf79ee-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-secrets-2gf9t" to be "success or failure"
Jan 21 05:47:59.241: INFO: Pod "pod-secrets-1bbf79ee-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.423368ms
Jan 21 05:48:01.255: INFO: Pod "pod-secrets-1bbf79ee-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025563083s
STEP: Saw pod success
Jan 21 05:48:01.255: INFO: Pod "pod-secrets-1bbf79ee-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:48:01.260: INFO: Trying to get logs from node k8s05 pod pod-secrets-1bbf79ee-1d40-11e9-b032-0a580af40356 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:48:01.301: INFO: Waiting for pod pod-secrets-1bbf79ee-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:48:01.307: INFO: Pod pod-secrets-1bbf79ee-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:48:01.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2gf9t" for this suite.
Jan 21 05:48:07.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:48:07.425: INFO: namespace: e2e-tests-secrets-2gf9t, resource: bindings, ignored listing per whitelist
Jan 21 05:48:07.501: INFO: namespace e2e-tests-secrets-2gf9t deletion completed in 6.184622157s

â€¢ [SLOW TEST:8.429 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:48:07.501: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 21 05:48:13.711: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 21 05:48:13.716: INFO: Pod pod-with-poststart-http-hook still exists
Jan 21 05:48:15.717: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 21 05:48:15.724: INFO: Pod pod-with-poststart-http-hook still exists
Jan 21 05:48:17.717: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 21 05:48:17.724: INFO: Pod pod-with-poststart-http-hook still exists
Jan 21 05:48:19.717: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 21 05:48:19.723: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:48:19.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-zpm4m" for this suite.
Jan 21 05:48:41.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:48:41.883: INFO: namespace: e2e-tests-container-lifecycle-hook-zpm4m, resource: bindings, ignored listing per whitelist
Jan 21 05:48:41.965: INFO: namespace e2e-tests-container-lifecycle-hook-zpm4m deletion completed in 22.235462448s

â€¢ [SLOW TEST:34.464 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:48:41.966: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-3551efa2-1d40-11e9-b032-0a580af40356
STEP: Creating configMap with name cm-test-opt-upd-3551f078-1d40-11e9-b032-0a580af40356
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3551efa2-1d40-11e9-b032-0a580af40356
STEP: Updating configmap cm-test-opt-upd-3551f078-1d40-11e9-b032-0a580af40356
STEP: Creating configMap with name cm-test-opt-create-3551f0a7-1d40-11e9-b032-0a580af40356
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:50:09.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fck79" for this suite.
Jan 21 05:50:31.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:50:31.247: INFO: namespace: e2e-tests-projected-fck79, resource: bindings, ignored listing per whitelist
Jan 21 05:50:31.365: INFO: namespace e2e-tests-projected-fck79 deletion completed in 22.185402981s

â€¢ [SLOW TEST:109.399 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:50:31.365: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:50:38.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-qndp9" for this suite.
Jan 21 05:51:00.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:51:00.604: INFO: namespace: e2e-tests-replication-controller-qndp9, resource: bindings, ignored listing per whitelist
Jan 21 05:51:00.703: INFO: namespace e2e-tests-replication-controller-qndp9 deletion completed in 22.158495936s

â€¢ [SLOW TEST:29.338 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:51:00.703: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 05:51:00.842: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 21 05:51:00.865: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:00.865: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:00.865: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:00.869: INFO: Number of nodes with available pods: 0
Jan 21 05:51:00.869: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:51:01.880: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:01.880: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:01.880: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:01.887: INFO: Number of nodes with available pods: 0
Jan 21 05:51:01.887: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:51:02.880: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:02.880: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:02.880: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:02.889: INFO: Number of nodes with available pods: 1
Jan 21 05:51:02.890: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 21 05:51:02.927: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:02.947: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:02.947: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:02.947: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:03.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:03.961: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:03.961: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:03.961: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:04.970: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:05.016: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:05.016: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:05.016: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:05.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:05.959: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:05.959: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:05.959: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:06.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:06.960: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:06.960: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:06.960: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:07.953: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:07.959: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:07.959: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:07.959: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:08.953: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:08.959: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:08.959: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:08.959: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:09.952: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:09.958: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:09.958: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:09.958: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:10.960: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:10.965: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:10.965: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:10.965: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:11.952: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:11.959: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:11.959: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:11.959: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:12.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:12.972: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:12.972: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:12.972: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:13.955: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:13.962: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:13.962: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:13.962: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:14.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:14.961: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:14.961: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:14.962: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:15.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:15.961: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:15.961: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:15.961: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:16.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:16.960: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:16.961: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:16.961: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:17.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:17.963: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:17.963: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:17.963: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:18.953: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:18.959: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:18.959: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:18.959: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:19.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:19.961: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:19.961: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:19.961: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:20.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:20.962: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:20.962: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:20.962: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:21.966: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:21.975: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:21.975: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:21.975: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:22.953: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:22.962: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:22.962: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:22.962: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:23.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:23.961: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:23.961: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:23.961: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:24.955: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:24.962: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:24.962: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:24.962: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:25.955: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:25.963: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:25.963: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:25.964: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:26.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:26.962: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:26.962: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:26.962: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:27.953: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:27.962: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:27.962: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:27.962: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:28.953: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:28.960: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:28.961: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:28.961: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:29.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:29.962: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:29.962: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:29.963: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:30.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:30.962: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:30.962: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:30.963: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:31.953: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:31.961: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:31.961: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:31.961: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:32.959: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:32.965: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:32.965: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:32.965: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:33.953: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:33.959: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:33.959: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:33.959: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:34.953: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:34.960: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:34.960: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:34.960: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:35.954: INFO: Wrong image for pod: daemon-set-2bt42. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Jan 21 05:51:35.954: INFO: Pod daemon-set-2bt42 is not available
Jan 21 05:51:35.962: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:35.962: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:35.962: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:36.953: INFO: Pod daemon-set-hgf4p is not available
Jan 21 05:51:36.960: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:36.960: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:36.960: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 21 05:51:36.964: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:36.965: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:36.965: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:36.968: INFO: Number of nodes with available pods: 0
Jan 21 05:51:36.968: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:51:37.977: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:37.977: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:37.977: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:37.982: INFO: Number of nodes with available pods: 0
Jan 21 05:51:37.982: INFO: Node k8s05 is running more than one daemon pod
Jan 21 05:51:38.977: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:38.977: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:38.977: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 05:51:38.984: INFO: Number of nodes with available pods: 1
Jan 21 05:51:38.984: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-ltghm, will wait for the garbage collector to delete the pods
Jan 21 05:51:39.091: INFO: Deleting DaemonSet.extensions daemon-set took: 22.093074ms
Jan 21 05:51:39.191: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.368584ms
Jan 21 05:51:43.006: INFO: Number of nodes with available pods: 0
Jan 21 05:51:43.006: INFO: Number of running nodes: 0, number of available pods: 0
Jan 21 05:51:43.010: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-ltghm/daemonsets","resourceVersion":"40489"},"items":null}

Jan 21 05:51:43.018: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-ltghm/pods","resourceVersion":"40489"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:51:43.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-ltghm" for this suite.
Jan 21 05:51:49.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:51:49.161: INFO: namespace: e2e-tests-daemonsets-ltghm, resource: bindings, ignored listing per whitelist
Jan 21 05:51:49.263: INFO: namespace e2e-tests-daemonsets-ltghm deletion completed in 6.212592297s

â€¢ [SLOW TEST:48.560 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:51:49.264: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-a4f5a586-1d40-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:51:49.462: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a4f8fb67-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-xb888" to be "success or failure"
Jan 21 05:51:49.474: INFO: Pod "pod-projected-configmaps-a4f8fb67-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.504308ms
Jan 21 05:51:51.481: INFO: Pod "pod-projected-configmaps-a4f8fb67-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018323593s
STEP: Saw pod success
Jan 21 05:51:51.481: INFO: Pod "pod-projected-configmaps-a4f8fb67-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:51:51.486: INFO: Trying to get logs from node k8s05 pod pod-projected-configmaps-a4f8fb67-1d40-11e9-b032-0a580af40356 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:51:51.565: INFO: Waiting for pod pod-projected-configmaps-a4f8fb67-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:51:51.574: INFO: Pod pod-projected-configmaps-a4f8fb67-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:51:51.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xb888" for this suite.
Jan 21 05:51:57.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:51:57.775: INFO: namespace: e2e-tests-projected-xb888, resource: bindings, ignored listing per whitelist
Jan 21 05:51:57.791: INFO: namespace e2e-tests-projected-xb888 deletion completed in 6.209569727s

â€¢ [SLOW TEST:8.528 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:51:57.792: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:51:57.919: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa03e8d9-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-vj8c2" to be "success or failure"
Jan 21 05:51:57.924: INFO: Pod "downwardapi-volume-aa03e8d9-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 5.150253ms
Jan 21 05:51:59.931: INFO: Pod "downwardapi-volume-aa03e8d9-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011932442s
STEP: Saw pod success
Jan 21 05:51:59.931: INFO: Pod "downwardapi-volume-aa03e8d9-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:51:59.935: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-aa03e8d9-1d40-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:51:59.977: INFO: Waiting for pod downwardapi-volume-aa03e8d9-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:51:59.981: INFO: Pod downwardapi-volume-aa03e8d9-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:51:59.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-vj8c2" for this suite.
Jan 21 05:52:06.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:52:06.083: INFO: namespace: e2e-tests-projected-vj8c2, resource: bindings, ignored listing per whitelist
Jan 21 05:52:06.135: INFO: namespace e2e-tests-projected-vj8c2 deletion completed in 6.147705201s

â€¢ [SLOW TEST:8.343 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:52:06.135: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-aefb19aa-1d40-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:52:06.252: INFO: Waiting up to 5m0s for pod "pod-secrets-aefcaab1-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-secrets-tb847" to be "success or failure"
Jan 21 05:52:06.260: INFO: Pod "pod-secrets-aefcaab1-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 7.77823ms
Jan 21 05:52:08.265: INFO: Pod "pod-secrets-aefcaab1-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013513628s
STEP: Saw pod success
Jan 21 05:52:08.265: INFO: Pod "pod-secrets-aefcaab1-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:52:08.269: INFO: Trying to get logs from node k8s05 pod pod-secrets-aefcaab1-1d40-11e9-b032-0a580af40356 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:52:08.309: INFO: Waiting for pod pod-secrets-aefcaab1-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:52:08.315: INFO: Pod pod-secrets-aefcaab1-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:52:08.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tb847" for this suite.
Jan 21 05:52:14.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:52:14.454: INFO: namespace: e2e-tests-secrets-tb847, resource: bindings, ignored listing per whitelist
Jan 21 05:52:14.478: INFO: namespace e2e-tests-secrets-tb847 deletion completed in 6.157339067s

â€¢ [SLOW TEST:8.344 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:52:14.479: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 21 05:52:14.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-cspwd'
Jan 21 05:52:14.745: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 21 05:52:14.745: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1268
Jan 21 05:52:16.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-cspwd'
Jan 21 05:52:16.887: INFO: stderr: ""
Jan 21 05:52:16.887: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:52:16.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cspwd" for this suite.
Jan 21 05:52:22.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:52:23.045: INFO: namespace: e2e-tests-kubectl-cspwd, resource: bindings, ignored listing per whitelist
Jan 21 05:52:23.101: INFO: namespace e2e-tests-kubectl-cspwd deletion completed in 6.204764107s

â€¢ [SLOW TEST:8.623 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:52:23.102: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:52:23.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b91b0fe2-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-s75tl" to be "success or failure"
Jan 21 05:52:23.246: INFO: Pod "downwardapi-volume-b91b0fe2-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.414708ms
Jan 21 05:52:25.260: INFO: Pod "downwardapi-volume-b91b0fe2-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02584455s
STEP: Saw pod success
Jan 21 05:52:25.260: INFO: Pod "downwardapi-volume-b91b0fe2-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:52:25.269: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-b91b0fe2-1d40-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:52:25.306: INFO: Waiting for pod downwardapi-volume-b91b0fe2-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:52:25.311: INFO: Pod downwardapi-volume-b91b0fe2-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:52:25.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-s75tl" for this suite.
Jan 21 05:52:31.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:52:31.391: INFO: namespace: e2e-tests-downward-api-s75tl, resource: bindings, ignored listing per whitelist
Jan 21 05:52:31.466: INFO: namespace e2e-tests-downward-api-s75tl deletion completed in 6.147898252s

â€¢ [SLOW TEST:8.365 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:52:31.467: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Jan 21 05:52:31.598: INFO: Waiting up to 5m0s for pod "client-containers-be178b49-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-containers-ffkb9" to be "success or failure"
Jan 21 05:52:31.605: INFO: Pod "client-containers-be178b49-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 6.638989ms
Jan 21 05:52:33.612: INFO: Pod "client-containers-be178b49-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013222614s
STEP: Saw pod success
Jan 21 05:52:33.612: INFO: Pod "client-containers-be178b49-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:52:33.616: INFO: Trying to get logs from node k8s05 pod client-containers-be178b49-1d40-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:52:33.650: INFO: Waiting for pod client-containers-be178b49-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:52:33.655: INFO: Pod client-containers-be178b49-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:52:33.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-ffkb9" for this suite.
Jan 21 05:52:39.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:52:39.875: INFO: namespace: e2e-tests-containers-ffkb9, resource: bindings, ignored listing per whitelist
Jan 21 05:52:39.880: INFO: namespace e2e-tests-containers-ffkb9 deletion completed in 6.215393735s

â€¢ [SLOW TEST:8.413 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:52:39.880: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1527
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 21 05:52:40.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-9kvlv'
Jan 21 05:52:40.162: INFO: stderr: ""
Jan 21 05:52:40.162: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1532
Jan 21 05:52:40.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-9kvlv'
Jan 21 05:52:48.232: INFO: stderr: ""
Jan 21 05:52:48.232: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:52:48.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-9kvlv" for this suite.
Jan 21 05:52:54.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:52:54.416: INFO: namespace: e2e-tests-kubectl-9kvlv, resource: bindings, ignored listing per whitelist
Jan 21 05:52:54.419: INFO: namespace e2e-tests-kubectl-9kvlv deletion completed in 6.165751882s

â€¢ [SLOW TEST:14.539 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:52:54.420: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:52:56.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-jjsd5" for this suite.
Jan 21 05:53:02.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:53:02.744: INFO: namespace: e2e-tests-emptydir-wrapper-jjsd5, resource: bindings, ignored listing per whitelist
Jan 21 05:53:02.843: INFO: namespace e2e-tests-emptydir-wrapper-jjsd5 deletion completed in 6.215250563s

â€¢ [SLOW TEST:8.423 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:53:02.843: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 21 05:53:02.977: INFO: Waiting up to 5m0s for pod "pod-d0cb000e-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-xlcwz" to be "success or failure"
Jan 21 05:53:02.987: INFO: Pod "pod-d0cb000e-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 10.406753ms
Jan 21 05:53:04.994: INFO: Pod "pod-d0cb000e-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017320596s
STEP: Saw pod success
Jan 21 05:53:04.994: INFO: Pod "pod-d0cb000e-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:53:05.025: INFO: Trying to get logs from node k8s05 pod pod-d0cb000e-1d40-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:53:05.165: INFO: Waiting for pod pod-d0cb000e-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:53:05.173: INFO: Pod pod-d0cb000e-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:53:05.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xlcwz" for this suite.
Jan 21 05:53:11.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:53:11.330: INFO: namespace: e2e-tests-emptydir-xlcwz, resource: bindings, ignored listing per whitelist
Jan 21 05:53:11.431: INFO: namespace e2e-tests-emptydir-xlcwz deletion completed in 6.227147611s

â€¢ [SLOW TEST:8.588 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:53:11.431: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0121 05:53:21.758844      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 21 05:53:21.759: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:53:21.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kwf6g" for this suite.
Jan 21 05:53:27.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:53:27.915: INFO: namespace: e2e-tests-gc-kwf6g, resource: bindings, ignored listing per whitelist
Jan 21 05:53:27.933: INFO: namespace e2e-tests-gc-kwf6g deletion completed in 6.169035032s

â€¢ [SLOW TEST:16.502 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:53:27.933: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 21 05:53:28.053: INFO: Waiting up to 5m0s for pod "downward-api-dfbdb78d-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-tgjgj" to be "success or failure"
Jan 21 05:53:28.063: INFO: Pod "downward-api-dfbdb78d-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.914024ms
Jan 21 05:53:30.071: INFO: Pod "downward-api-dfbdb78d-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017200877s
STEP: Saw pod success
Jan 21 05:53:30.071: INFO: Pod "downward-api-dfbdb78d-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:53:30.075: INFO: Trying to get logs from node k8s05 pod downward-api-dfbdb78d-1d40-11e9-b032-0a580af40356 container dapi-container: <nil>
STEP: delete the pod
Jan 21 05:53:30.119: INFO: Waiting for pod downward-api-dfbdb78d-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:53:30.124: INFO: Pod downward-api-dfbdb78d-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:53:30.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-tgjgj" for this suite.
Jan 21 05:53:36.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:53:36.228: INFO: namespace: e2e-tests-downward-api-tgjgj, resource: bindings, ignored listing per whitelist
Jan 21 05:53:36.326: INFO: namespace e2e-tests-downward-api-tgjgj deletion completed in 6.195495479s

â€¢ [SLOW TEST:8.393 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:53:36.327: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-e4beaca3-1d40-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 05:53:36.465: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4c14268-1d40-11e9-b032-0a580af40356" in namespace "e2e-tests-configmap-vgltp" to be "success or failure"
Jan 21 05:53:36.471: INFO: Pod "pod-configmaps-e4c14268-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 5.795202ms
Jan 21 05:53:38.479: INFO: Pod "pod-configmaps-e4c14268-1d40-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013932316s
Jan 21 05:53:40.486: INFO: Pod "pod-configmaps-e4c14268-1d40-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021053063s
STEP: Saw pod success
Jan 21 05:53:40.486: INFO: Pod "pod-configmaps-e4c14268-1d40-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:53:40.494: INFO: Trying to get logs from node k8s05 pod pod-configmaps-e4c14268-1d40-11e9-b032-0a580af40356 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 05:53:40.537: INFO: Waiting for pod pod-configmaps-e4c14268-1d40-11e9-b032-0a580af40356 to disappear
Jan 21 05:53:40.545: INFO: Pod pod-configmaps-e4c14268-1d40-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:53:40.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-vgltp" for this suite.
Jan 21 05:53:46.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:53:46.600: INFO: namespace: e2e-tests-configmap-vgltp, resource: bindings, ignored listing per whitelist
Jan 21 05:53:46.712: INFO: namespace e2e-tests-configmap-vgltp deletion completed in 6.160056728s

â€¢ [SLOW TEST:10.386 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:53:46.713: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0121 05:54:26.918484      15 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 21 05:54:26.918: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:54:26.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kt4sx" for this suite.
Jan 21 05:54:32.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:54:33.170: INFO: namespace: e2e-tests-gc-kt4sx, resource: bindings, ignored listing per whitelist
Jan 21 05:54:33.170: INFO: namespace e2e-tests-gc-kt4sx deletion completed in 6.244285401s

â€¢ [SLOW TEST:46.458 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:54:33.171: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 05:54:33.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06a3db3b-1d41-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-hnd2l" to be "success or failure"
Jan 21 05:54:33.327: INFO: Pod "downwardapi-volume-06a3db3b-1d41-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 7.613906ms
Jan 21 05:54:35.334: INFO: Pod "downwardapi-volume-06a3db3b-1d41-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015015984s
Jan 21 05:54:37.359: INFO: Pod "downwardapi-volume-06a3db3b-1d41-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040220578s
STEP: Saw pod success
Jan 21 05:54:37.360: INFO: Pod "downwardapi-volume-06a3db3b-1d41-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:54:37.366: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-06a3db3b-1d41-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 05:54:37.404: INFO: Waiting for pod downwardapi-volume-06a3db3b-1d41-11e9-b032-0a580af40356 to disappear
Jan 21 05:54:37.418: INFO: Pod downwardapi-volume-06a3db3b-1d41-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:54:37.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hnd2l" for this suite.
Jan 21 05:54:43.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:54:43.569: INFO: namespace: e2e-tests-downward-api-hnd2l, resource: bindings, ignored listing per whitelist
Jan 21 05:54:43.617: INFO: namespace e2e-tests-downward-api-hnd2l deletion completed in 6.192849203s

â€¢ [SLOW TEST:10.446 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:54:43.617: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Jan 21 05:54:43.820: INFO: Waiting up to 5m0s for pod "downward-api-0ce09d0a-1d41-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-2nkpb" to be "success or failure"
Jan 21 05:54:43.832: INFO: Pod "downward-api-0ce09d0a-1d41-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.937339ms
Jan 21 05:54:45.842: INFO: Pod "downward-api-0ce09d0a-1d41-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022039563s
STEP: Saw pod success
Jan 21 05:54:45.842: INFO: Pod "downward-api-0ce09d0a-1d41-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:54:45.848: INFO: Trying to get logs from node k8s05 pod downward-api-0ce09d0a-1d41-11e9-b032-0a580af40356 container dapi-container: <nil>
STEP: delete the pod
Jan 21 05:54:45.901: INFO: Waiting for pod downward-api-0ce09d0a-1d41-11e9-b032-0a580af40356 to disappear
Jan 21 05:54:45.910: INFO: Pod downward-api-0ce09d0a-1d41-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:54:45.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2nkpb" for this suite.
Jan 21 05:54:51.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:54:52.060: INFO: namespace: e2e-tests-downward-api-2nkpb, resource: bindings, ignored listing per whitelist
Jan 21 05:54:52.109: INFO: namespace e2e-tests-downward-api-2nkpb deletion completed in 6.188532675s

â€¢ [SLOW TEST:8.492 seconds]
[sig-node] Downward API
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:54:52.110: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-11e917aa-1d41-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:54:52.237: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-11eb3fc5-1d41-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-24jqv" to be "success or failure"
Jan 21 05:54:52.245: INFO: Pod "pod-projected-secrets-11eb3fc5-1d41-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 8.696739ms
Jan 21 05:54:54.251: INFO: Pod "pod-projected-secrets-11eb3fc5-1d41-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014752572s
STEP: Saw pod success
Jan 21 05:54:54.251: INFO: Pod "pod-projected-secrets-11eb3fc5-1d41-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:54:54.259: INFO: Trying to get logs from node k8s05 pod pod-projected-secrets-11eb3fc5-1d41-11e9-b032-0a580af40356 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:54:54.300: INFO: Waiting for pod pod-projected-secrets-11eb3fc5-1d41-11e9-b032-0a580af40356 to disappear
Jan 21 05:54:54.305: INFO: Pod pod-projected-secrets-11eb3fc5-1d41-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:54:54.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-24jqv" for this suite.
Jan 21 05:55:00.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:55:00.361: INFO: namespace: e2e-tests-projected-24jqv, resource: bindings, ignored listing per whitelist
Jan 21 05:55:00.484: INFO: namespace e2e-tests-projected-24jqv deletion completed in 6.171207682s

â€¢ [SLOW TEST:8.375 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:55:00.485: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-5fnzm
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 21 05:55:00.583: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 21 05:55:24.698: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.35:8080/dial?request=hostName&protocol=udp&host=10.244.3.34&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-5fnzm PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 05:55:24.698: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 05:55:24.898: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:55:24.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-5fnzm" for this suite.
Jan 21 05:55:46.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:55:47.004: INFO: namespace: e2e-tests-pod-network-test-5fnzm, resource: bindings, ignored listing per whitelist
Jan 21 05:55:47.122: INFO: namespace e2e-tests-pod-network-test-5fnzm deletion completed in 22.213784136s

â€¢ [SLOW TEST:46.637 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:55:47.122: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-qkzjd/secret-test-32b586ae-1d41-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:55:47.264: INFO: Waiting up to 5m0s for pod "pod-configmaps-32b78eac-1d41-11e9-b032-0a580af40356" in namespace "e2e-tests-secrets-qkzjd" to be "success or failure"
Jan 21 05:55:47.275: INFO: Pod "pod-configmaps-32b78eac-1d41-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 10.349097ms
Jan 21 05:55:49.284: INFO: Pod "pod-configmaps-32b78eac-1d41-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019978524s
STEP: Saw pod success
Jan 21 05:55:49.284: INFO: Pod "pod-configmaps-32b78eac-1d41-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:55:49.290: INFO: Trying to get logs from node k8s05 pod pod-configmaps-32b78eac-1d41-11e9-b032-0a580af40356 container env-test: <nil>
STEP: delete the pod
Jan 21 05:55:49.338: INFO: Waiting for pod pod-configmaps-32b78eac-1d41-11e9-b032-0a580af40356 to disappear
Jan 21 05:55:49.355: INFO: Pod pod-configmaps-32b78eac-1d41-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:55:49.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-qkzjd" for this suite.
Jan 21 05:55:55.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:55:55.440: INFO: namespace: e2e-tests-secrets-qkzjd, resource: bindings, ignored listing per whitelist
Jan 21 05:55:55.538: INFO: namespace e2e-tests-secrets-qkzjd deletion completed in 6.175190337s

â€¢ [SLOW TEST:8.416 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:55:55.538: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:295
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Jan 21 05:55:55.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 create -f - --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:55:56.016: INFO: stderr: ""
Jan 21 05:55:56.016: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 05:55:56.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:55:56.181: INFO: stderr: ""
Jan 21 05:55:56.181: INFO: stdout: "update-demo-nautilus-6z69w update-demo-nautilus-zxdqq "
Jan 21 05:55:56.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-6z69w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:55:56.307: INFO: stderr: ""
Jan 21 05:55:56.308: INFO: stdout: ""
Jan 21 05:55:56.308: INFO: update-demo-nautilus-6z69w is created but not running
Jan 21 05:56:01.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:01.500: INFO: stderr: ""
Jan 21 05:56:01.500: INFO: stdout: "update-demo-nautilus-6z69w update-demo-nautilus-zxdqq "
Jan 21 05:56:01.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-6z69w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:01.654: INFO: stderr: ""
Jan 21 05:56:01.654: INFO: stdout: "true"
Jan 21 05:56:01.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-6z69w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:01.779: INFO: stderr: ""
Jan 21 05:56:01.779: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 21 05:56:01.779: INFO: validating pod update-demo-nautilus-6z69w
Jan 21 05:56:01.791: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 05:56:01.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 05:56:01.791: INFO: update-demo-nautilus-6z69w is verified up and running
Jan 21 05:56:01.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-zxdqq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:01.939: INFO: stderr: ""
Jan 21 05:56:01.939: INFO: stdout: "true"
Jan 21 05:56:01.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-zxdqq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:02.106: INFO: stderr: ""
Jan 21 05:56:02.106: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 21 05:56:02.106: INFO: validating pod update-demo-nautilus-zxdqq
Jan 21 05:56:02.115: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 05:56:02.115: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 05:56:02.115: INFO: update-demo-nautilus-zxdqq is verified up and running
STEP: scaling down the replication controller
Jan 21 05:56:02.117: INFO: scanned /root for discovery docs: <nil>
Jan 21 05:56:02.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:03.383: INFO: stderr: ""
Jan 21 05:56:03.383: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 05:56:03.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:03.581: INFO: stderr: ""
Jan 21 05:56:03.581: INFO: stdout: "update-demo-nautilus-6z69w update-demo-nautilus-zxdqq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 21 05:56:08.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:08.764: INFO: stderr: ""
Jan 21 05:56:08.764: INFO: stdout: "update-demo-nautilus-zxdqq "
Jan 21 05:56:08.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-zxdqq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:08.923: INFO: stderr: ""
Jan 21 05:56:08.923: INFO: stdout: "true"
Jan 21 05:56:08.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-zxdqq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:09.048: INFO: stderr: ""
Jan 21 05:56:09.048: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 21 05:56:09.048: INFO: validating pod update-demo-nautilus-zxdqq
Jan 21 05:56:09.064: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 05:56:09.064: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 05:56:09.064: INFO: update-demo-nautilus-zxdqq is verified up and running
STEP: scaling up the replication controller
Jan 21 05:56:09.065: INFO: scanned /root for discovery docs: <nil>
Jan 21 05:56:09.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:10.279: INFO: stderr: ""
Jan 21 05:56:10.279: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 21 05:56:10.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:10.454: INFO: stderr: ""
Jan 21 05:56:10.454: INFO: stdout: "update-demo-nautilus-tqvgq update-demo-nautilus-zxdqq "
Jan 21 05:56:10.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-tqvgq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:10.566: INFO: stderr: ""
Jan 21 05:56:10.566: INFO: stdout: "true"
Jan 21 05:56:10.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-tqvgq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:10.674: INFO: stderr: ""
Jan 21 05:56:10.674: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 21 05:56:10.674: INFO: validating pod update-demo-nautilus-tqvgq
Jan 21 05:56:10.681: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 05:56:10.681: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 05:56:10.681: INFO: update-demo-nautilus-tqvgq is verified up and running
Jan 21 05:56:10.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-zxdqq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:10.815: INFO: stderr: ""
Jan 21 05:56:10.816: INFO: stdout: "true"
Jan 21 05:56:10.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods update-demo-nautilus-zxdqq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:10.950: INFO: stderr: ""
Jan 21 05:56:10.950: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 21 05:56:10.950: INFO: validating pod update-demo-nautilus-zxdqq
Jan 21 05:56:10.956: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 21 05:56:10.957: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 21 05:56:10.957: INFO: update-demo-nautilus-zxdqq is verified up and running
STEP: using delete to clean up resources
Jan 21 05:56:10.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:11.113: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 21 05:56:11.113: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 21 05:56:11.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-rg4vb'
Jan 21 05:56:11.277: INFO: stderr: "No resources found.\n"
Jan 21 05:56:11.278: INFO: stdout: ""
Jan 21 05:56:11.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 get pods -l name=update-demo --namespace=e2e-tests-kubectl-rg4vb -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 21 05:56:11.410: INFO: stderr: ""
Jan 21 05:56:11.410: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:56:11.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rg4vb" for this suite.
Jan 21 05:56:33.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:56:33.484: INFO: namespace: e2e-tests-kubectl-rg4vb, resource: bindings, ignored listing per whitelist
Jan 21 05:56:33.577: INFO: namespace e2e-tests-kubectl-rg4vb deletion completed in 22.158918515s

â€¢ [SLOW TEST:38.039 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:56:33.578: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 21 05:56:38.244: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4e64c790-1d41-11e9-b032-0a580af40356"
Jan 21 05:56:38.244: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4e64c790-1d41-11e9-b032-0a580af40356" in namespace "e2e-tests-pods-76bxx" to be "terminated due to deadline exceeded"
Jan 21 05:56:38.261: INFO: Pod "pod-update-activedeadlineseconds-4e64c790-1d41-11e9-b032-0a580af40356": Phase="Running", Reason="", readiness=true. Elapsed: 17.033105ms
Jan 21 05:56:40.272: INFO: Pod "pod-update-activedeadlineseconds-4e64c790-1d41-11e9-b032-0a580af40356": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.02729053s
Jan 21 05:56:40.272: INFO: Pod "pod-update-activedeadlineseconds-4e64c790-1d41-11e9-b032-0a580af40356" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:56:40.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-76bxx" for this suite.
Jan 21 05:56:46.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:56:46.379: INFO: namespace: e2e-tests-pods-76bxx, resource: bindings, ignored listing per whitelist
Jan 21 05:56:46.494: INFO: namespace e2e-tests-pods-76bxx deletion completed in 6.205951934s

â€¢ [SLOW TEST:12.917 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:56:46.495: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-56171e5a-1d41-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 05:56:46.619: INFO: Waiting up to 5m0s for pod "pod-secrets-5618f4b9-1d41-11e9-b032-0a580af40356" in namespace "e2e-tests-secrets-nfh9f" to be "success or failure"
Jan 21 05:56:46.628: INFO: Pod "pod-secrets-5618f4b9-1d41-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.384515ms
Jan 21 05:56:48.635: INFO: Pod "pod-secrets-5618f4b9-1d41-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016318437s
STEP: Saw pod success
Jan 21 05:56:48.635: INFO: Pod "pod-secrets-5618f4b9-1d41-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:56:48.640: INFO: Trying to get logs from node k8s05 pod pod-secrets-5618f4b9-1d41-11e9-b032-0a580af40356 container secret-volume-test: <nil>
STEP: delete the pod
Jan 21 05:56:48.691: INFO: Waiting for pod pod-secrets-5618f4b9-1d41-11e9-b032-0a580af40356 to disappear
Jan 21 05:56:48.697: INFO: Pod pod-secrets-5618f4b9-1d41-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:56:48.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nfh9f" for this suite.
Jan 21 05:56:54.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:56:54.824: INFO: namespace: e2e-tests-secrets-nfh9f, resource: bindings, ignored listing per whitelist
Jan 21 05:56:54.922: INFO: namespace e2e-tests-secrets-nfh9f deletion completed in 6.217473906s

â€¢ [SLOW TEST:8.427 seconds]
[sig-storage] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:56:54.922: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:57:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-cfs9c" for this suite.
Jan 21 05:57:07.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:57:07.366: INFO: namespace: e2e-tests-namespaces-cfs9c, resource: bindings, ignored listing per whitelist
Jan 21 05:57:07.451: INFO: namespace e2e-tests-namespaces-cfs9c deletion completed in 6.186906823s
STEP: Destroying namespace "e2e-tests-nsdeletetest-7qjx2" for this suite.
Jan 21 05:57:07.455: INFO: Namespace e2e-tests-nsdeletetest-7qjx2 was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-s9k6s" for this suite.
Jan 21 05:57:13.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:57:13.632: INFO: namespace: e2e-tests-nsdeletetest-s9k6s, resource: bindings, ignored listing per whitelist
Jan 21 05:57:13.659: INFO: namespace e2e-tests-nsdeletetest-s9k6s deletion completed in 6.203299851s

â€¢ [SLOW TEST:18.736 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:57:13.659: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Jan 21 05:57:14.317: INFO: Waiting up to 5m0s for pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-6ggt6" in namespace "e2e-tests-svcaccounts-jhsfs" to be "success or failure"
Jan 21 05:57:14.339: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-6ggt6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.939036ms
Jan 21 05:57:16.344: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-6ggt6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027206778s
Jan 21 05:57:18.350: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-6ggt6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033076245s
STEP: Saw pod success
Jan 21 05:57:18.350: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-6ggt6" satisfied condition "success or failure"
Jan 21 05:57:18.354: INFO: Trying to get logs from node k8s05 pod pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-6ggt6 container token-test: <nil>
STEP: delete the pod
Jan 21 05:57:18.397: INFO: Waiting for pod pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-6ggt6 to disappear
Jan 21 05:57:18.404: INFO: Pod pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-6ggt6 no longer exists
STEP: Creating a pod to test consume service account root CA
Jan 21 05:57:18.412: INFO: Waiting up to 5m0s for pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-pb4pz" in namespace "e2e-tests-svcaccounts-jhsfs" to be "success or failure"
Jan 21 05:57:18.438: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-pb4pz": Phase="Pending", Reason="", readiness=false. Elapsed: 26.117089ms
Jan 21 05:57:20.447: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-pb4pz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034908823s
Jan 21 05:57:22.455: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-pb4pz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042683441s
STEP: Saw pod success
Jan 21 05:57:22.455: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-pb4pz" satisfied condition "success or failure"
Jan 21 05:57:22.461: INFO: Trying to get logs from node k8s05 pod pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-pb4pz container root-ca-test: <nil>
STEP: delete the pod
Jan 21 05:57:22.513: INFO: Waiting for pod pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-pb4pz to disappear
Jan 21 05:57:22.518: INFO: Pod pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-pb4pz no longer exists
STEP: Creating a pod to test consume service account namespace
Jan 21 05:57:22.530: INFO: Waiting up to 5m0s for pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-5vmzt" in namespace "e2e-tests-svcaccounts-jhsfs" to be "success or failure"
Jan 21 05:57:22.540: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-5vmzt": Phase="Pending", Reason="", readiness=false. Elapsed: 10.110454ms
Jan 21 05:57:24.558: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-5vmzt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027864236s
Jan 21 05:57:26.564: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-5vmzt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033942293s
STEP: Saw pod success
Jan 21 05:57:26.564: INFO: Pod "pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-5vmzt" satisfied condition "success or failure"
Jan 21 05:57:26.569: INFO: Trying to get logs from node k8s05 pod pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-5vmzt container namespace-test: <nil>
STEP: delete the pod
Jan 21 05:57:26.614: INFO: Waiting for pod pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-5vmzt to disappear
Jan 21 05:57:26.619: INFO: Pod pod-service-account-6699adac-1d41-11e9-b032-0a580af40356-5vmzt no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:57:26.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-jhsfs" for this suite.
Jan 21 05:57:32.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:57:32.707: INFO: namespace: e2e-tests-svcaccounts-jhsfs, resource: bindings, ignored listing per whitelist
Jan 21 05:57:32.782: INFO: namespace e2e-tests-svcaccounts-jhsfs deletion completed in 6.156455151s

â€¢ [SLOW TEST:19.123 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:57:32.782: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 21 05:57:32.888: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:57:36.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-6lp7z" for this suite.
Jan 21 05:57:58.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:57:58.283: INFO: namespace: e2e-tests-init-container-6lp7z, resource: bindings, ignored listing per whitelist
Jan 21 05:57:58.351: INFO: namespace e2e-tests-init-container-6lp7z deletion completed in 22.184927411s

â€¢ [SLOW TEST:25.569 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:57:58.352: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 21 05:57:58.471: INFO: Waiting up to 5m0s for pod "pod-80eb8548-1d41-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-nlksx" to be "success or failure"
Jan 21 05:57:58.480: INFO: Pod "pod-80eb8548-1d41-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.401555ms
Jan 21 05:58:00.486: INFO: Pod "pod-80eb8548-1d41-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015222798s
STEP: Saw pod success
Jan 21 05:58:00.486: INFO: Pod "pod-80eb8548-1d41-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:58:00.490: INFO: Trying to get logs from node k8s05 pod pod-80eb8548-1d41-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 05:58:00.528: INFO: Waiting for pod pod-80eb8548-1d41-11e9-b032-0a580af40356 to disappear
Jan 21 05:58:00.533: INFO: Pod pod-80eb8548-1d41-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:58:00.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-nlksx" for this suite.
Jan 21 05:58:06.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:58:06.697: INFO: namespace: e2e-tests-emptydir-nlksx, resource: bindings, ignored listing per whitelist
Jan 21 05:58:06.775: INFO: namespace e2e-tests-emptydir-nlksx deletion completed in 6.234572588s

â€¢ [SLOW TEST:8.424 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:58:06.775: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Jan 21 05:58:06.931: INFO: Waiting up to 5m0s for pod "var-expansion-85f5aea9-1d41-11e9-b032-0a580af40356" in namespace "e2e-tests-var-expansion-kj8sd" to be "success or failure"
Jan 21 05:58:06.944: INFO: Pod "var-expansion-85f5aea9-1d41-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 12.299832ms
Jan 21 05:58:08.958: INFO: Pod "var-expansion-85f5aea9-1d41-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026844757s
STEP: Saw pod success
Jan 21 05:58:08.958: INFO: Pod "var-expansion-85f5aea9-1d41-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 05:58:08.964: INFO: Trying to get logs from node k8s05 pod var-expansion-85f5aea9-1d41-11e9-b032-0a580af40356 container dapi-container: <nil>
STEP: delete the pod
Jan 21 05:58:09.024: INFO: Waiting for pod var-expansion-85f5aea9-1d41-11e9-b032-0a580af40356 to disappear
Jan 21 05:58:09.031: INFO: Pod var-expansion-85f5aea9-1d41-11e9-b032-0a580af40356 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:58:09.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-kj8sd" for this suite.
Jan 21 05:58:15.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:58:15.157: INFO: namespace: e2e-tests-var-expansion-kj8sd, resource: bindings, ignored listing per whitelist
Jan 21 05:58:15.236: INFO: namespace e2e-tests-var-expansion-kj8sd deletion completed in 6.197491998s

â€¢ [SLOW TEST:8.461 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:58:15.236: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Jan 21 05:58:17.372: INFO: Pod pod-hostip-8afb2ff0-1d41-11e9-b032-0a580af40356 has hostIP: 192.168.1.21
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 05:58:17.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-zh9qp" for this suite.
Jan 21 05:58:39.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 05:58:39.490: INFO: namespace: e2e-tests-pods-zh9qp, resource: bindings, ignored listing per whitelist
Jan 21 05:58:39.573: INFO: namespace e2e-tests-pods-zh9qp deletion completed in 22.191652119s

â€¢ [SLOW TEST:24.336 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 05:58:39.573: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-jvdck
Jan 21 05:58:41.735: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-jvdck
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 05:58:41.739: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:02:42.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-jvdck" for this suite.
Jan 21 06:02:48.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:02:49.064: INFO: namespace: e2e-tests-container-probe-jvdck, resource: bindings, ignored listing per whitelist
Jan 21 06:02:49.141: INFO: namespace e2e-tests-container-probe-jvdck deletion completed in 6.203959118s

â€¢ [SLOW TEST:249.567 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:02:49.141: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 21 06:02:51.838: INFO: Successfully updated pod "annotationupdate2e40f59c-1d42-11e9-b032-0a580af40356"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:02:53.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cb28t" for this suite.
Jan 21 06:03:15.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:03:15.970: INFO: namespace: e2e-tests-projected-cb28t, resource: bindings, ignored listing per whitelist
Jan 21 06:03:16.109: INFO: namespace e2e-tests-projected-cb28t deletion completed in 22.205893041s

â€¢ [SLOW TEST:26.968 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:03:16.109: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Jan 21 06:03:16.252: INFO: Waiting up to 5m0s for pod "client-containers-3e5597f7-1d42-11e9-b032-0a580af40356" in namespace "e2e-tests-containers-wnc9h" to be "success or failure"
Jan 21 06:03:16.269: INFO: Pod "client-containers-3e5597f7-1d42-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 16.772193ms
Jan 21 06:03:18.275: INFO: Pod "client-containers-3e5597f7-1d42-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022925536s
STEP: Saw pod success
Jan 21 06:03:18.275: INFO: Pod "client-containers-3e5597f7-1d42-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:03:18.280: INFO: Trying to get logs from node k8s05 pod client-containers-3e5597f7-1d42-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 06:03:18.321: INFO: Waiting for pod client-containers-3e5597f7-1d42-11e9-b032-0a580af40356 to disappear
Jan 21 06:03:18.331: INFO: Pod client-containers-3e5597f7-1d42-11e9-b032-0a580af40356 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:03:18.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-wnc9h" for this suite.
Jan 21 06:03:24.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:03:24.441: INFO: namespace: e2e-tests-containers-wnc9h, resource: bindings, ignored listing per whitelist
Jan 21 06:03:24.478: INFO: namespace e2e-tests-containers-wnc9h deletion completed in 6.138690819s

â€¢ [SLOW TEST:8.369 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:03:24.478: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 21 06:03:31.655: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:03:32.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-6rx8s" for this suite.
Jan 21 06:03:54.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:03:54.748: INFO: namespace: e2e-tests-replicaset-6rx8s, resource: bindings, ignored listing per whitelist
Jan 21 06:03:54.862: INFO: namespace e2e-tests-replicaset-6rx8s deletion completed in 22.172827329s

â€¢ [SLOW TEST:30.384 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:03:54.863: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-556c2ef7-1d42-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 06:03:55.010: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-556d919f-1d42-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-qw5tz" to be "success or failure"
Jan 21 06:03:55.020: INFO: Pod "pod-projected-secrets-556d919f-1d42-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.758057ms
Jan 21 06:03:57.026: INFO: Pod "pod-projected-secrets-556d919f-1d42-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015279626s
STEP: Saw pod success
Jan 21 06:03:57.026: INFO: Pod "pod-projected-secrets-556d919f-1d42-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:03:57.029: INFO: Trying to get logs from node k8s05 pod pod-projected-secrets-556d919f-1d42-11e9-b032-0a580af40356 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 06:03:57.070: INFO: Waiting for pod pod-projected-secrets-556d919f-1d42-11e9-b032-0a580af40356 to disappear
Jan 21 06:03:57.082: INFO: Pod pod-projected-secrets-556d919f-1d42-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:03:57.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-qw5tz" for this suite.
Jan 21 06:04:03.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:04:03.238: INFO: namespace: e2e-tests-projected-qw5tz, resource: bindings, ignored listing per whitelist
Jan 21 06:04:03.292: INFO: namespace e2e-tests-projected-qw5tz deletion completed in 6.201881569s

â€¢ [SLOW TEST:8.430 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:04:03.293: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-8pl65
Jan 21 06:04:05.431: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-8pl65
STEP: checking the pod's current state and verifying that restartCount is present
Jan 21 06:04:05.436: INFO: Initial restart count of pod liveness-http is 0
Jan 21 06:04:29.546: INFO: Restart count of pod e2e-tests-container-probe-8pl65/liveness-http is now 1 (24.110797587s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:04:29.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-8pl65" for this suite.
Jan 21 06:04:35.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:04:35.729: INFO: namespace: e2e-tests-container-probe-8pl65, resource: bindings, ignored listing per whitelist
Jan 21 06:04:35.762: INFO: namespace e2e-tests-container-probe-8pl65 deletion completed in 6.178478039s

â€¢ [SLOW TEST:32.469 seconds]
[k8s.io] Probing container
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:04:35.762: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-6dccf310-1d42-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 06:04:35.896: INFO: Waiting up to 5m0s for pod "pod-secrets-6dcec3c9-1d42-11e9-b032-0a580af40356" in namespace "e2e-tests-secrets-772rr" to be "success or failure"
Jan 21 06:04:35.903: INFO: Pod "pod-secrets-6dcec3c9-1d42-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 7.143384ms
Jan 21 06:04:37.909: INFO: Pod "pod-secrets-6dcec3c9-1d42-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012924015s
STEP: Saw pod success
Jan 21 06:04:37.909: INFO: Pod "pod-secrets-6dcec3c9-1d42-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:04:37.913: INFO: Trying to get logs from node k8s05 pod pod-secrets-6dcec3c9-1d42-11e9-b032-0a580af40356 container secret-env-test: <nil>
STEP: delete the pod
Jan 21 06:04:37.954: INFO: Waiting for pod pod-secrets-6dcec3c9-1d42-11e9-b032-0a580af40356 to disappear
Jan 21 06:04:37.959: INFO: Pod pod-secrets-6dcec3c9-1d42-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:04:37.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-772rr" for this suite.
Jan 21 06:04:43.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:04:44.022: INFO: namespace: e2e-tests-secrets-772rr, resource: bindings, ignored listing per whitelist
Jan 21 06:04:44.146: INFO: namespace e2e-tests-secrets-772rr deletion completed in 6.178915141s

â€¢ [SLOW TEST:8.384 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:04:44.146: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-72cd4ef0-1d42-11e9-b032-0a580af40356
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:04:48.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-gmglb" for this suite.
Jan 21 06:05:10.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:05:10.423: INFO: namespace: e2e-tests-configmap-gmglb, resource: bindings, ignored listing per whitelist
Jan 21 06:05:10.516: INFO: namespace e2e-tests-configmap-gmglb deletion completed in 22.173705674s

â€¢ [SLOW TEST:26.370 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:05:10.516: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-9dx69
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-9dx69
STEP: Deleting pre-stop pod
Jan 21 06:05:19.696: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:05:19.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-9dx69" for this suite.
Jan 21 06:05:57.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:05:57.905: INFO: namespace: e2e-tests-prestop-9dx69, resource: bindings, ignored listing per whitelist
Jan 21 06:05:57.953: INFO: namespace e2e-tests-prestop-9dx69 deletion completed in 38.228494048s

â€¢ [SLOW TEST:47.437 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:05:57.954: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-9ecb016b-1d42-11e9-b032-0a580af40356
STEP: Creating a pod to test consume configMaps
Jan 21 06:05:58.090: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9eccd91b-1d42-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-9jftf" to be "success or failure"
Jan 21 06:05:58.098: INFO: Pod "pod-projected-configmaps-9eccd91b-1d42-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 8.384997ms
Jan 21 06:06:00.106: INFO: Pod "pod-projected-configmaps-9eccd91b-1d42-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015691131s
STEP: Saw pod success
Jan 21 06:06:00.106: INFO: Pod "pod-projected-configmaps-9eccd91b-1d42-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:06:00.120: INFO: Trying to get logs from node k8s05 pod pod-projected-configmaps-9eccd91b-1d42-11e9-b032-0a580af40356 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 21 06:06:00.162: INFO: Waiting for pod pod-projected-configmaps-9eccd91b-1d42-11e9-b032-0a580af40356 to disappear
Jan 21 06:06:00.170: INFO: Pod pod-projected-configmaps-9eccd91b-1d42-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:06:00.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9jftf" for this suite.
Jan 21 06:06:06.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:06:06.314: INFO: namespace: e2e-tests-projected-9jftf, resource: bindings, ignored listing per whitelist
Jan 21 06:06:06.383: INFO: namespace e2e-tests-projected-9jftf deletion completed in 6.205224712s

â€¢ [SLOW TEST:8.430 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:06:06.384: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 21 06:06:10.616: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:10.623: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:12.624: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:12.643: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:14.623: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:14.629: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:16.623: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:16.631: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:18.624: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:18.662: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:20.623: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:20.630: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:22.624: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:22.655: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:24.624: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:24.649: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:26.624: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:26.630: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:28.624: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:28.631: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:30.623: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:30.639: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:32.623: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:32.629: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:34.624: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:34.636: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:36.624: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:36.628: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 21 06:06:38.623: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 21 06:06:38.630: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:06:38.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-vv8p9" for this suite.
Jan 21 06:07:00.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:07:00.712: INFO: namespace: e2e-tests-container-lifecycle-hook-vv8p9, resource: bindings, ignored listing per whitelist
Jan 21 06:07:00.798: INFO: namespace e2e-tests-container-lifecycle-hook-vv8p9 deletion completed in 22.159835445s

â€¢ [SLOW TEST:54.414 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:07:00.798: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 21 06:07:00.938: INFO: Waiting up to 5m0s for pod "pod-c441eee9-1d42-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-dldvr" to be "success or failure"
Jan 21 06:07:00.949: INFO: Pod "pod-c441eee9-1d42-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.03739ms
Jan 21 06:07:02.977: INFO: Pod "pod-c441eee9-1d42-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039137797s
STEP: Saw pod success
Jan 21 06:07:02.977: INFO: Pod "pod-c441eee9-1d42-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:07:02.982: INFO: Trying to get logs from node k8s05 pod pod-c441eee9-1d42-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 06:07:03.052: INFO: Waiting for pod pod-c441eee9-1d42-11e9-b032-0a580af40356 to disappear
Jan 21 06:07:03.058: INFO: Pod pod-c441eee9-1d42-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:07:03.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dldvr" for this suite.
Jan 21 06:07:09.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:07:09.149: INFO: namespace: e2e-tests-emptydir-dldvr, resource: bindings, ignored listing per whitelist
Jan 21 06:07:09.245: INFO: namespace e2e-tests-emptydir-dldvr deletion completed in 6.179487587s

â€¢ [SLOW TEST:8.447 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:07:09.245: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-tj7d
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 06:07:09.368: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tj7d" in namespace "e2e-tests-subpath-jzdsl" to be "success or failure"
Jan 21 06:07:09.374: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.315637ms
Jan 21 06:07:11.379: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010988632s
Jan 21 06:07:13.410: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 4.041993936s
Jan 21 06:07:15.441: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 6.072397607s
Jan 21 06:07:17.453: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 8.084273074s
Jan 21 06:07:19.460: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 10.091271327s
Jan 21 06:07:21.468: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 12.100070511s
Jan 21 06:07:23.512: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 14.143299129s
Jan 21 06:07:25.520: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 16.151923768s
Jan 21 06:07:27.525: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 18.156599805s
Jan 21 06:07:29.530: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 20.162054563s
Jan 21 06:07:31.545: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Running", Reason="", readiness=false. Elapsed: 22.176874655s
Jan 21 06:07:33.559: INFO: Pod "pod-subpath-test-configmap-tj7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.190669338s
STEP: Saw pod success
Jan 21 06:07:33.559: INFO: Pod "pod-subpath-test-configmap-tj7d" satisfied condition "success or failure"
Jan 21 06:07:33.565: INFO: Trying to get logs from node k8s05 pod pod-subpath-test-configmap-tj7d container test-container-subpath-configmap-tj7d: <nil>
STEP: delete the pod
Jan 21 06:07:33.603: INFO: Waiting for pod pod-subpath-test-configmap-tj7d to disappear
Jan 21 06:07:33.608: INFO: Pod pod-subpath-test-configmap-tj7d no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tj7d
Jan 21 06:07:33.608: INFO: Deleting pod "pod-subpath-test-configmap-tj7d" in namespace "e2e-tests-subpath-jzdsl"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:07:33.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-jzdsl" for this suite.
Jan 21 06:07:39.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:07:39.742: INFO: namespace: e2e-tests-subpath-jzdsl, resource: bindings, ignored listing per whitelist
Jan 21 06:07:39.805: INFO: namespace e2e-tests-subpath-jzdsl deletion completed in 6.18774457s

â€¢ [SLOW TEST:30.560 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:07:39.806: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-rxnx6
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Jan 21 06:07:39.953: INFO: Found 0 stateful pods, waiting for 3
Jan 21 06:07:49.969: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 06:07:49.969: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 06:07:49.969: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 06:07:49.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-rxnx6 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 06:07:50.351: INFO: stderr: ""
Jan 21 06:07:50.351: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 06:07:50.351: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 21 06:08:00.412: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 21 06:08:10.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-rxnx6 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 06:08:10.813: INFO: stderr: ""
Jan 21 06:08:10.813: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 21 06:08:10.813: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 21 06:08:20.900: INFO: Waiting for StatefulSet e2e-tests-statefulset-rxnx6/ss2 to complete update
Jan 21 06:08:20.900: INFO: Waiting for Pod e2e-tests-statefulset-rxnx6/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 21 06:08:20.900: INFO: Waiting for Pod e2e-tests-statefulset-rxnx6/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 21 06:08:30.925: INFO: Waiting for StatefulSet e2e-tests-statefulset-rxnx6/ss2 to complete update
Jan 21 06:08:30.926: INFO: Waiting for Pod e2e-tests-statefulset-rxnx6/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Jan 21 06:08:40.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-rxnx6 ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 06:08:41.286: INFO: stderr: ""
Jan 21 06:08:41.286: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 06:08:41.286: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 21 06:08:51.372: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 21 06:09:01.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-rxnx6 ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 06:09:01.718: INFO: stderr: ""
Jan 21 06:09:01.718: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 21 06:09:01.718: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 21 06:09:21.764: INFO: Deleting all statefulset in ns e2e-tests-statefulset-rxnx6
Jan 21 06:09:21.776: INFO: Scaling statefulset ss2 to 0
Jan 21 06:09:41.817: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 06:09:41.823: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:09:41.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-rxnx6" for this suite.
Jan 21 06:09:47.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:09:47.944: INFO: namespace: e2e-tests-statefulset-rxnx6, resource: bindings, ignored listing per whitelist
Jan 21 06:09:48.102: INFO: namespace e2e-tests-statefulset-rxnx6 deletion completed in 6.233165709s

â€¢ [SLOW TEST:128.297 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:09:48.103: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 21 06:09:48.241: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:48.242: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:48.242: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:48.247: INFO: Number of nodes with available pods: 0
Jan 21 06:09:48.247: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:09:49.257: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:49.257: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:49.257: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:49.263: INFO: Number of nodes with available pods: 0
Jan 21 06:09:49.263: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:09:50.255: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:50.255: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:50.255: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:50.265: INFO: Number of nodes with available pods: 1
Jan 21 06:09:50.265: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 21 06:09:50.304: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:50.304: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:50.304: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:50.316: INFO: Number of nodes with available pods: 0
Jan 21 06:09:50.316: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:09:51.327: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:51.328: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:51.328: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:51.336: INFO: Number of nodes with available pods: 0
Jan 21 06:09:51.336: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:09:52.350: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:52.350: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:52.350: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:09:52.358: INFO: Number of nodes with available pods: 1
Jan 21 06:09:52.358: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-rf9ck, will wait for the garbage collector to delete the pods
Jan 21 06:09:52.444: INFO: Deleting DaemonSet.extensions daemon-set took: 17.39922ms
Jan 21 06:09:52.545: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.284187ms
Jan 21 06:10:26.759: INFO: Number of nodes with available pods: 0
Jan 21 06:10:26.759: INFO: Number of running nodes: 0, number of available pods: 0
Jan 21 06:10:26.764: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-rf9ck/daemonsets","resourceVersion":"44491"},"items":null}

Jan 21 06:10:26.769: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-rf9ck/pods","resourceVersion":"44491"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:10:26.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-rf9ck" for this suite.
Jan 21 06:10:32.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:10:32.922: INFO: namespace: e2e-tests-daemonsets-rf9ck, resource: bindings, ignored listing per whitelist
Jan 21 06:10:32.985: INFO: namespace e2e-tests-daemonsets-rf9ck deletion completed in 6.198146886s

â€¢ [SLOW TEST:44.883 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:10:32.985: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 06:10:33.092: INFO: Creating deployment "test-recreate-deployment"
Jan 21 06:10:33.104: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 21 06:10:33.125: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 21 06:10:35.136: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 21 06:10:35.142: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 21 06:10:35.155: INFO: Updating deployment test-recreate-deployment
Jan 21 06:10:35.155: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 21 06:10:35.318: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-vkc4d,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vkc4d/deployments/test-recreate-deployment,UID:42b83968-1d43-11e9-9faa-fa163e6f5c57,ResourceVersion:44568,Generation:2,CreationTimestamp:2019-01-21 06:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-01-21 06:10:35 +0000 UTC 2019-01-21 06:10:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-01-21 06:10:35 +0000 UTC 2019-01-21 06:10:33 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-697fbf54bf" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jan 21 06:10:35.323: INFO: New ReplicaSet "test-recreate-deployment-697fbf54bf" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf,GenerateName:,Namespace:e2e-tests-deployment-vkc4d,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vkc4d/replicasets/test-recreate-deployment-697fbf54bf,UID:43fa5ece-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:44564,Generation:1,CreationTimestamp:2019-01-21 06:10:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 42b83968-1d43-11e9-9faa-fa163e6f5c57 0xc000837997 0xc000837998}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 21 06:10:35.323: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 21 06:10:35.324: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5dfdcc846d,GenerateName:,Namespace:e2e-tests-deployment-vkc4d,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-vkc4d/replicasets/test-recreate-deployment-5dfdcc846d,UID:42ba9a2d-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:44556,Generation:2,CreationTimestamp:2019-01-21 06:10:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 42b83968-1d43-11e9-9faa-fa163e6f5c57 0xc000837717 0xc000837718}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5dfdcc846d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 21 06:10:35.331: INFO: Pod "test-recreate-deployment-697fbf54bf-vxf4p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-697fbf54bf-vxf4p,GenerateName:test-recreate-deployment-697fbf54bf-,Namespace:e2e-tests-deployment-vkc4d,SelfLink:/api/v1/namespaces/e2e-tests-deployment-vkc4d/pods/test-recreate-deployment-697fbf54bf-vxf4p,UID:43fc9efa-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:44567,Generation:0,CreationTimestamp:2019-01-21 06:10:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 697fbf54bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-697fbf54bf 43fa5ece-1d43-11e9-b27d-fa163eb7f963 0xc001297a67 0xc001297a68}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-zvfp8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zvfp8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zvfp8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001297dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001297e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:10:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:10:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:10:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:10:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:,StartTime:2019-01-21 06:10:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:10:35.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-vkc4d" for this suite.
Jan 21 06:10:41.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:10:41.472: INFO: namespace: e2e-tests-deployment-vkc4d, resource: bindings, ignored listing per whitelist
Jan 21 06:10:41.532: INFO: namespace e2e-tests-deployment-vkc4d deletion completed in 6.192796654s

â€¢ [SLOW TEST:8.547 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:10:41.532: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Jan 21 06:10:41.644: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-453531067 proxy --unix-socket=/tmp/kubectl-proxy-unix130986974/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:10:41.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4dfzv" for this suite.
Jan 21 06:10:47.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:10:47.817: INFO: namespace: e2e-tests-kubectl-4dfzv, resource: bindings, ignored listing per whitelist
Jan 21 06:10:47.953: INFO: namespace e2e-tests-kubectl-4dfzv deletion completed in 6.191714669s

â€¢ [SLOW TEST:6.421 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:10:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 21 06:10:48.053: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 21 06:10:48.065: INFO: Waiting for terminating namespaces to be deleted...
Jan 21 06:10:48.070: INFO: 
Logging pods the kubelet thinks is on node k8s05 before test
Jan 21 06:10:48.089: INFO: kube-proxy-q2nqf from kube-system started at 2019-01-21 02:11:36 +0000 UTC (1 container statuses recorded)
Jan 21 06:10:48.089: INFO: 	Container kube-proxy ready: true, restart count 1
Jan 21 06:10:48.089: INFO: sonobuoy from heptio-sonobuoy started at 2019-01-21 04:55:03 +0000 UTC (1 container statuses recorded)
Jan 21 06:10:48.089: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 21 06:10:48.089: INFO: kube-flannel-ds-9gnm8 from kube-system started at 2019-01-21 02:11:36 +0000 UTC (1 container statuses recorded)
Jan 21 06:10:48.089: INFO: 	Container kube-flannel ready: true, restart count 1
Jan 21 06:10:48.089: INFO: sonobuoy-e2e-job-4ff7b51a2b72420b from heptio-sonobuoy started at 2019-01-21 04:55:06 +0000 UTC (2 container statuses recorded)
Jan 21 06:10:48.089: INFO: 	Container e2e ready: true, restart count 0
Jan 21 06:10:48.089: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 21 06:10:48.089: INFO: sonobuoy-systemd-logs-daemon-set-713071a3b78f48ee-9xf85 from heptio-sonobuoy started at 2019-01-21 04:55:06 +0000 UTC (2 container statuses recorded)
Jan 21 06:10:48.089: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Jan 21 06:10:48.089: INFO: 	Container sonobuoy-worker ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node k8s05
Jan 21 06:10:48.147: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s05
Jan 21 06:10:48.147: INFO: Pod sonobuoy-e2e-job-4ff7b51a2b72420b requesting resource cpu=0m on Node k8s05
Jan 21 06:10:48.147: INFO: Pod sonobuoy-systemd-logs-daemon-set-713071a3b78f48ee-9xf85 requesting resource cpu=0m on Node k8s05
Jan 21 06:10:48.147: INFO: Pod kube-flannel-ds-9gnm8 requesting resource cpu=100m on Node k8s05
Jan 21 06:10:48.147: INFO: Pod kube-proxy-q2nqf requesting resource cpu=0m on Node k8s05
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4bb1daa0-1d43-11e9-b032-0a580af40356.157bc839f3cf8747], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-t4gdt/filler-pod-4bb1daa0-1d43-11e9-b032-0a580af40356 to k8s05]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4bb1daa0-1d43-11e9-b032-0a580af40356.157bc83a21a06d80], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4bb1daa0-1d43-11e9-b032-0a580af40356.157bc83a2462b9c4], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4bb1daa0-1d43-11e9-b032-0a580af40356.157bc83a2f4b4f7c], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.157bc83a6c690e39], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node k8s05
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:10:51.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-t4gdt" for this suite.
Jan 21 06:10:57.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:10:57.382: INFO: namespace: e2e-tests-sched-pred-t4gdt, resource: bindings, ignored listing per whitelist
Jan 21 06:10:57.459: INFO: namespace e2e-tests-sched-pred-t4gdt deletion completed in 6.216289298s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:9.506 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:10:57.459: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 21 06:10:57.599: INFO: Waiting up to 5m0s for pod "pod-5150f88c-1d43-11e9-b032-0a580af40356" in namespace "e2e-tests-emptydir-cfgcg" to be "success or failure"
Jan 21 06:10:57.610: INFO: Pod "pod-5150f88c-1d43-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.846372ms
Jan 21 06:10:59.625: INFO: Pod "pod-5150f88c-1d43-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026751666s
STEP: Saw pod success
Jan 21 06:10:59.625: INFO: Pod "pod-5150f88c-1d43-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:10:59.630: INFO: Trying to get logs from node k8s05 pod pod-5150f88c-1d43-11e9-b032-0a580af40356 container test-container: <nil>
STEP: delete the pod
Jan 21 06:10:59.671: INFO: Waiting for pod pod-5150f88c-1d43-11e9-b032-0a580af40356 to disappear
Jan 21 06:10:59.675: INFO: Pod pod-5150f88c-1d43-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:10:59.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-cfgcg" for this suite.
Jan 21 06:11:05.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:11:05.805: INFO: namespace: e2e-tests-emptydir-cfgcg, resource: bindings, ignored listing per whitelist
Jan 21 06:11:05.873: INFO: namespace e2e-tests-emptydir-cfgcg deletion completed in 6.189953471s

â€¢ [SLOW TEST:8.413 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:11:05.873: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 06:11:06.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56551d81-1d43-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-g9xwk" to be "success or failure"
Jan 21 06:11:06.021: INFO: Pod "downwardapi-volume-56551d81-1d43-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 9.272516ms
Jan 21 06:11:08.029: INFO: Pod "downwardapi-volume-56551d81-1d43-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017659568s
STEP: Saw pod success
Jan 21 06:11:08.029: INFO: Pod "downwardapi-volume-56551d81-1d43-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:11:08.036: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-56551d81-1d43-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 06:11:08.087: INFO: Waiting for pod downwardapi-volume-56551d81-1d43-11e9-b032-0a580af40356 to disappear
Jan 21 06:11:08.094: INFO: Pod downwardapi-volume-56551d81-1d43-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:11:08.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-g9xwk" for this suite.
Jan 21 06:11:14.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:11:14.193: INFO: namespace: e2e-tests-projected-g9xwk, resource: bindings, ignored listing per whitelist
Jan 21 06:11:14.303: INFO: namespace e2e-tests-projected-g9xwk deletion completed in 6.201625619s

â€¢ [SLOW TEST:8.431 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:11:14.304: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1399
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 21 06:11:14.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-692km'
Jan 21 06:11:14.851: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 21 06:11:14.851: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1404
Jan 21 06:11:18.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-692km'
Jan 21 06:11:19.045: INFO: stderr: ""
Jan 21 06:11:19.045: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:11:19.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-692km" for this suite.
Jan 21 06:11:41.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:11:41.178: INFO: namespace: e2e-tests-kubectl-692km, resource: bindings, ignored listing per whitelist
Jan 21 06:11:41.265: INFO: namespace e2e-tests-kubectl-692km deletion completed in 22.209673691s

â€¢ [SLOW TEST:26.962 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:11:41.266: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 06:11:41.470: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b75d545-1d43-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-829j5" to be "success or failure"
Jan 21 06:11:41.481: INFO: Pod "downwardapi-volume-6b75d545-1d43-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 11.312086ms
Jan 21 06:11:43.503: INFO: Pod "downwardapi-volume-6b75d545-1d43-11e9-b032-0a580af40356": Phase="Running", Reason="", readiness=true. Elapsed: 2.033207594s
Jan 21 06:11:45.512: INFO: Pod "downwardapi-volume-6b75d545-1d43-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042416872s
STEP: Saw pod success
Jan 21 06:11:45.512: INFO: Pod "downwardapi-volume-6b75d545-1d43-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:11:45.516: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-6b75d545-1d43-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 06:11:45.550: INFO: Waiting for pod downwardapi-volume-6b75d545-1d43-11e9-b032-0a580af40356 to disappear
Jan 21 06:11:45.554: INFO: Pod downwardapi-volume-6b75d545-1d43-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:11:45.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-829j5" for this suite.
Jan 21 06:11:51.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:11:51.672: INFO: namespace: e2e-tests-downward-api-829j5, resource: bindings, ignored listing per whitelist
Jan 21 06:11:51.759: INFO: namespace e2e-tests-downward-api-829j5 deletion completed in 6.199469089s

â€¢ [SLOW TEST:10.493 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:11:51.760: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-pqbn
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 06:11:51.927: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pqbn" in namespace "e2e-tests-subpath-bxbng" to be "success or failure"
Jan 21 06:11:51.953: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Pending", Reason="", readiness=false. Elapsed: 26.058931ms
Jan 21 06:11:53.967: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040281197s
Jan 21 06:11:55.974: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 4.046913188s
Jan 21 06:11:57.987: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 6.059505475s
Jan 21 06:12:00.004: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 8.076904452s
Jan 21 06:12:02.014: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 10.086879793s
Jan 21 06:12:04.028: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 12.100382441s
Jan 21 06:12:06.036: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 14.10893098s
Jan 21 06:12:08.042: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 16.115116993s
Jan 21 06:12:10.052: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 18.124995766s
Jan 21 06:12:12.058: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 20.13126378s
Jan 21 06:12:14.073: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Running", Reason="", readiness=false. Elapsed: 22.145978556s
Jan 21 06:12:16.081: INFO: Pod "pod-subpath-test-configmap-pqbn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.153618219s
STEP: Saw pod success
Jan 21 06:12:16.081: INFO: Pod "pod-subpath-test-configmap-pqbn" satisfied condition "success or failure"
Jan 21 06:12:16.086: INFO: Trying to get logs from node k8s05 pod pod-subpath-test-configmap-pqbn container test-container-subpath-configmap-pqbn: <nil>
STEP: delete the pod
Jan 21 06:12:16.123: INFO: Waiting for pod pod-subpath-test-configmap-pqbn to disappear
Jan 21 06:12:16.126: INFO: Pod pod-subpath-test-configmap-pqbn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pqbn
Jan 21 06:12:16.127: INFO: Deleting pod "pod-subpath-test-configmap-pqbn" in namespace "e2e-tests-subpath-bxbng"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:12:16.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-bxbng" for this suite.
Jan 21 06:12:22.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:12:22.276: INFO: namespace: e2e-tests-subpath-bxbng, resource: bindings, ignored listing per whitelist
Jan 21 06:12:22.366: INFO: namespace e2e-tests-subpath-bxbng deletion completed in 6.222804384s

â€¢ [SLOW TEST:30.607 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:12:22.366: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Jan 21 06:12:22.464: INFO: Creating deployment "nginx-deployment"
Jan 21 06:12:22.479: INFO: Waiting for observed generation 1
Jan 21 06:12:24.498: INFO: Waiting for all required pods to come up
Jan 21 06:12:24.508: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 21 06:12:30.532: INFO: Waiting for deployment "nginx-deployment" to complete
Jan 21 06:12:30.545: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jan 21 06:12:30.561: INFO: Updating deployment nginx-deployment
Jan 21 06:12:30.561: INFO: Waiting for observed generation 2
Jan 21 06:12:32.580: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 21 06:12:32.586: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 21 06:12:32.591: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 21 06:12:32.608: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 21 06:12:32.608: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 21 06:12:32.613: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 21 06:12:32.622: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jan 21 06:12:32.622: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jan 21 06:12:32.637: INFO: Updating deployment nginx-deployment
Jan 21 06:12:32.637: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jan 21 06:12:32.651: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 21 06:12:32.662: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 21 06:12:32.700: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4hvkr/deployments/nginx-deployment,UID:83e92940-1d43-11e9-9faa-fa163e6f5c57,ResourceVersion:45184,Generation:3,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-01-21 06:12:30 +0000 UTC 2019-01-21 06:12:22 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-65bbdb5f8" is progressing.} {Available False 2019-01-21 06:12:32 +0000 UTC 2019-01-21 06:12:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jan 21 06:12:32.717: INFO: New ReplicaSet "nginx-deployment-65bbdb5f8" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8,GenerateName:,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4hvkr/replicasets/nginx-deployment-65bbdb5f8,UID:88bb9b54-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45179,Generation:3,CreationTimestamp:2019-01-21 06:12:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 83e92940-1d43-11e9-9faa-fa163e6f5c57 0xc0012e2c77 0xc0012e2c78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 21 06:12:32.717: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jan 21 06:12:32.718: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965,GenerateName:,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-4hvkr/replicasets/nginx-deployment-555b55d965,UID:83eb3d4a-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45178,Generation:3,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 83e92940-1d43-11e9-9faa-fa163e6f5c57 0xc0012e2937 0xc0012e2938}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jan 21 06:12:32.741: INFO: Pod "nginx-deployment-555b55d965-2rh9t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-2rh9t,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-2rh9t,UID:8a01d4df-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45196,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0012e95d7 0xc0012e95d8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012e9640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012e9660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.741: INFO: Pod "nginx-deployment-555b55d965-5kz2x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5kz2x,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-5kz2x,UID:83f6f21a-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45100,Generation:0,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0012e96b7 0xc0012e96b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012e9760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012e9780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.96,StartTime:2019-01-21 06:12:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-21 06:12:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://8d791da14fd5751171f849b5e63512de3dc960b1831fb8af52378422c6bddbf6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.742: INFO: Pod "nginx-deployment-555b55d965-5qqxp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5qqxp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-5qqxp,UID:89ff2bf0-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45191,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0012e9840 0xc0012e9841}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012e98b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012e98d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:32 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.742: INFO: Pod "nginx-deployment-555b55d965-5rg7r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-5rg7r,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-5rg7r,UID:83ee9718-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45065,Generation:0,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0012e9940 0xc0012e9941}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012e99b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012e99d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.88,StartTime:2019-01-21 06:12:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-21 06:12:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://5075437c009a95f4cf7c655407df9f76e19b959054ac356a51ec428bf5fd9e50}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.742: INFO: Pod "nginx-deployment-555b55d965-6wtsp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-6wtsp,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-6wtsp,UID:83f08194-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45095,Generation:0,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0012e9a90 0xc0012e9a91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012e9b00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012e9b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.91,StartTime:2019-01-21 06:12:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-21 06:12:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://6e166bc5b8e6d810a7ecd509f5de57758539c5f9ec45fc76afa9d5d41d219505}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.742: INFO: Pod "nginx-deployment-555b55d965-b79c5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-b79c5,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-b79c5,UID:8a022706-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45198,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0012e9be0 0xc0012e9be1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012e9c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012e9c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.742: INFO: Pod "nginx-deployment-555b55d965-dqskw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-dqskw,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-dqskw,UID:83f6e916-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45088,Generation:0,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0012e9ce7 0xc0012e9ce8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012e9e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012e9e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.97,StartTime:2019-01-21 06:12:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-21 06:12:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://e09b0745a14a59dd525ea6404dad14832563986b7588d670cd389ed7d8c35a89}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.743: INFO: Pod "nginx-deployment-555b55d965-hnj7f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-hnj7f,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-hnj7f,UID:83f351c0-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45070,Generation:0,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0012e9ef0 0xc0012e9ef1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0012e9f60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0012e9f80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.92,StartTime:2019-01-21 06:12:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-21 06:12:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://cafe904742ab150f1f6198800c9fb000112292d3a4d5377a81490c65be426d2d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.743: INFO: Pod "nginx-deployment-555b55d965-jwzrh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-jwzrh,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-jwzrh,UID:89fda840-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45188,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc001764080 0xc001764081}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001764120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001764200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:32 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.743: INFO: Pod "nginx-deployment-555b55d965-k2gbt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-k2gbt,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-k2gbt,UID:8a020c1d-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45197,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc001764270 0xc001764271}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017642d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017642f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.743: INFO: Pod "nginx-deployment-555b55d965-kl4z9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-kl4z9,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-kl4z9,UID:8a02386e-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45194,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc001764467 0xc001764468}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001764550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001764570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.743: INFO: Pod "nginx-deployment-555b55d965-lpx5b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-lpx5b,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-lpx5b,UID:83f36eb4-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45082,Generation:0,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0017645c7 0xc0017645c8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001764650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001764680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.93,StartTime:2019-01-21 06:12:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-21 06:12:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://522802b571210025af130a483c559f5ec50f5b7a88d2af58649ce0e55065ffce}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.743: INFO: Pod "nginx-deployment-555b55d965-qd9c9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-qd9c9,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-qd9c9,UID:89fefea3-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45187,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc001765110 0xc001765111}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001765390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017653b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.744: INFO: Pod "nginx-deployment-555b55d965-vjshn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-vjshn,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-vjshn,UID:83f3f25a-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45061,Generation:0,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0017654a7 0xc0017654a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017655a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017655c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.90,StartTime:2019-01-21 06:12:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-21 06:12:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://28a741126720fa911421a7aca8923963ac027245b21a3883826fba918110d2d6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.744: INFO: Pod "nginx-deployment-555b55d965-z5q5x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-555b55d965-z5q5x,GenerateName:nginx-deployment-555b55d965-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-555b55d965-z5q5x,UID:83f38612-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45076,Generation:0,CreationTimestamp:2019-01-21 06:12:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 555b55d965,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-555b55d965 83eb3d4a-1d43-11e9-b27d-fa163eb7f963 0xc0017657a0 0xc0017657a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017658e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001765900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:22 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.89,StartTime:2019-01-21 06:12:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-01-21 06:12:25 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96 docker://9e0b3e3e816eef6307b0994d3b9b2c3275099b4d43c43fcc74d9c734cb3b9fe9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.744: INFO: Pod "nginx-deployment-65bbdb5f8-6kswb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-6kswb,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-65bbdb5f8-6kswb,UID:8a005c30-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45193,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 88bb9b54-1d43-11e9-b27d-fa163eb7f963 0xc001765b50 0xc001765b51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001765d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001765d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.744: INFO: Pod "nginx-deployment-65bbdb5f8-772cc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-772cc,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-65bbdb5f8-772cc,UID:8a002dbb-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45190,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 88bb9b54-1d43-11e9-b27d-fa163eb7f963 0xc001765e77 0xc001765e78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001765ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001765f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.744: INFO: Pod "nginx-deployment-65bbdb5f8-8cl5l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-8cl5l,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-65bbdb5f8-8cl5l,UID:88dad726-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45171,Generation:0,CreationTimestamp:2019-01-21 06:12:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 88bb9b54-1d43-11e9-b27d-fa163eb7f963 0xc000dfe397 0xc000dfe398}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dfe480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dfe4f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:,StartTime:2019-01-21 06:12:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.745: INFO: Pod "nginx-deployment-65bbdb5f8-gzrtj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-gzrtj,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-65bbdb5f8-gzrtj,UID:89fe2a4a-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45189,Generation:0,CreationTimestamp:2019-01-21 06:12:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 88bb9b54-1d43-11e9-b27d-fa163eb7f963 0xc000dfe670 0xc000dfe671}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dfe740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dfe760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:32 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.745: INFO: Pod "nginx-deployment-65bbdb5f8-jgxp6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-jgxp6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-65bbdb5f8-jgxp6,UID:88bf3f8b-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45149,Generation:0,CreationTimestamp:2019-01-21 06:12:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 88bb9b54-1d43-11e9-b27d-fa163eb7f963 0xc000dfebe0 0xc000dfebe1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dfecd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dfecf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:,StartTime:2019-01-21 06:12:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.745: INFO: Pod "nginx-deployment-65bbdb5f8-jjrjf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-jjrjf,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-65bbdb5f8-jjrjf,UID:88d2cd65-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45169,Generation:0,CreationTimestamp:2019-01-21 06:12:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 88bb9b54-1d43-11e9-b27d-fa163eb7f963 0xc000dfedc0 0xc000dfedc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dfee40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dfee60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:,StartTime:2019-01-21 06:12:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.745: INFO: Pod "nginx-deployment-65bbdb5f8-rq4zd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-rq4zd,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-65bbdb5f8-rq4zd,UID:88bfb233-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45159,Generation:0,CreationTimestamp:2019-01-21 06:12:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 88bb9b54-1d43-11e9-b27d-fa163eb7f963 0xc000dff150 0xc000dff151}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dff1d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dff1f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:,StartTime:2019-01-21 06:12:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 21 06:12:32.745: INFO: Pod "nginx-deployment-65bbdb5f8-tlvd6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-65bbdb5f8-tlvd6,GenerateName:nginx-deployment-65bbdb5f8-,Namespace:e2e-tests-deployment-4hvkr,SelfLink:/api/v1/namespaces/e2e-tests-deployment-4hvkr/pods/nginx-deployment-65bbdb5f8-tlvd6,UID:88bd27fe-1d43-11e9-b27d-fa163eb7f963,ResourceVersion:45139,Generation:0,CreationTimestamp:2019-01-21 06:12:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 65bbdb5f8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-65bbdb5f8 88bb9b54-1d43-11e9-b27d-fa163eb7f963 0xc0016cc3e0 0xc0016cc3e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-rvq7r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rvq7r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rvq7r true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cc4a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cc4f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:30 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:,StartTime:2019-01-21 06:12:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:12:32.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-4hvkr" for this suite.
Jan 21 06:12:40.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:12:40.994: INFO: namespace: e2e-tests-deployment-4hvkr, resource: bindings, ignored listing per whitelist
Jan 21 06:12:40.998: INFO: namespace e2e-tests-deployment-4hvkr deletion completed in 8.232387703s

â€¢ [SLOW TEST:18.632 seconds]
[sig-apps] Deployment
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:12:40.998: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 21 06:12:49.142: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-8f04c555-1d43-11e9-b032-0a580af40356,GenerateName:,Namespace:e2e-tests-events-sqr67,SelfLink:/api/v1/namespaces/e2e-tests-events-sqr67/pods/send-events-8f04c555-1d43-11e9-b032-0a580af40356,UID:8f049c25-1d43-11e9-9faa-fa163e6f5c57,ResourceVersion:45458,Generation:0,CreationTimestamp:2019-01-21 06:12:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 97989542,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-cfgl9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cfgl9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-cfgl9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s05,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003132140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003132160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-01-21 06:12:41 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.21,PodIP:10.244.3.123,StartTime:2019-01-21 06:12:41 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-01-21 06:12:44 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 docker://ecdbaf06c00247179c3ec002d7b0dab0a00368b5ea02be743934d24943f1b7a9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jan 21 06:12:51.152: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 21 06:12:53.159: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:12:53.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-sqr67" for this suite.
Jan 21 06:13:31.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:13:31.310: INFO: namespace: e2e-tests-events-sqr67, resource: bindings, ignored listing per whitelist
Jan 21 06:13:31.389: INFO: namespace e2e-tests-events-sqr67 deletion completed in 38.200967923s

â€¢ [SLOW TEST:50.391 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:13:31.389: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Jan 21 06:13:33.559: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-ad0fc157-1d43-11e9-b032-0a580af40356", GenerateName:"", Namespace:"e2e-tests-pods-8wb92", SelfLink:"/api/v1/namespaces/e2e-tests-pods-8wb92/pods/pod-submit-remove-ad0fc157-1d43-11e9-b032-0a580af40356", UID:"ad112541-1d43-11e9-9faa-fa163e6f5c57", ResourceVersion:"45562", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63683648011, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"501619215"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-kdhnd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001980200), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kdhnd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0020da1d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s05", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000dcc360), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0020da430)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0020da450)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0020da458), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0020da45c)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683648011, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683648013, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683648013, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683648011, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.21", PodIP:"10.244.3.124", StartTime:(*v1.Time)(0xc001a62760), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001a62780), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:e3f77f7f4a6bb5e7820e013fa60b96602b34f5704e796cfd94b561ae73adcf96", ContainerID:"docker://e40ad3dda8b6acc5abb147606eef44e29414a7cbb580bbbf76fc9c9a1878580b"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:13:48.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-8wb92" for this suite.
Jan 21 06:13:54.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:13:54.325: INFO: namespace: e2e-tests-pods-8wb92, resource: bindings, ignored listing per whitelist
Jan 21 06:13:54.419: INFO: namespace e2e-tests-pods-8wb92 deletion completed in 6.166789498s

â€¢ [SLOW TEST:23.030 seconds]
[k8s.io] Pods
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:13:54.419: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 21 06:13:54.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-xdfnd'
Jan 21 06:13:54.708: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 21 06:13:54.708: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Jan 21 06:13:54.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-xdfnd'
Jan 21 06:13:54.861: INFO: stderr: ""
Jan 21 06:13:54.861: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:13:54.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-xdfnd" for this suite.
Jan 21 06:14:00.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:14:00.977: INFO: namespace: e2e-tests-kubectl-xdfnd, resource: bindings, ignored listing per whitelist
Jan 21 06:14:01.030: INFO: namespace e2e-tests-kubectl-xdfnd deletion completed in 6.161374489s

â€¢ [SLOW TEST:6.610 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:14:01.030: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 21 06:14:01.175: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:14:04.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-wtf86" for this suite.
Jan 21 06:14:10.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:14:10.340: INFO: namespace: e2e-tests-init-container-wtf86, resource: bindings, ignored listing per whitelist
Jan 21 06:14:10.398: INFO: namespace e2e-tests-init-container-wtf86 deletion completed in 6.239837664s

â€¢ [SLOW TEST:9.369 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:14:10.399: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Jan 21 06:14:13.104: INFO: Successfully updated pod "labelsupdatec450e916-1d43-11e9-b032-0a580af40356"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:14:15.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zjpr5" for this suite.
Jan 21 06:14:37.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:14:37.312: INFO: namespace: e2e-tests-projected-zjpr5, resource: bindings, ignored listing per whitelist
Jan 21 06:14:37.348: INFO: namespace e2e-tests-projected-zjpr5 deletion completed in 22.210014354s

â€¢ [SLOW TEST:26.949 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:14:37.348: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 21 06:14:37.978: INFO: Pod name wrapped-volume-race-d4aaaaef-1d43-11e9-b032-0a580af40356: Found 0 pods out of 5
Jan 21 06:14:42.995: INFO: Pod name wrapped-volume-race-d4aaaaef-1d43-11e9-b032-0a580af40356: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d4aaaaef-1d43-11e9-b032-0a580af40356 in namespace e2e-tests-emptydir-wrapper-zmrzv, will wait for the garbage collector to delete the pods
Jan 21 06:14:53.114: INFO: Deleting ReplicationController wrapped-volume-race-d4aaaaef-1d43-11e9-b032-0a580af40356 took: 22.352357ms
Jan 21 06:14:53.215: INFO: Terminating ReplicationController wrapped-volume-race-d4aaaaef-1d43-11e9-b032-0a580af40356 pods took: 100.322627ms
STEP: Creating RC which spawns configmap-volume pods
Jan 21 06:15:30.465: INFO: Pod name wrapped-volume-race-f3f09395-1d43-11e9-b032-0a580af40356: Found 0 pods out of 5
Jan 21 06:15:35.486: INFO: Pod name wrapped-volume-race-f3f09395-1d43-11e9-b032-0a580af40356: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f3f09395-1d43-11e9-b032-0a580af40356 in namespace e2e-tests-emptydir-wrapper-zmrzv, will wait for the garbage collector to delete the pods
Jan 21 06:15:45.607: INFO: Deleting ReplicationController wrapped-volume-race-f3f09395-1d43-11e9-b032-0a580af40356 took: 21.112082ms
Jan 21 06:15:45.808: INFO: Terminating ReplicationController wrapped-volume-race-f3f09395-1d43-11e9-b032-0a580af40356 pods took: 200.307754ms
STEP: Creating RC which spawns configmap-volume pods
Jan 21 06:16:28.370: INFO: Pod name wrapped-volume-race-16725e58-1d44-11e9-b032-0a580af40356: Found 0 pods out of 5
Jan 21 06:16:33.387: INFO: Pod name wrapped-volume-race-16725e58-1d44-11e9-b032-0a580af40356: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-16725e58-1d44-11e9-b032-0a580af40356 in namespace e2e-tests-emptydir-wrapper-zmrzv, will wait for the garbage collector to delete the pods
Jan 21 06:16:43.510: INFO: Deleting ReplicationController wrapped-volume-race-16725e58-1d44-11e9-b032-0a580af40356 took: 26.793455ms
Jan 21 06:16:43.610: INFO: Terminating ReplicationController wrapped-volume-race-16725e58-1d44-11e9-b032-0a580af40356 pods took: 100.344569ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:17:29.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wrapper-zmrzv" for this suite.
Jan 21 06:17:37.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:17:37.257: INFO: namespace: e2e-tests-emptydir-wrapper-zmrzv, resource: bindings, ignored listing per whitelist
Jan 21 06:17:37.290: INFO: namespace e2e-tests-emptydir-wrapper-zmrzv deletion completed in 8.175055518s

â€¢ [SLOW TEST:179.942 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Slow] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:17:37.290: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
Jan 21 06:17:41.481: INFO: error from create uninitialized namespace: <nil>
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:18:05.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-bmn9k" for this suite.
Jan 21 06:18:11.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:18:11.753: INFO: namespace: e2e-tests-namespaces-bmn9k, resource: bindings, ignored listing per whitelist
Jan 21 06:18:11.777: INFO: namespace e2e-tests-namespaces-bmn9k deletion completed in 6.195146269s
STEP: Destroying namespace "e2e-tests-nsdeletetest-m69kl" for this suite.
Jan 21 06:18:11.782: INFO: Namespace e2e-tests-nsdeletetest-m69kl was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-nkx58" for this suite.
Jan 21 06:18:17.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:18:17.966: INFO: namespace: e2e-tests-nsdeletetest-nkx58, resource: bindings, ignored listing per whitelist
Jan 21 06:18:17.980: INFO: namespace e2e-tests-nsdeletetest-nkx58 deletion completed in 6.197935507s

â€¢ [SLOW TEST:40.690 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:18:17.980: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-mk8sr.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-mk8sr.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-mk8sr.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default;check="$$(dig +tcp +noall +answer +search kubernetes.default A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc;check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-mk8sr.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-mk8sr.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-mk8sr.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 21 06:18:22.358: INFO: DNS probes using e2e-tests-dns-mk8sr/dns-test-57e380e5-1d44-11e9-b032-0a580af40356 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:18:22.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-mk8sr" for this suite.
Jan 21 06:18:28.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:18:28.549: INFO: namespace: e2e-tests-dns-mk8sr, resource: bindings, ignored listing per whitelist
Jan 21 06:18:28.617: INFO: namespace e2e-tests-dns-mk8sr deletion completed in 6.184159721s

â€¢ [SLOW TEST:10.636 seconds]
[sig-network] DNS
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:18:28.617: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 21 06:18:28.789: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:28.789: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:28.790: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:28.800: INFO: Number of nodes with available pods: 0
Jan 21 06:18:28.800: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:29.807: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:29.807: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:29.807: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:29.811: INFO: Number of nodes with available pods: 0
Jan 21 06:18:29.811: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:30.811: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:30.811: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:30.811: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:30.817: INFO: Number of nodes with available pods: 1
Jan 21 06:18:30.817: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 21 06:18:30.847: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:30.847: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:30.847: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:30.851: INFO: Number of nodes with available pods: 0
Jan 21 06:18:30.851: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:31.860: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:31.860: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:31.861: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:31.865: INFO: Number of nodes with available pods: 0
Jan 21 06:18:31.865: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:32.871: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:32.871: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:32.871: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:32.885: INFO: Number of nodes with available pods: 0
Jan 21 06:18:32.885: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:33.863: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:33.863: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:33.863: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:33.870: INFO: Number of nodes with available pods: 0
Jan 21 06:18:33.870: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:34.861: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:34.862: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:34.862: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:34.868: INFO: Number of nodes with available pods: 0
Jan 21 06:18:34.868: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:35.863: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:35.863: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:35.863: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:35.869: INFO: Number of nodes with available pods: 0
Jan 21 06:18:35.869: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:36.860: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:36.860: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:36.860: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:36.865: INFO: Number of nodes with available pods: 0
Jan 21 06:18:36.865: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:37.861: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:37.861: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:37.861: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:37.867: INFO: Number of nodes with available pods: 0
Jan 21 06:18:37.867: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:38.860: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:38.860: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:38.860: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:38.864: INFO: Number of nodes with available pods: 0
Jan 21 06:18:38.864: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:39.860: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:39.861: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:39.861: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:39.865: INFO: Number of nodes with available pods: 0
Jan 21 06:18:39.865: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:40.860: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:40.860: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:40.860: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:40.865: INFO: Number of nodes with available pods: 0
Jan 21 06:18:40.865: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:41.860: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:41.860: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:41.860: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:41.865: INFO: Number of nodes with available pods: 0
Jan 21 06:18:41.865: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:42.859: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:42.859: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:42.859: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:42.864: INFO: Number of nodes with available pods: 0
Jan 21 06:18:42.864: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:43.869: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:43.869: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:43.869: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:43.874: INFO: Number of nodes with available pods: 0
Jan 21 06:18:43.874: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:44.861: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:44.861: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:44.862: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:44.868: INFO: Number of nodes with available pods: 0
Jan 21 06:18:44.868: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:45.861: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:45.861: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:45.861: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:45.866: INFO: Number of nodes with available pods: 0
Jan 21 06:18:45.866: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:46.862: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:46.862: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:46.862: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:46.870: INFO: Number of nodes with available pods: 0
Jan 21 06:18:46.870: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:47.862: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:47.862: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:47.862: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:47.875: INFO: Number of nodes with available pods: 0
Jan 21 06:18:47.875: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:48.861: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:48.861: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:48.861: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:48.866: INFO: Number of nodes with available pods: 0
Jan 21 06:18:48.866: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:49.858: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:49.858: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:49.858: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:49.863: INFO: Number of nodes with available pods: 0
Jan 21 06:18:49.863: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:50.859: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:50.859: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:50.859: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:50.864: INFO: Number of nodes with available pods: 0
Jan 21 06:18:50.864: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:51.859: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:51.859: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:51.859: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:51.864: INFO: Number of nodes with available pods: 0
Jan 21 06:18:51.864: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:52.865: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:52.865: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:52.865: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:52.869: INFO: Number of nodes with available pods: 0
Jan 21 06:18:52.869: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:53.858: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:53.859: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:53.859: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:53.866: INFO: Number of nodes with available pods: 0
Jan 21 06:18:53.866: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:54.868: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:54.868: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:54.868: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:54.873: INFO: Number of nodes with available pods: 0
Jan 21 06:18:54.873: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:55.864: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:55.864: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:55.864: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:55.869: INFO: Number of nodes with available pods: 0
Jan 21 06:18:55.869: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:56.860: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:56.860: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:56.860: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:56.867: INFO: Number of nodes with available pods: 0
Jan 21 06:18:56.867: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:57.858: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:57.858: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:57.858: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:57.862: INFO: Number of nodes with available pods: 0
Jan 21 06:18:57.862: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:58.860: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:58.861: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:58.861: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:58.865: INFO: Number of nodes with available pods: 0
Jan 21 06:18:58.865: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:18:59.859: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:59.859: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:59.859: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:18:59.869: INFO: Number of nodes with available pods: 0
Jan 21 06:18:59.869: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:00.863: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:00.863: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:00.863: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:00.871: INFO: Number of nodes with available pods: 0
Jan 21 06:19:00.871: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:01.862: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:01.862: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:01.862: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:01.868: INFO: Number of nodes with available pods: 0
Jan 21 06:19:01.868: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:02.875: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:02.875: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:02.875: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:02.887: INFO: Number of nodes with available pods: 0
Jan 21 06:19:02.887: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:03.859: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:03.859: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:03.859: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:03.864: INFO: Number of nodes with available pods: 0
Jan 21 06:19:03.864: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:04.865: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:04.865: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:04.865: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:04.889: INFO: Number of nodes with available pods: 0
Jan 21 06:19:04.889: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:05.859: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:05.859: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:05.859: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:05.865: INFO: Number of nodes with available pods: 0
Jan 21 06:19:05.866: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:06.862: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:06.863: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:06.863: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:06.869: INFO: Number of nodes with available pods: 0
Jan 21 06:19:06.869: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:07.879: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:07.880: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:07.880: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:07.895: INFO: Number of nodes with available pods: 0
Jan 21 06:19:07.895: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:08.861: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:08.861: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:08.861: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:08.867: INFO: Number of nodes with available pods: 0
Jan 21 06:19:08.867: INFO: Node k8s05 is running more than one daemon pod
Jan 21 06:19:09.869: INFO: DaemonSet pods can't tolerate node k8s01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:09.869: INFO: DaemonSet pods can't tolerate node k8s02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:09.869: INFO: DaemonSet pods can't tolerate node k8s03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 21 06:19:09.874: INFO: Number of nodes with available pods: 1
Jan 21 06:19:09.874: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace e2e-tests-daemonsets-6zcrn, will wait for the garbage collector to delete the pods
Jan 21 06:19:09.946: INFO: Deleting DaemonSet.extensions daemon-set took: 13.710615ms
Jan 21 06:19:10.046: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.236695ms
Jan 21 06:19:48.258: INFO: Number of nodes with available pods: 0
Jan 21 06:19:48.258: INFO: Number of running nodes: 0, number of available pods: 0
Jan 21 06:19:48.263: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-6zcrn/daemonsets","resourceVersion":"47281"},"items":null}

Jan 21 06:19:48.268: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-6zcrn/pods","resourceVersion":"47281"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:19:48.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-6zcrn" for this suite.
Jan 21 06:19:54.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:19:54.454: INFO: namespace: e2e-tests-daemonsets-6zcrn, resource: bindings, ignored listing per whitelist
Jan 21 06:19:54.488: INFO: namespace e2e-tests-daemonsets-6zcrn deletion completed in 6.197675836s

â€¢ [SLOW TEST:85.871 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:19:54.488: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-rhl94
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 21 06:19:54.607: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 21 06:20:14.742: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.147:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-rhl94 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 21 06:20:14.742: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
Jan 21 06:20:14.895: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:20:14.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-rhl94" for this suite.
Jan 21 06:20:36.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:20:36.951: INFO: namespace: e2e-tests-pod-network-test-rhl94, resource: bindings, ignored listing per whitelist
Jan 21 06:20:37.081: INFO: namespace e2e-tests-pod-network-test-rhl94 deletion completed in 22.178674131s

â€¢ [SLOW TEST:42.593 seconds]
[sig-network] Networking
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:20:37.081: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Jan 21 06:20:37.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aacbe616-1d44-11e9-b032-0a580af40356" in namespace "e2e-tests-downward-api-pd792" to be "success or failure"
Jan 21 06:20:37.226: INFO: Pod "downwardapi-volume-aacbe616-1d44-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 8.647283ms
Jan 21 06:20:39.233: INFO: Pod "downwardapi-volume-aacbe616-1d44-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015369216s
STEP: Saw pod success
Jan 21 06:20:39.233: INFO: Pod "downwardapi-volume-aacbe616-1d44-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:20:39.238: INFO: Trying to get logs from node k8s05 pod downwardapi-volume-aacbe616-1d44-11e9-b032-0a580af40356 container client-container: <nil>
STEP: delete the pod
Jan 21 06:20:39.298: INFO: Waiting for pod downwardapi-volume-aacbe616-1d44-11e9-b032-0a580af40356 to disappear
Jan 21 06:20:39.304: INFO: Pod downwardapi-volume-aacbe616-1d44-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:20:39.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-pd792" for this suite.
Jan 21 06:20:45.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:20:45.403: INFO: namespace: e2e-tests-downward-api-pd792, resource: bindings, ignored listing per whitelist
Jan 21 06:20:45.485: INFO: namespace e2e-tests-downward-api-pd792 deletion completed in 6.170894969s

â€¢ [SLOW TEST:8.404 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:20:45.485: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-tmvb
STEP: Creating a pod to test atomic-volume-subpath
Jan 21 06:20:45.614: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-tmvb" in namespace "e2e-tests-subpath-fdc9w" to be "success or failure"
Jan 21 06:20:45.620: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.413438ms
Jan 21 06:20:47.626: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011821932s
Jan 21 06:20:49.634: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 4.019870598s
Jan 21 06:20:51.642: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 6.027216284s
Jan 21 06:20:53.658: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 8.04339924s
Jan 21 06:20:55.665: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 10.050758358s
Jan 21 06:20:57.671: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 12.057031405s
Jan 21 06:20:59.678: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 14.063111241s
Jan 21 06:21:01.698: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 16.083215649s
Jan 21 06:21:03.713: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 18.098982984s
Jan 21 06:21:05.721: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 20.106866083s
Jan 21 06:21:07.728: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Running", Reason="", readiness=false. Elapsed: 22.113125842s
Jan 21 06:21:09.733: INFO: Pod "pod-subpath-test-projected-tmvb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.118331449s
STEP: Saw pod success
Jan 21 06:21:09.733: INFO: Pod "pod-subpath-test-projected-tmvb" satisfied condition "success or failure"
Jan 21 06:21:09.738: INFO: Trying to get logs from node k8s05 pod pod-subpath-test-projected-tmvb container test-container-subpath-projected-tmvb: <nil>
STEP: delete the pod
Jan 21 06:21:09.784: INFO: Waiting for pod pod-subpath-test-projected-tmvb to disappear
Jan 21 06:21:09.790: INFO: Pod pod-subpath-test-projected-tmvb no longer exists
STEP: Deleting pod pod-subpath-test-projected-tmvb
Jan 21 06:21:09.790: INFO: Deleting pod "pod-subpath-test-projected-tmvb" in namespace "e2e-tests-subpath-fdc9w"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:21:09.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-fdc9w" for this suite.
Jan 21 06:21:15.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:21:15.881: INFO: namespace: e2e-tests-subpath-fdc9w, resource: bindings, ignored listing per whitelist
Jan 21 06:21:15.976: INFO: namespace e2e-tests-subpath-fdc9w deletion completed in 6.173979737s

â€¢ [SLOW TEST:30.490 seconds]
[sig-storage] Subpath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:21:15.976: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-c1fa1423-1d44-11e9-b032-0a580af40356
STEP: Creating a pod to test consume secrets
Jan 21 06:21:16.149: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c2008c21-1d44-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-rjshl" to be "success or failure"
Jan 21 06:21:16.153: INFO: Pod "pod-projected-secrets-c2008c21-1d44-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055588ms
Jan 21 06:21:18.161: INFO: Pod "pod-projected-secrets-c2008c21-1d44-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012498004s
STEP: Saw pod success
Jan 21 06:21:18.161: INFO: Pod "pod-projected-secrets-c2008c21-1d44-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:21:18.167: INFO: Trying to get logs from node k8s05 pod pod-projected-secrets-c2008c21-1d44-11e9-b032-0a580af40356 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 21 06:21:18.215: INFO: Waiting for pod pod-projected-secrets-c2008c21-1d44-11e9-b032-0a580af40356 to disappear
Jan 21 06:21:18.221: INFO: Pod pod-projected-secrets-c2008c21-1d44-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:21:18.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rjshl" for this suite.
Jan 21 06:21:24.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:21:24.301: INFO: namespace: e2e-tests-projected-rjshl, resource: bindings, ignored listing per whitelist
Jan 21 06:21:24.428: INFO: namespace e2e-tests-projected-rjshl deletion completed in 6.198842883s

â€¢ [SLOW TEST:8.452 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:21:24.429: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Jan 21 06:21:24.540: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-fjxps" to be "success or failure"
Jan 21 06:21:24.554: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.279671ms
Jan 21 06:21:26.559: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019394909s
STEP: Saw pod success
Jan 21 06:21:26.559: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 21 06:21:26.564: INFO: Trying to get logs from node k8s05 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 21 06:21:26.599: INFO: Waiting for pod pod-host-path-test to disappear
Jan 21 06:21:26.602: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:21:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-fjxps" for this suite.
Jan 21 06:21:32.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:21:32.783: INFO: namespace: e2e-tests-hostpath-fjxps, resource: bindings, ignored listing per whitelist
Jan 21 06:21:32.830: INFO: namespace e2e-tests-hostpath-fjxps deletion completed in 6.220911208s

â€¢ [SLOW TEST:8.402 seconds]
[sig-storage] HostPath
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:21:32.831: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 21 06:21:33.018: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-86frz,SelfLink:/api/v1/namespaces/e2e-tests-watch-86frz/configmaps/e2e-watch-test-label-changed,UID:cc098512-1d44-11e9-9faa-fa163e6f5c57,ResourceVersion:47656,Generation:0,CreationTimestamp:2019-01-21 06:21:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 21 06:21:33.019: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-86frz,SelfLink:/api/v1/namespaces/e2e-tests-watch-86frz/configmaps/e2e-watch-test-label-changed,UID:cc098512-1d44-11e9-9faa-fa163e6f5c57,ResourceVersion:47657,Generation:0,CreationTimestamp:2019-01-21 06:21:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 21 06:21:33.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-86frz,SelfLink:/api/v1/namespaces/e2e-tests-watch-86frz/configmaps/e2e-watch-test-label-changed,UID:cc098512-1d44-11e9-9faa-fa163e6f5c57,ResourceVersion:47658,Generation:0,CreationTimestamp:2019-01-21 06:21:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 21 06:21:43.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-86frz,SelfLink:/api/v1/namespaces/e2e-tests-watch-86frz/configmaps/e2e-watch-test-label-changed,UID:cc098512-1d44-11e9-9faa-fa163e6f5c57,ResourceVersion:47677,Generation:0,CreationTimestamp:2019-01-21 06:21:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 21 06:21:43.089: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-86frz,SelfLink:/api/v1/namespaces/e2e-tests-watch-86frz/configmaps/e2e-watch-test-label-changed,UID:cc098512-1d44-11e9-9faa-fa163e6f5c57,ResourceVersion:47678,Generation:0,CreationTimestamp:2019-01-21 06:21:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 21 06:21:43.104: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-86frz,SelfLink:/api/v1/namespaces/e2e-tests-watch-86frz/configmaps/e2e-watch-test-label-changed,UID:cc098512-1d44-11e9-9faa-fa163e6f5c57,ResourceVersion:47679,Generation:0,CreationTimestamp:2019-01-21 06:21:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:21:43.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-86frz" for this suite.
Jan 21 06:21:49.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:21:49.147: INFO: namespace: e2e-tests-watch-86frz, resource: bindings, ignored listing per whitelist
Jan 21 06:21:49.325: INFO: namespace e2e-tests-watch-86frz deletion completed in 6.212950929s

â€¢ [SLOW TEST:16.494 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:21:49.325: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-d5dc2c75-1d44-11e9-b032-0a580af40356
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d5dc2c75-1d44-11e9-b032-0a580af40356
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:21:53.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-6k5rr" for this suite.
Jan 21 06:22:15.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:22:15.637: INFO: namespace: e2e-tests-configmap-6k5rr, resource: bindings, ignored listing per whitelist
Jan 21 06:22:15.771: INFO: namespace e2e-tests-configmap-6k5rr deletion completed in 22.19808714s

â€¢ [SLOW TEST:26.446 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:22:15.772: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 21 06:22:15.881: INFO: PodSpec: initContainers in spec.initContainers
Jan 21 06:23:01.073: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e59dba0f-1d44-11e9-b032-0a580af40356", GenerateName:"", Namespace:"e2e-tests-init-container-g8qjm", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-g8qjm/pods/pod-init-e59dba0f-1d44-11e9-b032-0a580af40356", UID:"e59ddf9e-1d44-11e9-9faa-fa163e6f5c57", ResourceVersion:"47875", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63683648535, loc:(*time.Location)(0x7b33b80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"881184254"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-4b25z", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002dcef00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4b25z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4b25z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4b25z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002da9af8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s05", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0014a0900), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002da9b90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002da9bb0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002da9bb8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002da9bbc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683648535, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683648535, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683648535, loc:(*time.Location)(0x7b33b80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63683648535, loc:(*time.Location)(0x7b33b80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.21", PodIP:"10.244.3.154", StartTime:(*v1.Time)(0xc00313eba0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00312c1c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00312c230)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://169243621de7e78864beec2abef7e5a5dec04eb43c8dae89894db058804bf917"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00313ebe0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00313ebc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:23:01.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-g8qjm" for this suite.
Jan 21 06:23:23.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:23:23.187: INFO: namespace: e2e-tests-init-container-g8qjm, resource: bindings, ignored listing per whitelist
Jan 21 06:23:23.271: INFO: namespace e2e-tests-init-container-g8qjm deletion completed in 22.176674928s

â€¢ [SLOW TEST:67.499 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:23:23.271: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-0ddc3d84-1d45-11e9-b032-0a580af40356
STEP: Creating secret with name secret-projected-all-test-volume-0ddc3d30-1d45-11e9-b032-0a580af40356
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 21 06:23:23.452: INFO: Waiting up to 5m0s for pod "projected-volume-0ddc3ce0-1d45-11e9-b032-0a580af40356" in namespace "e2e-tests-projected-v7bjz" to be "success or failure"
Jan 21 06:23:23.482: INFO: Pod "projected-volume-0ddc3ce0-1d45-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 30.46032ms
Jan 21 06:23:25.490: INFO: Pod "projected-volume-0ddc3ce0-1d45-11e9-b032-0a580af40356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038337468s
Jan 21 06:23:27.497: INFO: Pod "projected-volume-0ddc3ce0-1d45-11e9-b032-0a580af40356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044696617s
STEP: Saw pod success
Jan 21 06:23:27.497: INFO: Pod "projected-volume-0ddc3ce0-1d45-11e9-b032-0a580af40356" satisfied condition "success or failure"
Jan 21 06:23:27.502: INFO: Trying to get logs from node k8s05 pod projected-volume-0ddc3ce0-1d45-11e9-b032-0a580af40356 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 21 06:23:27.550: INFO: Waiting for pod projected-volume-0ddc3ce0-1d45-11e9-b032-0a580af40356 to disappear
Jan 21 06:23:27.562: INFO: Pod projected-volume-0ddc3ce0-1d45-11e9-b032-0a580af40356 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:23:27.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v7bjz" for this suite.
Jan 21 06:23:33.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:23:33.620: INFO: namespace: e2e-tests-projected-v7bjz, resource: bindings, ignored listing per whitelist
Jan 21 06:23:33.754: INFO: namespace e2e-tests-projected-v7bjz deletion completed in 6.184417315s

â€¢ [SLOW TEST:10.483 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:23:33.755: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Jan 21 06:23:33.867: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:23:38.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-whwb4" for this suite.
Jan 21 06:23:44.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:23:44.137: INFO: namespace: e2e-tests-init-container-whwb4, resource: bindings, ignored listing per whitelist
Jan 21 06:23:44.230: INFO: namespace e2e-tests-init-container-whwb4 deletion completed in 6.201942217s

â€¢ [SLOW TEST:10.475 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:23:44.230: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:85
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:23:44.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-k944g" for this suite.
Jan 21 06:23:50.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:23:50.455: INFO: namespace: e2e-tests-services-k944g, resource: bindings, ignored listing per whitelist
Jan 21 06:23:50.545: INFO: namespace e2e-tests-services-k944g deletion completed in 6.176807855s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:90

â€¢ [SLOW TEST:6.315 seconds]
[sig-network] Services
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:23:50.545: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-2xbnz
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-2xbnz
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-2xbnz
Jan 21 06:23:50.684: INFO: Found 0 stateful pods, waiting for 1
Jan 21 06:24:00.705: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 21 06:24:00.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-2xbnz ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 06:24:01.013: INFO: stderr: ""
Jan 21 06:24:01.013: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 06:24:01.013: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 21 06:24:01.019: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 21 06:24:11.036: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 06:24:11.036: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 06:24:11.065: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999086s
Jan 21 06:24:12.070: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992454831s
Jan 21 06:24:13.076: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987037819s
Jan 21 06:24:14.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981559168s
Jan 21 06:24:15.090: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.975419883s
Jan 21 06:24:16.095: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.967676542s
Jan 21 06:24:17.102: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.962463509s
Jan 21 06:24:18.109: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.955716101s
Jan 21 06:24:19.117: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.948467465s
Jan 21 06:24:20.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 940.68486ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-2xbnz
Jan 21 06:24:21.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-2xbnz ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 06:24:21.474: INFO: stderr: ""
Jan 21 06:24:21.474: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 21 06:24:21.474: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 21 06:24:21.481: INFO: Found 1 stateful pods, waiting for 3
Jan 21 06:24:31.499: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 06:24:31.499: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 06:24:31.499: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 21 06:24:31.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-2xbnz ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 06:24:31.729: INFO: stderr: ""
Jan 21 06:24:31.729: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 06:24:31.729: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 21 06:24:31.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-2xbnz ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 06:24:32.014: INFO: stderr: ""
Jan 21 06:24:32.014: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 06:24:32.014: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 21 06:24:32.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-2xbnz ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 21 06:24:32.296: INFO: stderr: ""
Jan 21 06:24:32.296: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 21 06:24:32.296: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 21 06:24:32.296: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 06:24:32.301: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 21 06:24:42.326: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 06:24:42.326: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 06:24:42.326: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 21 06:24:42.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999552s
Jan 21 06:24:43.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991102717s
Jan 21 06:24:44.371: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98529566s
Jan 21 06:24:45.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975090533s
Jan 21 06:24:46.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965337784s
Jan 21 06:24:47.397: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957361483s
Jan 21 06:24:48.405: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.949949296s
Jan 21 06:24:49.413: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94262964s
Jan 21 06:24:50.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.934665643s
Jan 21 06:24:51.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.644052ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-2xbnz
Jan 21 06:24:52.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-2xbnz ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 06:24:52.831: INFO: stderr: ""
Jan 21 06:24:52.832: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 21 06:24:52.832: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 21 06:24:52.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-2xbnz ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 06:24:53.155: INFO: stderr: ""
Jan 21 06:24:53.155: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 21 06:24:53.155: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 21 06:24:53.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 exec --namespace=e2e-tests-statefulset-2xbnz ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 21 06:24:53.465: INFO: stderr: ""
Jan 21 06:24:53.465: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 21 06:24:53.465: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 21 06:24:53.465: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 21 06:25:03.502: INFO: Deleting all statefulset in ns e2e-tests-statefulset-2xbnz
Jan 21 06:25:03.507: INFO: Scaling statefulset ss to 0
Jan 21 06:25:03.524: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 06:25:03.528: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:25:03.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-2xbnz" for this suite.
Jan 21 06:25:09.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:25:09.634: INFO: namespace: e2e-tests-statefulset-2xbnz, resource: bindings, ignored listing per whitelist
Jan 21 06:25:09.748: INFO: namespace e2e-tests-statefulset-2xbnz deletion completed in 6.183613415s

â€¢ [SLOW TEST:79.203 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:25:09.749: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-rkfkc
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Jan 21 06:25:09.905: INFO: Found 0 stateful pods, waiting for 3
Jan 21 06:25:19.919: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 06:25:19.919: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 06:25:19.919: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 21 06:25:19.957: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 21 06:25:30.012: INFO: Updating stateful set ss2
Jan 21 06:25:30.031: INFO: Waiting for Pod e2e-tests-statefulset-rkfkc/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jan 21 06:25:40.144: INFO: Found 1 stateful pods, waiting for 3
Jan 21 06:25:50.163: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 06:25:50.163: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 21 06:25:50.163: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 21 06:25:50.206: INFO: Updating stateful set ss2
Jan 21 06:25:50.222: INFO: Waiting for Pod e2e-tests-statefulset-rkfkc/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 21 06:26:00.264: INFO: Updating stateful set ss2
Jan 21 06:26:00.277: INFO: Waiting for StatefulSet e2e-tests-statefulset-rkfkc/ss2 to complete update
Jan 21 06:26:00.277: INFO: Waiting for Pod e2e-tests-statefulset-rkfkc/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jan 21 06:26:10.319: INFO: Waiting for StatefulSet e2e-tests-statefulset-rkfkc/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 21 06:26:20.291: INFO: Deleting all statefulset in ns e2e-tests-statefulset-rkfkc
Jan 21 06:26:20.297: INFO: Scaling statefulset ss2 to 0
Jan 21 06:26:30.360: INFO: Waiting for statefulset status.replicas updated to 0
Jan 21 06:26:30.364: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:26:30.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-rkfkc" for this suite.
Jan 21 06:26:36.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:26:36.452: INFO: namespace: e2e-tests-statefulset-rkfkc, resource: bindings, ignored listing per whitelist
Jan 21 06:26:36.573: INFO: namespace e2e-tests-statefulset-rkfkc deletion completed in 6.182450791s

â€¢ [SLOW TEST:86.824 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:153
STEP: Creating a kubernetes client
Jan 21 06:26:36.574: INFO: >>> kubeConfig: /tmp/kubeconfig-453531067
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:243
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 21 06:26:36.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-4hg8g'
Jan 21 06:26:37.151: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 21 06:26:37.151: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jan 21 06:26:37.178: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-2zqlx]
Jan 21 06:26:37.178: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-2zqlx" in namespace "e2e-tests-kubectl-4hg8g" to be "running and ready"
Jan 21 06:26:37.189: INFO: Pod "e2e-test-nginx-rc-2zqlx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.124059ms
Jan 21 06:26:39.195: INFO: Pod "e2e-test-nginx-rc-2zqlx": Phase="Running", Reason="", readiness=true. Elapsed: 2.016633321s
Jan 21 06:26:39.195: INFO: Pod "e2e-test-nginx-rc-2zqlx" satisfied condition "running and ready"
Jan 21 06:26:39.195: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-2zqlx]
Jan 21 06:26:39.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-4hg8g'
Jan 21 06:26:39.413: INFO: stderr: ""
Jan 21 06:26:39.413: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1303
Jan 21 06:26:39.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-453531067 delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-4hg8g'
Jan 21 06:26:39.621: INFO: stderr: ""
Jan 21 06:26:39.621: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:154
Jan 21 06:26:39.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-4hg8g" for this suite.
Jan 21 06:26:45.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 21 06:26:45.734: INFO: namespace: e2e-tests-kubectl-4hg8g, resource: bindings, ignored listing per whitelist
Jan 21 06:26:45.857: INFO: namespace e2e-tests-kubectl-4hg8g deletion completed in 6.224972121s

â€¢ [SLOW TEST:9.284 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.13.0-rc.2.1+ddf47ac13c1a94/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSJan 21 06:26:45.859: INFO: Running AfterSuite actions on all nodes
Jan 21 06:26:45.874: INFO: Running AfterSuite actions on node 1
Jan 21 06:26:45.874: INFO: Skipping dumping logs from cluster

Ran 200 of 1946 Specs in 5388.776 seconds
SUCCESS! -- 200 Passed | 0 Failed | 0 Pending | 1746 Skipped PASS

Ginkgo ran 1 suite in 1h29m50.268871216s
Test Suite Passed
