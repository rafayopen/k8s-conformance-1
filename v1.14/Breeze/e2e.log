I0522 01:41:56.138426      16 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-734452489
I0522 01:41:56.138565      16 e2e.go:240] Starting e2e run "c7a5f7e4-7c32-11e9-8c5e-b202ea6dae39" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1558489314 - Will randomize all specs
Will run 204 of 3585 specs

May 22 01:41:56.320: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 01:41:56.322: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 22 01:41:56.340: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 22 01:41:56.371: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 22 01:41:56.371: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
May 22 01:41:56.371: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 22 01:41:56.380: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-amd64' (0 seconds elapsed)
May 22 01:41:56.380: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
May 22 01:41:56.380: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
May 22 01:41:56.380: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
May 22 01:41:56.380: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
May 22 01:41:56.380: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 22 01:41:56.380: INFO: e2e test version: v1.14.2
May 22 01:41:56.381: INFO: kube-apiserver version: v1.14.2
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:41:56.382: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename namespaces
May 22 01:41:56.423: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
May 22 01:41:56.433: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2081
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5176
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6087
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:42:21.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2081" for this suite.
May 22 01:42:27.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:42:28.039: INFO: namespace namespaces-2081 deletion completed in 6.1706813s
STEP: Destroying namespace "nsdeletetest-5176" for this suite.
May 22 01:42:28.047: INFO: Namespace nsdeletetest-5176 was already deleted
STEP: Destroying namespace "nsdeletetest-6087" for this suite.
May 22 01:42:34.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:42:34.213: INFO: namespace nsdeletetest-6087 deletion completed in 6.165975657s

• [SLOW TEST:37.832 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:42:34.214: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7763
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-rqmg
STEP: Creating a pod to test atomic-volume-subpath
May 22 01:42:34.436: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rqmg" in namespace "subpath-7763" to be "success or failure"
May 22 01:42:34.441: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.894993ms
May 22 01:42:36.445: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009173426s
May 22 01:42:38.455: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Running", Reason="", readiness=true. Elapsed: 4.019015311s
May 22 01:42:40.461: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Running", Reason="", readiness=true. Elapsed: 6.024938494s
May 22 01:42:42.466: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Running", Reason="", readiness=true. Elapsed: 8.029732878s
May 22 01:42:44.471: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Running", Reason="", readiness=true. Elapsed: 10.035080117s
May 22 01:42:46.480: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Running", Reason="", readiness=true. Elapsed: 12.04409924s
May 22 01:42:48.491: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Running", Reason="", readiness=true. Elapsed: 14.05449823s
May 22 01:42:50.496: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Running", Reason="", readiness=true. Elapsed: 16.06017592s
May 22 01:42:52.501: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Running", Reason="", readiness=true. Elapsed: 18.064290267s
May 22 01:42:54.506: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Running", Reason="", readiness=true. Elapsed: 20.069719125s
May 22 01:42:56.511: INFO: Pod "pod-subpath-test-projected-rqmg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.074682933s
STEP: Saw pod success
May 22 01:42:56.511: INFO: Pod "pod-subpath-test-projected-rqmg" satisfied condition "success or failure"
May 22 01:42:56.515: INFO: Trying to get logs from node worker01 pod pod-subpath-test-projected-rqmg container test-container-subpath-projected-rqmg: <nil>
STEP: delete the pod
May 22 01:42:56.565: INFO: Waiting for pod pod-subpath-test-projected-rqmg to disappear
May 22 01:42:56.572: INFO: Pod pod-subpath-test-projected-rqmg no longer exists
STEP: Deleting pod pod-subpath-test-projected-rqmg
May 22 01:42:56.572: INFO: Deleting pod "pod-subpath-test-projected-rqmg" in namespace "subpath-7763"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:42:56.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7763" for this suite.
May 22 01:43:02.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:43:02.761: INFO: namespace subpath-7763 deletion completed in 6.17103304s

• [SLOW TEST:28.547 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:43:02.761: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
May 22 01:43:02.957: INFO: Waiting up to 5m0s for pod "var-expansion-f00fbebf-7c32-11e9-8c5e-b202ea6dae39" in namespace "var-expansion-3673" to be "success or failure"
May 22 01:43:02.970: INFO: Pod "var-expansion-f00fbebf-7c32-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.728941ms
May 22 01:43:04.975: INFO: Pod "var-expansion-f00fbebf-7c32-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017522601s
May 22 01:43:06.984: INFO: Pod "var-expansion-f00fbebf-7c32-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026504901s
STEP: Saw pod success
May 22 01:43:06.984: INFO: Pod "var-expansion-f00fbebf-7c32-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:43:06.998: INFO: Trying to get logs from node worker01 pod var-expansion-f00fbebf-7c32-11e9-8c5e-b202ea6dae39 container dapi-container: <nil>
STEP: delete the pod
May 22 01:43:07.058: INFO: Waiting for pod var-expansion-f00fbebf-7c32-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:43:07.062: INFO: Pod var-expansion-f00fbebf-7c32-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:43:07.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3673" for this suite.
May 22 01:43:13.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:43:13.230: INFO: namespace var-expansion-3673 deletion completed in 6.153079448s

• [SLOW TEST:10.469 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:43:13.230: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f64c29b7-7c32-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 01:43:13.412: INFO: Waiting up to 5m0s for pod "pod-secrets-f64d3394-7c32-11e9-8c5e-b202ea6dae39" in namespace "secrets-5680" to be "success or failure"
May 22 01:43:13.422: INFO: Pod "pod-secrets-f64d3394-7c32-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.898664ms
May 22 01:43:15.426: INFO: Pod "pod-secrets-f64d3394-7c32-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013976278s
STEP: Saw pod success
May 22 01:43:15.426: INFO: Pod "pod-secrets-f64d3394-7c32-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:43:15.431: INFO: Trying to get logs from node worker01 pod pod-secrets-f64d3394-7c32-11e9-8c5e-b202ea6dae39 container secret-volume-test: <nil>
STEP: delete the pod
May 22 01:43:15.475: INFO: Waiting for pod pod-secrets-f64d3394-7c32-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:43:15.479: INFO: Pod pod-secrets-f64d3394-7c32-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:43:15.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5680" for this suite.
May 22 01:43:21.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:43:21.637: INFO: namespace secrets-5680 deletion completed in 6.148859886s

• [SLOW TEST:8.407 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:43:21.637: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-fb4f7846-7c32-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 01:43:21.821: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb50a7f0-7c32-11e9-8c5e-b202ea6dae39" in namespace "projected-470" to be "success or failure"
May 22 01:43:21.833: INFO: Pod "pod-projected-configmaps-fb50a7f0-7c32-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029747ms
May 22 01:43:23.838: INFO: Pod "pod-projected-configmaps-fb50a7f0-7c32-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016703032s
STEP: Saw pod success
May 22 01:43:23.838: INFO: Pod "pod-projected-configmaps-fb50a7f0-7c32-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:43:23.845: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-fb50a7f0-7c32-11e9-8c5e-b202ea6dae39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 01:43:23.889: INFO: Waiting for pod pod-projected-configmaps-fb50a7f0-7c32-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:43:23.902: INFO: Pod pod-projected-configmaps-fb50a7f0-7c32-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:43:23.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-470" for this suite.
May 22 01:43:29.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:43:30.095: INFO: namespace projected-470 deletion completed in 6.184145272s

• [SLOW TEST:8.459 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:43:30.095: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 22 01:43:30.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7534'
May 22 01:43:30.440: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 01:43:30.441: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
May 22 01:43:30.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete jobs e2e-test-nginx-job --namespace=kubectl-7534'
May 22 01:43:30.539: INFO: stderr: ""
May 22 01:43:30.539: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:43:30.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7534" for this suite.
May 22 01:43:36.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:43:36.705: INFO: namespace kubectl-7534 deletion completed in 6.157793647s

• [SLOW TEST:6.609 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:43:36.705: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2163
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 01:43:36.888: INFO: Waiting up to 5m0s for pod "downwardapi-volume-044ab1a4-7c33-11e9-8c5e-b202ea6dae39" in namespace "downward-api-2163" to be "success or failure"
May 22 01:43:36.903: INFO: Pod "downwardapi-volume-044ab1a4-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 15.522825ms
May 22 01:43:38.908: INFO: Pod "downwardapi-volume-044ab1a4-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019880829s
May 22 01:43:40.919: INFO: Pod "downwardapi-volume-044ab1a4-7c33-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030886009s
STEP: Saw pod success
May 22 01:43:40.919: INFO: Pod "downwardapi-volume-044ab1a4-7c33-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:43:40.934: INFO: Trying to get logs from node worker01 pod downwardapi-volume-044ab1a4-7c33-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 01:43:40.984: INFO: Waiting for pod downwardapi-volume-044ab1a4-7c33-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:43:40.992: INFO: Pod downwardapi-volume-044ab1a4-7c33-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:43:40.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2163" for this suite.
May 22 01:43:47.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:43:47.206: INFO: namespace downward-api-2163 deletion completed in 6.204644321s

• [SLOW TEST:10.502 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:43:47.207: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9239
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-0a8f0650-7c33-11e9-8c5e-b202ea6dae39
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0a8f0650-7c33-11e9-8c5e-b202ea6dae39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:43:51.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9239" for this suite.
May 22 01:44:13.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:44:13.677: INFO: namespace configmap-9239 deletion completed in 22.166718454s

• [SLOW TEST:26.470 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:44:13.677: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
May 22 01:44:14.395: INFO: created pod pod-service-account-defaultsa
May 22 01:44:14.395: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 22 01:44:14.411: INFO: created pod pod-service-account-mountsa
May 22 01:44:14.411: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 22 01:44:14.422: INFO: created pod pod-service-account-nomountsa
May 22 01:44:14.422: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 22 01:44:14.451: INFO: created pod pod-service-account-defaultsa-mountspec
May 22 01:44:14.451: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 22 01:44:14.466: INFO: created pod pod-service-account-mountsa-mountspec
May 22 01:44:14.466: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 22 01:44:14.481: INFO: created pod pod-service-account-nomountsa-mountspec
May 22 01:44:14.481: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 22 01:44:14.500: INFO: created pod pod-service-account-defaultsa-nomountspec
May 22 01:44:14.500: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 22 01:44:14.519: INFO: created pod pod-service-account-mountsa-nomountspec
May 22 01:44:14.519: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 22 01:44:14.540: INFO: created pod pod-service-account-nomountsa-nomountspec
May 22 01:44:14.540: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:44:14.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1317" for this suite.
May 22 01:44:36.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:44:36.730: INFO: namespace svcaccounts-1317 deletion completed in 22.180689656s

• [SLOW TEST:23.053 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:44:36.730: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3237
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-28136f87-7c33-11e9-8c5e-b202ea6dae39
STEP: Creating configMap with name cm-test-opt-upd-28136fde-7c33-11e9-8c5e-b202ea6dae39
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-28136f87-7c33-11e9-8c5e-b202ea6dae39
STEP: Updating configmap cm-test-opt-upd-28136fde-7c33-11e9-8c5e-b202ea6dae39
STEP: Creating configMap with name cm-test-opt-create-28136ff7-7c33-11e9-8c5e-b202ea6dae39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:46:13.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3237" for this suite.
May 22 01:46:36.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:46:36.144: INFO: namespace projected-3237 deletion completed in 22.158503658s

• [SLOW TEST:119.413 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:46:36.144: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7164
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 22 01:46:36.353: INFO: Found 0 stateful pods, waiting for 3
May 22 01:46:46.358: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 01:46:46.358: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 01:46:46.358: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 22 01:46:46.397: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 22 01:46:56.457: INFO: Updating stateful set ss2
May 22 01:46:56.472: INFO: Waiting for Pod statefulset-7164/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 22 01:47:06.553: INFO: Found 2 stateful pods, waiting for 3
May 22 01:47:16.560: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 01:47:16.560: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 01:47:16.560: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 22 01:47:16.608: INFO: Updating stateful set ss2
May 22 01:47:16.621: INFO: Waiting for Pod statefulset-7164/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 22 01:47:26.673: INFO: Updating stateful set ss2
May 22 01:47:26.692: INFO: Waiting for StatefulSet statefulset-7164/ss2 to complete update
May 22 01:47:26.692: INFO: Waiting for Pod statefulset-7164/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 01:47:36.707: INFO: Deleting all statefulset in ns statefulset-7164
May 22 01:47:36.713: INFO: Scaling statefulset ss2 to 0
May 22 01:47:46.758: INFO: Waiting for statefulset status.replicas updated to 0
May 22 01:47:46.763: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:47:46.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7164" for this suite.
May 22 01:47:52.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:47:52.961: INFO: namespace statefulset-7164 deletion completed in 6.158323054s

• [SLOW TEST:76.817 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:47:52.961: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-1231
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1231 to expose endpoints map[]
May 22 01:47:53.171: INFO: Get endpoints failed (9.088249ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May 22 01:47:54.175: INFO: successfully validated that service endpoint-test2 in namespace services-1231 exposes endpoints map[] (1.013216258s elapsed)
STEP: Creating pod pod1 in namespace services-1231
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1231 to expose endpoints map[pod1:[80]]
May 22 01:47:56.228: INFO: successfully validated that service endpoint-test2 in namespace services-1231 exposes endpoints map[pod1:[80]] (2.039781197s elapsed)
STEP: Creating pod pod2 in namespace services-1231
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1231 to expose endpoints map[pod1:[80] pod2:[80]]
May 22 01:47:58.318: INFO: successfully validated that service endpoint-test2 in namespace services-1231 exposes endpoints map[pod1:[80] pod2:[80]] (2.070749518s elapsed)
STEP: Deleting pod pod1 in namespace services-1231
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1231 to expose endpoints map[pod2:[80]]
May 22 01:47:59.376: INFO: successfully validated that service endpoint-test2 in namespace services-1231 exposes endpoints map[pod2:[80]] (1.047066931s elapsed)
STEP: Deleting pod pod2 in namespace services-1231
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1231 to expose endpoints map[]
May 22 01:48:00.425: INFO: successfully validated that service endpoint-test2 in namespace services-1231 exposes endpoints map[] (1.019016424s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:48:00.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1231" for this suite.
May 22 01:48:22.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:48:22.640: INFO: namespace services-1231 deletion completed in 22.160743894s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.679 seconds]
[sig-network] Services
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:48:22.640: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-aeb9bfa7-7c33-11e9-8c5e-b202ea6dae39
May 22 01:48:22.828: INFO: Pod name my-hostname-basic-aeb9bfa7-7c33-11e9-8c5e-b202ea6dae39: Found 0 pods out of 1
May 22 01:48:27.832: INFO: Pod name my-hostname-basic-aeb9bfa7-7c33-11e9-8c5e-b202ea6dae39: Found 1 pods out of 1
May 22 01:48:27.833: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-aeb9bfa7-7c33-11e9-8c5e-b202ea6dae39" are running
May 22 01:48:27.839: INFO: Pod "my-hostname-basic-aeb9bfa7-7c33-11e9-8c5e-b202ea6dae39-n5tvt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 01:48:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 01:48:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 01:48:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 01:46:17 +0000 UTC Reason: Message:}])
May 22 01:48:27.839: INFO: Trying to dial the pod
May 22 01:48:32.852: INFO: Controller my-hostname-basic-aeb9bfa7-7c33-11e9-8c5e-b202ea6dae39: Got expected result from replica 1 [my-hostname-basic-aeb9bfa7-7c33-11e9-8c5e-b202ea6dae39-n5tvt]: "my-hostname-basic-aeb9bfa7-7c33-11e9-8c5e-b202ea6dae39-n5tvt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:48:32.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-783" for this suite.
May 22 01:48:38.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:48:39.021: INFO: namespace replication-controller-783 deletion completed in 6.161558821s

• [SLOW TEST:16.381 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:48:39.021: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8873
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
May 22 01:48:39.228: INFO: Waiting up to 5m0s for pod "pod-b87ead56-7c33-11e9-8c5e-b202ea6dae39" in namespace "emptydir-8873" to be "success or failure"
May 22 01:48:39.239: INFO: Pod "pod-b87ead56-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.322277ms
May 22 01:48:41.244: INFO: Pod "pod-b87ead56-7c33-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015201766s
STEP: Saw pod success
May 22 01:48:41.244: INFO: Pod "pod-b87ead56-7c33-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:48:41.247: INFO: Trying to get logs from node worker01 pod pod-b87ead56-7c33-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 01:48:41.294: INFO: Waiting for pod pod-b87ead56-7c33-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:48:41.305: INFO: Pod pod-b87ead56-7c33-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:48:41.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8873" for this suite.
May 22 01:48:47.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:48:47.472: INFO: namespace emptydir-8873 deletion completed in 6.156200843s

• [SLOW TEST:8.451 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:48:47.472: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 01:48:47.659: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd86b658-7c33-11e9-8c5e-b202ea6dae39" in namespace "projected-4666" to be "success or failure"
May 22 01:48:47.677: INFO: Pod "downwardapi-volume-bd86b658-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 17.984381ms
May 22 01:48:49.682: INFO: Pod "downwardapi-volume-bd86b658-7c33-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02252162s
STEP: Saw pod success
May 22 01:48:49.682: INFO: Pod "downwardapi-volume-bd86b658-7c33-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:48:49.686: INFO: Trying to get logs from node worker01 pod downwardapi-volume-bd86b658-7c33-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 01:48:49.730: INFO: Waiting for pod downwardapi-volume-bd86b658-7c33-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:48:49.742: INFO: Pod downwardapi-volume-bd86b658-7c33-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:48:49.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4666" for this suite.
May 22 01:48:55.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:48:55.917: INFO: namespace projected-4666 deletion completed in 6.164084787s

• [SLOW TEST:8.445 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:48:55.917: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-c2914f5f-7c33-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 01:48:56.128: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c293612b-7c33-11e9-8c5e-b202ea6dae39" in namespace "projected-2288" to be "success or failure"
May 22 01:48:56.134: INFO: Pod "pod-projected-configmaps-c293612b-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 5.939319ms
May 22 01:48:58.139: INFO: Pod "pod-projected-configmaps-c293612b-7c33-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010587765s
STEP: Saw pod success
May 22 01:48:58.139: INFO: Pod "pod-projected-configmaps-c293612b-7c33-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:48:58.145: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-c293612b-7c33-11e9-8c5e-b202ea6dae39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 01:48:58.187: INFO: Waiting for pod pod-projected-configmaps-c293612b-7c33-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:48:58.195: INFO: Pod pod-projected-configmaps-c293612b-7c33-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:48:58.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2288" for this suite.
May 22 01:49:04.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:49:04.354: INFO: namespace projected-2288 deletion completed in 6.147476313s

• [SLOW TEST:8.437 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:49:04.354: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-c7973a7d-7c33-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 01:49:04.555: INFO: Waiting up to 5m0s for pod "pod-configmaps-c798a2c6-7c33-11e9-8c5e-b202ea6dae39" in namespace "configmap-701" to be "success or failure"
May 22 01:49:04.567: INFO: Pod "pod-configmaps-c798a2c6-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.302683ms
May 22 01:49:06.575: INFO: Pod "pod-configmaps-c798a2c6-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020014309s
May 22 01:49:08.580: INFO: Pod "pod-configmaps-c798a2c6-7c33-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024888098s
STEP: Saw pod success
May 22 01:49:08.580: INFO: Pod "pod-configmaps-c798a2c6-7c33-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:49:08.590: INFO: Trying to get logs from node worker01 pod pod-configmaps-c798a2c6-7c33-11e9-8c5e-b202ea6dae39 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 01:49:08.626: INFO: Waiting for pod pod-configmaps-c798a2c6-7c33-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:49:08.630: INFO: Pod pod-configmaps-c798a2c6-7c33-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:49:08.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-701" for this suite.
May 22 01:49:14.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:49:14.811: INFO: namespace configmap-701 deletion completed in 6.171128001s

• [SLOW TEST:10.457 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:49:14.811: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
May 22 01:49:17.519: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6271 pod-service-account-ce20c5e9-7c33-11e9-8c5e-b202ea6dae39 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 22 01:49:17.838: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6271 pod-service-account-ce20c5e9-7c33-11e9-8c5e-b202ea6dae39 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 22 01:49:18.176: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6271 pod-service-account-ce20c5e9-7c33-11e9-8c5e-b202ea6dae39 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:49:18.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6271" for this suite.
May 22 01:49:24.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:49:24.714: INFO: namespace svcaccounts-6271 deletion completed in 6.168419038s

• [SLOW TEST:9.903 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:49:24.714: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 01:49:24.897: INFO: Waiting up to 5m0s for pod "downward-api-d3b95c22-7c33-11e9-8c5e-b202ea6dae39" in namespace "downward-api-8502" to be "success or failure"
May 22 01:49:24.911: INFO: Pod "downward-api-d3b95c22-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.7918ms
May 22 01:49:26.915: INFO: Pod "downward-api-d3b95c22-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018002459s
May 22 01:49:28.919: INFO: Pod "downward-api-d3b95c22-7c33-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022112642s
STEP: Saw pod success
May 22 01:49:28.919: INFO: Pod "downward-api-d3b95c22-7c33-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:49:28.925: INFO: Trying to get logs from node worker01 pod downward-api-d3b95c22-7c33-11e9-8c5e-b202ea6dae39 container dapi-container: <nil>
STEP: delete the pod
May 22 01:49:28.968: INFO: Waiting for pod downward-api-d3b95c22-7c33-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:49:28.978: INFO: Pod downward-api-d3b95c22-7c33-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:49:28.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8502" for this suite.
May 22 01:49:35.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:49:35.196: INFO: namespace downward-api-8502 deletion completed in 6.207934046s

• [SLOW TEST:10.482 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:49:35.196: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 01:49:35.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9f808cb-7c33-11e9-8c5e-b202ea6dae39" in namespace "downward-api-3721" to be "success or failure"
May 22 01:49:35.413: INFO: Pod "downwardapi-volume-d9f808cb-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 15.095545ms
May 22 01:49:37.424: INFO: Pod "downwardapi-volume-d9f808cb-7c33-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026323059s
May 22 01:49:39.429: INFO: Pod "downwardapi-volume-d9f808cb-7c33-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030821167s
STEP: Saw pod success
May 22 01:49:39.429: INFO: Pod "downwardapi-volume-d9f808cb-7c33-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:49:39.433: INFO: Trying to get logs from node worker01 pod downwardapi-volume-d9f808cb-7c33-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 01:49:39.478: INFO: Waiting for pod downwardapi-volume-d9f808cb-7c33-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:49:39.482: INFO: Pod downwardapi-volume-d9f808cb-7c33-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:49:39.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3721" for this suite.
May 22 01:49:45.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:49:45.661: INFO: namespace downward-api-3721 deletion completed in 6.167439777s

• [SLOW TEST:10.465 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:49:45.661: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 22 01:49:45.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6859'
May 22 01:49:45.923: INFO: stderr: ""
May 22 01:49:45.923: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 22 01:49:50.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pod e2e-test-nginx-pod --namespace=kubectl-6859 -o json'
May 22 01:49:51.047: INFO: stderr: ""
May 22 01:49:51.047: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-05-22T01:47:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6859\",\n        \"resourceVersion\": \"128745\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6859/pods/e2e-test-nginx-pod\",\n        \"uid\": \"9a3c23a4-7c33-11e9-b253-000c290a0f16\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-pdplt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker01\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-pdplt\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-pdplt\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-22T01:49:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-22T01:49:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-22T01:49:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-22T01:47:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://01b01b46f2fe3e36f95ef8981c87fca8f7454b2d80fdb3cd4e11c478040b29ca\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-22T01:49:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.9.21\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.226\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-22T01:49:45Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 22 01:49:51.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 replace -f - --namespace=kubectl-6859'
May 22 01:49:51.319: INFO: stderr: ""
May 22 01:49:51.319: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
May 22 01:49:51.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete pods e2e-test-nginx-pod --namespace=kubectl-6859'
May 22 01:50:02.323: INFO: stderr: ""
May 22 01:50:02.323: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:50:02.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6859" for this suite.
May 22 01:50:08.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:50:08.509: INFO: namespace kubectl-6859 deletion completed in 6.177763051s

• [SLOW TEST:22.848 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:50:08.509: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 22 01:50:12.762: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:12.773: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:14.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:14.784: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:16.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:16.784: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:18.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:18.779: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:20.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:20.788: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:22.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:22.783: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:24.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:24.783: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:26.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:26.784: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:28.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:28.780: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:30.774: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:30.781: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:32.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:32.786: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:34.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:34.783: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:36.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:36.780: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:38.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:38.788: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:40.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:40.784: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 01:50:42.773: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 01:50:42.787: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:50:42.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6460" for this suite.
May 22 01:51:04.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:51:04.988: INFO: namespace container-lifecycle-hook-6460 deletion completed in 22.168905838s

• [SLOW TEST:56.479 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:51:04.988: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-0f7d644d-7c34-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 01:51:05.200: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f80324c-7c34-11e9-8c5e-b202ea6dae39" in namespace "projected-4958" to be "success or failure"
May 22 01:51:05.208: INFO: Pod "pod-projected-configmaps-0f80324c-7c34-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.142892ms
May 22 01:51:07.212: INFO: Pod "pod-projected-configmaps-0f80324c-7c34-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01258804s
STEP: Saw pod success
May 22 01:51:07.212: INFO: Pod "pod-projected-configmaps-0f80324c-7c34-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:51:07.216: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-0f80324c-7c34-11e9-8c5e-b202ea6dae39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 01:51:07.260: INFO: Waiting for pod pod-projected-configmaps-0f80324c-7c34-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:51:07.268: INFO: Pod pod-projected-configmaps-0f80324c-7c34-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:51:07.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4958" for this suite.
May 22 01:51:13.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:51:13.447: INFO: namespace projected-4958 deletion completed in 6.169918656s

• [SLOW TEST:8.460 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:51:13.448: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2013
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 01:51:13.629: INFO: Waiting up to 5m0s for pod "downward-api-14880e42-7c34-11e9-8c5e-b202ea6dae39" in namespace "downward-api-2013" to be "success or failure"
May 22 01:51:13.639: INFO: Pod "downward-api-14880e42-7c34-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.254659ms
May 22 01:51:15.645: INFO: Pod "downward-api-14880e42-7c34-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015687476s
STEP: Saw pod success
May 22 01:51:15.645: INFO: Pod "downward-api-14880e42-7c34-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:51:15.649: INFO: Trying to get logs from node worker01 pod downward-api-14880e42-7c34-11e9-8c5e-b202ea6dae39 container dapi-container: <nil>
STEP: delete the pod
May 22 01:51:15.692: INFO: Waiting for pod downward-api-14880e42-7c34-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:51:15.697: INFO: Pod downward-api-14880e42-7c34-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:51:15.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2013" for this suite.
May 22 01:51:21.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:51:21.879: INFO: namespace downward-api-2013 deletion completed in 6.172015706s

• [SLOW TEST:8.431 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:51:21.879: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-2614
STEP: Creating secret with name secret-test-1991bb3c-7c34-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 01:51:22.246: INFO: Waiting up to 5m0s for pod "pod-secrets-19aba9c6-7c34-11e9-8c5e-b202ea6dae39" in namespace "secrets-867" to be "success or failure"
May 22 01:51:22.253: INFO: Pod "pod-secrets-19aba9c6-7c34-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.334345ms
May 22 01:51:24.258: INFO: Pod "pod-secrets-19aba9c6-7c34-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01172773s
STEP: Saw pod success
May 22 01:51:24.258: INFO: Pod "pod-secrets-19aba9c6-7c34-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:51:24.266: INFO: Trying to get logs from node worker01 pod pod-secrets-19aba9c6-7c34-11e9-8c5e-b202ea6dae39 container secret-volume-test: <nil>
STEP: delete the pod
May 22 01:51:24.308: INFO: Waiting for pod pod-secrets-19aba9c6-7c34-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:51:24.312: INFO: Pod pod-secrets-19aba9c6-7c34-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:51:24.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-867" for this suite.
May 22 01:51:30.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:51:30.483: INFO: namespace secrets-867 deletion completed in 6.159684132s
STEP: Destroying namespace "secret-namespace-2614" for this suite.
May 22 01:51:36.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:51:36.640: INFO: namespace secret-namespace-2614 deletion completed in 6.156571617s

• [SLOW TEST:14.761 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:51:36.640: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:51:36.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5417" for this suite.
May 22 01:51:42.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:51:43.002: INFO: namespace services-5417 deletion completed in 6.163123522s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.363 seconds]
[sig-network] Services
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:51:43.003: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 22 01:51:47.257: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 22 01:51:47.265: INFO: Pod pod-with-poststart-http-hook still exists
May 22 01:51:49.266: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 22 01:51:49.272: INFO: Pod pod-with-poststart-http-hook still exists
May 22 01:51:51.266: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 22 01:51:51.272: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:51:51.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9029" for this suite.
May 22 01:52:13.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:52:13.469: INFO: namespace container-lifecycle-hook-9029 deletion completed in 22.187811768s

• [SLOW TEST:30.466 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:52:13.469: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 22 01:52:13.641: INFO: Waiting up to 5m0s for pod "pod-384db523-7c34-11e9-8c5e-b202ea6dae39" in namespace "emptydir-5713" to be "success or failure"
May 22 01:52:13.649: INFO: Pod "pod-384db523-7c34-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 7.883443ms
May 22 01:52:15.654: INFO: Pod "pod-384db523-7c34-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01277096s
STEP: Saw pod success
May 22 01:52:15.654: INFO: Pod "pod-384db523-7c34-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:52:15.659: INFO: Trying to get logs from node worker01 pod pod-384db523-7c34-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 01:52:15.695: INFO: Waiting for pod pod-384db523-7c34-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:52:15.704: INFO: Pod pod-384db523-7c34-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:52:15.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5713" for this suite.
May 22 01:52:21.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:52:21.892: INFO: namespace emptydir-5713 deletion completed in 6.173740751s

• [SLOW TEST:8.423 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:52:21.892: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:52:24.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7314" for this suite.
May 22 01:53:22.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:53:22.295: INFO: namespace kubelet-test-7314 deletion completed in 58.166140205s

• [SLOW TEST:60.403 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:53:22.295: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-6153f894-7c34-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 01:53:22.482: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-615565dc-7c34-11e9-8c5e-b202ea6dae39" in namespace "projected-1191" to be "success or failure"
May 22 01:53:22.493: INFO: Pod "pod-projected-secrets-615565dc-7c34-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 11.175969ms
May 22 01:53:24.498: INFO: Pod "pod-projected-secrets-615565dc-7c34-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016411224s
May 22 01:53:26.508: INFO: Pod "pod-projected-secrets-615565dc-7c34-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025612837s
STEP: Saw pod success
May 22 01:53:26.508: INFO: Pod "pod-projected-secrets-615565dc-7c34-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:53:26.523: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-615565dc-7c34-11e9-8c5e-b202ea6dae39 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 01:53:26.559: INFO: Waiting for pod pod-projected-secrets-615565dc-7c34-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:53:26.567: INFO: Pod pod-projected-secrets-615565dc-7c34-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:53:26.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1191" for this suite.
May 22 01:53:32.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:53:32.723: INFO: namespace projected-1191 deletion completed in 6.148415188s

• [SLOW TEST:10.428 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:53:32.723: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4931
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 01:53:32.915: INFO: Waiting up to 5m0s for pod "downwardapi-volume-678c859b-7c34-11e9-8c5e-b202ea6dae39" in namespace "projected-4931" to be "success or failure"
May 22 01:53:32.926: INFO: Pod "downwardapi-volume-678c859b-7c34-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.832583ms
May 22 01:53:34.930: INFO: Pod "downwardapi-volume-678c859b-7c34-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015557641s
STEP: Saw pod success
May 22 01:53:34.930: INFO: Pod "downwardapi-volume-678c859b-7c34-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:53:34.934: INFO: Trying to get logs from node worker01 pod downwardapi-volume-678c859b-7c34-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 01:53:34.973: INFO: Waiting for pod downwardapi-volume-678c859b-7c34-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:53:34.980: INFO: Pod downwardapi-volume-678c859b-7c34-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:53:34.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4931" for this suite.
May 22 01:53:41.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:53:41.160: INFO: namespace projected-4931 deletion completed in 6.168923714s

• [SLOW TEST:8.438 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:53:41.161: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2162
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 01:53:41.364: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"33531074-7c34-11e9-b253-000c290a0f16", Controller:(*bool)(0xc0017284fa), BlockOwnerDeletion:(*bool)(0xc0017284fb)}}
May 22 01:53:41.380: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"334f31bf-7c34-11e9-b253-000c290a0f16", Controller:(*bool)(0xc001ca1d22), BlockOwnerDeletion:(*bool)(0xc001ca1d23)}}
May 22 01:53:41.393: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3350e442-7c34-11e9-b253-000c290a0f16", Controller:(*bool)(0xc001ca1f0a), BlockOwnerDeletion:(*bool)(0xc001ca1f0b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:53:46.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2162" for this suite.
May 22 01:53:52.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:53:52.590: INFO: namespace gc-2162 deletion completed in 6.16263491s

• [SLOW TEST:11.430 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:53:52.590: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1740
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 22 01:53:52.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-1740'
May 22 01:53:53.080: INFO: stderr: ""
May 22 01:53:53.080: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 22 01:53:54.084: INFO: Selector matched 1 pods for map[app:redis]
May 22 01:53:54.084: INFO: Found 0 / 1
May 22 01:53:55.086: INFO: Selector matched 1 pods for map[app:redis]
May 22 01:53:55.086: INFO: Found 1 / 1
May 22 01:53:55.086: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 22 01:53:55.097: INFO: Selector matched 1 pods for map[app:redis]
May 22 01:53:55.097: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 22 01:53:55.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 patch pod redis-master-4j6r2 --namespace=kubectl-1740 -p {"metadata":{"annotations":{"x":"y"}}}'
May 22 01:53:55.189: INFO: stderr: ""
May 22 01:53:55.189: INFO: stdout: "pod/redis-master-4j6r2 patched\n"
STEP: checking annotations
May 22 01:53:55.196: INFO: Selector matched 1 pods for map[app:redis]
May 22 01:53:55.196: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:53:55.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1740" for this suite.
May 22 01:54:17.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:54:17.397: INFO: namespace kubectl-1740 deletion completed in 22.189224172s

• [SLOW TEST:24.807 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:54:17.397: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
May 22 01:54:17.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 api-versions'
May 22 01:54:17.629: INFO: stderr: ""
May 22 01:54:17.629: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:54:17.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3615" for this suite.
May 22 01:54:23.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:54:23.823: INFO: namespace kubectl-3615 deletion completed in 6.188798141s

• [SLOW TEST:6.426 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:54:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-8m4t
STEP: Creating a pod to test atomic-volume-subpath
May 22 01:54:24.025: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8m4t" in namespace "subpath-7490" to be "success or failure"
May 22 01:54:24.035: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Pending", Reason="", readiness=false. Elapsed: 9.723894ms
May 22 01:54:26.042: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 2.016788551s
May 22 01:54:28.052: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 4.027018178s
May 22 01:54:30.064: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 6.038391603s
May 22 01:54:32.068: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 8.042186448s
May 22 01:54:34.078: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 10.052266707s
May 22 01:54:36.086: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 12.060770696s
May 22 01:54:38.099: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 14.073903323s
May 22 01:54:40.105: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 16.079615074s
May 22 01:54:42.119: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 18.093584083s
May 22 01:54:44.125: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Running", Reason="", readiness=true. Elapsed: 20.099432746s
May 22 01:54:46.129: INFO: Pod "pod-subpath-test-configmap-8m4t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.10335094s
STEP: Saw pod success
May 22 01:54:46.129: INFO: Pod "pod-subpath-test-configmap-8m4t" satisfied condition "success or failure"
May 22 01:54:46.137: INFO: Trying to get logs from node worker01 pod pod-subpath-test-configmap-8m4t container test-container-subpath-configmap-8m4t: <nil>
STEP: delete the pod
May 22 01:54:46.179: INFO: Waiting for pod pod-subpath-test-configmap-8m4t to disappear
May 22 01:54:46.186: INFO: Pod pod-subpath-test-configmap-8m4t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8m4t
May 22 01:54:46.186: INFO: Deleting pod "pod-subpath-test-configmap-8m4t" in namespace "subpath-7490"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:54:46.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7490" for this suite.
May 22 01:54:52.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:54:52.386: INFO: namespace subpath-7490 deletion completed in 6.17940693s

• [SLOW TEST:28.563 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:54:52.386: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 22 01:54:52.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1708'
May 22 01:54:52.640: INFO: stderr: ""
May 22 01:54:52.641: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
May 22 01:54:52.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete pods e2e-test-nginx-pod --namespace=kubectl-1708'
May 22 01:55:02.320: INFO: stderr: ""
May 22 01:55:02.320: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:55:02.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1708" for this suite.
May 22 01:55:08.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:55:08.489: INFO: namespace kubectl-1708 deletion completed in 6.159346246s

• [SLOW TEST:16.103 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:55:08.489: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 22 01:55:08.669: INFO: Pod name pod-release: Found 0 pods out of 1
May 22 01:55:13.676: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:55:14.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6498" for this suite.
May 22 01:55:20.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:55:20.898: INFO: namespace replication-controller-6498 deletion completed in 6.179298503s

• [SLOW TEST:12.409 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:55:20.899: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-a8066804-7c34-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 01:55:21.110: INFO: Waiting up to 5m0s for pod "pod-configmaps-a8089b98-7c34-11e9-8c5e-b202ea6dae39" in namespace "configmap-9777" to be "success or failure"
May 22 01:55:21.123: INFO: Pod "pod-configmaps-a8089b98-7c34-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.961847ms
May 22 01:55:23.134: INFO: Pod "pod-configmaps-a8089b98-7c34-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024326506s
STEP: Saw pod success
May 22 01:55:23.135: INFO: Pod "pod-configmaps-a8089b98-7c34-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:55:23.154: INFO: Trying to get logs from node worker01 pod pod-configmaps-a8089b98-7c34-11e9-8c5e-b202ea6dae39 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 01:55:23.203: INFO: Waiting for pod pod-configmaps-a8089b98-7c34-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:55:23.212: INFO: Pod pod-configmaps-a8089b98-7c34-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:55:23.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9777" for this suite.
May 22 01:55:29.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:55:29.382: INFO: namespace configmap-9777 deletion completed in 6.15826602s

• [SLOW TEST:8.483 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:55:29.382: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1138
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 22 01:55:34.101: INFO: Successfully updated pod "annotationupdatead146e88-7c34-11e9-8c5e-b202ea6dae39"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:55:36.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1138" for this suite.
May 22 01:55:58.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:55:58.306: INFO: namespace downward-api-1138 deletion completed in 22.17485023s

• [SLOW TEST:28.924 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:55:58.306: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
May 22 01:56:00.538: INFO: Pod pod-hostip-be536985-7c34-11e9-8c5e-b202ea6dae39 has hostIP: 192.168.9.21
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:56:00.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1878" for this suite.
May 22 01:56:20.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:56:20.696: INFO: namespace pods-1878 deletion completed in 20.15278612s

• [SLOW TEST:22.390 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:56:20.696: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 01:56:20.877: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbaad00e-7c34-11e9-8c5e-b202ea6dae39" in namespace "downward-api-1754" to be "success or failure"
May 22 01:56:20.888: INFO: Pod "downwardapi-volume-cbaad00e-7c34-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.51245ms
May 22 01:56:22.893: INFO: Pod "downwardapi-volume-cbaad00e-7c34-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015383783s
STEP: Saw pod success
May 22 01:56:22.893: INFO: Pod "downwardapi-volume-cbaad00e-7c34-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 01:56:22.899: INFO: Trying to get logs from node worker01 pod downwardapi-volume-cbaad00e-7c34-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 01:56:22.940: INFO: Waiting for pod downwardapi-volume-cbaad00e-7c34-11e9-8c5e-b202ea6dae39 to disappear
May 22 01:56:22.945: INFO: Pod downwardapi-volume-cbaad00e-7c34-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:56:22.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1754" for this suite.
May 22 01:56:28.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:56:29.114: INFO: namespace downward-api-1754 deletion completed in 6.158858424s

• [SLOW TEST:8.418 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:56:29.114: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 22 01:56:31.862: INFO: Successfully updated pod "annotationupdated0af1afc-7c34-11e9-8c5e-b202ea6dae39"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:56:33.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9616" for this suite.
May 22 01:56:55.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 01:56:56.091: INFO: namespace projected-9616 deletion completed in 22.174407433s

• [SLOW TEST:26.977 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 01:56:56.091: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4428
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 22 01:56:56.566: INFO: Pod name wrapped-volume-race-e0efdfa7-7c34-11e9-8c5e-b202ea6dae39: Found 0 pods out of 5
May 22 01:57:01.573: INFO: Pod name wrapped-volume-race-e0efdfa7-7c34-11e9-8c5e-b202ea6dae39: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e0efdfa7-7c34-11e9-8c5e-b202ea6dae39 in namespace emptydir-wrapper-4428, will wait for the garbage collector to delete the pods
May 22 01:57:13.695: INFO: Deleting ReplicationController wrapped-volume-race-e0efdfa7-7c34-11e9-8c5e-b202ea6dae39 took: 8.920758ms
May 22 01:57:13.996: INFO: Terminating ReplicationController wrapped-volume-race-e0efdfa7-7c34-11e9-8c5e-b202ea6dae39 pods took: 300.884889ms
STEP: Creating RC which spawns configmap-volume pods
May 22 01:57:52.419: INFO: Pod name wrapped-volume-race-0239d57b-7c35-11e9-8c5e-b202ea6dae39: Found 0 pods out of 5
May 22 01:57:57.425: INFO: Pod name wrapped-volume-race-0239d57b-7c35-11e9-8c5e-b202ea6dae39: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0239d57b-7c35-11e9-8c5e-b202ea6dae39 in namespace emptydir-wrapper-4428, will wait for the garbage collector to delete the pods
May 22 01:58:09.559: INFO: Deleting ReplicationController wrapped-volume-race-0239d57b-7c35-11e9-8c5e-b202ea6dae39 took: 22.700533ms
May 22 01:58:09.861: INFO: Terminating ReplicationController wrapped-volume-race-0239d57b-7c35-11e9-8c5e-b202ea6dae39 pods took: 301.811442ms
STEP: Creating RC which spawns configmap-volume pods
May 22 01:58:52.380: INFO: Pod name wrapped-volume-race-25f7c733-7c35-11e9-8c5e-b202ea6dae39: Found 0 pods out of 5
May 22 01:58:57.386: INFO: Pod name wrapped-volume-race-25f7c733-7c35-11e9-8c5e-b202ea6dae39: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-25f7c733-7c35-11e9-8c5e-b202ea6dae39 in namespace emptydir-wrapper-4428, will wait for the garbage collector to delete the pods
May 22 01:59:07.473: INFO: Deleting ReplicationController wrapped-volume-race-25f7c733-7c35-11e9-8c5e-b202ea6dae39 took: 7.853727ms
May 22 01:59:07.774: INFO: Terminating ReplicationController wrapped-volume-race-25f7c733-7c35-11e9-8c5e-b202ea6dae39 pods took: 300.705027ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 01:59:53.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4428" for this suite.
May 22 01:59:59.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:00:00.059: INFO: namespace emptydir-wrapper-4428 deletion completed in 6.177072942s

• [SLOW TEST:183.967 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:00:00.059: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:00:00.248: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 22 02:00:00.268: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:00.268: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:00.268: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:00.276: INFO: Number of nodes with available pods: 0
May 22 02:00:00.276: INFO: Node worker01 is running more than one daemon pod
May 22 02:00:01.281: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:01.282: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:01.282: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:01.286: INFO: Number of nodes with available pods: 0
May 22 02:00:01.286: INFO: Node worker01 is running more than one daemon pod
May 22 02:00:02.286: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:02.286: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:02.286: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:02.296: INFO: Number of nodes with available pods: 1
May 22 02:00:02.296: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 22 02:00:02.362: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:02.370: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:02.370: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:02.370: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:03.376: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:03.382: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:03.382: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:03.382: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:04.375: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:04.381: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:04.381: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:04.381: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:05.375: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:05.375: INFO: Pod daemon-set-vt5jm is not available
May 22 02:00:05.381: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:05.381: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:05.381: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:06.375: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:06.375: INFO: Pod daemon-set-vt5jm is not available
May 22 02:00:06.380: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:06.381: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:06.381: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:07.381: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:07.381: INFO: Pod daemon-set-vt5jm is not available
May 22 02:00:07.390: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:07.390: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:07.390: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:08.374: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:08.374: INFO: Pod daemon-set-vt5jm is not available
May 22 02:00:08.379: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:08.379: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:08.379: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:09.374: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:09.374: INFO: Pod daemon-set-vt5jm is not available
May 22 02:00:09.379: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:09.379: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:09.379: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:10.378: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:10.378: INFO: Pod daemon-set-vt5jm is not available
May 22 02:00:10.388: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:10.388: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:10.388: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:11.374: INFO: Wrong image for pod: daemon-set-vt5jm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 22 02:00:11.374: INFO: Pod daemon-set-vt5jm is not available
May 22 02:00:11.379: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:11.379: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:11.379: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:12.386: INFO: Pod daemon-set-88pxj is not available
May 22 02:00:12.391: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:12.392: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:12.392: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 22 02:00:12.399: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:12.399: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:12.399: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:12.407: INFO: Number of nodes with available pods: 0
May 22 02:00:12.407: INFO: Node worker01 is running more than one daemon pod
May 22 02:00:13.414: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:13.414: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:13.414: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:13.418: INFO: Number of nodes with available pods: 0
May 22 02:00:13.418: INFO: Node worker01 is running more than one daemon pod
May 22 02:00:14.412: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:14.412: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:14.412: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:00:14.416: INFO: Number of nodes with available pods: 1
May 22 02:00:14.416: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2821, will wait for the garbage collector to delete the pods
May 22 02:00:14.498: INFO: Deleting DaemonSet.extensions daemon-set took: 9.638398ms
May 22 02:00:14.799: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.90335ms
May 22 02:00:22.403: INFO: Number of nodes with available pods: 0
May 22 02:00:22.403: INFO: Number of running nodes: 0, number of available pods: 0
May 22 02:00:22.406: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2821/daemonsets","resourceVersion":"131499"},"items":null}

May 22 02:00:22.411: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2821/pods","resourceVersion":"131499"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:00:22.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2821" for this suite.
May 22 02:00:28.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:00:28.573: INFO: namespace daemonsets-2821 deletion completed in 6.148395238s

• [SLOW TEST:28.514 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:00:28.573: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-5f689e34-7c35-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:00:28.754: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f69cfd7-7c35-11e9-8c5e-b202ea6dae39" in namespace "projected-3036" to be "success or failure"
May 22 02:00:28.763: INFO: Pod "pod-projected-secrets-5f69cfd7-7c35-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.707767ms
May 22 02:00:30.774: INFO: Pod "pod-projected-secrets-5f69cfd7-7c35-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019461384s
STEP: Saw pod success
May 22 02:00:30.774: INFO: Pod "pod-projected-secrets-5f69cfd7-7c35-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:00:30.784: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-5f69cfd7-7c35-11e9-8c5e-b202ea6dae39 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 02:00:30.844: INFO: Waiting for pod pod-projected-secrets-5f69cfd7-7c35-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:00:30.848: INFO: Pod pod-projected-secrets-5f69cfd7-7c35-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:00:30.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3036" for this suite.
May 22 02:00:36.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:00:37.037: INFO: namespace projected-3036 deletion completed in 6.180421803s

• [SLOW TEST:8.464 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:00:37.037: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9201
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
May 22 02:00:37.212: INFO: Waiting up to 5m0s for pod "pod-647498cf-7c35-11e9-8c5e-b202ea6dae39" in namespace "emptydir-9201" to be "success or failure"
May 22 02:00:37.219: INFO: Pod "pod-647498cf-7c35-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 7.18273ms
May 22 02:00:39.230: INFO: Pod "pod-647498cf-7c35-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018131188s
May 22 02:00:41.236: INFO: Pod "pod-647498cf-7c35-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024234723s
STEP: Saw pod success
May 22 02:00:41.236: INFO: Pod "pod-647498cf-7c35-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:00:41.242: INFO: Trying to get logs from node worker01 pod pod-647498cf-7c35-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:00:41.279: INFO: Waiting for pod pod-647498cf-7c35-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:00:41.293: INFO: Pod pod-647498cf-7c35-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:00:41.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9201" for this suite.
May 22 02:00:47.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:00:47.475: INFO: namespace emptydir-9201 deletion completed in 6.174526966s

• [SLOW TEST:10.438 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:00:47.475: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 22 02:00:53.718: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 22 02:00:53.722: INFO: Pod pod-with-prestop-http-hook still exists
May 22 02:00:55.723: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 22 02:00:55.734: INFO: Pod pod-with-prestop-http-hook still exists
May 22 02:00:57.723: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 22 02:00:57.732: INFO: Pod pod-with-prestop-http-hook still exists
May 22 02:00:59.723: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 22 02:00:59.729: INFO: Pod pod-with-prestop-http-hook still exists
May 22 02:01:01.723: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 22 02:01:01.734: INFO: Pod pod-with-prestop-http-hook still exists
May 22 02:01:03.723: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 22 02:01:03.727: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:01:03.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8264" for this suite.
May 22 02:01:25.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:01:25.917: INFO: namespace container-lifecycle-hook-8264 deletion completed in 22.174434326s

• [SLOW TEST:38.441 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:01:25.917: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7414
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-7414
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7414
May 22 02:01:26.120: INFO: Found 0 stateful pods, waiting for 1
May 22 02:01:36.131: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 22 02:01:36.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:01:36.441: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:01:36.441: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:01:36.441: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 02:01:36.445: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 22 02:01:46.450: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 22 02:01:46.450: INFO: Waiting for statefulset status.replicas updated to 0
May 22 02:01:46.468: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:01:46.468: INFO: ss-0  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:01:46.468: INFO: 
May 22 02:01:46.468: INFO: StatefulSet ss has not reached scale 3, at 1
May 22 02:01:47.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995071321s
May 22 02:01:48.494: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989581815s
May 22 02:01:49.508: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96868822s
May 22 02:01:50.515: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.954990999s
May 22 02:01:51.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.948220077s
May 22 02:01:52.523: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.94361001s
May 22 02:01:53.530: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.938940419s
May 22 02:01:54.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.9323749s
May 22 02:01:55.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.273492ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7414
May 22 02:01:56.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:01:56.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 02:01:56.847: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 02:01:56.847: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 02:01:56.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:01:57.186: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 22 02:01:57.186: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 02:01:57.186: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 02:01:57.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:01:57.519: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 22 02:01:57.519: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 02:01:57.519: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 02:01:57.528: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 02:01:57.528: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 02:01:57.528: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 22 02:01:57.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:01:57.844: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:01:57.844: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:01:57.844: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 02:01:57.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:01:58.194: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:01:58.194: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:01:58.194: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 02:01:58.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:01:58.645: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:01:58.645: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:01:58.645: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 02:01:58.645: INFO: Waiting for statefulset status.replicas updated to 0
May 22 02:01:58.649: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 22 02:02:08.657: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 22 02:02:08.657: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 22 02:02:08.657: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 22 02:02:08.673: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:08.673: INFO: ss-0  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:08.673: INFO: ss-1  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:08.673: INFO: ss-2  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:08.673: INFO: 
May 22 02:02:08.673: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 02:02:09.677: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:09.677: INFO: ss-0  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:09.677: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:09.677: INFO: ss-2  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:09.677: INFO: 
May 22 02:02:09.677: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 02:02:10.688: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:10.688: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:10.689: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:10.689: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:10.689: INFO: 
May 22 02:02:10.689: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 02:02:11.694: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:11.694: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:11.694: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:11.694: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:11.694: INFO: 
May 22 02:02:11.694: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 02:02:12.698: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:12.698: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:12.698: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:12.698: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:12.698: INFO: 
May 22 02:02:12.698: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 02:02:13.703: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:13.703: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:13.703: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:13.703: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:13.703: INFO: 
May 22 02:02:13.703: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 02:02:14.711: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:14.711: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:14.711: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:14.711: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:14.711: INFO: 
May 22 02:02:14.711: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 02:02:15.722: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:15.722: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:15.722: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:15.722: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:15.723: INFO: 
May 22 02:02:15.723: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 02:02:16.727: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:16.727: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:16.727: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:16.727: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:16.727: INFO: 
May 22 02:02:16.727: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 02:02:17.731: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
May 22 02:02:17.731: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:32 +0000 UTC  }]
May 22 02:02:17.731: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:17.731: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:01:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:00:54 +0000 UTC  }]
May 22 02:02:17.731: INFO: 
May 22 02:02:17.731: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7414
May 22 02:02:18.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:02:18.854: INFO: rc: 1
May 22 02:02:18.854: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001254ab0 exit status 1 <nil> <nil> true [0xc002b025c0 0xc002b025f8 0xc002b02628] [0xc002b025c0 0xc002b025f8 0xc002b02628] [0xc002b025e8 0xc002b02618] [0x9c00a0 0x9c00a0] 0xc00277faa0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 22 02:02:28.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:02:28.927: INFO: rc: 1
May 22 02:02:28.927: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d09800 exit status 1 <nil> <nil> true [0xc002a361f0 0xc002a36208 0xc002a36220] [0xc002a361f0 0xc002a36208 0xc002a36220] [0xc002a36200 0xc002a36218] [0x9c00a0 0x9c00a0] 0xc001f72780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:02:38.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:02:39.013: INFO: rc: 1
May 22 02:02:39.013: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001254e10 exit status 1 <nil> <nil> true [0xc002b02648 0xc002b02680 0xc002b026d0] [0xc002b02648 0xc002b02680 0xc002b026d0] [0xc002b02670 0xc002b026a0] [0x9c00a0 0x9c00a0] 0xc00277fe00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:02:49.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:02:49.083: INFO: rc: 1
May 22 02:02:49.083: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d09b60 exit status 1 <nil> <nil> true [0xc002a36228 0xc002a36240 0xc002a36258] [0xc002a36228 0xc002a36240 0xc002a36258] [0xc002a36238 0xc002a36250] [0x9c00a0 0x9c00a0] 0xc001f72c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:02:59.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:02:59.152: INFO: rc: 1
May 22 02:02:59.152: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001255170 exit status 1 <nil> <nil> true [0xc002b026e8 0xc002b02758 0xc002b02798] [0xc002b026e8 0xc002b02758 0xc002b02798] [0xc002b02748 0xc002b02780] [0x9c00a0 0x9c00a0] 0xc000b64180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:03:09.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:03:09.220: INFO: rc: 1
May 22 02:03:09.221: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001255620 exit status 1 <nil> <nil> true [0xc002b027a8 0xc002b027e8 0xc002b02820] [0xc002b027a8 0xc002b027e8 0xc002b02820] [0xc002b027d0 0xc002b02810] [0x9c00a0 0x9c00a0] 0xc000b644e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:03:19.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:03:19.291: INFO: rc: 1
May 22 02:03:19.291: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001255980 exit status 1 <nil> <nil> true [0xc002b02838 0xc002b028a0 0xc002b02950] [0xc002b02838 0xc002b028a0 0xc002b02950] [0xc002b02870 0xc002b02908] [0x9c00a0 0x9c00a0] 0xc000b64840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:03:29.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:03:29.364: INFO: rc: 1
May 22 02:03:29.364: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d09ef0 exit status 1 <nil> <nil> true [0xc002a36260 0xc002a36278 0xc002a36290] [0xc002a36260 0xc002a36278 0xc002a36290] [0xc002a36270 0xc002a36288] [0x9c00a0 0x9c00a0] 0xc001f72fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:03:39.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:03:39.452: INFO: rc: 1
May 22 02:03:39.452: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000db4390 exit status 1 <nil> <nil> true [0xc002a36298 0xc002a362b0 0xc002a362c8] [0xc002a36298 0xc002a362b0 0xc002a362c8] [0xc002a362a8 0xc002a362c0] [0x9c00a0 0x9c00a0] 0xc001f73320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:03:49.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:03:49.523: INFO: rc: 1
May 22 02:03:49.523: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000db4690 exit status 1 <nil> <nil> true [0xc002a362d8 0xc002a362f0 0xc002a36308] [0xc002a362d8 0xc002a362f0 0xc002a36308] [0xc002a362e8 0xc002a36300] [0x9c00a0 0x9c00a0] 0xc001f73620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:03:59.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:03:59.608: INFO: rc: 1
May 22 02:03:59.608: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001422330 exit status 1 <nil> <nil> true [0xc002a36018 0xc002a36030 0xc002a36048] [0xc002a36018 0xc002a36030 0xc002a36048] [0xc002a36028 0xc002a36040] [0x9c00a0 0x9c00a0] 0xc00277e360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:04:09.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:04:09.699: INFO: rc: 1
May 22 02:04:09.699: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d08300 exit status 1 <nil> <nil> true [0xc002b02018 0xc002b02030 0xc002b02048] [0xc002b02018 0xc002b02030 0xc002b02048] [0xc002b02028 0xc002b02040] [0x9c00a0 0x9c00a0] 0xc0029fe4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:04:19.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:04:19.783: INFO: rc: 1
May 22 02:04:19.783: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d087b0 exit status 1 <nil> <nil> true [0xc002b02050 0xc002b02068 0xc002b02080] [0xc002b02050 0xc002b02068 0xc002b02080] [0xc002b02060 0xc002b02078] [0x9c00a0 0x9c00a0] 0xc0029fe8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:04:29.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:04:29.851: INFO: rc: 1
May 22 02:04:29.851: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014226c0 exit status 1 <nil> <nil> true [0xc002a36050 0xc002a36068 0xc002a36080] [0xc002a36050 0xc002a36068 0xc002a36080] [0xc002a36060 0xc002a36078] [0x9c00a0 0x9c00a0] 0xc00277e6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:04:39.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:04:39.956: INFO: rc: 1
May 22 02:04:39.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d08b10 exit status 1 <nil> <nil> true [0xc002b02088 0xc002b020a0 0xc002b020b8] [0xc002b02088 0xc002b020a0 0xc002b020b8] [0xc002b02098 0xc002b020b0] [0x9c00a0 0x9c00a0] 0xc0029fed80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:04:49.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:04:50.022: INFO: rc: 1
May 22 02:04:50.022: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001422a50 exit status 1 <nil> <nil> true [0xc002a36088 0xc002a360a0 0xc002a360b8] [0xc002a36088 0xc002a360a0 0xc002a360b8] [0xc002a36098 0xc002a360b0] [0x9c00a0 0x9c00a0] 0xc00277ea20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:05:00.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:05:00.111: INFO: rc: 1
May 22 02:05:00.111: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d08e70 exit status 1 <nil> <nil> true [0xc002b020c0 0xc002b020d8 0xc002b020f0] [0xc002b020c0 0xc002b020d8 0xc002b020f0] [0xc002b020d0 0xc002b020e8] [0x9c00a0 0x9c00a0] 0xc0029ff3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:05:10.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:05:10.192: INFO: rc: 1
May 22 02:05:10.192: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d091a0 exit status 1 <nil> <nil> true [0xc002b020f8 0xc002b02110 0xc002b02128] [0xc002b020f8 0xc002b02110 0xc002b02128] [0xc002b02108 0xc002b02120] [0x9c00a0 0x9c00a0] 0xc0029ff920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:05:20.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:05:20.290: INFO: rc: 1
May 22 02:05:20.290: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d09500 exit status 1 <nil> <nil> true [0xc002b02130 0xc002b02148 0xc002b02160] [0xc002b02130 0xc002b02148 0xc002b02160] [0xc002b02140 0xc002b02158] [0x9c00a0 0x9c00a0] 0xc0029ffc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:05:30.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:05:30.357: INFO: rc: 1
May 22 02:05:30.357: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d09860 exit status 1 <nil> <nil> true [0xc002b02168 0xc002b02180 0xc002b02198] [0xc002b02168 0xc002b02180 0xc002b02198] [0xc002b02178 0xc002b02190] [0x9c00a0 0x9c00a0] 0xc0028ca060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:05:40.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:05:40.437: INFO: rc: 1
May 22 02:05:40.437: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d09bc0 exit status 1 <nil> <nil> true [0xc002b021a0 0xc002b021b8 0xc002b021d0] [0xc002b021a0 0xc002b021b8 0xc002b021d0] [0xc002b021b0 0xc002b021c8] [0x9c00a0 0x9c00a0] 0xc0028ca420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:05:50.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:05:50.518: INFO: rc: 1
May 22 02:05:50.518: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001422300 exit status 1 <nil> <nil> true [0xc002a36020 0xc002a36038 0xc002a36050] [0xc002a36020 0xc002a36038 0xc002a36050] [0xc002a36030 0xc002a36048] [0x9c00a0 0x9c00a0] 0xc0029fe4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:06:00.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:06:00.595: INFO: rc: 1
May 22 02:06:00.595: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001422660 exit status 1 <nil> <nil> true [0xc002a36058 0xc002a36070 0xc002a36088] [0xc002a36058 0xc002a36070 0xc002a36088] [0xc002a36068 0xc002a36080] [0x9c00a0 0x9c00a0] 0xc0029fe8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:06:10.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:06:10.660: INFO: rc: 1
May 22 02:06:10.660: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014229f0 exit status 1 <nil> <nil> true [0xc002a36090 0xc002a360a8 0xc002a360c0] [0xc002a36090 0xc002a360a8 0xc002a360c0] [0xc002a360a0 0xc002a360b8] [0x9c00a0 0x9c00a0] 0xc0029fed80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:06:20.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:06:20.749: INFO: rc: 1
May 22 02:06:20.749: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001422d50 exit status 1 <nil> <nil> true [0xc002a360c8 0xc002a360e0 0xc002a360f8] [0xc002a360c8 0xc002a360e0 0xc002a360f8] [0xc002a360d8 0xc002a360f0] [0x9c00a0 0x9c00a0] 0xc0029ff3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:06:30.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:06:30.814: INFO: rc: 1
May 22 02:06:30.814: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001423080 exit status 1 <nil> <nil> true [0xc002a36100 0xc002a36118 0xc002a36130] [0xc002a36100 0xc002a36118 0xc002a36130] [0xc002a36110 0xc002a36128] [0x9c00a0 0x9c00a0] 0xc0029ff920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:06:40.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:06:40.883: INFO: rc: 1
May 22 02:06:40.883: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014233b0 exit status 1 <nil> <nil> true [0xc002a36138 0xc002a36150 0xc002a36168] [0xc002a36138 0xc002a36150 0xc002a36168] [0xc002a36148 0xc002a36160] [0x9c00a0 0x9c00a0] 0xc0029ffc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:06:50.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:06:50.956: INFO: rc: 1
May 22 02:06:50.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014236e0 exit status 1 <nil> <nil> true [0xc002a36170 0xc002a36188 0xc002a361a0] [0xc002a36170 0xc002a36188 0xc002a361a0] [0xc002a36180 0xc002a36198] [0x9c00a0 0x9c00a0] 0xc00277e060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:07:00.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:07:01.060: INFO: rc: 1
May 22 02:07:01.061: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d08480 exit status 1 <nil> <nil> true [0xc002b02018 0xc002b02030 0xc002b02048] [0xc002b02018 0xc002b02030 0xc002b02048] [0xc002b02028 0xc002b02040] [0x9c00a0 0x9c00a0] 0xc0028ca300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:07:11.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:07:11.150: INFO: rc: 1
May 22 02:07:11.150: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001423a10 exit status 1 <nil> <nil> true [0xc002a361a8 0xc002a361c0 0xc002a361d8] [0xc002a361a8 0xc002a361c0 0xc002a361d8] [0xc002a361b8 0xc002a361d0] [0x9c00a0 0x9c00a0] 0xc00277e480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 02:07:21.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-7414 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:07:21.251: INFO: rc: 1
May 22 02:07:21.251: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
May 22 02:07:21.251: INFO: Scaling statefulset ss to 0
May 22 02:07:21.266: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 02:07:21.271: INFO: Deleting all statefulset in ns statefulset-7414
May 22 02:07:21.278: INFO: Scaling statefulset ss to 0
May 22 02:07:21.297: INFO: Waiting for statefulset status.replicas updated to 0
May 22 02:07:21.302: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:07:21.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7414" for this suite.
May 22 02:07:27.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:07:27.507: INFO: namespace statefulset-7414 deletion completed in 6.173996206s

• [SLOW TEST:361.590 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:07:27.507: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:07:27.684: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 22 02:07:32.690: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 22 02:07:32.690: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 02:07:32.737: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-5878,SelfLink:/apis/apps/v1/namespaces/deployment-5878/deployments/test-cleanup-deployment,UID:4fe5bbc2-7c36-11e9-b253-000c290a0f16,ResourceVersion:132610,Generation:1,CreationTimestamp:2019-05-22 02:07:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 22 02:07:32.744: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-5878,SelfLink:/apis/apps/v1/namespaces/deployment-5878/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:5bffa953-7c36-11e9-85c4-000c296e7615,ResourceVersion:132612,Generation:1,CreationTimestamp:2019-05-22 02:07:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 4fe5bbc2-7c36-11e9-b253-000c290a0f16 0xc00105bf07 0xc00105bf08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 02:07:32.744: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 22 02:07:32.744: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-5878,SelfLink:/apis/apps/v1/namespaces/deployment-5878/replicasets/test-cleanup-controller,UID:4c9fa6d5-7c36-11e9-b253-000c290a0f16,ResourceVersion:132611,Generation:1,CreationTimestamp:2019-05-22 02:07:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 4fe5bbc2-7c36-11e9-b253-000c290a0f16 0xc00105be37 0xc00105be38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 22 02:07:32.756: INFO: Pod "test-cleanup-controller-mn76t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-mn76t,GenerateName:test-cleanup-controller-,Namespace:deployment-5878,SelfLink:/api/v1/namespaces/deployment-5878/pods/test-cleanup-controller-mn76t,UID:58fc1811-7c36-11e9-85c4-000c296e7615,ResourceVersion:132603,Generation:0,CreationTimestamp:2019-05-22 02:07:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 4c9fa6d5-7c36-11e9-b253-000c290a0f16 0xc000394527 0xc000394528}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7zq5l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7zq5l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7zq5l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003946b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000394790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:07:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:07:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:07:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:07:06 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.19,StartTime:2019-05-22 02:07:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 02:07:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://995ffb0c29bceef9c2fe03e7bcb96b4479b64d921dd5371885fcc59422f47eb7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:07:32.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5878" for this suite.
May 22 02:07:38.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:07:38.936: INFO: namespace deployment-5878 deletion completed in 6.164877931s

• [SLOW TEST:11.429 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:07:38.936: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7835.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7835.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 22 02:07:41.186: INFO: DNS probes using dns-7835/dns-test-5fed965f-7c36-11e9-8c5e-b202ea6dae39 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:07:41.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7835" for this suite.
May 22 02:07:47.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:07:47.382: INFO: namespace dns-7835 deletion completed in 6.170764077s

• [SLOW TEST:8.446 seconds]
[sig-network] DNS
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:07:47.382: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 02:07:47.558: INFO: Waiting up to 5m0s for pod "downward-api-64f603f6-7c36-11e9-8c5e-b202ea6dae39" in namespace "downward-api-3788" to be "success or failure"
May 22 02:07:47.566: INFO: Pod "downward-api-64f603f6-7c36-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610226ms
May 22 02:07:49.571: INFO: Pod "downward-api-64f603f6-7c36-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013172203s
STEP: Saw pod success
May 22 02:07:49.571: INFO: Pod "downward-api-64f603f6-7c36-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:07:49.575: INFO: Trying to get logs from node worker01 pod downward-api-64f603f6-7c36-11e9-8c5e-b202ea6dae39 container dapi-container: <nil>
STEP: delete the pod
May 22 02:07:49.600: INFO: Waiting for pod downward-api-64f603f6-7c36-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:07:49.610: INFO: Pod downward-api-64f603f6-7c36-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:07:49.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3788" for this suite.
May 22 02:07:55.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:07:55.808: INFO: namespace downward-api-3788 deletion completed in 6.186202872s

• [SLOW TEST:8.426 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:07:55.808: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2091
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:07:58.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2091" for this suite.
May 22 02:08:46.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:08:46.219: INFO: namespace kubelet-test-2091 deletion completed in 48.183625213s

• [SLOW TEST:50.411 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:08:46.219: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 22 02:08:46.400: INFO: Waiting up to 5m0s for pod "pod-88088f35-7c36-11e9-8c5e-b202ea6dae39" in namespace "emptydir-1201" to be "success or failure"
May 22 02:08:46.410: INFO: Pod "pod-88088f35-7c36-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.72879ms
May 22 02:08:48.415: INFO: Pod "pod-88088f35-7c36-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015047037s
May 22 02:08:50.419: INFO: Pod "pod-88088f35-7c36-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019357421s
STEP: Saw pod success
May 22 02:08:50.419: INFO: Pod "pod-88088f35-7c36-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:08:50.423: INFO: Trying to get logs from node worker01 pod pod-88088f35-7c36-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:08:50.450: INFO: Waiting for pod pod-88088f35-7c36-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:08:50.457: INFO: Pod pod-88088f35-7c36-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:08:50.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1201" for this suite.
May 22 02:08:56.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:08:56.624: INFO: namespace emptydir-1201 deletion completed in 6.153516708s

• [SLOW TEST:10.405 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:08:56.624: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:08:56.784: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:09:00.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1549" for this suite.
May 22 02:09:44.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:09:45.011: INFO: namespace pods-1549 deletion completed in 44.168517563s

• [SLOW TEST:48.387 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:09:45.011: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 22 02:09:45.213: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6614,SelfLink:/api/v1/namespaces/watch-6614/configmaps/e2e-watch-test-label-changed,UID:a609df4b-7c36-11e9-b253-000c290a0f16,ResourceVersion:133049,Generation:0,CreationTimestamp:2019-05-22 02:09:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 02:09:45.214: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6614,SelfLink:/api/v1/namespaces/watch-6614/configmaps/e2e-watch-test-label-changed,UID:a609df4b-7c36-11e9-b253-000c290a0f16,ResourceVersion:133050,Generation:0,CreationTimestamp:2019-05-22 02:09:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 22 02:09:45.214: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6614,SelfLink:/api/v1/namespaces/watch-6614/configmaps/e2e-watch-test-label-changed,UID:a609df4b-7c36-11e9-b253-000c290a0f16,ResourceVersion:133051,Generation:0,CreationTimestamp:2019-05-22 02:09:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 22 02:09:55.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6614,SelfLink:/api/v1/namespaces/watch-6614/configmaps/e2e-watch-test-label-changed,UID:a609df4b-7c36-11e9-b253-000c290a0f16,ResourceVersion:133071,Generation:0,CreationTimestamp:2019-05-22 02:09:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 02:09:55.296: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6614,SelfLink:/api/v1/namespaces/watch-6614/configmaps/e2e-watch-test-label-changed,UID:a609df4b-7c36-11e9-b253-000c290a0f16,ResourceVersion:133072,Generation:0,CreationTimestamp:2019-05-22 02:09:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 22 02:09:55.296: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6614,SelfLink:/api/v1/namespaces/watch-6614/configmaps/e2e-watch-test-label-changed,UID:a609df4b-7c36-11e9-b253-000c290a0f16,ResourceVersion:133073,Generation:0,CreationTimestamp:2019-05-22 02:09:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:09:55.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6614" for this suite.
May 22 02:10:01.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:10:01.447: INFO: namespace watch-6614 deletion completed in 6.143540197s

• [SLOW TEST:16.436 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:10:01.447: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0522 02:10:02.207025      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 02:10:02.207: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:10:02.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5541" for this suite.
May 22 02:10:08.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:10:08.371: INFO: namespace gc-5541 deletion completed in 6.153641071s

• [SLOW TEST:6.924 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:10:08.371: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:10:10.637: INFO: Waiting up to 5m0s for pod "client-envvars-ba3ee1e0-7c36-11e9-8c5e-b202ea6dae39" in namespace "pods-1158" to be "success or failure"
May 22 02:10:10.649: INFO: Pod "client-envvars-ba3ee1e0-7c36-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 11.860964ms
May 22 02:10:12.658: INFO: Pod "client-envvars-ba3ee1e0-7c36-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020227013s
STEP: Saw pod success
May 22 02:10:12.658: INFO: Pod "client-envvars-ba3ee1e0-7c36-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:10:12.666: INFO: Trying to get logs from node worker01 pod client-envvars-ba3ee1e0-7c36-11e9-8c5e-b202ea6dae39 container env3cont: <nil>
STEP: delete the pod
May 22 02:10:12.717: INFO: Waiting for pod client-envvars-ba3ee1e0-7c36-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:10:12.725: INFO: Pod client-envvars-ba3ee1e0-7c36-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:10:12.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1158" for this suite.
May 22 02:10:54.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:10:54.897: INFO: namespace pods-1158 deletion completed in 42.162054931s

• [SLOW TEST:46.525 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:10:54.897: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-d4bbaa38-7c36-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 02:10:55.090: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d4bd0b2e-7c36-11e9-8c5e-b202ea6dae39" in namespace "projected-4555" to be "success or failure"
May 22 02:10:55.097: INFO: Pod "pod-projected-configmaps-d4bd0b2e-7c36-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 7.290063ms
May 22 02:10:57.105: INFO: Pod "pod-projected-configmaps-d4bd0b2e-7c36-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015240454s
STEP: Saw pod success
May 22 02:10:57.105: INFO: Pod "pod-projected-configmaps-d4bd0b2e-7c36-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:10:57.112: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-d4bd0b2e-7c36-11e9-8c5e-b202ea6dae39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 02:10:57.173: INFO: Waiting for pod pod-projected-configmaps-d4bd0b2e-7c36-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:10:57.185: INFO: Pod pod-projected-configmaps-d4bd0b2e-7c36-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:10:57.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4555" for this suite.
May 22 02:11:03.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:11:03.369: INFO: namespace projected-4555 deletion completed in 6.176203013s

• [SLOW TEST:8.472 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:11:03.369: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:11:07.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6170" for this suite.
May 22 02:11:13.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:11:13.724: INFO: namespace kubelet-test-6170 deletion completed in 6.160832432s

• [SLOW TEST:10.355 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:11:13.724: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-dff3d24e-7c36-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 02:11:13.914: INFO: Waiting up to 5m0s for pod "pod-configmaps-dff531c2-7c36-11e9-8c5e-b202ea6dae39" in namespace "configmap-6036" to be "success or failure"
May 22 02:11:13.923: INFO: Pod "pod-configmaps-dff531c2-7c36-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.343442ms
May 22 02:11:15.930: INFO: Pod "pod-configmaps-dff531c2-7c36-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015887628s
May 22 02:11:17.935: INFO: Pod "pod-configmaps-dff531c2-7c36-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020642702s
STEP: Saw pod success
May 22 02:11:17.935: INFO: Pod "pod-configmaps-dff531c2-7c36-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:11:17.939: INFO: Trying to get logs from node worker01 pod pod-configmaps-dff531c2-7c36-11e9-8c5e-b202ea6dae39 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 02:11:17.967: INFO: Waiting for pod pod-configmaps-dff531c2-7c36-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:11:17.980: INFO: Pod pod-configmaps-dff531c2-7c36-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:11:17.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6036" for this suite.
May 22 02:11:24.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:11:24.159: INFO: namespace configmap-6036 deletion completed in 6.1696174s

• [SLOW TEST:10.435 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:11:24.159: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-110
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-e62d8281-7c36-11e9-8c5e-b202ea6dae39
STEP: Creating secret with name s-test-opt-upd-e62d832d-7c36-11e9-8c5e-b202ea6dae39
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e62d8281-7c36-11e9-8c5e-b202ea6dae39
STEP: Updating secret s-test-opt-upd-e62d832d-7c36-11e9-8c5e-b202ea6dae39
STEP: Creating secret with name s-test-opt-create-e62d833a-7c36-11e9-8c5e-b202ea6dae39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:11:28.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-110" for this suite.
May 22 02:11:52.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:11:52.715: INFO: namespace projected-110 deletion completed in 24.183989018s

• [SLOW TEST:28.556 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:11:52.715: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 22 02:11:55.452: INFO: Successfully updated pod "pod-update-f730cbd5-7c36-11e9-8c5e-b202ea6dae39"
STEP: verifying the updated pod is in kubernetes
May 22 02:11:55.469: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:11:55.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4798" for this suite.
May 22 02:12:17.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:12:17.648: INFO: namespace pods-4798 deletion completed in 22.168278087s

• [SLOW TEST:24.932 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:12:17.648: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:12:17.827: INFO: Waiting up to 5m0s for pod "downwardapi-volume-060d5cd1-7c37-11e9-8c5e-b202ea6dae39" in namespace "projected-8771" to be "success or failure"
May 22 02:12:17.835: INFO: Pod "downwardapi-volume-060d5cd1-7c37-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 7.932853ms
May 22 02:12:19.842: INFO: Pod "downwardapi-volume-060d5cd1-7c37-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015478961s
STEP: Saw pod success
May 22 02:12:19.842: INFO: Pod "downwardapi-volume-060d5cd1-7c37-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:12:19.850: INFO: Trying to get logs from node worker01 pod downwardapi-volume-060d5cd1-7c37-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:12:19.900: INFO: Waiting for pod downwardapi-volume-060d5cd1-7c37-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:12:19.904: INFO: Pod downwardapi-volume-060d5cd1-7c37-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:12:19.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8771" for this suite.
May 22 02:12:25.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:12:26.102: INFO: namespace projected-8771 deletion completed in 6.189046751s

• [SLOW TEST:8.454 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:12:26.102: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:12:26.301: INFO: Conformance test suite needs a cluster with at least 2 nodes.
May 22 02:12:26.301: INFO: Create a RollingUpdate DaemonSet
May 22 02:12:26.311: INFO: Check that daemon pods launch on every node of the cluster
May 22 02:12:26.320: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:26.320: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:26.320: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:26.329: INFO: Number of nodes with available pods: 0
May 22 02:12:26.329: INFO: Node worker01 is running more than one daemon pod
May 22 02:12:27.335: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:27.335: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:27.335: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:27.340: INFO: Number of nodes with available pods: 0
May 22 02:12:27.340: INFO: Node worker01 is running more than one daemon pod
May 22 02:12:28.336: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:28.336: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:28.336: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:28.342: INFO: Number of nodes with available pods: 0
May 22 02:12:28.342: INFO: Node worker01 is running more than one daemon pod
May 22 02:12:29.337: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:29.337: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:29.337: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:29.344: INFO: Number of nodes with available pods: 1
May 22 02:12:29.344: INFO: Number of running nodes: 1, number of available pods: 1
May 22 02:12:29.344: INFO: Update the DaemonSet to trigger a rollout
May 22 02:12:29.358: INFO: Updating DaemonSet daemon-set
May 22 02:12:33.380: INFO: Roll back the DaemonSet before rollout is complete
May 22 02:12:33.391: INFO: Updating DaemonSet daemon-set
May 22 02:12:33.391: INFO: Make sure DaemonSet rollback is complete
May 22 02:12:33.398: INFO: Wrong image for pod: daemon-set-9vjps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 22 02:12:33.398: INFO: Pod daemon-set-9vjps is not available
May 22 02:12:33.405: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:33.405: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:33.405: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:34.410: INFO: Wrong image for pod: daemon-set-9vjps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 22 02:12:34.410: INFO: Pod daemon-set-9vjps is not available
May 22 02:12:34.416: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:34.416: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:34.416: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:35.411: INFO: Pod daemon-set-v2kzb is not available
May 22 02:12:35.416: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:35.416: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 02:12:35.416: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2096, will wait for the garbage collector to delete the pods
May 22 02:12:35.492: INFO: Deleting DaemonSet.extensions daemon-set took: 10.307284ms
May 22 02:12:35.792: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.658043ms
May 22 02:13:44.998: INFO: Number of nodes with available pods: 0
May 22 02:13:44.998: INFO: Number of running nodes: 0, number of available pods: 0
May 22 02:13:45.003: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2096/daemonsets","resourceVersion":"133832"},"items":null}

May 22 02:13:45.008: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2096/pods","resourceVersion":"133832"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:13:45.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2096" for this suite.
May 22 02:13:51.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:13:51.213: INFO: namespace daemonsets-2096 deletion completed in 6.182182996s

• [SLOW TEST:85.111 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:13:51.213: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3881
May 22 02:13:53.410: INFO: Started pod liveness-http in namespace container-probe-3881
STEP: checking the pod's current state and verifying that restartCount is present
May 22 02:13:53.414: INFO: Initial restart count of pod liveness-http is 0
May 22 02:14:19.523: INFO: Restart count of pod container-probe-3881/liveness-http is now 1 (26.108151485s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:14:19.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3881" for this suite.
May 22 02:14:25.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:14:25.734: INFO: namespace container-probe-3881 deletion completed in 6.166791408s

• [SLOW TEST:34.520 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:14:25.734: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:14:25.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5268311b-7c37-11e9-8c5e-b202ea6dae39" in namespace "projected-6664" to be "success or failure"
May 22 02:14:25.941: INFO: Pod "downwardapi-volume-5268311b-7c37-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.101168ms
May 22 02:14:27.954: INFO: Pod "downwardapi-volume-5268311b-7c37-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025280152s
May 22 02:14:29.969: INFO: Pod "downwardapi-volume-5268311b-7c37-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040115667s
STEP: Saw pod success
May 22 02:14:29.969: INFO: Pod "downwardapi-volume-5268311b-7c37-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:14:29.980: INFO: Trying to get logs from node worker01 pod downwardapi-volume-5268311b-7c37-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:14:30.030: INFO: Waiting for pod downwardapi-volume-5268311b-7c37-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:14:30.036: INFO: Pod downwardapi-volume-5268311b-7c37-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:14:30.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6664" for this suite.
May 22 02:14:36.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:14:36.234: INFO: namespace projected-6664 deletion completed in 6.189883892s

• [SLOW TEST:10.500 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:14:36.234: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May 22 02:14:42.463: INFO: 10 pods remaining
May 22 02:14:42.463: INFO: 10 pods has nil DeletionTimestamp
May 22 02:14:42.463: INFO: 
May 22 02:14:43.457: INFO: 10 pods remaining
May 22 02:14:43.457: INFO: 10 pods has nil DeletionTimestamp
May 22 02:14:43.457: INFO: 
STEP: Gathering metrics
W0522 02:14:44.460560      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 02:14:44.460: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:14:44.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3829" for this suite.
May 22 02:14:52.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:14:52.633: INFO: namespace gc-3829 deletion completed in 8.166224902s

• [SLOW TEST:16.398 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:14:52.633: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5390
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 22 02:14:52.802: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 22 02:15:08.892: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.50 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5390 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 02:15:08.892: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 02:15:10.153: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:15:10.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5390" for this suite.
May 22 02:15:32.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:15:32.322: INFO: namespace pod-network-test-5390 deletion completed in 22.163459027s

• [SLOW TEST:39.689 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:15:32.322: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-7a17c203-7c37-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:15:32.519: INFO: Waiting up to 5m0s for pod "pod-secrets-7a1926fb-7c37-11e9-8c5e-b202ea6dae39" in namespace "secrets-2835" to be "success or failure"
May 22 02:15:32.528: INFO: Pod "pod-secrets-7a1926fb-7c37-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.221885ms
May 22 02:15:34.534: INFO: Pod "pod-secrets-7a1926fb-7c37-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015318134s
May 22 02:15:36.540: INFO: Pod "pod-secrets-7a1926fb-7c37-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020939541s
STEP: Saw pod success
May 22 02:15:36.540: INFO: Pod "pod-secrets-7a1926fb-7c37-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:15:36.545: INFO: Trying to get logs from node worker01 pod pod-secrets-7a1926fb-7c37-11e9-8c5e-b202ea6dae39 container secret-volume-test: <nil>
STEP: delete the pod
May 22 02:15:36.576: INFO: Waiting for pod pod-secrets-7a1926fb-7c37-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:15:36.585: INFO: Pod pod-secrets-7a1926fb-7c37-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:15:36.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2835" for this suite.
May 22 02:15:42.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:15:42.805: INFO: namespace secrets-2835 deletion completed in 6.202792608s

• [SLOW TEST:10.483 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:15:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 22 02:15:42.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2273'
May 22 02:15:43.126: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 02:15:43.126: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 22 02:15:43.149: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-8mstp]
May 22 02:15:43.149: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-8mstp" in namespace "kubectl-2273" to be "running and ready"
May 22 02:15:43.162: INFO: Pod "e2e-test-nginx-rc-8mstp": Phase="Pending", Reason="", readiness=false. Elapsed: 13.51719ms
May 22 02:15:45.171: INFO: Pod "e2e-test-nginx-rc-8mstp": Phase="Running", Reason="", readiness=true. Elapsed: 2.022258537s
May 22 02:15:45.171: INFO: Pod "e2e-test-nginx-rc-8mstp" satisfied condition "running and ready"
May 22 02:15:45.171: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-8mstp]
May 22 02:15:45.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 logs rc/e2e-test-nginx-rc --namespace=kubectl-2273'
May 22 02:15:45.300: INFO: stderr: ""
May 22 02:15:45.300: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
May 22 02:15:45.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete rc e2e-test-nginx-rc --namespace=kubectl-2273'
May 22 02:15:45.388: INFO: stderr: ""
May 22 02:15:45.388: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:15:45.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2273" for this suite.
May 22 02:15:51.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:15:51.567: INFO: namespace kubectl-2273 deletion completed in 6.17119039s

• [SLOW TEST:8.762 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:15:51.567: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3654
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 22 02:15:51.750: INFO: Waiting up to 5m0s for pod "pod-859010dd-7c37-11e9-8c5e-b202ea6dae39" in namespace "emptydir-3654" to be "success or failure"
May 22 02:15:51.760: INFO: Pod "pod-859010dd-7c37-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.737591ms
May 22 02:15:53.764: INFO: Pod "pod-859010dd-7c37-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014248775s
STEP: Saw pod success
May 22 02:15:53.764: INFO: Pod "pod-859010dd-7c37-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:15:53.769: INFO: Trying to get logs from node worker01 pod pod-859010dd-7c37-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:15:53.803: INFO: Waiting for pod pod-859010dd-7c37-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:15:53.810: INFO: Pod pod-859010dd-7c37-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:15:53.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3654" for this suite.
May 22 02:15:59.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:16:00.017: INFO: namespace emptydir-3654 deletion completed in 6.191703251s

• [SLOW TEST:8.450 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:16:00.017: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
May 22 02:16:00.188: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-734452489 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:16:00.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1774" for this suite.
May 22 02:16:06.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:16:06.476: INFO: namespace kubectl-1774 deletion completed in 6.197133914s

• [SLOW TEST:6.459 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:16:06.476: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 22 02:16:06.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6576'
May 22 02:16:06.730: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 02:16:06.730: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May 22 02:16:06.746: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
May 22 02:16:06.749: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 22 02:16:06.788: INFO: scanned /root for discovery docs: <nil>
May 22 02:16:06.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6576'
May 22 02:16:22.653: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 22 02:16:22.653: INFO: stdout: "Created e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c\nScaling up e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 22 02:16:22.653: INFO: stdout: "Created e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c\nScaling up e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 22 02:16:22.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-6576'
May 22 02:16:22.755: INFO: stderr: ""
May 22 02:16:22.755: INFO: stdout: "e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c-tq7hq "
May 22 02:16:22.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c-tq7hq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6576'
May 22 02:16:22.828: INFO: stderr: ""
May 22 02:16:22.828: INFO: stdout: "true"
May 22 02:16:22.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c-tq7hq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6576'
May 22 02:16:22.904: INFO: stderr: ""
May 22 02:16:22.904: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May 22 02:16:22.904: INFO: e2e-test-nginx-rc-a1e27edde090de6926f391eff678d43c-tq7hq is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
May 22 02:16:22.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete rc e2e-test-nginx-rc --namespace=kubectl-6576'
May 22 02:16:22.982: INFO: stderr: ""
May 22 02:16:22.982: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:16:22.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6576" for this suite.
May 22 02:16:29.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:16:29.168: INFO: namespace kubectl-6576 deletion completed in 6.169244154s

• [SLOW TEST:22.692 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:16:29.168: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-8248
May 22 02:16:31.361: INFO: Started pod liveness-exec in namespace container-probe-8248
STEP: checking the pod's current state and verifying that restartCount is present
May 22 02:16:31.366: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:20:32.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8248" for this suite.
May 22 02:20:38.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:20:38.529: INFO: namespace container-probe-8248 deletion completed in 6.171631438s

• [SLOW TEST:249.361 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:20:38.529: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:20:38.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-309c5d9d-7c38-11e9-8c5e-b202ea6dae39" in namespace "projected-4264" to be "success or failure"
May 22 02:20:38.738: INFO: Pod "downwardapi-volume-309c5d9d-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 11.504504ms
May 22 02:20:40.743: INFO: Pod "downwardapi-volume-309c5d9d-7c38-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016953039s
STEP: Saw pod success
May 22 02:20:40.743: INFO: Pod "downwardapi-volume-309c5d9d-7c38-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:20:40.748: INFO: Trying to get logs from node worker01 pod downwardapi-volume-309c5d9d-7c38-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:20:40.778: INFO: Waiting for pod downwardapi-volume-309c5d9d-7c38-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:20:40.788: INFO: Pod downwardapi-volume-309c5d9d-7c38-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:20:40.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4264" for this suite.
May 22 02:20:46.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:20:46.991: INFO: namespace projected-4264 deletion completed in 6.192169049s

• [SLOW TEST:8.461 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:20:46.991: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2588.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2588.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2588.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2588.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2588.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2588.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 22 02:20:49.242: INFO: DNS probes using dns-2588/dns-test-35a57760-7c38-11e9-8c5e-b202ea6dae39 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:20:49.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2588" for this suite.
May 22 02:20:55.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:20:55.431: INFO: namespace dns-2588 deletion completed in 6.159319552s

• [SLOW TEST:8.441 seconds]
[sig-network] DNS
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:20:55.432: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:20:55.604: INFO: Creating deployment "nginx-deployment"
May 22 02:20:55.617: INFO: Waiting for observed generation 1
May 22 02:20:57.633: INFO: Waiting for all required pods to come up
May 22 02:20:57.638: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 22 02:21:03.651: INFO: Waiting for deployment "nginx-deployment" to complete
May 22 02:21:03.661: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 22 02:21:03.675: INFO: Updating deployment nginx-deployment
May 22 02:21:03.675: INFO: Waiting for observed generation 2
May 22 02:21:05.688: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 22 02:21:05.692: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 22 02:21:05.695: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 22 02:21:05.708: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 22 02:21:05.708: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 22 02:21:05.711: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 22 02:21:05.719: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 22 02:21:05.719: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 22 02:21:05.742: INFO: Updating deployment nginx-deployment
May 22 02:21:05.742: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 22 02:21:05.773: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 22 02:21:05.785: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 02:21:05.833: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3477,SelfLink:/apis/apps/v1/namespaces/deployment-3477/deployments/nginx-deployment,UID:3aaef1ce-7c38-11e9-b253-000c290a0f16,ResourceVersion:135441,Generation:3,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-05-22 02:21:03 +0000 UTC 2019-05-22 02:20:55 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-05-22 02:21:05 +0000 UTC 2019-05-22 02:21:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

May 22 02:21:05.855: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-3477,SelfLink:/apis/apps/v1/namespaces/deployment-3477/replicasets/nginx-deployment-5f9595f595,UID:3f7e6654-7c38-11e9-85c4-000c296e7615,ResourceVersion:135427,Generation:3,CreationTimestamp:2019-05-22 02:21:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 3aaef1ce-7c38-11e9-b253-000c290a0f16 0xc000745f87 0xc000745f88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 02:21:05.855: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 22 02:21:05.855: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-3477,SelfLink:/apis/apps/v1/namespaces/deployment-3477/replicasets/nginx-deployment-6f478d8d8,UID:3ab1d888-7c38-11e9-85c4-000c296e7615,ResourceVersion:135425,Generation:3,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 3aaef1ce-7c38-11e9-b253-000c290a0f16 0xc0005bc847 0xc0005bc848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 22 02:21:05.893: INFO: Pod "nginx-deployment-5f9595f595-2l455" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-2l455,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-2l455,UID:40cb79da-7c38-11e9-85c4-000c296e7615,ResourceVersion:135460,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc0005bdf57 0xc0005bdf58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00047b0c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00047b2c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.893: INFO: Pod "nginx-deployment-5f9595f595-5dj6x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-5dj6x,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-5dj6x,UID:3f838c24-7c38-11e9-85c4-000c296e7615,ResourceVersion:135418,Generation:0,CreationTimestamp:2019-05-22 02:21:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc00047b830 0xc00047b831}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00047ba40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00047ba60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:,StartTime:2019-05-22 02:21:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.894: INFO: Pod "nginx-deployment-5f9595f595-5tsn9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-5tsn9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-5tsn9,UID:3f8d603e-7c38-11e9-85c4-000c296e7615,ResourceVersion:135419,Generation:0,CreationTimestamp:2019-05-22 02:21:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc00047bc70 0xc00047bc71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00047bd80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00047bdc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:,StartTime:2019-05-22 02:21:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.894: INFO: Pod "nginx-deployment-5f9595f595-72hzp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-72hzp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-72hzp,UID:3f836787-7c38-11e9-85c4-000c296e7615,ResourceVersion:135417,Generation:0,CreationTimestamp:2019-05-22 02:21:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc000054090 0xc000054091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000054480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0000544d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:,StartTime:2019-05-22 02:21:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.894: INFO: Pod "nginx-deployment-5f9595f595-bdk5c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-bdk5c,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-bdk5c,UID:40c8d3d6-7c38-11e9-85c4-000c296e7615,ResourceVersion:135453,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc000054600 0xc000054601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000546a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0000546d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.894: INFO: Pod "nginx-deployment-5f9595f595-d8pkp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-d8pkp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-d8pkp,UID:40c35c32-7c38-11e9-85c4-000c296e7615,ResourceVersion:135442,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc000054790 0xc000054791}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000054800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000054820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.894: INFO: Pod "nginx-deployment-5f9595f595-dlx2n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-dlx2n,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-dlx2n,UID:40bf4fb5-7c38-11e9-85c4-000c296e7615,ResourceVersion:135445,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc000054890 0xc000054891}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000054910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000054930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.894: INFO: Pod "nginx-deployment-5f9595f595-l56b2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-l56b2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-l56b2,UID:40c8d57a-7c38-11e9-85c4-000c296e7615,ResourceVersion:135457,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc000054a20 0xc000054a21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000054ae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000054b30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.894: INFO: Pod "nginx-deployment-5f9595f595-nglm2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-nglm2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-nglm2,UID:40c84f84-7c38-11e9-85c4-000c296e7615,ResourceVersion:135458,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc000054df0 0xc000054df1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000054e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000054eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.895: INFO: Pod "nginx-deployment-5f9595f595-q44rs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-q44rs,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-q44rs,UID:3f8ef34e-7c38-11e9-85c4-000c296e7615,ResourceVersion:135420,Generation:0,CreationTimestamp:2019-05-22 02:21:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc000054fb0 0xc000054fb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000550d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000055110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:,StartTime:2019-05-22 02:21:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.895: INFO: Pod "nginx-deployment-5f9595f595-tfksk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-tfksk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-tfksk,UID:3f80dd63-7c38-11e9-85c4-000c296e7615,ResourceVersion:135400,Generation:0,CreationTimestamp:2019-05-22 02:21:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc0000552a0 0xc0000552a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000554c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000055500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:,StartTime:2019-05-22 02:21:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.895: INFO: Pod "nginx-deployment-5f9595f595-vn8gs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-vn8gs,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-vn8gs,UID:40c33bb1-7c38-11e9-85c4-000c296e7615,ResourceVersion:135440,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc000055660 0xc000055661}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000055740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000055770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.895: INFO: Pod "nginx-deployment-5f9595f595-ws5t6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-ws5t6,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-5f9595f595-ws5t6,UID:40c870af-7c38-11e9-85c4-000c296e7615,ResourceVersion:135455,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3f7e6654-7c38-11e9-85c4-000c296e7615 0xc000055820 0xc000055821}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000055900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000055930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.895: INFO: Pod "nginx-deployment-6f478d8d8-44s58" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-44s58,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-44s58,UID:40c1ed65-7c38-11e9-85c4-000c296e7615,ResourceVersion:135438,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000055af0 0xc000055af1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000055c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000055c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.895: INFO: Pod "nginx-deployment-6f478d8d8-6fwfs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6fwfs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-6fwfs,UID:3ab64ee8-7c38-11e9-85c4-000c296e7615,ResourceVersion:135328,Generation:0,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000055d50 0xc000055d51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000055e40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000055e80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.62,StartTime:2019-05-22 02:20:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 02:20:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b680173bd55f3696af410508342a82c3577786cde8523a04f4eae210abfc42af}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.896: INFO: Pod "nginx-deployment-6f478d8d8-7fpj6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7fpj6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-7fpj6,UID:40c7ad56-7c38-11e9-85c4-000c296e7615,ResourceVersion:135456,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000394090 0xc000394091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000394260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000394310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.896: INFO: Pod "nginx-deployment-6f478d8d8-8s6tq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8s6tq,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-8s6tq,UID:3abb1114-7c38-11e9-85c4-000c296e7615,ResourceVersion:135322,Generation:0,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000394530 0xc000394531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003946b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000394790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.67,StartTime:2019-05-22 02:20:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 02:21:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2140937e4d9e90e125fd18aeb60ae1af0ac2b21399a3dfee82cb9b3db67bdf39}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.896: INFO: Pod "nginx-deployment-6f478d8d8-98gws" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-98gws,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-98gws,UID:3ab82ca6-7c38-11e9-85c4-000c296e7615,ResourceVersion:135343,Generation:0,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000394c10 0xc000394c11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000394eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000394ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.69,StartTime:2019-05-22 02:20:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 02:21:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a473afb7aab3cbaef03e593dd0873e504d32c8adcf8e070dcb352cf4a0487d8a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.896: INFO: Pod "nginx-deployment-6f478d8d8-b9tv8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-b9tv8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-b9tv8,UID:40c24479-7c38-11e9-85c4-000c296e7615,ResourceVersion:135452,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc0003951a0 0xc0003951a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000395290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000395310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.897: INFO: Pod "nginx-deployment-6f478d8d8-czbmb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-czbmb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-czbmb,UID:40c63706-7c38-11e9-85c4-000c296e7615,ResourceVersion:135449,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000395e80 0xc000395e81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aca140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aca180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.897: INFO: Pod "nginx-deployment-6f478d8d8-d8wbn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-d8wbn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-d8wbn,UID:40bebd7a-7c38-11e9-85c4-000c296e7615,ResourceVersion:135443,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000aca310 0xc000aca311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aca400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aca420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.897: INFO: Pod "nginx-deployment-6f478d8d8-dpmpc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dpmpc,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-dpmpc,UID:3ab6032e-7c38-11e9-85c4-000c296e7615,ResourceVersion:135362,Generation:0,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000aca4b0 0xc000aca4b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aca520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aca550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.61,StartTime:2019-05-22 02:20:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 02:20:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6565d7183c2a1d2385702f9a75a372856eead8a0e81c7e49f2aab7bcb72363fe}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.897: INFO: Pod "nginx-deployment-6f478d8d8-glgs9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-glgs9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-glgs9,UID:3ab83fd5-7c38-11e9-85c4-000c296e7615,ResourceVersion:135331,Generation:0,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000aca6e0 0xc000aca6e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aca760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aca780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.65,StartTime:2019-05-22 02:20:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 02:21:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4e240a405aee520fbf2726b27ac44f78381b7b1a3ecfa1b91e5fc0b2266be993}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.897: INFO: Pod "nginx-deployment-6f478d8d8-k6kfp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-k6kfp,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-k6kfp,UID:40c6d0b6-7c38-11e9-85c4-000c296e7615,ResourceVersion:135450,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000aca990 0xc000aca991}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000acaac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000acac30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.897: INFO: Pod "nginx-deployment-6f478d8d8-lnzms" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lnzms,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-lnzms,UID:40bc783b-7c38-11e9-85c4-000c296e7615,ResourceVersion:135430,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000acacf0 0xc000acacf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000acae60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000acae90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.898: INFO: Pod "nginx-deployment-6f478d8d8-mkv68" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mkv68,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-mkv68,UID:3ab82521-7c38-11e9-85c4-000c296e7615,ResourceVersion:135357,Generation:0,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000acaf10 0xc000acaf11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000acb410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000acb430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.63,StartTime:2019-05-22 02:20:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 02:20:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b2feaee8d07649196fafe74440e2a69b136bcfc71c5bc4867c0b979f6235f3ff}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.898: INFO: Pod "nginx-deployment-6f478d8d8-qfz9n" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qfz9n,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-qfz9n,UID:3ab4dfb1-7c38-11e9-85c4-000c296e7615,ResourceVersion:135325,Generation:0,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000acb750 0xc000acb751}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000acb900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000acb960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.60,StartTime:2019-05-22 02:20:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 02:20:58 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://995f84380cbcee41f439babab8e9a737cae43371cbf0303652b98da028f7600a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.898: INFO: Pod "nginx-deployment-6f478d8d8-qrh58" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qrh58,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-qrh58,UID:40c6fb83-7c38-11e9-85c4-000c296e7615,ResourceVersion:135447,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000acbbc0 0xc000acbbc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000acbce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000acbd80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.898: INFO: Pod "nginx-deployment-6f478d8d8-t95ks" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-t95ks,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-t95ks,UID:3ab8479f-7c38-11e9-85c4-000c296e7615,ResourceVersion:135349,Generation:0,CreationTimestamp:2019-05-22 02:20:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000acbf10 0xc000acbf11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b780d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b78120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:20:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.64,StartTime:2019-05-22 02:20:55 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 02:20:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://bac79fdf32d0ef02c350a124b1c5cceec3c677abb232a428c5259a7bf1fa1462}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.898: INFO: Pod "nginx-deployment-6f478d8d8-tlq2l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-tlq2l,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-tlq2l,UID:40beefc6-7c38-11e9-85c4-000c296e7615,ResourceVersion:135446,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000b78ad0 0xc000b78ad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b78b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b78b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:21:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.899: INFO: Pod "nginx-deployment-6f478d8d8-w9g8b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-w9g8b,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-w9g8b,UID:40c1cc1e-7c38-11e9-85c4-000c296e7615,ResourceVersion:135437,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000b78be0 0xc000b78be1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b78c50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b78c70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.899: INFO: Pod "nginx-deployment-6f478d8d8-x8v5q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-x8v5q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-x8v5q,UID:40c22473-7c38-11e9-85c4-000c296e7615,ResourceVersion:135439,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000b78ce0 0xc000b78ce1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b78d50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b78d70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 02:21:05.899: INFO: Pod "nginx-deployment-6f478d8d8-xwxhd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xwxhd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3477,SelfLink:/api/v1/namespaces/deployment-3477/pods/nginx-deployment-6f478d8d8-xwxhd,UID:40c68713-7c38-11e9-85c4-000c296e7615,ResourceVersion:135448,Generation:0,CreationTimestamp:2019-05-22 02:21:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3ab1d888-7c38-11e9-85c4-000c296e7615 0xc000b78df0 0xc000b78df1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-866pk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-866pk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-866pk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b78e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b78e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:21:05.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3477" for this suite.
May 22 02:21:13.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:21:14.142: INFO: namespace deployment-3477 deletion completed in 8.218516263s

• [SLOW TEST:18.711 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:21:14.143: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-45d5b632-7c38-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:21:14.337: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39" in namespace "projected-3678" to be "success or failure"
May 22 02:21:14.351: INFO: Pod "pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.97509ms
May 22 02:21:16.359: INFO: Pod "pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021340741s
May 22 02:21:18.366: INFO: Pod "pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028782421s
May 22 02:21:20.371: INFO: Pod "pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033985134s
May 22 02:21:22.379: INFO: Pod "pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.041271343s
STEP: Saw pod success
May 22 02:21:22.379: INFO: Pod "pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:21:22.387: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 02:21:22.430: INFO: Waiting for pod pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:21:22.440: INFO: Pod pod-projected-secrets-45d6c695-7c38-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:21:22.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3678" for this suite.
May 22 02:21:28.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:21:28.640: INFO: namespace projected-3678 deletion completed in 6.189648204s

• [SLOW TEST:14.498 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:21:28.641: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9500
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 22 02:21:28.821: INFO: Waiting up to 5m0s for pod "pod-4e78e9a2-7c38-11e9-8c5e-b202ea6dae39" in namespace "emptydir-9500" to be "success or failure"
May 22 02:21:28.835: INFO: Pod "pod-4e78e9a2-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.230797ms
May 22 02:21:30.840: INFO: Pod "pod-4e78e9a2-7c38-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018772544s
STEP: Saw pod success
May 22 02:21:30.840: INFO: Pod "pod-4e78e9a2-7c38-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:21:30.845: INFO: Trying to get logs from node worker01 pod pod-4e78e9a2-7c38-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:21:30.879: INFO: Waiting for pod pod-4e78e9a2-7c38-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:21:30.891: INFO: Pod pod-4e78e9a2-7c38-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:21:30.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9500" for this suite.
May 22 02:21:36.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:21:37.097: INFO: namespace emptydir-9500 deletion completed in 6.191211817s

• [SLOW TEST:8.456 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:21:37.097: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8042
May 22 02:21:39.309: INFO: Started pod liveness-http in namespace container-probe-8042
STEP: checking the pod's current state and verifying that restartCount is present
May 22 02:21:39.314: INFO: Initial restart count of pod liveness-http is 0
May 22 02:21:51.361: INFO: Restart count of pod container-probe-8042/liveness-http is now 1 (12.046923328s elapsed)
May 22 02:22:11.423: INFO: Restart count of pod container-probe-8042/liveness-http is now 2 (32.109517403s elapsed)
May 22 02:22:31.501: INFO: Restart count of pod container-probe-8042/liveness-http is now 3 (52.187168306s elapsed)
May 22 02:22:51.580: INFO: Restart count of pod container-probe-8042/liveness-http is now 4 (1m12.265692286s elapsed)
May 22 02:24:01.862: INFO: Restart count of pod container-probe-8042/liveness-http is now 5 (2m22.547742722s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:24:01.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8042" for this suite.
May 22 02:24:07.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:24:08.092: INFO: namespace container-probe-8042 deletion completed in 6.201001357s

• [SLOW TEST:150.995 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:24:08.092: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2998
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 02:24:08.273: INFO: Waiting up to 5m0s for pod "downward-api-ad83134e-7c38-11e9-8c5e-b202ea6dae39" in namespace "downward-api-2998" to be "success or failure"
May 22 02:24:08.284: INFO: Pod "downward-api-ad83134e-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.314257ms
May 22 02:24:10.289: INFO: Pod "downward-api-ad83134e-7c38-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015248305s
STEP: Saw pod success
May 22 02:24:10.289: INFO: Pod "downward-api-ad83134e-7c38-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:24:10.293: INFO: Trying to get logs from node worker01 pod downward-api-ad83134e-7c38-11e9-8c5e-b202ea6dae39 container dapi-container: <nil>
STEP: delete the pod
May 22 02:24:10.333: INFO: Waiting for pod downward-api-ad83134e-7c38-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:24:10.346: INFO: Pod downward-api-ad83134e-7c38-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:24:10.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2998" for this suite.
May 22 02:24:16.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:24:16.547: INFO: namespace downward-api-2998 deletion completed in 6.190526071s

• [SLOW TEST:8.455 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:24:16.547: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3054
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:24:16.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b28e109b-7c38-11e9-8c5e-b202ea6dae39" in namespace "downward-api-3054" to be "success or failure"
May 22 02:24:16.747: INFO: Pod "downwardapi-volume-b28e109b-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.708131ms
May 22 02:24:18.752: INFO: Pod "downwardapi-volume-b28e109b-7c38-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017110846s
STEP: Saw pod success
May 22 02:24:18.752: INFO: Pod "downwardapi-volume-b28e109b-7c38-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:24:18.757: INFO: Trying to get logs from node worker01 pod downwardapi-volume-b28e109b-7c38-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:24:18.789: INFO: Waiting for pod downwardapi-volume-b28e109b-7c38-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:24:18.798: INFO: Pod downwardapi-volume-b28e109b-7c38-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:24:18.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3054" for this suite.
May 22 02:24:24.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:24:24.999: INFO: namespace downward-api-3054 deletion completed in 6.188363664s

• [SLOW TEST:8.452 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:24:24.999: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0522 02:24:35.309421      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 02:24:35.309: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:24:35.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-438" for this suite.
May 22 02:24:41.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:24:41.505: INFO: namespace gc-438 deletion completed in 6.190534979s

• [SLOW TEST:16.506 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:24:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 22 02:24:41.679: INFO: Waiting up to 5m0s for pod "pod-c16d6a98-7c38-11e9-8c5e-b202ea6dae39" in namespace "emptydir-4255" to be "success or failure"
May 22 02:24:41.689: INFO: Pod "pod-c16d6a98-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.781615ms
May 22 02:24:43.694: INFO: Pod "pod-c16d6a98-7c38-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014612432s
STEP: Saw pod success
May 22 02:24:43.694: INFO: Pod "pod-c16d6a98-7c38-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:24:43.699: INFO: Trying to get logs from node worker01 pod pod-c16d6a98-7c38-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:24:43.730: INFO: Waiting for pod pod-c16d6a98-7c38-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:24:43.743: INFO: Pod pod-c16d6a98-7c38-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:24:43.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4255" for this suite.
May 22 02:24:49.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:24:49.935: INFO: namespace emptydir-4255 deletion completed in 6.180266726s

• [SLOW TEST:8.429 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:24:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:25:50.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3202" for this suite.
May 22 02:26:12.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:26:12.303: INFO: namespace container-probe-3202 deletion completed in 22.160140632s

• [SLOW TEST:82.368 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:26:12.304: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2519
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-2519/secret-test-f78c8e25-7c38-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:26:12.491: INFO: Waiting up to 5m0s for pod "pod-configmaps-f78d9215-7c38-11e9-8c5e-b202ea6dae39" in namespace "secrets-2519" to be "success or failure"
May 22 02:26:12.500: INFO: Pod "pod-configmaps-f78d9215-7c38-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.913904ms
May 22 02:26:14.506: INFO: Pod "pod-configmaps-f78d9215-7c38-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015466487s
STEP: Saw pod success
May 22 02:26:14.507: INFO: Pod "pod-configmaps-f78d9215-7c38-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:26:14.514: INFO: Trying to get logs from node worker01 pod pod-configmaps-f78d9215-7c38-11e9-8c5e-b202ea6dae39 container env-test: <nil>
STEP: delete the pod
May 22 02:26:14.554: INFO: Waiting for pod pod-configmaps-f78d9215-7c38-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:26:14.567: INFO: Pod pod-configmaps-f78d9215-7c38-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:26:14.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2519" for this suite.
May 22 02:26:20.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:26:20.770: INFO: namespace secrets-2519 deletion completed in 6.192325088s

• [SLOW TEST:8.466 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:26:20.770: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-v86jr in namespace proxy-8288
I0522 02:26:20.981349      16 runners.go:184] Created replication controller with name: proxy-service-v86jr, namespace: proxy-8288, replica count: 1
I0522 02:26:22.031976      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 02:26:23.032832      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0522 02:26:24.033109      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0522 02:26:25.033842      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0522 02:26:26.034561      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0522 02:26:27.035156      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0522 02:26:28.035721      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0522 02:26:29.036360      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0522 02:26:30.036690      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0522 02:26:31.037060      16 runners.go:184] proxy-service-v86jr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 22 02:26:31.041: INFO: setup took 10.103530463s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 22 02:26:31.053: INFO: (0) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 10.951048ms)
May 22 02:26:31.053: INFO: (0) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 11.601029ms)
May 22 02:26:31.061: INFO: (0) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 18.626205ms)
May 22 02:26:31.064: INFO: (0) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 22.539417ms)
May 22 02:26:31.064: INFO: (0) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 21.091308ms)
May 22 02:26:31.064: INFO: (0) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 20.948905ms)
May 22 02:26:31.064: INFO: (0) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 21.635153ms)
May 22 02:26:31.069: INFO: (0) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 26.427707ms)
May 22 02:26:31.069: INFO: (0) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 25.67381ms)
May 22 02:26:31.075: INFO: (0) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 32.043707ms)
May 22 02:26:31.075: INFO: (0) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 32.663472ms)
May 22 02:26:31.077: INFO: (0) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 34.266548ms)
May 22 02:26:31.077: INFO: (0) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 34.080196ms)
May 22 02:26:31.077: INFO: (0) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 33.521041ms)
May 22 02:26:31.077: INFO: (0) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 35.080349ms)
May 22 02:26:31.077: INFO: (0) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 35.469125ms)
May 22 02:26:31.092: INFO: (1) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 13.653444ms)
May 22 02:26:31.093: INFO: (1) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 14.393248ms)
May 22 02:26:31.093: INFO: (1) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 14.526752ms)
May 22 02:26:31.093: INFO: (1) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 14.171605ms)
May 22 02:26:31.093: INFO: (1) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 14.314876ms)
May 22 02:26:31.093: INFO: (1) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 14.272078ms)
May 22 02:26:31.093: INFO: (1) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 15.973768ms)
May 22 02:26:31.093: INFO: (1) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 14.931667ms)
May 22 02:26:31.093: INFO: (1) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 14.770973ms)
May 22 02:26:31.093: INFO: (1) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 15.476486ms)
May 22 02:26:31.096: INFO: (1) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 17.847887ms)
May 22 02:26:31.098: INFO: (1) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 20.111951ms)
May 22 02:26:31.099: INFO: (1) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 20.985954ms)
May 22 02:26:31.099: INFO: (1) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 20.886837ms)
May 22 02:26:31.099: INFO: (1) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 21.633206ms)
May 22 02:26:31.099: INFO: (1) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 21.813099ms)
May 22 02:26:31.115: INFO: (2) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 14.746116ms)
May 22 02:26:31.115: INFO: (2) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 14.222719ms)
May 22 02:26:31.115: INFO: (2) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 15.087221ms)
May 22 02:26:31.115: INFO: (2) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 14.165296ms)
May 22 02:26:31.115: INFO: (2) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 15.264459ms)
May 22 02:26:31.115: INFO: (2) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 14.748378ms)
May 22 02:26:31.115: INFO: (2) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 14.318954ms)
May 22 02:26:31.115: INFO: (2) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 15.692322ms)
May 22 02:26:31.115: INFO: (2) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 14.302855ms)
May 22 02:26:31.116: INFO: (2) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 14.982745ms)
May 22 02:26:31.120: INFO: (2) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 18.758402ms)
May 22 02:26:31.121: INFO: (2) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 20.558351ms)
May 22 02:26:31.123: INFO: (2) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 23.064128ms)
May 22 02:26:31.123: INFO: (2) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 22.797308ms)
May 22 02:26:31.123: INFO: (2) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 24.000946ms)
May 22 02:26:31.125: INFO: (2) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 23.747723ms)
May 22 02:26:31.136: INFO: (3) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 9.078908ms)
May 22 02:26:31.136: INFO: (3) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 10.394306ms)
May 22 02:26:31.136: INFO: (3) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 9.70957ms)
May 22 02:26:31.136: INFO: (3) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 11.34832ms)
May 22 02:26:31.136: INFO: (3) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 11.047303ms)
May 22 02:26:31.136: INFO: (3) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 9.606101ms)
May 22 02:26:31.136: INFO: (3) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 10.836269ms)
May 22 02:26:31.138: INFO: (3) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 13.275666ms)
May 22 02:26:31.138: INFO: (3) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 12.034206ms)
May 22 02:26:31.138: INFO: (3) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 12.540068ms)
May 22 02:26:31.138: INFO: (3) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 11.531891ms)
May 22 02:26:31.140: INFO: (3) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 14.269204ms)
May 22 02:26:31.142: INFO: (3) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 15.848654ms)
May 22 02:26:31.142: INFO: (3) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 15.5749ms)
May 22 02:26:31.142: INFO: (3) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 17.039686ms)
May 22 02:26:31.142: INFO: (3) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 16.957685ms)
May 22 02:26:31.165: INFO: (4) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 21.845475ms)
May 22 02:26:31.165: INFO: (4) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 22.289744ms)
May 22 02:26:31.165: INFO: (4) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 21.712449ms)
May 22 02:26:31.165: INFO: (4) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 22.311773ms)
May 22 02:26:31.165: INFO: (4) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 22.963539ms)
May 22 02:26:31.169: INFO: (4) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 26.177835ms)
May 22 02:26:31.170: INFO: (4) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 27.501222ms)
May 22 02:26:31.170: INFO: (4) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 26.820322ms)
May 22 02:26:31.170: INFO: (4) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 26.771145ms)
May 22 02:26:31.170: INFO: (4) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 27.634839ms)
May 22 02:26:31.173: INFO: (4) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 31.129336ms)
May 22 02:26:31.177: INFO: (4) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 33.58723ms)
May 22 02:26:31.178: INFO: (4) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 34.561319ms)
May 22 02:26:31.178: INFO: (4) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 35.280473ms)
May 22 02:26:31.178: INFO: (4) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 34.695091ms)
May 22 02:26:31.178: INFO: (4) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 35.270289ms)
May 22 02:26:31.198: INFO: (5) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 17.897187ms)
May 22 02:26:31.198: INFO: (5) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 16.54395ms)
May 22 02:26:31.198: INFO: (5) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 16.328548ms)
May 22 02:26:31.198: INFO: (5) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 19.066731ms)
May 22 02:26:31.198: INFO: (5) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 18.491534ms)
May 22 02:26:31.199: INFO: (5) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 18.534826ms)
May 22 02:26:31.199: INFO: (5) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 19.277341ms)
May 22 02:26:31.200: INFO: (5) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 19.096157ms)
May 22 02:26:31.202: INFO: (5) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 23.21769ms)
May 22 02:26:31.203: INFO: (5) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 24.050277ms)
May 22 02:26:31.206: INFO: (5) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 24.473384ms)
May 22 02:26:31.211: INFO: (5) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 30.561875ms)
May 22 02:26:31.211: INFO: (5) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 31.135986ms)
May 22 02:26:31.211: INFO: (5) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 31.740722ms)
May 22 02:26:31.211: INFO: (5) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 33.078894ms)
May 22 02:26:31.211: INFO: (5) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 31.907034ms)
May 22 02:26:31.230: INFO: (6) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 16.513524ms)
May 22 02:26:31.230: INFO: (6) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 15.155191ms)
May 22 02:26:31.235: INFO: (6) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 20.169627ms)
May 22 02:26:31.235: INFO: (6) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 21.01169ms)
May 22 02:26:31.235: INFO: (6) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 21.192482ms)
May 22 02:26:31.235: INFO: (6) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 21.424109ms)
May 22 02:26:31.235: INFO: (6) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 22.068585ms)
May 22 02:26:31.238: INFO: (6) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 23.252262ms)
May 22 02:26:31.238: INFO: (6) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 26.278012ms)
May 22 02:26:31.238: INFO: (6) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 25.496611ms)
May 22 02:26:31.238: INFO: (6) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 25.876555ms)
May 22 02:26:31.239: INFO: (6) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 24.316644ms)
May 22 02:26:31.242: INFO: (6) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 29.258171ms)
May 22 02:26:31.247: INFO: (6) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 33.342275ms)
May 22 02:26:31.247: INFO: (6) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 34.168482ms)
May 22 02:26:31.247: INFO: (6) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 34.41968ms)
May 22 02:26:31.260: INFO: (7) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 12.598716ms)
May 22 02:26:31.266: INFO: (7) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 18.679898ms)
May 22 02:26:31.267: INFO: (7) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 19.158666ms)
May 22 02:26:31.268: INFO: (7) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 19.116471ms)
May 22 02:26:31.268: INFO: (7) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 20.240681ms)
May 22 02:26:31.270: INFO: (7) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 21.065802ms)
May 22 02:26:31.270: INFO: (7) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 21.817395ms)
May 22 02:26:31.270: INFO: (7) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 22.298792ms)
May 22 02:26:31.270: INFO: (7) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 21.65173ms)
May 22 02:26:31.271: INFO: (7) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 21.266323ms)
May 22 02:26:31.273: INFO: (7) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 23.532318ms)
May 22 02:26:31.274: INFO: (7) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 25.434041ms)
May 22 02:26:31.281: INFO: (7) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 33.309639ms)
May 22 02:26:31.281: INFO: (7) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 31.881467ms)
May 22 02:26:31.281: INFO: (7) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 32.434648ms)
May 22 02:26:31.281: INFO: (7) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 32.613282ms)
May 22 02:26:31.294: INFO: (8) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 12.969215ms)
May 22 02:26:31.297: INFO: (8) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 15.17374ms)
May 22 02:26:31.300: INFO: (8) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 19.036074ms)
May 22 02:26:31.301: INFO: (8) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 17.886783ms)
May 22 02:26:31.301: INFO: (8) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 18.555483ms)
May 22 02:26:31.303: INFO: (8) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 19.865973ms)
May 22 02:26:31.303: INFO: (8) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 20.272026ms)
May 22 02:26:31.303: INFO: (8) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 20.531344ms)
May 22 02:26:31.303: INFO: (8) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 21.620061ms)
May 22 02:26:31.303: INFO: (8) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 20.767141ms)
May 22 02:26:31.309: INFO: (8) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 26.574886ms)
May 22 02:26:31.312: INFO: (8) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 29.665978ms)
May 22 02:26:31.312: INFO: (8) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 29.824087ms)
May 22 02:26:31.312: INFO: (8) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 29.301887ms)
May 22 02:26:31.312: INFO: (8) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 28.571522ms)
May 22 02:26:31.312: INFO: (8) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 28.66811ms)
May 22 02:26:31.329: INFO: (9) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 17.166301ms)
May 22 02:26:31.335: INFO: (9) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 22.557328ms)
May 22 02:26:31.337: INFO: (9) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 24.904151ms)
May 22 02:26:31.337: INFO: (9) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 25.438508ms)
May 22 02:26:31.337: INFO: (9) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 25.037286ms)
May 22 02:26:31.337: INFO: (9) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 25.400579ms)
May 22 02:26:31.337: INFO: (9) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 25.356797ms)
May 22 02:26:31.337: INFO: (9) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 25.294225ms)
May 22 02:26:31.338: INFO: (9) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 26.072188ms)
May 22 02:26:31.338: INFO: (9) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 26.084067ms)
May 22 02:26:31.338: INFO: (9) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 25.95405ms)
May 22 02:26:31.338: INFO: (9) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 25.908671ms)
May 22 02:26:31.346: INFO: (9) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 33.847112ms)
May 22 02:26:31.351: INFO: (9) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 38.775493ms)
May 22 02:26:31.351: INFO: (9) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 38.98051ms)
May 22 02:26:31.351: INFO: (9) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 39.034919ms)
May 22 02:26:31.365: INFO: (10) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 13.599442ms)
May 22 02:26:31.369: INFO: (10) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 17.93413ms)
May 22 02:26:31.370: INFO: (10) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 16.711154ms)
May 22 02:26:31.371: INFO: (10) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 16.809795ms)
May 22 02:26:31.371: INFO: (10) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 17.192854ms)
May 22 02:26:31.374: INFO: (10) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 19.200753ms)
May 22 02:26:31.374: INFO: (10) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 22.18615ms)
May 22 02:26:31.374: INFO: (10) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 21.041008ms)
May 22 02:26:31.374: INFO: (10) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 22.460354ms)
May 22 02:26:31.374: INFO: (10) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 22.085589ms)
May 22 02:26:31.377: INFO: (10) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 24.062503ms)
May 22 02:26:31.378: INFO: (10) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 22.909729ms)
May 22 02:26:31.378: INFO: (10) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 23.393447ms)
May 22 02:26:31.378: INFO: (10) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 24.513812ms)
May 22 02:26:31.378: INFO: (10) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 25.191078ms)
May 22 02:26:31.378: INFO: (10) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 25.649812ms)
May 22 02:26:31.391: INFO: (11) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 11.996994ms)
May 22 02:26:31.396: INFO: (11) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 15.7273ms)
May 22 02:26:31.396: INFO: (11) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 17.386555ms)
May 22 02:26:31.396: INFO: (11) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 16.681541ms)
May 22 02:26:31.396: INFO: (11) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 17.747183ms)
May 22 02:26:31.396: INFO: (11) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 16.361849ms)
May 22 02:26:31.396: INFO: (11) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 14.398267ms)
May 22 02:26:31.397: INFO: (11) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 16.289125ms)
May 22 02:26:31.399: INFO: (11) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 19.349911ms)
May 22 02:26:31.405: INFO: (11) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 26.239473ms)
May 22 02:26:31.405: INFO: (11) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 23.851969ms)
May 22 02:26:31.405: INFO: (11) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 26.154317ms)
May 22 02:26:31.406: INFO: (11) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 25.030702ms)
May 22 02:26:31.408: INFO: (11) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 27.753129ms)
May 22 02:26:31.409: INFO: (11) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 28.909774ms)
May 22 02:26:31.409: INFO: (11) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 28.721363ms)
May 22 02:26:31.424: INFO: (12) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 14.376854ms)
May 22 02:26:31.424: INFO: (12) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 14.847403ms)
May 22 02:26:31.432: INFO: (12) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 21.252618ms)
May 22 02:26:31.432: INFO: (12) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 21.129156ms)
May 22 02:26:31.445: INFO: (12) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 34.57342ms)
May 22 02:26:31.445: INFO: (12) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 34.477178ms)
May 22 02:26:31.445: INFO: (12) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 35.036877ms)
May 22 02:26:31.445: INFO: (12) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 34.354426ms)
May 22 02:26:31.445: INFO: (12) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 35.398442ms)
May 22 02:26:31.445: INFO: (12) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 35.27695ms)
May 22 02:26:31.445: INFO: (12) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 33.990718ms)
May 22 02:26:31.445: INFO: (12) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 35.644703ms)
May 22 02:26:31.449: INFO: (12) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 37.446417ms)
May 22 02:26:31.452: INFO: (12) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 41.405532ms)
May 22 02:26:31.452: INFO: (12) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 39.987397ms)
May 22 02:26:31.452: INFO: (12) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 40.318184ms)
May 22 02:26:31.471: INFO: (13) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 16.720418ms)
May 22 02:26:31.476: INFO: (13) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 23.927505ms)
May 22 02:26:31.476: INFO: (13) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 22.753282ms)
May 22 02:26:31.476: INFO: (13) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 22.085478ms)
May 22 02:26:31.476: INFO: (13) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 22.659904ms)
May 22 02:26:31.476: INFO: (13) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 22.533888ms)
May 22 02:26:31.476: INFO: (13) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 24.456754ms)
May 22 02:26:31.478: INFO: (13) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 25.322862ms)
May 22 02:26:31.479: INFO: (13) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 25.882783ms)
May 22 02:26:31.479: INFO: (13) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 26.66626ms)
May 22 02:26:31.483: INFO: (13) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 30.788918ms)
May 22 02:26:31.487: INFO: (13) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 33.523231ms)
May 22 02:26:31.487: INFO: (13) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 34.566909ms)
May 22 02:26:31.487: INFO: (13) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 34.720476ms)
May 22 02:26:31.487: INFO: (13) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 33.477393ms)
May 22 02:26:31.490: INFO: (13) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 36.241301ms)
May 22 02:26:31.502: INFO: (14) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 12.212308ms)
May 22 02:26:31.503: INFO: (14) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 13.3236ms)
May 22 02:26:31.503: INFO: (14) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 12.884558ms)
May 22 02:26:31.507: INFO: (14) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 17.443969ms)
May 22 02:26:31.507: INFO: (14) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 15.362176ms)
May 22 02:26:31.507: INFO: (14) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 16.544315ms)
May 22 02:26:31.509: INFO: (14) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 18.191239ms)
May 22 02:26:31.511: INFO: (14) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 20.74916ms)
May 22 02:26:31.511: INFO: (14) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 20.216475ms)
May 22 02:26:31.511: INFO: (14) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 19.109092ms)
May 22 02:26:31.512: INFO: (14) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 19.744949ms)
May 22 02:26:31.513: INFO: (14) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 23.126587ms)
May 22 02:26:31.514: INFO: (14) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 23.150216ms)
May 22 02:26:31.514: INFO: (14) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 22.327304ms)
May 22 02:26:31.520: INFO: (14) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 28.570998ms)
May 22 02:26:31.520: INFO: (14) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 28.845427ms)
May 22 02:26:31.532: INFO: (15) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 12.095243ms)
May 22 02:26:31.545: INFO: (15) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 22.524297ms)
May 22 02:26:31.545: INFO: (15) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 22.932198ms)
May 22 02:26:31.545: INFO: (15) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 22.787303ms)
May 22 02:26:31.545: INFO: (15) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 24.607372ms)
May 22 02:26:31.546: INFO: (15) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 24.452964ms)
May 22 02:26:31.546: INFO: (15) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 24.07286ms)
May 22 02:26:31.546: INFO: (15) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 23.466365ms)
May 22 02:26:31.547: INFO: (15) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 24.80215ms)
May 22 02:26:31.547: INFO: (15) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 23.323129ms)
May 22 02:26:31.547: INFO: (15) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 25.508284ms)
May 22 02:26:31.553: INFO: (15) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 30.104573ms)
May 22 02:26:31.554: INFO: (15) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 32.359127ms)
May 22 02:26:31.554: INFO: (15) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 30.855111ms)
May 22 02:26:31.554: INFO: (15) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 33.484294ms)
May 22 02:26:31.554: INFO: (15) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 31.276298ms)
May 22 02:26:31.567: INFO: (16) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 12.621445ms)
May 22 02:26:31.571: INFO: (16) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 16.197275ms)
May 22 02:26:31.571: INFO: (16) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 15.509931ms)
May 22 02:26:31.571: INFO: (16) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 15.767969ms)
May 22 02:26:31.574: INFO: (16) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 17.879719ms)
May 22 02:26:31.574: INFO: (16) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 18.645631ms)
May 22 02:26:31.574: INFO: (16) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 19.123266ms)
May 22 02:26:31.575: INFO: (16) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 20.51271ms)
May 22 02:26:31.575: INFO: (16) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 20.300301ms)
May 22 02:26:31.577: INFO: (16) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 21.326192ms)
May 22 02:26:31.580: INFO: (16) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 25.42613ms)
May 22 02:26:31.580: INFO: (16) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 26.027661ms)
May 22 02:26:31.580: INFO: (16) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 24.949106ms)
May 22 02:26:31.580: INFO: (16) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 25.806283ms)
May 22 02:26:31.580: INFO: (16) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 24.863588ms)
May 22 02:26:31.580: INFO: (16) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 25.966527ms)
May 22 02:26:31.595: INFO: (17) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 14.824563ms)
May 22 02:26:31.596: INFO: (17) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 15.192323ms)
May 22 02:26:31.598: INFO: (17) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 17.106504ms)
May 22 02:26:31.604: INFO: (17) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 22.466068ms)
May 22 02:26:31.604: INFO: (17) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 22.653537ms)
May 22 02:26:31.604: INFO: (17) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 22.744651ms)
May 22 02:26:31.608: INFO: (17) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 26.43015ms)
May 22 02:26:31.608: INFO: (17) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 26.765647ms)
May 22 02:26:31.609: INFO: (17) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 26.977556ms)
May 22 02:26:31.610: INFO: (17) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 28.800354ms)
May 22 02:26:31.614: INFO: (17) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 32.793791ms)
May 22 02:26:31.617: INFO: (17) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 35.615624ms)
May 22 02:26:31.617: INFO: (17) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 36.448807ms)
May 22 02:26:31.617: INFO: (17) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 35.741149ms)
May 22 02:26:31.617: INFO: (17) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 36.321515ms)
May 22 02:26:31.618: INFO: (17) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 36.60143ms)
May 22 02:26:31.629: INFO: (18) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 11.528552ms)
May 22 02:26:31.637: INFO: (18) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 16.559911ms)
May 22 02:26:31.638: INFO: (18) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 19.271886ms)
May 22 02:26:31.638: INFO: (18) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 18.829584ms)
May 22 02:26:31.638: INFO: (18) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 19.041719ms)
May 22 02:26:31.639: INFO: (18) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 19.600194ms)
May 22 02:26:31.639: INFO: (18) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 19.588843ms)
May 22 02:26:31.639: INFO: (18) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 19.432235ms)
May 22 02:26:31.640: INFO: (18) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 20.943993ms)
May 22 02:26:31.641: INFO: (18) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 22.535713ms)
May 22 02:26:31.647: INFO: (18) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 29.138057ms)
May 22 02:26:31.647: INFO: (18) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 27.721062ms)
May 22 02:26:31.649: INFO: (18) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 30.913396ms)
May 22 02:26:31.649: INFO: (18) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 29.62082ms)
May 22 02:26:31.649: INFO: (18) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 29.527529ms)
May 22 02:26:31.649: INFO: (18) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 29.245041ms)
May 22 02:26:31.663: INFO: (19) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:162/proxy/: bar (200; 11.362253ms)
May 22 02:26:31.669: INFO: (19) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:460/proxy/: tls baz (200; 17.965567ms)
May 22 02:26:31.669: INFO: (19) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:1080/proxy/rewriteme">... (200; 16.168894ms)
May 22 02:26:31.669: INFO: (19) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:160/proxy/: foo (200; 17.289559ms)
May 22 02:26:31.671: INFO: (19) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7:1080/proxy/rewriteme">test<... (200; 20.421221ms)
May 22 02:26:31.671: INFO: (19) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:462/proxy/: tls qux (200; 21.519003ms)
May 22 02:26:31.671: INFO: (19) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:162/proxy/: bar (200; 18.249111ms)
May 22 02:26:31.671: INFO: (19) /api/v1/namespaces/proxy-8288/pods/http:proxy-service-v86jr-vshr7:160/proxy/: foo (200; 21.321337ms)
May 22 02:26:31.672: INFO: (19) /api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/https:proxy-service-v86jr-vshr7:443/proxy/tlsrewritem... (200; 20.136084ms)
May 22 02:26:31.673: INFO: (19) /api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/: <a href="/api/v1/namespaces/proxy-8288/pods/proxy-service-v86jr-vshr7/proxy/rewriteme">test</a> (200; 21.690747ms)
May 22 02:26:31.674: INFO: (19) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname2/proxy/: tls qux (200; 21.411709ms)
May 22 02:26:31.677: INFO: (19) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname2/proxy/: bar (200; 26.967136ms)
May 22 02:26:31.679: INFO: (19) /api/v1/namespaces/proxy-8288/services/https:proxy-service-v86jr:tlsportname1/proxy/: tls baz (200; 28.774835ms)
May 22 02:26:31.680: INFO: (19) /api/v1/namespaces/proxy-8288/services/http:proxy-service-v86jr:portname1/proxy/: foo (200; 27.89188ms)
May 22 02:26:31.684: INFO: (19) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname1/proxy/: foo (200; 34.302918ms)
May 22 02:26:31.684: INFO: (19) /api/v1/namespaces/proxy-8288/services/proxy-service-v86jr:portname2/proxy/: bar (200; 33.18065ms)
STEP: deleting ReplicationController proxy-service-v86jr in namespace proxy-8288, will wait for the garbage collector to delete the pods
May 22 02:26:31.748: INFO: Deleting ReplicationController proxy-service-v86jr took: 8.148447ms
May 22 02:26:32.050: INFO: Terminating ReplicationController proxy-service-v86jr pods took: 301.721335ms
[AfterEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:26:42.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8288" for this suite.
May 22 02:26:48.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:26:48.544: INFO: namespace proxy-8288 deletion completed in 6.187791492s

• [SLOW TEST:27.774 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:26:48.545: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 22 02:26:48.739: INFO: Waiting up to 5m0s for pod "pod-0d281f9b-7c39-11e9-8c5e-b202ea6dae39" in namespace "emptydir-3867" to be "success or failure"
May 22 02:26:48.747: INFO: Pod "pod-0d281f9b-7c39-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118749ms
May 22 02:26:50.752: INFO: Pod "pod-0d281f9b-7c39-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013113262s
STEP: Saw pod success
May 22 02:26:50.752: INFO: Pod "pod-0d281f9b-7c39-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:26:50.758: INFO: Trying to get logs from node worker01 pod pod-0d281f9b-7c39-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:26:50.796: INFO: Waiting for pod pod-0d281f9b-7c39-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:26:50.802: INFO: Pod pod-0d281f9b-7c39-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:26:50.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3867" for this suite.
May 22 02:26:56.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:26:57.007: INFO: namespace emptydir-3867 deletion completed in 6.190379118s

• [SLOW TEST:8.462 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:26:57.008: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 22 02:26:57.187: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May 22 02:27:04.262: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:27:04.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6011" for this suite.
May 22 02:27:10.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:27:10.427: INFO: namespace pods-6011 deletion completed in 6.154299325s

• [SLOW TEST:13.419 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:27:10.427: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:27:10.615: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a3183b7-7c39-11e9-8c5e-b202ea6dae39" in namespace "downward-api-3723" to be "success or failure"
May 22 02:27:10.637: INFO: Pod "downwardapi-volume-1a3183b7-7c39-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 21.494305ms
May 22 02:27:12.642: INFO: Pod "downwardapi-volume-1a3183b7-7c39-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026764333s
STEP: Saw pod success
May 22 02:27:12.642: INFO: Pod "downwardapi-volume-1a3183b7-7c39-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:27:12.647: INFO: Trying to get logs from node worker01 pod downwardapi-volume-1a3183b7-7c39-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:27:12.679: INFO: Waiting for pod downwardapi-volume-1a3183b7-7c39-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:27:12.694: INFO: Pod downwardapi-volume-1a3183b7-7c39-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:27:12.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3723" for this suite.
May 22 02:27:18.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:27:18.883: INFO: namespace downward-api-3723 deletion completed in 6.17916562s

• [SLOW TEST:8.456 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:27:18.883: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3954
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:27:39.109: INFO: Container started at 2019-05-22 02:27:20 +0000 UTC, pod became ready at 2019-05-22 02:27:37 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:27:39.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3954" for this suite.
May 22 02:28:01.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:28:01.330: INFO: namespace container-probe-3954 deletion completed in 22.210747116s

• [SLOW TEST:42.447 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:28:01.330: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-38890fb4-7c39-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:28:01.525: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-388a6499-7c39-11e9-8c5e-b202ea6dae39" in namespace "projected-9879" to be "success or failure"
May 22 02:28:01.531: INFO: Pod "pod-projected-secrets-388a6499-7c39-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.597808ms
May 22 02:28:03.541: INFO: Pod "pod-projected-secrets-388a6499-7c39-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016500843s
May 22 02:28:05.547: INFO: Pod "pod-projected-secrets-388a6499-7c39-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022566035s
STEP: Saw pod success
May 22 02:28:05.547: INFO: Pod "pod-projected-secrets-388a6499-7c39-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:28:05.555: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-388a6499-7c39-11e9-8c5e-b202ea6dae39 container secret-volume-test: <nil>
STEP: delete the pod
May 22 02:28:05.599: INFO: Waiting for pod pod-projected-secrets-388a6499-7c39-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:28:05.608: INFO: Pod pod-projected-secrets-388a6499-7c39-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:28:05.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9879" for this suite.
May 22 02:28:11.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:28:11.781: INFO: namespace projected-9879 deletion completed in 6.164340321s

• [SLOW TEST:10.451 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:28:11.782: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9492
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 22 02:28:11.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9492'
May 22 02:28:12.125: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 02:28:12.125: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
May 22 02:28:14.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9492'
May 22 02:28:14.236: INFO: stderr: ""
May 22 02:28:14.236: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:28:14.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9492" for this suite.
May 22 02:28:36.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:28:36.417: INFO: namespace kubectl-9492 deletion completed in 22.171939823s

• [SLOW TEST:24.635 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:28:36.417: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0522 02:28:46.657015      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 02:28:46.657: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:28:46.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1948" for this suite.
May 22 02:28:52.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:28:52.843: INFO: namespace gc-1948 deletion completed in 6.174216402s

• [SLOW TEST:16.426 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:28:52.843: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-573c462c-7c39-11e9-8c5e-b202ea6dae39
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:28:53.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4597" for this suite.
May 22 02:28:59.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:28:59.221: INFO: namespace configmap-4597 deletion completed in 6.200677525s

• [SLOW TEST:6.378 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:28:59.221: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6167
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-5b0af40d-7c39-11e9-8c5e-b202ea6dae39
STEP: Creating secret with name s-test-opt-upd-5b0af467-7c39-11e9-8c5e-b202ea6dae39
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-5b0af40d-7c39-11e9-8c5e-b202ea6dae39
STEP: Updating secret s-test-opt-upd-5b0af467-7c39-11e9-8c5e-b202ea6dae39
STEP: Creating secret with name s-test-opt-create-5b0af48b-7c39-11e9-8c5e-b202ea6dae39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:29:03.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6167" for this suite.
May 22 02:29:25.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:29:25.755: INFO: namespace secrets-6167 deletion completed in 22.16935033s

• [SLOW TEST:26.534 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:29:25.755: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8134
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:29:25.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6adbe1cc-7c39-11e9-8c5e-b202ea6dae39" in namespace "projected-8134" to be "success or failure"
May 22 02:29:25.955: INFO: Pod "downwardapi-volume-6adbe1cc-7c39-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.369624ms
May 22 02:29:27.960: INFO: Pod "downwardapi-volume-6adbe1cc-7c39-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014647161s
STEP: Saw pod success
May 22 02:29:27.960: INFO: Pod "downwardapi-volume-6adbe1cc-7c39-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:29:27.965: INFO: Trying to get logs from node worker01 pod downwardapi-volume-6adbe1cc-7c39-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:29:27.996: INFO: Waiting for pod downwardapi-volume-6adbe1cc-7c39-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:29:28.007: INFO: Pod downwardapi-volume-6adbe1cc-7c39-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:29:28.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8134" for this suite.
May 22 02:29:34.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:29:34.186: INFO: namespace projected-8134 deletion completed in 6.169818817s

• [SLOW TEST:8.430 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:29:34.186: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
May 22 02:29:34.364: INFO: Waiting up to 5m0s for pod "var-expansion-6fe12b3d-7c39-11e9-8c5e-b202ea6dae39" in namespace "var-expansion-6124" to be "success or failure"
May 22 02:29:34.370: INFO: Pod "var-expansion-6fe12b3d-7c39-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.363321ms
May 22 02:29:36.375: INFO: Pod "var-expansion-6fe12b3d-7c39-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01110754s
STEP: Saw pod success
May 22 02:29:36.375: INFO: Pod "var-expansion-6fe12b3d-7c39-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:29:36.380: INFO: Trying to get logs from node worker01 pod var-expansion-6fe12b3d-7c39-11e9-8c5e-b202ea6dae39 container dapi-container: <nil>
STEP: delete the pod
May 22 02:29:36.414: INFO: Waiting for pod var-expansion-6fe12b3d-7c39-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:29:36.423: INFO: Pod var-expansion-6fe12b3d-7c39-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:29:36.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6124" for this suite.
May 22 02:29:42.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:29:42.605: INFO: namespace var-expansion-6124 deletion completed in 6.168525802s

• [SLOW TEST:8.419 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:29:42.605: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 22 02:29:42.768: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 22 02:29:42.798: INFO: Waiting for terminating namespaces to be deleted...
May 22 02:29:42.808: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
May 22 02:29:42.825: INFO: kube-flannel-ds-amd64-bd2jm from kube-system started at 2019-05-20 03:44:11 +0000 UTC (1 container statuses recorded)
May 22 02:29:42.825: INFO: 	Container kube-flannel ready: true, restart count 5
May 22 02:29:42.825: INFO: sonobuoy-e2e-job-858a03c85b4b4f46 from heptio-sonobuoy started at 2019-05-22 01:41:45 +0000 UTC (2 container statuses recorded)
May 22 02:29:42.825: INFO: 	Container e2e ready: true, restart count 0
May 22 02:29:42.825: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 02:29:42.825: INFO: tiller-deploy-6b9dd779b9-rlphv from kube-system started at 2019-05-20 03:44:51 +0000 UTC (1 container statuses recorded)
May 22 02:29:42.825: INFO: 	Container tiller ready: true, restart count 9
May 22 02:29:42.825: INFO: sonobuoy-systemd-logs-daemon-set-e3be60816e5648ce-qxw5h from heptio-sonobuoy started at 2019-05-22 01:41:46 +0000 UTC (2 container statuses recorded)
May 22 02:29:42.825: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 02:29:42.825: INFO: 	Container systemd-logs ready: true, restart count 0
May 22 02:29:42.825: INFO: kube-proxy-l7wpq from kube-system started at 2019-05-20 03:44:11 +0000 UTC (1 container statuses recorded)
May 22 02:29:42.825: INFO: 	Container kube-proxy ready: true, restart count 5
May 22 02:29:42.825: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-22 01:41:43 +0000 UTC (1 container statuses recorded)
May 22 02:29:42.825: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15a0e062105fd94b], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:29:43.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3578" for this suite.
May 22 02:29:49.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:29:50.083: INFO: namespace sched-pred-3578 deletion completed in 6.201578023s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.478 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:29:50.083: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:29:50.245: INFO: Creating deployment "test-recreate-deployment"
May 22 02:29:50.257: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 22 02:29:50.275: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 22 02:29:52.287: INFO: Waiting deployment "test-recreate-deployment" to complete
May 22 02:29:52.292: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694088990, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694088990, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694088990, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694088990, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 02:29:54.298: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 22 02:29:54.313: INFO: Updating deployment test-recreate-deployment
May 22 02:29:54.313: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 02:29:54.436: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-8588,SelfLink:/apis/apps/v1/namespaces/deployment-8588/deployments/test-recreate-deployment,UID:795b2623-7c39-11e9-b253-000c290a0f16,ResourceVersion:137562,Generation:2,CreationTimestamp:2019-05-22 02:29:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-22 02:29:54 +0000 UTC 2019-05-22 02:29:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-22 02:29:54 +0000 UTC 2019-05-22 02:29:50 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 22 02:29:54.448: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-8588,SelfLink:/apis/apps/v1/namespaces/deployment-8588/replicasets/test-recreate-deployment-c9cbd8684,UID:7bd091e8-7c39-11e9-85c4-000c296e7615,ResourceVersion:137558,Generation:1,CreationTimestamp:2019-05-22 02:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 795b2623-7c39-11e9-b253-000c290a0f16 0xc002f27320 0xc002f27321}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 02:29:54.448: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 22 02:29:54.448: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-8588,SelfLink:/apis/apps/v1/namespaces/deployment-8588/replicasets/test-recreate-deployment-7d57d5ff7c,UID:795cef99-7c39-11e9-85c4-000c296e7615,ResourceVersion:137550,Generation:2,CreationTimestamp:2019-05-22 02:29:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 795b2623-7c39-11e9-b253-000c290a0f16 0xc002f27257 0xc002f27258}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 02:29:54.457: INFO: Pod "test-recreate-deployment-c9cbd8684-lfltp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-lfltp,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-8588,SelfLink:/api/v1/namespaces/deployment-8588/pods/test-recreate-deployment-c9cbd8684-lfltp,UID:7bd175de-7c39-11e9-85c4-000c296e7615,ResourceVersion:137561,Generation:0,CreationTimestamp:2019-05-22 02:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 7bd091e8-7c39-11e9-85c4-000c296e7615 0xc002f27b50 0xc002f27b51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-j2tbc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-j2tbc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-j2tbc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002f27bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002f27be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:29:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:29:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:29:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:29:54 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:,StartTime:2019-05-22 02:29:54 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:29:54.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8588" for this suite.
May 22 02:30:00.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:30:00.641: INFO: namespace deployment-8588 deletion completed in 6.174293296s

• [SLOW TEST:10.558 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:30:00.641: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[]
May 22 02:30:00.841: INFO: Get endpoints failed (10.700536ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May 22 02:30:01.848: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[] (1.018256153s elapsed)
STEP: Creating pod pod1 in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[pod1:[100]]
May 22 02:30:03.898: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[pod1:[100]] (2.034925251s elapsed)
STEP: Creating pod pod2 in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[pod1:[100] pod2:[101]]
May 22 02:30:06.990: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[pod1:[100] pod2:[101]] (3.083332369s elapsed)
STEP: Deleting pod pod1 in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[pod2:[101]]
May 22 02:30:08.048: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[pod2:[101]] (1.042243523s elapsed)
STEP: Deleting pod pod2 in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[]
May 22 02:30:09.074: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[] (1.010880251s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:30:09.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9118" for this suite.
May 22 02:30:15.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:30:15.311: INFO: namespace services-9118 deletion completed in 6.174272855s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:14.670 seconds]
[sig-network] Services
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:30:15.311: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-88650d39-7c39-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:30:15.508: INFO: Waiting up to 5m0s for pod "pod-secrets-88667827-7c39-11e9-8c5e-b202ea6dae39" in namespace "secrets-3307" to be "success or failure"
May 22 02:30:15.517: INFO: Pod "pod-secrets-88667827-7c39-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.877167ms
May 22 02:30:17.524: INFO: Pod "pod-secrets-88667827-7c39-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01643248s
STEP: Saw pod success
May 22 02:30:17.525: INFO: Pod "pod-secrets-88667827-7c39-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:30:17.529: INFO: Trying to get logs from node worker01 pod pod-secrets-88667827-7c39-11e9-8c5e-b202ea6dae39 container secret-volume-test: <nil>
STEP: delete the pod
May 22 02:30:17.558: INFO: Waiting for pod pod-secrets-88667827-7c39-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:30:17.563: INFO: Pod pod-secrets-88667827-7c39-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:30:17.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3307" for this suite.
May 22 02:30:23.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:30:23.757: INFO: namespace secrets-3307 deletion completed in 6.182650705s

• [SLOW TEST:8.445 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:30:23.757: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1111
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8d6f0901-7c39-11e9-8c5e-b202ea6dae39
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8d6f0901-7c39-11e9-8c5e-b202ea6dae39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:30:28.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1111" for this suite.
May 22 02:30:50.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:30:50.216: INFO: namespace projected-1111 deletion completed in 22.192346892s

• [SLOW TEST:26.459 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:30:50.216: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2764
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-9d3259b3-7c39-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 02:30:50.404: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9d33bb4b-7c39-11e9-8c5e-b202ea6dae39" in namespace "projected-2764" to be "success or failure"
May 22 02:30:50.414: INFO: Pod "pod-projected-configmaps-9d33bb4b-7c39-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.668232ms
May 22 02:30:52.419: INFO: Pod "pod-projected-configmaps-9d33bb4b-7c39-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01586524s
STEP: Saw pod success
May 22 02:30:52.420: INFO: Pod "pod-projected-configmaps-9d33bb4b-7c39-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:30:52.424: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-9d33bb4b-7c39-11e9-8c5e-b202ea6dae39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 02:30:52.463: INFO: Waiting for pod pod-projected-configmaps-9d33bb4b-7c39-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:30:52.474: INFO: Pod pod-projected-configmaps-9d33bb4b-7c39-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:30:52.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2764" for this suite.
May 22 02:30:58.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:30:58.654: INFO: namespace projected-2764 deletion completed in 6.174506915s

• [SLOW TEST:8.438 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:30:58.654: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 22 02:30:58.852: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2164,SelfLink:/api/v1/namespaces/watch-2164/configmaps/e2e-watch-test-watch-closed,UID:a23b763d-7c39-11e9-b253-000c290a0f16,ResourceVersion:137852,Generation:0,CreationTimestamp:2019-05-22 02:30:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 02:30:58.852: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2164,SelfLink:/api/v1/namespaces/watch-2164/configmaps/e2e-watch-test-watch-closed,UID:a23b763d-7c39-11e9-b253-000c290a0f16,ResourceVersion:137853,Generation:0,CreationTimestamp:2019-05-22 02:30:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 22 02:30:58.887: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2164,SelfLink:/api/v1/namespaces/watch-2164/configmaps/e2e-watch-test-watch-closed,UID:a23b763d-7c39-11e9-b253-000c290a0f16,ResourceVersion:137854,Generation:0,CreationTimestamp:2019-05-22 02:30:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 02:30:58.888: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2164,SelfLink:/api/v1/namespaces/watch-2164/configmaps/e2e-watch-test-watch-closed,UID:a23b763d-7c39-11e9-b253-000c290a0f16,ResourceVersion:137855,Generation:0,CreationTimestamp:2019-05-22 02:30:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:30:58.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2164" for this suite.
May 22 02:31:04.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:31:05.089: INFO: namespace watch-2164 deletion completed in 6.191684285s

• [SLOW TEST:6.434 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:31:05.089: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 22 02:31:05.273: INFO: Waiting up to 5m0s for pod "pod-a60fc6eb-7c39-11e9-8c5e-b202ea6dae39" in namespace "emptydir-5818" to be "success or failure"
May 22 02:31:05.286: INFO: Pod "pod-a60fc6eb-7c39-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.005209ms
May 22 02:31:07.299: INFO: Pod "pod-a60fc6eb-7c39-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026516344s
STEP: Saw pod success
May 22 02:31:07.299: INFO: Pod "pod-a60fc6eb-7c39-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:31:07.313: INFO: Trying to get logs from node worker01 pod pod-a60fc6eb-7c39-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:31:07.370: INFO: Waiting for pod pod-a60fc6eb-7c39-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:31:07.378: INFO: Pod pod-a60fc6eb-7c39-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:31:07.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5818" for this suite.
May 22 02:31:13.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:31:13.565: INFO: namespace emptydir-5818 deletion completed in 6.178832362s

• [SLOW TEST:8.477 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:31:13.565: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:31:17.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2986" for this suite.
May 22 02:31:23.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:31:24.001: INFO: namespace emptydir-wrapper-2986 deletion completed in 6.165440756s

• [SLOW TEST:10.436 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:31:24.001: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8885
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8885
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8885
May 22 02:31:24.218: INFO: Found 0 stateful pods, waiting for 1
May 22 02:31:34.223: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 22 02:31:34.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:31:34.542: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:31:34.542: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:31:34.542: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 02:31:34.546: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 22 02:31:44.557: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 22 02:31:44.557: INFO: Waiting for statefulset status.replicas updated to 0
May 22 02:31:44.606: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998975s
May 22 02:31:45.617: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.986199955s
May 22 02:31:46.624: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975033307s
May 22 02:31:47.629: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.967732561s
May 22 02:31:48.636: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.963098849s
May 22 02:31:49.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.955672218s
May 22 02:31:50.648: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951348811s
May 22 02:31:51.660: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.944281688s
May 22 02:31:52.668: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.931975709s
May 22 02:31:53.677: INFO: Verifying statefulset ss doesn't scale past 1 for another 923.528587ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8885
May 22 02:31:54.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:31:55.030: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 02:31:55.031: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 02:31:55.031: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 02:31:55.037: INFO: Found 1 stateful pods, waiting for 3
May 22 02:32:05.042: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 02:32:05.042: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 02:32:05.042: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 22 02:32:05.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:32:05.377: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:32:05.377: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:32:05.377: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 02:32:05.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:32:05.686: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:32:05.686: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:32:05.686: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 02:32:05.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:32:06.094: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:32:06.094: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:32:06.094: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 02:32:06.094: INFO: Waiting for statefulset status.replicas updated to 0
May 22 02:32:06.101: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 22 02:32:16.114: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 22 02:32:16.114: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 22 02:32:16.114: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 22 02:32:16.143: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999424s
May 22 02:32:17.150: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989356809s
May 22 02:32:18.162: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982541528s
May 22 02:32:19.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.969868026s
May 22 02:32:20.180: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964737752s
May 22 02:32:21.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.951805659s
May 22 02:32:22.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.938946242s
May 22 02:32:23.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.93361855s
May 22 02:32:24.218: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.926196507s
May 22 02:32:25.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 914.044726ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8885
May 22 02:32:26.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:32:26.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 02:32:26.579: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 02:32:26.579: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 02:32:26.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:32:26.890: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 02:32:26.890: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 02:32:26.890: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 02:32:26.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:32:27.501: INFO: rc: 126
May 22 02:32:27.501: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "process_linux.go:91: executing setns process caused \"exit status 21\"": unknown
 command terminated with exit code 126
 [] <nil> 0xc00246fc50 exit status 126 <nil> <nil> true [0xc001b4a528 0xc001b4a570 0xc001b4a590] [0xc001b4a528 0xc001b4a570 0xc001b4a590] [0xc001b4a550 0xc001b4a588] [0x9c00a0 0x9c00a0] 0xc002f48a80 <nil>}:
Command stdout:
OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "process_linux.go:91: executing setns process caused \"exit status 21\"": unknown

stderr:
command terminated with exit code 126

error:
exit status 126

May 22 02:32:37.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:32:37.583: INFO: rc: 1
May 22 02:32:37.583: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023dd9b0 exit status 1 <nil> <nil> true [0xc002b02240 0xc002b02270 0xc002b022a0] [0xc002b02240 0xc002b02270 0xc002b022a0] [0xc002b02260 0xc002b02290] [0x9c00a0 0x9c00a0] 0xc0027a2720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:32:47.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:32:47.657: INFO: rc: 1
May 22 02:32:47.657: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246ffb0 exit status 1 <nil> <nil> true [0xc001b4a598 0xc001b4a5c8 0xc001b4a610] [0xc001b4a598 0xc001b4a5c8 0xc001b4a610] [0xc001b4a5b8 0xc001b4a5f0] [0x9c00a0 0x9c00a0] 0xc002f49140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:32:57.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:32:57.737: INFO: rc: 1
May 22 02:32:57.737: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00297c330 exit status 1 <nil> <nil> true [0xc001b4a620 0xc001b4a648 0xc001b4a6a0] [0xc001b4a620 0xc001b4a648 0xc001b4a6a0] [0xc001b4a638 0xc001b4a670] [0x9c00a0 0x9c00a0] 0xc002f49800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:33:07.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:33:07.838: INFO: rc: 1
May 22 02:33:07.838: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00297c7b0 exit status 1 <nil> <nil> true [0xc001b4a6c8 0xc001b4a6f0 0xc001b4a738] [0xc001b4a6c8 0xc001b4a6f0 0xc001b4a738] [0xc001b4a6e8 0xc001b4a720] [0x9c00a0 0x9c00a0] 0xc002f49e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:33:17.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:33:17.945: INFO: rc: 1
May 22 02:33:17.945: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023ddd40 exit status 1 <nil> <nil> true [0xc002b022b8 0xc002b022e8 0xc002b02320] [0xc002b022b8 0xc002b022e8 0xc002b02320] [0xc002b022d8 0xc002b02310] [0x9c00a0 0x9c00a0] 0xc0027a2de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:33:27.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:33:28.027: INFO: rc: 1
May 22 02:33:28.027: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0015ea1e0 exit status 1 <nil> <nil> true [0xc002b02340 0xc002b02370 0xc002b02398] [0xc002b02340 0xc002b02370 0xc002b02398] [0xc002b02360 0xc002b02388] [0x9c00a0 0x9c00a0] 0xc0027a3440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:33:38.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:33:38.099: INFO: rc: 1
May 22 02:33:38.099: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0015ea690 exit status 1 <nil> <nil> true [0xc002b023b0 0xc002b023f8 0xc002b02438] [0xc002b023b0 0xc002b023f8 0xc002b02438] [0xc002b023e8 0xc002b02428] [0x9c00a0 0x9c00a0] 0xc0027a3800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:33:48.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:33:48.194: INFO: rc: 1
May 22 02:33:48.194: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0015ea9c0 exit status 1 <nil> <nil> true [0xc002b02448 0xc002b02478 0xc002b024b8] [0xc002b02448 0xc002b02478 0xc002b024b8] [0xc002b02468 0xc002b024a8] [0x9c00a0 0x9c00a0] 0xc0027a3b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:33:58.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:33:58.283: INFO: rc: 1
May 22 02:33:58.284: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00297cb70 exit status 1 <nil> <nil> true [0xc001b4a740 0xc001b4a7a8 0xc001b4a7f0] [0xc001b4a740 0xc001b4a7a8 0xc001b4a7f0] [0xc001b4a788 0xc001b4a7d8] [0x9c00a0 0x9c00a0] 0xc0021b22a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:34:08.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:34:08.366: INFO: rc: 1
May 22 02:34:08.366: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023dc300 exit status 1 <nil> <nil> true [0xc001b4a010 0xc001b4a088 0xc001b4a0d8] [0xc001b4a010 0xc001b4a088 0xc001b4a0d8] [0xc001b4a068 0xc001b4a0b8] [0x9c00a0 0x9c00a0] 0xc002f48480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:34:18.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:34:18.443: INFO: rc: 1
May 22 02:34:18.443: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246e450 exit status 1 <nil> <nil> true [0xc002b02018 0xc002b02030 0xc002b02048] [0xc002b02018 0xc002b02030 0xc002b02048] [0xc002b02028 0xc002b02040] [0x9c00a0 0x9c00a0] 0xc002f82600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:34:28.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:34:28.518: INFO: rc: 1
May 22 02:34:28.518: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023dc660 exit status 1 <nil> <nil> true [0xc001b4a0f0 0xc001b4a120 0xc001b4a180] [0xc001b4a0f0 0xc001b4a120 0xc001b4a180] [0xc001b4a118 0xc001b4a158] [0x9c00a0 0x9c00a0] 0xc002f48b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:34:38.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:34:38.632: INFO: rc: 1
May 22 02:34:38.633: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246e7b0 exit status 1 <nil> <nil> true [0xc002b02050 0xc002b02068 0xc002b02080] [0xc002b02050 0xc002b02068 0xc002b02080] [0xc002b02060 0xc002b02078] [0x9c00a0 0x9c00a0] 0xc002f82c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:34:48.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:34:48.729: INFO: rc: 1
May 22 02:34:48.729: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246eb10 exit status 1 <nil> <nil> true [0xc002b02088 0xc002b020a0 0xc002b020b8] [0xc002b02088 0xc002b020a0 0xc002b020b8] [0xc002b02098 0xc002b020b0] [0x9c00a0 0x9c00a0] 0xc002f83020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:34:58.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:34:58.832: INFO: rc: 1
May 22 02:34:58.832: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023dc9c0 exit status 1 <nil> <nil> true [0xc001b4a198 0xc001b4a1d8 0xc001b4a208] [0xc001b4a198 0xc001b4a1d8 0xc001b4a208] [0xc001b4a1b8 0xc001b4a1f8] [0x9c00a0 0x9c00a0] 0xc002f49200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:35:08.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:35:08.917: INFO: rc: 1
May 22 02:35:08.917: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246ee70 exit status 1 <nil> <nil> true [0xc002b020c0 0xc002b020d8 0xc002b020f0] [0xc002b020c0 0xc002b020d8 0xc002b020f0] [0xc002b020d0 0xc002b020e8] [0x9c00a0 0x9c00a0] 0xc002f83380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:35:18.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:35:19.001: INFO: rc: 1
May 22 02:35:19.001: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023dd1a0 exit status 1 <nil> <nil> true [0xc001b4a228 0xc001b4a268 0xc001b4a290] [0xc001b4a228 0xc001b4a268 0xc001b4a290] [0xc001b4a260 0xc001b4a280] [0x9c00a0 0x9c00a0] 0xc002f498c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:35:29.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:35:29.074: INFO: rc: 1
May 22 02:35:29.074: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246f1d0 exit status 1 <nil> <nil> true [0xc002b020f8 0xc002b02110 0xc002b02128] [0xc002b020f8 0xc002b02110 0xc002b02128] [0xc002b02108 0xc002b02120] [0x9c00a0 0x9c00a0] 0xc002f836e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:35:39.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:35:39.144: INFO: rc: 1
May 22 02:35:39.144: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023dd500 exit status 1 <nil> <nil> true [0xc001b4a2a0 0xc001b4a2d0 0xc001b4a300] [0xc001b4a2a0 0xc001b4a2d0 0xc001b4a300] [0xc001b4a2c8 0xc001b4a2e8] [0x9c00a0 0x9c00a0] 0xc002f49f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:35:49.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:35:49.235: INFO: rc: 1
May 22 02:35:49.235: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246f560 exit status 1 <nil> <nil> true [0xc002b02130 0xc002b02148 0xc002b02160] [0xc002b02130 0xc002b02148 0xc002b02160] [0xc002b02140 0xc002b02158] [0x9c00a0 0x9c00a0] 0xc002f83b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:35:59.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:35:59.318: INFO: rc: 1
May 22 02:35:59.318: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246f890 exit status 1 <nil> <nil> true [0xc002b02168 0xc002b02180 0xc002b02198] [0xc002b02168 0xc002b02180 0xc002b02198] [0xc002b02178 0xc002b02190] [0x9c00a0 0x9c00a0] 0xc002f83ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:36:09.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:36:09.404: INFO: rc: 1
May 22 02:36:09.405: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023dc330 exit status 1 <nil> <nil> true [0xc001b4a010 0xc001b4a088 0xc001b4a0d8] [0xc001b4a010 0xc001b4a088 0xc001b4a0d8] [0xc001b4a068 0xc001b4a0b8] [0x9c00a0 0x9c00a0] 0xc002f48480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:36:19.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:36:19.492: INFO: rc: 1
May 22 02:36:19.492: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246e480 exit status 1 <nil> <nil> true [0xc002b02018 0xc002b02030 0xc002b02048] [0xc002b02018 0xc002b02030 0xc002b02048] [0xc002b02028 0xc002b02040] [0x9c00a0 0x9c00a0] 0xc003008300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:36:29.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:36:29.585: INFO: rc: 1
May 22 02:36:29.585: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023dc6c0 exit status 1 <nil> <nil> true [0xc001b4a0f0 0xc001b4a120 0xc001b4a180] [0xc001b4a0f0 0xc001b4a120 0xc001b4a180] [0xc001b4a118 0xc001b4a158] [0x9c00a0 0x9c00a0] 0xc002f48b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:36:39.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:36:39.681: INFO: rc: 1
May 22 02:36:39.681: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246e810 exit status 1 <nil> <nil> true [0xc002b02050 0xc002b02068 0xc002b02080] [0xc002b02050 0xc002b02068 0xc002b02080] [0xc002b02060 0xc002b02078] [0x9c00a0 0x9c00a0] 0xc003008660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:36:49.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:36:49.768: INFO: rc: 1
May 22 02:36:49.768: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246eb70 exit status 1 <nil> <nil> true [0xc002b02088 0xc002b020a0 0xc002b020b8] [0xc002b02088 0xc002b020a0 0xc002b020b8] [0xc002b02098 0xc002b020b0] [0x9c00a0 0x9c00a0] 0xc003008c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:36:59.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:36:59.869: INFO: rc: 1
May 22 02:36:59.869: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023dca80 exit status 1 <nil> <nil> true [0xc001b4a198 0xc001b4a1d8 0xc001b4a208] [0xc001b4a198 0xc001b4a1d8 0xc001b4a208] [0xc001b4a1b8 0xc001b4a1f8] [0x9c00a0 0x9c00a0] 0xc002f49200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:37:09.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:37:09.960: INFO: rc: 1
May 22 02:37:09.960: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246ef00 exit status 1 <nil> <nil> true [0xc002b020c0 0xc002b020d8 0xc002b020f0] [0xc002b020c0 0xc002b020d8 0xc002b020f0] [0xc002b020d0 0xc002b020e8] [0x9c00a0 0x9c00a0] 0xc003009320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:37:19.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:37:20.044: INFO: rc: 1
May 22 02:37:20.044: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00246f290 exit status 1 <nil> <nil> true [0xc002b020f8 0xc002b02110 0xc002b02128] [0xc002b020f8 0xc002b02110 0xc002b02128] [0xc002b02108 0xc002b02120] [0x9c00a0 0x9c00a0] 0xc0030098c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 22 02:37:30.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-8885 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:37:30.154: INFO: rc: 1
May 22 02:37:30.154: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
May 22 02:37:30.154: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 02:37:30.174: INFO: Deleting all statefulset in ns statefulset-8885
May 22 02:37:30.181: INFO: Scaling statefulset ss to 0
May 22 02:37:30.202: INFO: Waiting for statefulset status.replicas updated to 0
May 22 02:37:30.206: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:37:30.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8885" for this suite.
May 22 02:37:36.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:37:36.407: INFO: namespace statefulset-8885 deletion completed in 6.165021085s

• [SLOW TEST:372.406 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:37:36.407: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
May 22 02:37:36.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 --namespace=kubectl-5108 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 22 02:37:38.812: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 22 02:37:38.812: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:37:40.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5108" for this suite.
May 22 02:37:46.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:37:46.986: INFO: namespace kubectl-5108 deletion completed in 6.160582355s

• [SLOW TEST:10.578 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:37:46.986: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:37:47.176: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 22 02:37:52.188: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 22 02:37:52.188: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 22 02:37:54.198: INFO: Creating deployment "test-rollover-deployment"
May 22 02:37:54.233: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 22 02:37:56.255: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 22 02:37:56.267: INFO: Ensure that both replica sets have 1 created replica
May 22 02:37:56.279: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 22 02:37:56.305: INFO: Updating deployment test-rollover-deployment
May 22 02:37:56.305: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 22 02:37:58.317: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 22 02:37:58.324: INFO: Make sure deployment "test-rollover-deployment" is complete
May 22 02:37:58.333: INFO: all replica sets need to contain the pod-template-hash label
May 22 02:37:58.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089478, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 02:38:00.341: INFO: all replica sets need to contain the pod-template-hash label
May 22 02:38:00.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089478, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 02:38:02.342: INFO: all replica sets need to contain the pod-template-hash label
May 22 02:38:02.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089478, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 02:38:04.342: INFO: all replica sets need to contain the pod-template-hash label
May 22 02:38:04.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089478, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 02:38:06.348: INFO: all replica sets need to contain the pod-template-hash label
May 22 02:38:06.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089478, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089474, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 02:38:08.343: INFO: 
May 22 02:38:08.343: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 02:38:08.355: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-1773,SelfLink:/apis/apps/v1/namespaces/deployment-1773/deployments/test-rollover-deployment,UID:99d066b7-7c3a-11e9-b253-000c290a0f16,ResourceVersion:138981,Generation:2,CreationTimestamp:2019-05-22 02:37:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-22 02:37:54 +0000 UTC 2019-05-22 02:37:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-22 02:38:08 +0000 UTC 2019-05-22 02:37:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 22 02:38:08.362: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-1773,SelfLink:/apis/apps/v1/namespaces/deployment-1773/replicasets/test-rollover-deployment-766b4d6c9d,UID:9b11a75f-7c3a-11e9-85c4-000c296e7615,ResourceVersion:138970,Generation:2,CreationTimestamp:2019-05-22 02:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 99d066b7-7c3a-11e9-b253-000c290a0f16 0xc002fe1157 0xc002fe1158}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 22 02:38:08.362: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 22 02:38:08.362: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-1773,SelfLink:/apis/apps/v1/namespaces/deployment-1773/replicasets/test-rollover-controller,UID:959db0f3-7c3a-11e9-b253-000c290a0f16,ResourceVersion:138979,Generation:2,CreationTimestamp:2019-05-22 02:37:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 99d066b7-7c3a-11e9-b253-000c290a0f16 0xc002fe0fa7 0xc002fe0fa8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 02:38:08.362: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-1773,SelfLink:/apis/apps/v1/namespaces/deployment-1773/replicasets/test-rollover-deployment-6455657675,UID:99d97098-7c3a-11e9-85c4-000c296e7615,ResourceVersion:138935,Generation:2,CreationTimestamp:2019-05-22 02:37:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 99d066b7-7c3a-11e9-b253-000c290a0f16 0xc002fe1077 0xc002fe1078}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 02:38:08.367: INFO: Pod "test-rollover-deployment-766b4d6c9d-sg7jx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-sg7jx,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-1773,SelfLink:/api/v1/namespaces/deployment-1773/pods/test-rollover-deployment-766b4d6c9d-sg7jx,UID:9b1a1f5e-7c3a-11e9-85c4-000c296e7615,ResourceVersion:138948,Generation:0,CreationTimestamp:2019-05-22 02:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 9b11a75f-7c3a-11e9-85c4-000c296e7615 0xc002f269a7 0xc002f269a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5qgss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5qgss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-5qgss true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002f26a20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002f26a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:37:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:37:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:37:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:37:56 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.127,StartTime:2019-05-22 02:37:56 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-22 02:37:57 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2b4b6f54bd8feb814c381a4ba5fc2ed0a01f76c15de7de48ddea9f0f1686ddb7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:38:08.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1773" for this suite.
May 22 02:38:14.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:38:14.559: INFO: namespace deployment-1773 deletion completed in 6.185779128s

• [SLOW TEST:27.574 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:38:14.560: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-181
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:38:14.734: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 22 02:38:14.752: INFO: Pod name sample-pod: Found 0 pods out of 1
May 22 02:38:19.757: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 22 02:38:19.757: INFO: Creating deployment "test-rolling-update-deployment"
May 22 02:38:19.764: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 22 02:38:19.772: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 22 02:38:21.781: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 22 02:38:21.785: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 02:38:21.799: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-181,SelfLink:/apis/apps/v1/namespaces/deployment-181/deployments/test-rolling-update-deployment,UID:a90be885-7c3a-11e9-b253-000c290a0f16,ResourceVersion:139091,Generation:1,CreationTimestamp:2019-05-22 02:38:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-22 02:38:19 +0000 UTC 2019-05-22 02:38:19 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-22 02:38:21 +0000 UTC 2019-05-22 02:38:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 22 02:38:21.805: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-181,SelfLink:/apis/apps/v1/namespaces/deployment-181/replicasets/test-rolling-update-deployment-67599b4d9,UID:a90ef8ea-7c3a-11e9-85c4-000c296e7615,ResourceVersion:139080,Generation:1,CreationTimestamp:2019-05-22 02:38:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment a90be885-7c3a-11e9-b253-000c290a0f16 0xc002752360 0xc002752361}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 22 02:38:21.805: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 22 02:38:21.805: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-181,SelfLink:/apis/apps/v1/namespaces/deployment-181/replicasets/test-rolling-update-controller,UID:a60dbf5f-7c3a-11e9-b253-000c290a0f16,ResourceVersion:139089,Generation:2,CreationTimestamp:2019-05-22 02:38:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment a90be885-7c3a-11e9-b253-000c290a0f16 0xc002752297 0xc002752298}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 02:38:21.809: INFO: Pod "test-rolling-update-deployment-67599b4d9-clnhv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-clnhv,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-181,SelfLink:/api/v1/namespaces/deployment-181/pods/test-rolling-update-deployment-67599b4d9-clnhv,UID:a9100948-7c3a-11e9-85c4-000c296e7615,ResourceVersion:139079,Generation:0,CreationTimestamp:2019-05-22 02:38:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 a90ef8ea-7c3a-11e9-85c4-000c296e7615 0xc002599950 0xc002599951}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9x88l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9x88l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-9x88l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025999c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025999e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:38:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:38:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:38:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:38:19 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.129,StartTime:2019-05-22 02:38:19 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-22 02:38:20 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://85027792c596e7d84ff21374690a46344cf3b77cc398cd32c6a9f7c76e87a7b4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:38:21.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-181" for this suite.
May 22 02:38:27.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:38:27.985: INFO: namespace deployment-181 deletion completed in 6.166966372s

• [SLOW TEST:13.425 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:38:27.985: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2591
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
May 22 02:38:28.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 cluster-info'
May 22 02:38:28.300: INFO: stderr: ""
May 22 02:38:28.300: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:38:28.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2591" for this suite.
May 22 02:38:34.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:38:34.481: INFO: namespace kubectl-2591 deletion completed in 6.174208505s

• [SLOW TEST:6.496 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:38:34.481: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-b1ebd98a-7c3a-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:38:34.673: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b1ed4108-7c3a-11e9-8c5e-b202ea6dae39" in namespace "projected-4745" to be "success or failure"
May 22 02:38:34.681: INFO: Pod "pod-projected-secrets-b1ed4108-7c3a-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 7.554653ms
May 22 02:38:36.686: INFO: Pod "pod-projected-secrets-b1ed4108-7c3a-11e9-8c5e-b202ea6dae39": Phase="Running", Reason="", readiness=true. Elapsed: 2.012629143s
May 22 02:38:38.692: INFO: Pod "pod-projected-secrets-b1ed4108-7c3a-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018413341s
STEP: Saw pod success
May 22 02:38:38.692: INFO: Pod "pod-projected-secrets-b1ed4108-7c3a-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:38:38.697: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-b1ed4108-7c3a-11e9-8c5e-b202ea6dae39 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 02:38:38.740: INFO: Waiting for pod pod-projected-secrets-b1ed4108-7c3a-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:38:38.750: INFO: Pod pod-projected-secrets-b1ed4108-7c3a-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:38:38.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4745" for this suite.
May 22 02:38:44.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:38:44.935: INFO: namespace projected-4745 deletion completed in 6.171538996s

• [SLOW TEST:10.454 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:38:44.935: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:38:45.120: INFO: (0) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 12.74123ms)
May 22 02:38:45.129: INFO: (1) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.087712ms)
May 22 02:38:45.136: INFO: (2) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.927598ms)
May 22 02:38:45.146: INFO: (3) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.763154ms)
May 22 02:38:45.156: INFO: (4) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.253696ms)
May 22 02:38:45.167: INFO: (5) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.491729ms)
May 22 02:38:45.177: INFO: (6) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.338145ms)
May 22 02:38:45.186: INFO: (7) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 8.650983ms)
May 22 02:38:45.196: INFO: (8) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.776699ms)
May 22 02:38:45.206: INFO: (9) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.314591ms)
May 22 02:38:45.216: INFO: (10) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.511173ms)
May 22 02:38:45.226: INFO: (11) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.635558ms)
May 22 02:38:45.235: INFO: (12) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.146887ms)
May 22 02:38:45.246: INFO: (13) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.96445ms)
May 22 02:38:45.256: INFO: (14) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.131032ms)
May 22 02:38:45.268: INFO: (15) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 12.425123ms)
May 22 02:38:45.279: INFO: (16) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.660976ms)
May 22 02:38:45.289: INFO: (17) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.419285ms)
May 22 02:38:45.298: INFO: (18) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.241209ms)
May 22 02:38:45.305: INFO: (19) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.401093ms)
[AfterEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:38:45.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3540" for this suite.
May 22 02:38:51.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:38:51.490: INFO: namespace proxy-3540 deletion completed in 6.178407197s

• [SLOW TEST:6.556 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:38:51.491: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1599
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-bc13b484-7c3a-11e9-8c5e-b202ea6dae39
STEP: Creating configMap with name cm-test-opt-upd-bc13b4ea-7c3a-11e9-8c5e-b202ea6dae39
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bc13b484-7c3a-11e9-8c5e-b202ea6dae39
STEP: Updating configmap cm-test-opt-upd-bc13b4ea-7c3a-11e9-8c5e-b202ea6dae39
STEP: Creating configMap with name cm-test-opt-create-bc13b50c-7c3a-11e9-8c5e-b202ea6dae39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:40:06.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1599" for this suite.
May 22 02:40:28.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:40:28.644: INFO: namespace configmap-1599 deletion completed in 22.172488561s

• [SLOW TEST:97.153 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:40:28.644: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 22 02:40:28.832: INFO: Waiting up to 5m0s for pod "pod-f5f8657a-7c3a-11e9-8c5e-b202ea6dae39" in namespace "emptydir-132" to be "success or failure"
May 22 02:40:28.841: INFO: Pod "pod-f5f8657a-7c3a-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.111691ms
May 22 02:40:30.846: INFO: Pod "pod-f5f8657a-7c3a-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013902949s
STEP: Saw pod success
May 22 02:40:30.846: INFO: Pod "pod-f5f8657a-7c3a-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:40:30.850: INFO: Trying to get logs from node worker01 pod pod-f5f8657a-7c3a-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:40:30.885: INFO: Waiting for pod pod-f5f8657a-7c3a-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:40:30.899: INFO: Pod pod-f5f8657a-7c3a-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:40:30.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-132" for this suite.
May 22 02:40:36.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:40:37.076: INFO: namespace emptydir-132 deletion completed in 6.168172183s

• [SLOW TEST:8.432 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:40:37.077: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:40:37.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3839" for this suite.
May 22 02:40:43.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:40:43.532: INFO: namespace kubelet-test-3839 deletion completed in 6.208891439s

• [SLOW TEST:6.456 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:40:43.533: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-fed7312f-7c3a-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 02:40:43.724: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fed8b000-7c3a-11e9-8c5e-b202ea6dae39" in namespace "projected-2576" to be "success or failure"
May 22 02:40:43.736: INFO: Pod "pod-projected-configmaps-fed8b000-7c3a-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 11.172366ms
May 22 02:40:45.747: INFO: Pod "pod-projected-configmaps-fed8b000-7c3a-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022787613s
May 22 02:40:47.759: INFO: Pod "pod-projected-configmaps-fed8b000-7c3a-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034205533s
STEP: Saw pod success
May 22 02:40:47.759: INFO: Pod "pod-projected-configmaps-fed8b000-7c3a-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:40:47.771: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-fed8b000-7c3a-11e9-8c5e-b202ea6dae39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 02:40:47.835: INFO: Waiting for pod pod-projected-configmaps-fed8b000-7c3a-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:40:47.843: INFO: Pod pod-projected-configmaps-fed8b000-7c3a-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:40:47.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2576" for this suite.
May 22 02:40:53.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:40:54.073: INFO: namespace projected-2576 deletion completed in 6.212447653s

• [SLOW TEST:10.541 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:40:54.073: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8198
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
May 22 02:40:54.628: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 22 02:40:56.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089654, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089654, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089654, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089654, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 02:40:59.636: INFO: Waited 923.132152ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:41:00.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8198" for this suite.
May 22 02:41:06.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:41:06.402: INFO: namespace aggregator-8198 deletion completed in 6.234089678s

• [SLOW TEST:12.329 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:41:06.402: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4475.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4475.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4475.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4475.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4475.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4475.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4475.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4475.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4475.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4475.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4475.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 204.104.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.104.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.104.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.104.204_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4475.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4475.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4475.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4475.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4475.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4475.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4475.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4475.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4475.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4475.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4475.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 204.104.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.104.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.104.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.104.204_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 22 02:41:10.689: INFO: Unable to read wheezy_udp@dns-test-service.dns-4475.svc.cluster.local from pod dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39: the server could not find the requested resource (get pods dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39)
May 22 02:41:10.703: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4475.svc.cluster.local from pod dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39: the server could not find the requested resource (get pods dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39)
May 22 02:41:10.721: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local from pod dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39: the server could not find the requested resource (get pods dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39)
May 22 02:41:10.732: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local from pod dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39: the server could not find the requested resource (get pods dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39)
May 22 02:41:10.779: INFO: Unable to read jessie_udp@dns-test-service.dns-4475.svc.cluster.local from pod dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39: the server could not find the requested resource (get pods dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39)
May 22 02:41:10.786: INFO: Unable to read jessie_tcp@dns-test-service.dns-4475.svc.cluster.local from pod dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39: the server could not find the requested resource (get pods dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39)
May 22 02:41:10.795: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local from pod dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39: the server could not find the requested resource (get pods dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39)
May 22 02:41:10.804: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local from pod dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39: the server could not find the requested resource (get pods dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39)
May 22 02:41:10.846: INFO: Lookups using dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39 failed for: [wheezy_udp@dns-test-service.dns-4475.svc.cluster.local wheezy_tcp@dns-test-service.dns-4475.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local jessie_udp@dns-test-service.dns-4475.svc.cluster.local jessie_tcp@dns-test-service.dns-4475.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4475.svc.cluster.local]

May 22 02:41:15.969: INFO: DNS probes using dns-4475/dns-test-0c7fa226-7c3b-11e9-8c5e-b202ea6dae39 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:41:16.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4475" for this suite.
May 22 02:41:22.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:41:22.300: INFO: namespace dns-4475 deletion completed in 6.192706532s

• [SLOW TEST:15.898 seconds]
[sig-network] DNS
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:41:22.300: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
May 22 02:41:22.482: INFO: Waiting up to 5m0s for pod "client-containers-15f309be-7c3b-11e9-8c5e-b202ea6dae39" in namespace "containers-5846" to be "success or failure"
May 22 02:41:22.490: INFO: Pod "client-containers-15f309be-7c3b-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.458502ms
May 22 02:41:24.502: INFO: Pod "client-containers-15f309be-7c3b-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020592618s
STEP: Saw pod success
May 22 02:41:24.503: INFO: Pod "client-containers-15f309be-7c3b-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:41:24.516: INFO: Trying to get logs from node worker01 pod client-containers-15f309be-7c3b-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:41:24.575: INFO: Waiting for pod client-containers-15f309be-7c3b-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:41:24.584: INFO: Pod client-containers-15f309be-7c3b-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:41:24.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5846" for this suite.
May 22 02:41:30.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:41:30.771: INFO: namespace containers-5846 deletion completed in 6.1753397s

• [SLOW TEST:8.471 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:41:30.771: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-q7nw
STEP: Creating a pod to test atomic-volume-subpath
May 22 02:41:30.972: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-q7nw" in namespace "subpath-2190" to be "success or failure"
May 22 02:41:30.984: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Pending", Reason="", readiness=false. Elapsed: 11.479298ms
May 22 02:41:32.996: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023969436s
May 22 02:41:35.002: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Running", Reason="", readiness=true. Elapsed: 4.029510634s
May 22 02:41:37.007: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Running", Reason="", readiness=true. Elapsed: 6.035123702s
May 22 02:41:39.014: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Running", Reason="", readiness=true. Elapsed: 8.042163154s
May 22 02:41:41.023: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Running", Reason="", readiness=true. Elapsed: 10.050826243s
May 22 02:41:43.035: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Running", Reason="", readiness=true. Elapsed: 12.062849299s
May 22 02:41:45.045: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Running", Reason="", readiness=true. Elapsed: 14.072913063s
May 22 02:41:47.058: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Running", Reason="", readiness=true. Elapsed: 16.085511259s
May 22 02:41:49.065: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Running", Reason="", readiness=true. Elapsed: 18.092717798s
May 22 02:41:51.070: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Running", Reason="", readiness=true. Elapsed: 20.097435477s
May 22 02:41:53.074: INFO: Pod "pod-subpath-test-configmap-q7nw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.102291974s
STEP: Saw pod success
May 22 02:41:53.075: INFO: Pod "pod-subpath-test-configmap-q7nw" satisfied condition "success or failure"
May 22 02:41:53.079: INFO: Trying to get logs from node worker01 pod pod-subpath-test-configmap-q7nw container test-container-subpath-configmap-q7nw: <nil>
STEP: delete the pod
May 22 02:41:53.118: INFO: Waiting for pod pod-subpath-test-configmap-q7nw to disappear
May 22 02:41:53.132: INFO: Pod pod-subpath-test-configmap-q7nw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-q7nw
May 22 02:41:53.132: INFO: Deleting pod "pod-subpath-test-configmap-q7nw" in namespace "subpath-2190"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:41:53.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2190" for this suite.
May 22 02:41:59.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:41:59.332: INFO: namespace subpath-2190 deletion completed in 6.179907682s

• [SLOW TEST:28.561 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:41:59.332: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-2c07b0e2-7c3b-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:41:59.546: INFO: Waiting up to 5m0s for pod "pod-secrets-2c0938e7-7c3b-11e9-8c5e-b202ea6dae39" in namespace "secrets-699" to be "success or failure"
May 22 02:41:59.559: INFO: Pod "pod-secrets-2c0938e7-7c3b-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.470288ms
May 22 02:42:01.565: INFO: Pod "pod-secrets-2c0938e7-7c3b-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018748595s
May 22 02:42:03.571: INFO: Pod "pod-secrets-2c0938e7-7c3b-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02556777s
STEP: Saw pod success
May 22 02:42:03.571: INFO: Pod "pod-secrets-2c0938e7-7c3b-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:42:03.578: INFO: Trying to get logs from node worker01 pod pod-secrets-2c0938e7-7c3b-11e9-8c5e-b202ea6dae39 container secret-volume-test: <nil>
STEP: delete the pod
May 22 02:42:03.631: INFO: Waiting for pod pod-secrets-2c0938e7-7c3b-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:42:03.636: INFO: Pod pod-secrets-2c0938e7-7c3b-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:42:03.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-699" for this suite.
May 22 02:42:09.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:42:09.820: INFO: namespace secrets-699 deletion completed in 6.172755203s

• [SLOW TEST:10.488 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:42:09.820: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:42:09.993: INFO: Creating ReplicaSet my-hostname-basic-3246efdb-7c3b-11e9-8c5e-b202ea6dae39
May 22 02:42:10.017: INFO: Pod name my-hostname-basic-3246efdb-7c3b-11e9-8c5e-b202ea6dae39: Found 0 pods out of 1
May 22 02:42:15.023: INFO: Pod name my-hostname-basic-3246efdb-7c3b-11e9-8c5e-b202ea6dae39: Found 1 pods out of 1
May 22 02:42:15.023: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3246efdb-7c3b-11e9-8c5e-b202ea6dae39" is running
May 22 02:42:15.027: INFO: Pod "my-hostname-basic-3246efdb-7c3b-11e9-8c5e-b202ea6dae39-xv45p" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 02:42:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 02:42:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 02:42:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 02:42:10 +0000 UTC Reason: Message:}])
May 22 02:42:15.027: INFO: Trying to dial the pod
May 22 02:42:20.043: INFO: Controller my-hostname-basic-3246efdb-7c3b-11e9-8c5e-b202ea6dae39: Got expected result from replica 1 [my-hostname-basic-3246efdb-7c3b-11e9-8c5e-b202ea6dae39-xv45p]: "my-hostname-basic-3246efdb-7c3b-11e9-8c5e-b202ea6dae39-xv45p", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:42:20.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9163" for this suite.
May 22 02:42:26.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:42:26.229: INFO: namespace replicaset-9163 deletion completed in 6.181189358s

• [SLOW TEST:16.409 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:42:26.229: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 22 02:42:26.397: INFO: PodSpec: initContainers in spec.initContainers
May 22 02:43:11.366: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3c0e09f8-7c3b-11e9-8c5e-b202ea6dae39", GenerateName:"", Namespace:"init-container-3610", SelfLink:"/api/v1/namespaces/init-container-3610/pods/pod-init-3c0e09f8-7c3b-11e9-8c5e-b202ea6dae39", UID:"3c0f343b-7c3b-11e9-b253-000c290a0f16", ResourceVersion:"140136", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63694089746, loc:(*time.Location)(0x8a140e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"397852119"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-sg74c", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00234d480), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sg74c", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sg74c", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sg74c", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001f9ff08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker01", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0031d0ba0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f9ffa0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f9ffc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001f9ffc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001f9ffcc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089746, loc:(*time.Location)(0x8a140e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089746, loc:(*time.Location)(0x8a140e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089746, loc:(*time.Location)(0x8a140e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694089746, loc:(*time.Location)(0x8a140e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.9.21", PodIP:"10.244.3.140", StartTime:(*v1.Time)(0xc0019fb220), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002401260)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0024012d0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://d1f8a00a7e69607f73725b751a0aa6256aa8c24f35349059e30b98723c78f1c0"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0019fb260), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0019fb240), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:43:11.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3610" for this suite.
May 22 02:43:33.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:43:33.536: INFO: namespace init-container-3610 deletion completed in 22.160288924s

• [SLOW TEST:67.307 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:43:33.536: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-642c198d-7c3b-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:43:33.728: INFO: Waiting up to 5m0s for pod "pod-secrets-642d696b-7c3b-11e9-8c5e-b202ea6dae39" in namespace "secrets-7495" to be "success or failure"
May 22 02:43:33.736: INFO: Pod "pod-secrets-642d696b-7c3b-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080287ms
May 22 02:43:35.742: INFO: Pod "pod-secrets-642d696b-7c3b-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014064659s
May 22 02:43:37.749: INFO: Pod "pod-secrets-642d696b-7c3b-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02162285s
STEP: Saw pod success
May 22 02:43:37.749: INFO: Pod "pod-secrets-642d696b-7c3b-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:43:37.757: INFO: Trying to get logs from node worker01 pod pod-secrets-642d696b-7c3b-11e9-8c5e-b202ea6dae39 container secret-volume-test: <nil>
STEP: delete the pod
May 22 02:43:37.805: INFO: Waiting for pod pod-secrets-642d696b-7c3b-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:43:37.817: INFO: Pod pod-secrets-642d696b-7c3b-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:43:37.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7495" for this suite.
May 22 02:43:43.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:43:44.005: INFO: namespace secrets-7495 deletion completed in 6.17477882s

• [SLOW TEST:10.469 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:43:44.005: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9711
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-6a69c13a-7c3b-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:43:44.202: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6a6b73fa-7c3b-11e9-8c5e-b202ea6dae39" in namespace "projected-9711" to be "success or failure"
May 22 02:43:44.221: INFO: Pod "pod-projected-secrets-6a6b73fa-7c3b-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 18.939619ms
May 22 02:43:46.226: INFO: Pod "pod-projected-secrets-6a6b73fa-7c3b-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023864549s
STEP: Saw pod success
May 22 02:43:46.226: INFO: Pod "pod-projected-secrets-6a6b73fa-7c3b-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:43:46.230: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-6a6b73fa-7c3b-11e9-8c5e-b202ea6dae39 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 02:43:46.262: INFO: Waiting for pod pod-projected-secrets-6a6b73fa-7c3b-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:43:46.273: INFO: Pod pod-projected-secrets-6a6b73fa-7c3b-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:43:46.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9711" for this suite.
May 22 02:43:52.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:43:52.459: INFO: namespace projected-9711 deletion completed in 6.176739277s

• [SLOW TEST:8.454 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:43:52.459: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3133
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 22 02:43:55.224: INFO: Successfully updated pod "labelsupdate6f739a50-7c3b-11e9-8c5e-b202ea6dae39"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:43:57.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3133" for this suite.
May 22 02:44:19.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:44:19.453: INFO: namespace projected-3133 deletion completed in 22.183362715s

• [SLOW TEST:26.994 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:44:19.453: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3475
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3475
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3475
STEP: Creating statefulset with conflicting port in namespace statefulset-3475
STEP: Waiting until pod test-pod will start running in namespace statefulset-3475
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3475
May 22 02:44:23.765: INFO: Observed stateful pod in namespace: statefulset-3475, name: ss-0, uid: 81df1616-7c3b-11e9-85c4-000c296e7615, status phase: Pending. Waiting for statefulset controller to delete.
May 22 02:44:24.106: INFO: Observed stateful pod in namespace: statefulset-3475, name: ss-0, uid: 81df1616-7c3b-11e9-85c4-000c296e7615, status phase: Failed. Waiting for statefulset controller to delete.
May 22 02:44:24.127: INFO: Observed stateful pod in namespace: statefulset-3475, name: ss-0, uid: 81df1616-7c3b-11e9-85c4-000c296e7615, status phase: Failed. Waiting for statefulset controller to delete.
May 22 02:44:24.142: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3475
STEP: Removing pod with conflicting port in namespace statefulset-3475
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3475 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 02:44:28.192: INFO: Deleting all statefulset in ns statefulset-3475
May 22 02:44:28.203: INFO: Scaling statefulset ss to 0
May 22 02:44:48.244: INFO: Waiting for statefulset status.replicas updated to 0
May 22 02:44:48.248: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:44:48.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3475" for this suite.
May 22 02:44:54.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:44:54.440: INFO: namespace statefulset-3475 deletion completed in 6.159018462s

• [SLOW TEST:34.987 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:44:54.440: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:44:58.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3235" for this suite.
May 22 02:45:38.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:45:38.845: INFO: namespace kubelet-test-3235 deletion completed in 40.182988891s

• [SLOW TEST:44.405 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:45:38.845: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:45:39.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6776" for this suite.
May 22 02:46:01.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:46:01.240: INFO: namespace pods-6776 deletion completed in 22.165966138s

• [SLOW TEST:22.395 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:46:01.240: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:46:01.428: INFO: (0) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.730097ms)
May 22 02:46:01.438: INFO: (1) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.504867ms)
May 22 02:46:01.448: INFO: (2) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.812335ms)
May 22 02:46:01.458: INFO: (3) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.2022ms)
May 22 02:46:01.468: INFO: (4) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.864651ms)
May 22 02:46:01.479: INFO: (5) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.036528ms)
May 22 02:46:01.488: INFO: (6) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.37889ms)
May 22 02:46:01.498: INFO: (7) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.42746ms)
May 22 02:46:01.509: INFO: (8) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.78053ms)
May 22 02:46:01.519: INFO: (9) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.10774ms)
May 22 02:46:01.530: INFO: (10) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.549388ms)
May 22 02:46:01.551: INFO: (11) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 20.944812ms)
May 22 02:46:01.562: INFO: (12) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.013296ms)
May 22 02:46:01.573: INFO: (13) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.148854ms)
May 22 02:46:01.584: INFO: (14) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.417795ms)
May 22 02:46:01.593: INFO: (15) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 8.851419ms)
May 22 02:46:01.603: INFO: (16) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.655083ms)
May 22 02:46:01.614: INFO: (17) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.873689ms)
May 22 02:46:01.625: INFO: (18) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.20863ms)
May 22 02:46:01.638: INFO: (19) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 12.60437ms)
[AfterEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:46:01.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6427" for this suite.
May 22 02:46:07.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:46:07.836: INFO: namespace proxy-6427 deletion completed in 6.193199273s

• [SLOW TEST:6.596 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:46:07.837: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-8089/configmap-test-c02428ef-7c3b-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 02:46:08.035: INFO: Waiting up to 5m0s for pod "pod-configmaps-c02604c8-7c3b-11e9-8c5e-b202ea6dae39" in namespace "configmap-8089" to be "success or failure"
May 22 02:46:08.044: INFO: Pod "pod-configmaps-c02604c8-7c3b-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.962749ms
May 22 02:46:10.048: INFO: Pod "pod-configmaps-c02604c8-7c3b-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013527555s
STEP: Saw pod success
May 22 02:46:10.048: INFO: Pod "pod-configmaps-c02604c8-7c3b-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:46:10.053: INFO: Trying to get logs from node worker01 pod pod-configmaps-c02604c8-7c3b-11e9-8c5e-b202ea6dae39 container env-test: <nil>
STEP: delete the pod
May 22 02:46:10.087: INFO: Waiting for pod pod-configmaps-c02604c8-7c3b-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:46:10.102: INFO: Pod pod-configmaps-c02604c8-7c3b-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:46:10.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8089" for this suite.
May 22 02:46:16.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:46:16.313: INFO: namespace configmap-8089 deletion completed in 6.198817662s

• [SLOW TEST:8.476 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:46:16.313: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:46:21.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6946" for this suite.
May 22 02:46:43.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:46:43.729: INFO: namespace replication-controller-6946 deletion completed in 22.169735956s

• [SLOW TEST:27.416 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:46:43.729: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 22 02:46:43.963: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-332,SelfLink:/api/v1/namespaces/watch-332/configmaps/e2e-watch-test-resource-version,UID:d589eb69-7c3b-11e9-b253-000c290a0f16,ResourceVersion:140885,Generation:0,CreationTimestamp:2019-05-22 02:46:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 02:46:43.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-332,SelfLink:/api/v1/namespaces/watch-332/configmaps/e2e-watch-test-resource-version,UID:d589eb69-7c3b-11e9-b253-000c290a0f16,ResourceVersion:140886,Generation:0,CreationTimestamp:2019-05-22 02:46:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:46:43.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-332" for this suite.
May 22 02:46:49.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:46:50.144: INFO: namespace watch-332 deletion completed in 6.171223261s

• [SLOW TEST:6.415 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:46:50.145: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8607
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 22 02:46:53.376: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:46:54.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8607" for this suite.
May 22 02:47:16.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:47:16.571: INFO: namespace replicaset-8607 deletion completed in 22.163570129s

• [SLOW TEST:26.426 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:47:16.571: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
May 22 02:47:16.751: INFO: Waiting up to 5m0s for pod "client-containers-e91c35fc-7c3b-11e9-8c5e-b202ea6dae39" in namespace "containers-345" to be "success or failure"
May 22 02:47:16.762: INFO: Pod "client-containers-e91c35fc-7c3b-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.695284ms
May 22 02:47:18.768: INFO: Pod "client-containers-e91c35fc-7c3b-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016137211s
STEP: Saw pod success
May 22 02:47:18.768: INFO: Pod "client-containers-e91c35fc-7c3b-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:47:18.772: INFO: Trying to get logs from node worker01 pod client-containers-e91c35fc-7c3b-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:47:18.809: INFO: Waiting for pod client-containers-e91c35fc-7c3b-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:47:18.819: INFO: Pod client-containers-e91c35fc-7c3b-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:47:18.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-345" for this suite.
May 22 02:47:24.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:47:25.019: INFO: namespace containers-345 deletion completed in 6.19055252s

• [SLOW TEST:8.449 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:47:25.019: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:47:25.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee262329-7c3b-11e9-8c5e-b202ea6dae39" in namespace "projected-2809" to be "success or failure"
May 22 02:47:25.210: INFO: Pod "downwardapi-volume-ee262329-7c3b-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 7.873722ms
May 22 02:47:27.215: INFO: Pod "downwardapi-volume-ee262329-7c3b-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013475764s
STEP: Saw pod success
May 22 02:47:27.215: INFO: Pod "downwardapi-volume-ee262329-7c3b-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:47:27.221: INFO: Trying to get logs from node worker01 pod downwardapi-volume-ee262329-7c3b-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:47:27.263: INFO: Waiting for pod downwardapi-volume-ee262329-7c3b-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:47:27.270: INFO: Pod downwardapi-volume-ee262329-7c3b-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:47:27.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2809" for this suite.
May 22 02:47:33.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:47:33.450: INFO: namespace projected-2809 deletion completed in 6.168128536s

• [SLOW TEST:8.431 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:47:33.450: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 22 02:47:33.616: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:47:37.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2358" for this suite.
May 22 02:47:59.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:47:59.832: INFO: namespace init-container-2358 deletion completed in 22.165690635s

• [SLOW TEST:26.382 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:47:59.832: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 02:47:59.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 version --client'
May 22 02:48:00.059: INFO: stderr: ""
May 22 02:48:00.059: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.2\", GitCommit:\"66049e3b21efe110454d67df4fa62b08ea79a19b\", GitTreeState:\"clean\", BuildDate:\"2019-05-16T16:23:09Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 22 02:48:00.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-9480'
May 22 02:48:00.252: INFO: stderr: ""
May 22 02:48:00.252: INFO: stdout: "replicationcontroller/redis-master created\n"
May 22 02:48:00.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-9480'
May 22 02:48:00.745: INFO: stderr: ""
May 22 02:48:00.745: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 22 02:48:01.750: INFO: Selector matched 1 pods for map[app:redis]
May 22 02:48:01.750: INFO: Found 0 / 1
May 22 02:48:02.750: INFO: Selector matched 1 pods for map[app:redis]
May 22 02:48:02.750: INFO: Found 1 / 1
May 22 02:48:02.750: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 22 02:48:02.755: INFO: Selector matched 1 pods for map[app:redis]
May 22 02:48:02.755: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 22 02:48:02.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 describe pod redis-master-gwpb5 --namespace=kubectl-9480'
May 22 02:48:02.860: INFO: stderr: ""
May 22 02:48:02.860: INFO: stdout: "Name:               redis-master-gwpb5\nNamespace:          kubectl-9480\nPriority:           0\nPriorityClassName:  <none>\nNode:               worker01/192.168.9.21\nStart Time:         Wed, 22 May 2019 02:48:00 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.244.3.155\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://3990cfcf1dbac475e2327b7eff286cc06a7be784077429c74ba935615eea97d7\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 22 May 2019 02:48:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wx5ht (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wx5ht:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wx5ht\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9480/redis-master-gwpb5 to worker01\n  Normal  Pulled     1s    kubelet, worker01  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, worker01  Created container redis-master\n  Normal  Started    1s    kubelet, worker01  Started container redis-master\n"
May 22 02:48:02.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 describe rc redis-master --namespace=kubectl-9480'
May 22 02:48:02.961: INFO: stderr: ""
May 22 02:48:02.961: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9480\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-gwpb5\n"
May 22 02:48:02.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 describe service redis-master --namespace=kubectl-9480'
May 22 02:48:03.054: INFO: stderr: ""
May 22 02:48:03.054: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9480\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.103.217.136\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.3.155:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 22 02:48:03.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 describe node master01'
May 22 02:48:03.167: INFO: stderr: ""
May 22 02:48:03.167: INFO: stdout: "Name:               master01\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master01\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"be:5a:7b:ee:68:96\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.9.11\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 20 May 2019 03:42:44 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 22 May 2019 02:47:32 +0000   Mon, 20 May 2019 03:42:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 22 May 2019 02:47:32 +0000   Mon, 20 May 2019 03:42:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 22 May 2019 02:47:32 +0000   Mon, 20 May 2019 03:42:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 22 May 2019 02:47:32 +0000   Mon, 20 May 2019 03:43:19 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.9.11\n  Hostname:    master01\nCapacity:\n cpu:                2\n ephemeral-storage:  99688900Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3861512Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  91873290088\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3759112Ki\n pods:               110\nSystem Info:\n Machine ID:                 a8e78ff250274bc7bd50871834e3e9a6\n System UUID:                DB494D56-7B9B-BAF8-4BB0-575E2A0A0F16\n Boot ID:                    7e378553-5e9a-4477-8116-c50b3ed37328\n Kernel Version:             3.10.0-957.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.6\n Kubelet Version:            v1.14.2\n Kube-Proxy Version:         v1.14.2\nPodCIDR:                     10.244.0.0/24\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-e3be60816e5648ce-8mxjz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         66m\n  kube-system                coredns-d94dc7985-ddj4w                                    100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     47h\n  kube-system                coredns-d94dc7985-ncm5f                                    100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     47h\n  kube-system                kube-apiserver-master01                                    250m (12%)    0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                kube-controller-manager-master01                           200m (10%)    0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                kube-flannel-ds-amd64-jfdq7                                100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      47h\n  kube-system                kube-proxy-95n7z                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                kube-scheduler-master01                                    100m (5%)     0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                kubernetes-dashboard-6d69746b4c-8kf6w                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                850m (42%)  100m (5%)\n  memory             190Mi (5%)  390Mi (10%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
May 22 02:48:03.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 describe namespace kubectl-9480'
May 22 02:48:03.258: INFO: stderr: ""
May 22 02:48:03.258: INFO: stdout: "Name:         kubectl-9480\nLabels:       e2e-framework=kubectl\n              e2e-run=c7a5f7e4-7c32-11e9-8c5e-b202ea6dae39\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:48:03.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9480" for this suite.
May 22 02:48:25.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:48:25.434: INFO: namespace kubectl-9480 deletion completed in 22.16637468s

• [SLOW TEST:25.602 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:48:25.434: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:48:25.633: INFO: Waiting up to 5m0s for pod "downwardapi-volume-122868c8-7c3c-11e9-8c5e-b202ea6dae39" in namespace "downward-api-2307" to be "success or failure"
May 22 02:48:25.650: INFO: Pod "downwardapi-volume-122868c8-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 17.329209ms
May 22 02:48:27.655: INFO: Pod "downwardapi-volume-122868c8-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02234107s
May 22 02:48:29.662: INFO: Pod "downwardapi-volume-122868c8-7c3c-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029406671s
STEP: Saw pod success
May 22 02:48:29.662: INFO: Pod "downwardapi-volume-122868c8-7c3c-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:48:29.669: INFO: Trying to get logs from node worker01 pod downwardapi-volume-122868c8-7c3c-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:48:29.710: INFO: Waiting for pod downwardapi-volume-122868c8-7c3c-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:48:29.719: INFO: Pod downwardapi-volume-122868c8-7c3c-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:48:29.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2307" for this suite.
May 22 02:48:35.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:48:35.930: INFO: namespace downward-api-2307 deletion completed in 6.202622671s

• [SLOW TEST:10.496 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:48:35.930: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-186a2a77-7c3c-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 02:48:36.126: INFO: Waiting up to 5m0s for pod "pod-configmaps-186bdf57-7c3c-11e9-8c5e-b202ea6dae39" in namespace "configmap-8028" to be "success or failure"
May 22 02:48:36.136: INFO: Pod "pod-configmaps-186bdf57-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.744596ms
May 22 02:48:38.141: INFO: Pod "pod-configmaps-186bdf57-7c3c-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014875604s
STEP: Saw pod success
May 22 02:48:38.141: INFO: Pod "pod-configmaps-186bdf57-7c3c-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:48:38.147: INFO: Trying to get logs from node worker01 pod pod-configmaps-186bdf57-7c3c-11e9-8c5e-b202ea6dae39 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 02:48:38.179: INFO: Waiting for pod pod-configmaps-186bdf57-7c3c-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:48:38.193: INFO: Pod pod-configmaps-186bdf57-7c3c-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:48:38.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8028" for this suite.
May 22 02:48:44.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:48:44.371: INFO: namespace configmap-8028 deletion completed in 6.166690367s

• [SLOW TEST:8.441 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:48:44.371: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 22 02:48:44.559: INFO: Waiting up to 5m0s for pod "pod-1d727396-7c3c-11e9-8c5e-b202ea6dae39" in namespace "emptydir-9995" to be "success or failure"
May 22 02:48:44.566: INFO: Pod "pod-1d727396-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.302067ms
May 22 02:48:46.571: INFO: Pod "pod-1d727396-7c3c-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01211249s
STEP: Saw pod success
May 22 02:48:46.571: INFO: Pod "pod-1d727396-7c3c-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:48:46.576: INFO: Trying to get logs from node worker01 pod pod-1d727396-7c3c-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:48:46.613: INFO: Waiting for pod pod-1d727396-7c3c-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:48:46.621: INFO: Pod pod-1d727396-7c3c-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:48:46.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9995" for this suite.
May 22 02:48:52.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:48:52.833: INFO: namespace emptydir-9995 deletion completed in 6.19806787s

• [SLOW TEST:8.462 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:48:52.834: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 22 02:48:57.079: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:48:57.084: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:48:59.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:48:59.091: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:01.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:01.089: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:03.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:03.090: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:05.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:05.094: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:07.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:07.090: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:09.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:09.089: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:11.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:11.094: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:13.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:13.094: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:15.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:15.094: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:17.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:17.092: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:19.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:19.100: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:21.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:21.098: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 02:49:23.085: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 02:49:23.090: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:49:23.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2849" for this suite.
May 22 02:49:45.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:49:45.248: INFO: namespace container-lifecycle-hook-2849 deletion completed in 22.152193472s

• [SLOW TEST:52.414 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:49:45.248: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-99
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-41baf6a7-7c3c-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 02:49:45.439: INFO: Waiting up to 5m0s for pod "pod-secrets-41bc5232-7c3c-11e9-8c5e-b202ea6dae39" in namespace "secrets-99" to be "success or failure"
May 22 02:49:45.449: INFO: Pod "pod-secrets-41bc5232-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.658144ms
May 22 02:49:47.456: INFO: Pod "pod-secrets-41bc5232-7c3c-11e9-8c5e-b202ea6dae39": Phase="Running", Reason="", readiness=true. Elapsed: 2.016939656s
May 22 02:49:49.465: INFO: Pod "pod-secrets-41bc5232-7c3c-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026121298s
STEP: Saw pod success
May 22 02:49:49.466: INFO: Pod "pod-secrets-41bc5232-7c3c-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:49:49.475: INFO: Trying to get logs from node worker01 pod pod-secrets-41bc5232-7c3c-11e9-8c5e-b202ea6dae39 container secret-env-test: <nil>
STEP: delete the pod
May 22 02:49:49.524: INFO: Waiting for pod pod-secrets-41bc5232-7c3c-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:49:49.530: INFO: Pod pod-secrets-41bc5232-7c3c-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:49:49.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-99" for this suite.
May 22 02:49:55.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:49:55.701: INFO: namespace secrets-99 deletion completed in 6.162388215s

• [SLOW TEST:10.453 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:49:55.702: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-1809/configmap-test-47f6680b-7c3c-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 02:49:55.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-47f7a741-7c3c-11e9-8c5e-b202ea6dae39" in namespace "configmap-1809" to be "success or failure"
May 22 02:49:55.908: INFO: Pod "pod-configmaps-47f7a741-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047885ms
May 22 02:49:57.916: INFO: Pod "pod-configmaps-47f7a741-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017602865s
May 22 02:49:59.923: INFO: Pod "pod-configmaps-47f7a741-7c3c-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025077471s
STEP: Saw pod success
May 22 02:49:59.923: INFO: Pod "pod-configmaps-47f7a741-7c3c-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:49:59.933: INFO: Trying to get logs from node worker01 pod pod-configmaps-47f7a741-7c3c-11e9-8c5e-b202ea6dae39 container env-test: <nil>
STEP: delete the pod
May 22 02:49:59.991: INFO: Waiting for pod pod-configmaps-47f7a741-7c3c-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:50:00.001: INFO: Pod pod-configmaps-47f7a741-7c3c-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:50:00.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1809" for this suite.
May 22 02:50:06.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:50:06.186: INFO: namespace configmap-1809 deletion completed in 6.177289244s

• [SLOW TEST:10.484 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:50:06.186: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1072
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 22 02:50:08.404: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-4e351d87-7c3c-11e9-8c5e-b202ea6dae39,GenerateName:,Namespace:events-1072,SelfLink:/api/v1/namespaces/events-1072/pods/send-events-4e351d87-7c3c-11e9-8c5e-b202ea6dae39,UID:4e35cbdf-7c3c-11e9-b253-000c290a0f16,ResourceVersion:141626,Generation:0,CreationTimestamp:2019-05-22 02:50:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 349663160,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-2mbn2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-2mbn2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-2mbn2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00285f2d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00285f2f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:50:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:50:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:50:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 02:50:06 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.21,PodIP:10.244.3.163,StartTime:2019-05-22 02:50:06 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-22 02:50:07 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://4bda834b60178c993a2dc44cda8af41be29ed38299b2791aba9e4dc50de91e4f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 22 02:50:10.415: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 22 02:50:12.420: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:50:12.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1072" for this suite.
May 22 02:50:52.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:50:52.629: INFO: namespace events-1072 deletion completed in 40.188594851s

• [SLOW TEST:46.443 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:50:52.629: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 02:50:52.813: INFO: Waiting up to 5m0s for pod "downward-api-69e468c7-7c3c-11e9-8c5e-b202ea6dae39" in namespace "downward-api-2195" to be "success or failure"
May 22 02:50:52.822: INFO: Pod "downward-api-69e468c7-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.38983ms
May 22 02:50:54.826: INFO: Pod "downward-api-69e468c7-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01333289s
May 22 02:50:56.831: INFO: Pod "downward-api-69e468c7-7c3c-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018122506s
STEP: Saw pod success
May 22 02:50:56.831: INFO: Pod "downward-api-69e468c7-7c3c-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:50:56.836: INFO: Trying to get logs from node worker01 pod downward-api-69e468c7-7c3c-11e9-8c5e-b202ea6dae39 container dapi-container: <nil>
STEP: delete the pod
May 22 02:50:56.866: INFO: Waiting for pod downward-api-69e468c7-7c3c-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:50:56.874: INFO: Pod downward-api-69e468c7-7c3c-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:50:56.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2195" for this suite.
May 22 02:51:02.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:51:03.064: INFO: namespace downward-api-2195 deletion completed in 6.177962125s

• [SLOW TEST:10.435 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:51:03.064: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
May 22 02:51:03.252: INFO: Waiting up to 5m0s for pod "var-expansion-701d44fe-7c3c-11e9-8c5e-b202ea6dae39" in namespace "var-expansion-834" to be "success or failure"
May 22 02:51:03.267: INFO: Pod "var-expansion-701d44fe-7c3c-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 14.651932ms
May 22 02:51:05.272: INFO: Pod "var-expansion-701d44fe-7c3c-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019696092s
STEP: Saw pod success
May 22 02:51:05.272: INFO: Pod "var-expansion-701d44fe-7c3c-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:51:05.276: INFO: Trying to get logs from node worker01 pod var-expansion-701d44fe-7c3c-11e9-8c5e-b202ea6dae39 container dapi-container: <nil>
STEP: delete the pod
May 22 02:51:05.308: INFO: Waiting for pod var-expansion-701d44fe-7c3c-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:51:05.316: INFO: Pod var-expansion-701d44fe-7c3c-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:51:05.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-834" for this suite.
May 22 02:51:11.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:51:11.550: INFO: namespace var-expansion-834 deletion completed in 6.21897815s

• [SLOW TEST:8.485 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:51:11.550: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:51:36.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7036" for this suite.
May 22 02:51:42.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:51:42.294: INFO: namespace container-runtime-7036 deletion completed in 6.196979924s

• [SLOW TEST:30.744 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:51:42.294: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1186
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 22 02:51:42.494: INFO: Found 0 stateful pods, waiting for 3
May 22 02:51:52.503: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 02:51:52.503: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 02:51:52.503: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 22 02:51:52.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-1186 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:51:52.857: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:51:52.857: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:51:52.857: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 22 02:51:52.902: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 22 02:52:02.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-1186 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:52:03.267: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 02:52:03.267: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 02:52:03.267: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 02:52:13.298: INFO: Waiting for StatefulSet statefulset-1186/ss2 to complete update
May 22 02:52:13.298: INFO: Waiting for Pod statefulset-1186/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 22 02:52:13.298: INFO: Waiting for Pod statefulset-1186/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 22 02:52:23.308: INFO: Waiting for StatefulSet statefulset-1186/ss2 to complete update
May 22 02:52:23.308: INFO: Waiting for Pod statefulset-1186/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
May 22 02:52:33.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-1186 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 02:52:33.606: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 02:52:33.606: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 02:52:33.606: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 02:52:43.657: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 22 02:52:53.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 exec --namespace=statefulset-1186 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 02:52:54.001: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 02:52:54.001: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 02:52:54.001: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 02:53:14.036: INFO: Deleting all statefulset in ns statefulset-1186
May 22 02:53:14.042: INFO: Scaling statefulset ss2 to 0
May 22 02:53:34.088: INFO: Waiting for statefulset status.replicas updated to 0
May 22 02:53:34.095: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:53:34.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1186" for this suite.
May 22 02:53:42.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:53:42.327: INFO: namespace statefulset-1186 deletion completed in 8.185293323s

• [SLOW TEST:120.033 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:53:42.327: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 22 02:53:42.497: INFO: namespace kubectl-2335
May 22 02:53:42.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-2335'
May 22 02:53:42.833: INFO: stderr: ""
May 22 02:53:42.833: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 22 02:53:43.839: INFO: Selector matched 1 pods for map[app:redis]
May 22 02:53:43.839: INFO: Found 0 / 1
May 22 02:53:44.839: INFO: Selector matched 1 pods for map[app:redis]
May 22 02:53:44.839: INFO: Found 1 / 1
May 22 02:53:44.839: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 22 02:53:44.847: INFO: Selector matched 1 pods for map[app:redis]
May 22 02:53:44.847: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 22 02:53:44.847: INFO: wait on redis-master startup in kubectl-2335 
May 22 02:53:44.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 logs redis-master-ww266 redis-master --namespace=kubectl-2335'
May 22 02:53:44.949: INFO: stderr: ""
May 22 02:53:44.949: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 May 02:53:43.931 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 May 02:53:43.931 # Server started, Redis version 3.2.12\n1:M 22 May 02:53:43.931 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 May 02:53:43.931 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 22 02:53:44.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2335'
May 22 02:53:45.054: INFO: stderr: ""
May 22 02:53:45.054: INFO: stdout: "service/rm2 exposed\n"
May 22 02:53:45.061: INFO: Service rm2 in namespace kubectl-2335 found.
STEP: exposing service
May 22 02:53:47.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2335'
May 22 02:53:47.180: INFO: stderr: ""
May 22 02:53:47.180: INFO: stdout: "service/rm3 exposed\n"
May 22 02:53:47.189: INFO: Service rm3 in namespace kubectl-2335 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:53:49.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2335" for this suite.
May 22 02:54:13.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:54:13.383: INFO: namespace kubectl-2335 deletion completed in 24.180350164s

• [SLOW TEST:31.056 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:54:13.383: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1291
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-e18e9c52-7c3c-11e9-8c5e-b202ea6dae39
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:54:17.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1291" for this suite.
May 22 02:54:39.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:54:39.789: INFO: namespace configmap-1291 deletion completed in 22.161014624s

• [SLOW TEST:26.406 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:54:39.789: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0522 02:55:10.541098      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 02:55:10.541: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:55:10.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2347" for this suite.
May 22 02:55:16.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:55:16.731: INFO: namespace gc-2347 deletion completed in 6.180591959s

• [SLOW TEST:36.942 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:55:16.731: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:55:16.915: INFO: Waiting up to 5m0s for pod "downwardapi-volume-074f7b66-7c3d-11e9-8c5e-b202ea6dae39" in namespace "downward-api-558" to be "success or failure"
May 22 02:55:16.924: INFO: Pod "downwardapi-volume-074f7b66-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.90462ms
May 22 02:55:18.929: INFO: Pod "downwardapi-volume-074f7b66-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014145152s
May 22 02:55:20.937: INFO: Pod "downwardapi-volume-074f7b66-7c3d-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021981461s
STEP: Saw pod success
May 22 02:55:20.937: INFO: Pod "downwardapi-volume-074f7b66-7c3d-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:55:20.944: INFO: Trying to get logs from node worker01 pod downwardapi-volume-074f7b66-7c3d-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:55:20.994: INFO: Waiting for pod downwardapi-volume-074f7b66-7c3d-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:55:21.002: INFO: Pod downwardapi-volume-074f7b66-7c3d-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:55:21.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-558" for this suite.
May 22 02:55:27.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:55:27.188: INFO: namespace downward-api-558 deletion completed in 6.176071115s

• [SLOW TEST:10.456 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:55:27.188: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-d9s5
STEP: Creating a pod to test atomic-volume-subpath
May 22 02:55:27.389: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-d9s5" in namespace "subpath-8525" to be "success or failure"
May 22 02:55:27.400: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.697841ms
May 22 02:55:29.405: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 2.015503035s
May 22 02:55:31.417: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 4.027714942s
May 22 02:55:33.422: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 6.03207591s
May 22 02:55:35.427: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 8.037378398s
May 22 02:55:37.441: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 10.051747912s
May 22 02:55:39.446: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 12.056585885s
May 22 02:55:41.456: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 14.066802643s
May 22 02:55:43.463: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 16.073864928s
May 22 02:55:45.471: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 18.081119543s
May 22 02:55:47.482: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 20.092173482s
May 22 02:55:49.487: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Running", Reason="", readiness=true. Elapsed: 22.097970417s
May 22 02:55:51.497: INFO: Pod "pod-subpath-test-downwardapi-d9s5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.107422647s
STEP: Saw pod success
May 22 02:55:51.497: INFO: Pod "pod-subpath-test-downwardapi-d9s5" satisfied condition "success or failure"
May 22 02:55:51.507: INFO: Trying to get logs from node worker01 pod pod-subpath-test-downwardapi-d9s5 container test-container-subpath-downwardapi-d9s5: <nil>
STEP: delete the pod
May 22 02:55:51.564: INFO: Waiting for pod pod-subpath-test-downwardapi-d9s5 to disappear
May 22 02:55:51.568: INFO: Pod pod-subpath-test-downwardapi-d9s5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-d9s5
May 22 02:55:51.568: INFO: Deleting pod "pod-subpath-test-downwardapi-d9s5" in namespace "subpath-8525"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:55:51.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8525" for this suite.
May 22 02:55:57.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:55:57.755: INFO: namespace subpath-8525 deletion completed in 6.170675949s

• [SLOW TEST:30.567 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:55:57.755: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-2331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
May 22 02:55:57.939: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2331" to be "success or failure"
May 22 02:55:57.958: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 18.919775ms
May 22 02:55:59.963: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023984388s
STEP: Saw pod success
May 22 02:55:59.963: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 22 02:55:59.967: INFO: Trying to get logs from node worker01 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 22 02:56:00.000: INFO: Waiting for pod pod-host-path-test to disappear
May 22 02:56:00.008: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:56:00.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2331" for this suite.
May 22 02:56:06.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:56:06.201: INFO: namespace hostpath-2331 deletion completed in 6.184016592s

• [SLOW TEST:8.446 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:56:06.201: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
May 22 02:56:06.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-1352'
May 22 02:56:06.573: INFO: stderr: ""
May 22 02:56:06.573: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
May 22 02:56:07.579: INFO: Selector matched 1 pods for map[app:redis]
May 22 02:56:07.579: INFO: Found 0 / 1
May 22 02:56:08.584: INFO: Selector matched 1 pods for map[app:redis]
May 22 02:56:08.584: INFO: Found 1 / 1
May 22 02:56:08.584: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 22 02:56:08.593: INFO: Selector matched 1 pods for map[app:redis]
May 22 02:56:08.593: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 22 02:56:08.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 logs redis-master-zqk4l redis-master --namespace=kubectl-1352'
May 22 02:56:08.700: INFO: stderr: ""
May 22 02:56:08.701: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 May 02:56:07.660 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 May 02:56:07.660 # Server started, Redis version 3.2.12\n1:M 22 May 02:56:07.660 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 May 02:56:07.660 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 22 02:56:08.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 log redis-master-zqk4l redis-master --namespace=kubectl-1352 --tail=1'
May 22 02:56:08.795: INFO: stderr: ""
May 22 02:56:08.795: INFO: stdout: "1:M 22 May 02:56:07.660 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 22 02:56:08.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 log redis-master-zqk4l redis-master --namespace=kubectl-1352 --limit-bytes=1'
May 22 02:56:08.883: INFO: stderr: ""
May 22 02:56:08.883: INFO: stdout: " "
STEP: exposing timestamps
May 22 02:56:08.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 log redis-master-zqk4l redis-master --namespace=kubectl-1352 --tail=1 --timestamps'
May 22 02:56:08.975: INFO: stderr: ""
May 22 02:56:08.975: INFO: stdout: "2019-05-22T02:56:07.663569623Z 1:M 22 May 02:56:07.660 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 22 02:56:11.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 log redis-master-zqk4l redis-master --namespace=kubectl-1352 --since=1s'
May 22 02:56:11.593: INFO: stderr: ""
May 22 02:56:11.593: INFO: stdout: ""
May 22 02:56:11.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 log redis-master-zqk4l redis-master --namespace=kubectl-1352 --since=24h'
May 22 02:56:11.679: INFO: stderr: ""
May 22 02:56:11.679: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 May 02:56:07.660 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 May 02:56:07.660 # Server started, Redis version 3.2.12\n1:M 22 May 02:56:07.660 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 May 02:56:07.660 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
May 22 02:56:11.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-1352'
May 22 02:56:11.771: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 02:56:11.771: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 22 02:56:11.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get rc,svc -l name=nginx --no-headers --namespace=kubectl-1352'
May 22 02:56:11.854: INFO: stderr: "No resources found.\n"
May 22 02:56:11.854: INFO: stdout: ""
May 22 02:56:11.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -l name=nginx --namespace=kubectl-1352 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 02:56:11.931: INFO: stderr: ""
May 22 02:56:11.931: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:56:11.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1352" for this suite.
May 22 02:56:33.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:56:34.107: INFO: namespace kubectl-1352 deletion completed in 22.168408122s

• [SLOW TEST:27.906 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:56:34.108: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 22 02:56:34.273: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 22 02:56:34.294: INFO: Waiting for terminating namespaces to be deleted...
May 22 02:56:34.298: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
May 22 02:56:34.311: INFO: kube-proxy-l7wpq from kube-system started at 2019-05-20 03:44:11 +0000 UTC (1 container statuses recorded)
May 22 02:56:34.311: INFO: 	Container kube-proxy ready: true, restart count 5
May 22 02:56:34.311: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-22 01:41:43 +0000 UTC (1 container statuses recorded)
May 22 02:56:34.311: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 22 02:56:34.311: INFO: kube-flannel-ds-amd64-bd2jm from kube-system started at 2019-05-20 03:44:11 +0000 UTC (1 container statuses recorded)
May 22 02:56:34.311: INFO: 	Container kube-flannel ready: true, restart count 5
May 22 02:56:34.311: INFO: sonobuoy-e2e-job-858a03c85b4b4f46 from heptio-sonobuoy started at 2019-05-22 01:41:45 +0000 UTC (2 container statuses recorded)
May 22 02:56:34.311: INFO: 	Container e2e ready: true, restart count 0
May 22 02:56:34.311: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 02:56:34.311: INFO: tiller-deploy-6b9dd779b9-rlphv from kube-system started at 2019-05-20 03:44:51 +0000 UTC (1 container statuses recorded)
May 22 02:56:34.311: INFO: 	Container tiller ready: true, restart count 9
May 22 02:56:34.311: INFO: sonobuoy-systemd-logs-daemon-set-e3be60816e5648ce-qxw5h from heptio-sonobuoy started at 2019-05-22 01:41:46 +0000 UTC (2 container statuses recorded)
May 22 02:56:34.311: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 22 02:56:34.311: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node worker01
May 22 02:56:34.369: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker01
May 22 02:56:34.369: INFO: Pod sonobuoy-e2e-job-858a03c85b4b4f46 requesting resource cpu=0m on Node worker01
May 22 02:56:34.369: INFO: Pod sonobuoy-systemd-logs-daemon-set-e3be60816e5648ce-qxw5h requesting resource cpu=0m on Node worker01
May 22 02:56:34.369: INFO: Pod kube-flannel-ds-amd64-bd2jm requesting resource cpu=100m on Node worker01
May 22 02:56:34.369: INFO: Pod kube-proxy-l7wpq requesting resource cpu=0m on Node worker01
May 22 02:56:34.369: INFO: Pod tiller-deploy-6b9dd779b9-rlphv requesting resource cpu=0m on Node worker01
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-357c31d3-7c3d-11e9-8c5e-b202ea6dae39.15a0e1d9479d0108], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6615/filler-pod-357c31d3-7c3d-11e9-8c5e-b202ea6dae39 to worker01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-357c31d3-7c3d-11e9-8c5e-b202ea6dae39.15a0e1d976aefded], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-357c31d3-7c3d-11e9-8c5e-b202ea6dae39.15a0e1d979720c0d], Reason = [Created], Message = [Created container filler-pod-357c31d3-7c3d-11e9-8c5e-b202ea6dae39]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-357c31d3-7c3d-11e9-8c5e-b202ea6dae39.15a0e1d98ad91430], Reason = [Started], Message = [Started container filler-pod-357c31d3-7c3d-11e9-8c5e-b202ea6dae39]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15a0e1da37f30247], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node worker01
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:56:39.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6615" for this suite.
May 22 02:56:45.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:56:45.666: INFO: namespace sched-pred-6615 deletion completed in 6.197077286s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.558 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:56:45.666: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-3c50d323-7c3d-11e9-8c5e-b202ea6dae39
STEP: Creating secret with name secret-projected-all-test-volume-3c50d314-7c3d-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test Check all projections for projected volume plugin
May 22 02:56:45.866: INFO: Waiting up to 5m0s for pod "projected-volume-3c50d2ea-7c3d-11e9-8c5e-b202ea6dae39" in namespace "projected-3224" to be "success or failure"
May 22 02:56:45.875: INFO: Pod "projected-volume-3c50d2ea-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.36615ms
May 22 02:56:47.881: INFO: Pod "projected-volume-3c50d2ea-7c3d-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014747589s
STEP: Saw pod success
May 22 02:56:47.881: INFO: Pod "projected-volume-3c50d2ea-7c3d-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:56:47.886: INFO: Trying to get logs from node worker01 pod projected-volume-3c50d2ea-7c3d-11e9-8c5e-b202ea6dae39 container projected-all-volume-test: <nil>
STEP: delete the pod
May 22 02:56:47.921: INFO: Waiting for pod projected-volume-3c50d2ea-7c3d-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:56:47.936: INFO: Pod projected-volume-3c50d2ea-7c3d-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:56:47.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3224" for this suite.
May 22 02:56:53.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:56:54.118: INFO: namespace projected-3224 deletion completed in 6.168013175s

• [SLOW TEST:8.452 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:56:54.118: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 22 02:56:54.304: INFO: Waiting up to 5m0s for pod "pod-415c2198-7c3d-11e9-8c5e-b202ea6dae39" in namespace "emptydir-4745" to be "success or failure"
May 22 02:56:54.313: INFO: Pod "pod-415c2198-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.26997ms
May 22 02:56:56.318: INFO: Pod "pod-415c2198-7c3d-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014410832s
STEP: Saw pod success
May 22 02:56:56.318: INFO: Pod "pod-415c2198-7c3d-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:56:56.323: INFO: Trying to get logs from node worker01 pod pod-415c2198-7c3d-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 02:56:56.356: INFO: Waiting for pod pod-415c2198-7c3d-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:56:56.362: INFO: Pod pod-415c2198-7c3d-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:56:56.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4745" for this suite.
May 22 02:57:02.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:57:02.547: INFO: namespace emptydir-4745 deletion completed in 6.175657358s

• [SLOW TEST:8.429 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:57:02.547: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
May 22 02:57:02.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-6473'
May 22 02:57:02.934: INFO: stderr: ""
May 22 02:57:02.934: INFO: stdout: "pod/pause created\n"
May 22 02:57:02.934: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 22 02:57:02.934: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6473" to be "running and ready"
May 22 02:57:02.941: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685035ms
May 22 02:57:04.945: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011353907s
May 22 02:57:04.945: INFO: Pod "pause" satisfied condition "running and ready"
May 22 02:57:04.945: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
May 22 02:57:04.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 label pods pause testing-label=testing-label-value --namespace=kubectl-6473'
May 22 02:57:05.036: INFO: stderr: ""
May 22 02:57:05.036: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 22 02:57:05.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pod pause -L testing-label --namespace=kubectl-6473'
May 22 02:57:05.115: INFO: stderr: ""
May 22 02:57:05.115: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 22 02:57:05.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 label pods pause testing-label- --namespace=kubectl-6473'
May 22 02:57:05.202: INFO: stderr: ""
May 22 02:57:05.202: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 22 02:57:05.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pod pause -L testing-label --namespace=kubectl-6473'
May 22 02:57:05.277: INFO: stderr: ""
May 22 02:57:05.277: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
May 22 02:57:05.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-6473'
May 22 02:57:05.369: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 02:57:05.369: INFO: stdout: "pod \"pause\" force deleted\n"
May 22 02:57:05.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get rc,svc -l name=pause --no-headers --namespace=kubectl-6473'
May 22 02:57:05.470: INFO: stderr: "No resources found.\n"
May 22 02:57:05.470: INFO: stdout: ""
May 22 02:57:05.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -l name=pause --namespace=kubectl-6473 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 02:57:05.569: INFO: stderr: ""
May 22 02:57:05.569: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:57:05.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6473" for this suite.
May 22 02:57:11.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:57:11.742: INFO: namespace kubectl-6473 deletion completed in 6.165202929s

• [SLOW TEST:9.195 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:57:11.742: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6049
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-4bdc8076-7c3d-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 02:57:11.936: INFO: Waiting up to 5m0s for pod "pod-configmaps-4bdde7c4-7c3d-11e9-8c5e-b202ea6dae39" in namespace "configmap-6049" to be "success or failure"
May 22 02:57:11.947: INFO: Pod "pod-configmaps-4bdde7c4-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 11.282409ms
May 22 02:57:13.952: INFO: Pod "pod-configmaps-4bdde7c4-7c3d-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015887924s
STEP: Saw pod success
May 22 02:57:13.952: INFO: Pod "pod-configmaps-4bdde7c4-7c3d-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:57:13.957: INFO: Trying to get logs from node worker01 pod pod-configmaps-4bdde7c4-7c3d-11e9-8c5e-b202ea6dae39 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 02:57:13.991: INFO: Waiting for pod pod-configmaps-4bdde7c4-7c3d-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:57:14.004: INFO: Pod pod-configmaps-4bdde7c4-7c3d-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:57:14.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6049" for this suite.
May 22 02:57:20.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:57:20.196: INFO: namespace configmap-6049 deletion completed in 6.178846747s

• [SLOW TEST:8.454 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:57:20.196: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
May 22 02:57:20.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-4648'
May 22 02:57:20.568: INFO: stderr: ""
May 22 02:57:20.568: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 02:57:20.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4648'
May 22 02:57:20.680: INFO: stderr: ""
May 22 02:57:20.680: INFO: stdout: "update-demo-nautilus-dnqpf update-demo-nautilus-qrnqk "
May 22 02:57:20.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-dnqpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4648'
May 22 02:57:20.754: INFO: stderr: ""
May 22 02:57:20.754: INFO: stdout: ""
May 22 02:57:20.754: INFO: update-demo-nautilus-dnqpf is created but not running
May 22 02:57:25.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4648'
May 22 02:57:25.865: INFO: stderr: ""
May 22 02:57:25.865: INFO: stdout: "update-demo-nautilus-dnqpf update-demo-nautilus-qrnqk "
May 22 02:57:25.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-dnqpf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4648'
May 22 02:57:25.945: INFO: stderr: ""
May 22 02:57:25.945: INFO: stdout: "true"
May 22 02:57:25.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-dnqpf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4648'
May 22 02:57:26.021: INFO: stderr: ""
May 22 02:57:26.021: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 02:57:26.021: INFO: validating pod update-demo-nautilus-dnqpf
May 22 02:57:26.032: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 02:57:26.032: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 02:57:26.032: INFO: update-demo-nautilus-dnqpf is verified up and running
May 22 02:57:26.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-qrnqk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4648'
May 22 02:57:26.105: INFO: stderr: ""
May 22 02:57:26.105: INFO: stdout: "true"
May 22 02:57:26.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-qrnqk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4648'
May 22 02:57:26.176: INFO: stderr: ""
May 22 02:57:26.176: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 02:57:26.176: INFO: validating pod update-demo-nautilus-qrnqk
May 22 02:57:26.186: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 02:57:26.186: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 02:57:26.186: INFO: update-demo-nautilus-qrnqk is verified up and running
STEP: rolling-update to new replication controller
May 22 02:57:26.189: INFO: scanned /root for discovery docs: <nil>
May 22 02:57:26.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4648'
May 22 02:57:48.689: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 22 02:57:48.689: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 02:57:48.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4648'
May 22 02:57:48.788: INFO: stderr: ""
May 22 02:57:48.788: INFO: stdout: "update-demo-kitten-m6rnk update-demo-kitten-rvcdn "
May 22 02:57:48.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-kitten-m6rnk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4648'
May 22 02:57:48.863: INFO: stderr: ""
May 22 02:57:48.863: INFO: stdout: "true"
May 22 02:57:48.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-kitten-m6rnk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4648'
May 22 02:57:48.931: INFO: stderr: ""
May 22 02:57:48.931: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 22 02:57:48.931: INFO: validating pod update-demo-kitten-m6rnk
May 22 02:57:48.942: INFO: got data: {
  "image": "kitten.jpg"
}

May 22 02:57:48.942: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 22 02:57:48.942: INFO: update-demo-kitten-m6rnk is verified up and running
May 22 02:57:48.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-kitten-rvcdn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4648'
May 22 02:57:49.021: INFO: stderr: ""
May 22 02:57:49.021: INFO: stdout: "true"
May 22 02:57:49.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-kitten-rvcdn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4648'
May 22 02:57:49.095: INFO: stderr: ""
May 22 02:57:49.095: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 22 02:57:49.095: INFO: validating pod update-demo-kitten-rvcdn
May 22 02:57:49.105: INFO: got data: {
  "image": "kitten.jpg"
}

May 22 02:57:49.105: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 22 02:57:49.105: INFO: update-demo-kitten-rvcdn is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:57:49.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4648" for this suite.
May 22 02:58:01.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:58:01.306: INFO: namespace kubectl-4648 deletion completed in 12.19308172s

• [SLOW TEST:41.110 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:58:01.306: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 02:58:01.509: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6967834d-7c3d-11e9-8c5e-b202ea6dae39" in namespace "projected-8772" to be "success or failure"
May 22 02:58:01.522: INFO: Pod "downwardapi-volume-6967834d-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.814093ms
May 22 02:58:03.527: INFO: Pod "downwardapi-volume-6967834d-7c3d-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017343258s
STEP: Saw pod success
May 22 02:58:03.527: INFO: Pod "downwardapi-volume-6967834d-7c3d-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:58:03.531: INFO: Trying to get logs from node worker01 pod downwardapi-volume-6967834d-7c3d-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 02:58:03.570: INFO: Waiting for pod downwardapi-volume-6967834d-7c3d-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:58:03.584: INFO: Pod downwardapi-volume-6967834d-7c3d-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:58:03.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8772" for this suite.
May 22 02:58:09.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:58:09.788: INFO: namespace projected-8772 deletion completed in 6.192714038s

• [SLOW TEST:8.482 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:58:09.788: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-6e7516b3-7c3d-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 02:58:09.980: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6e76e1ce-7c3d-11e9-8c5e-b202ea6dae39" in namespace "projected-1608" to be "success or failure"
May 22 02:58:09.986: INFO: Pod "pod-projected-configmaps-6e76e1ce-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.349763ms
May 22 02:58:11.992: INFO: Pod "pod-projected-configmaps-6e76e1ce-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011976649s
May 22 02:58:14.003: INFO: Pod "pod-projected-configmaps-6e76e1ce-7c3d-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023700266s
STEP: Saw pod success
May 22 02:58:14.003: INFO: Pod "pod-projected-configmaps-6e76e1ce-7c3d-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 02:58:14.017: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-6e76e1ce-7c3d-11e9-8c5e-b202ea6dae39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 02:58:14.072: INFO: Waiting for pod pod-projected-configmaps-6e76e1ce-7c3d-11e9-8c5e-b202ea6dae39 to disappear
May 22 02:58:14.080: INFO: Pod pod-projected-configmaps-6e76e1ce-7c3d-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:58:14.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1608" for this suite.
May 22 02:58:20.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:58:20.287: INFO: namespace projected-1608 deletion completed in 6.196953509s

• [SLOW TEST:10.499 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:58:20.287: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4746
I0522 02:58:20.462626      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4746, replica count: 1
I0522 02:58:21.513360      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 02:58:22.513565      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 22 02:58:22.640: INFO: Created: latency-svc-gd7z7
May 22 02:58:22.660: INFO: Got endpoints: latency-svc-gd7z7 [46.012233ms]
May 22 02:58:22.702: INFO: Created: latency-svc-4sgfk
May 22 02:58:22.720: INFO: Created: latency-svc-87mts
May 22 02:58:22.721: INFO: Got endpoints: latency-svc-4sgfk [61.296395ms]
May 22 02:58:22.746: INFO: Got endpoints: latency-svc-87mts [85.977242ms]
May 22 02:58:22.752: INFO: Created: latency-svc-vtsxh
May 22 02:58:22.770: INFO: Got endpoints: latency-svc-vtsxh [110.211895ms]
May 22 02:58:22.803: INFO: Created: latency-svc-rv4ph
May 22 02:58:22.828: INFO: Got endpoints: latency-svc-rv4ph [168.333973ms]
May 22 02:58:22.856: INFO: Created: latency-svc-k92ch
May 22 02:58:22.869: INFO: Got endpoints: latency-svc-k92ch [209.23121ms]
May 22 02:58:22.917: INFO: Created: latency-svc-h5xc9
May 22 02:58:22.941: INFO: Got endpoints: latency-svc-h5xc9 [281.0014ms]
May 22 02:58:22.942: INFO: Created: latency-svc-k8gqg
May 22 02:58:22.972: INFO: Got endpoints: latency-svc-k8gqg [311.634987ms]
May 22 02:58:23.030: INFO: Created: latency-svc-8p2fd
May 22 02:58:23.058: INFO: Got endpoints: latency-svc-8p2fd [398.035697ms]
May 22 02:58:23.098: INFO: Created: latency-svc-knqnm
May 22 02:58:23.118: INFO: Got endpoints: latency-svc-knqnm [457.792833ms]
May 22 02:58:23.122: INFO: Created: latency-svc-wbz4j
May 22 02:58:23.147: INFO: Created: latency-svc-nbj85
May 22 02:58:23.151: INFO: Got endpoints: latency-svc-wbz4j [490.753142ms]
May 22 02:58:23.179: INFO: Got endpoints: latency-svc-nbj85 [518.569058ms]
May 22 02:58:23.188: INFO: Created: latency-svc-hmdkx
May 22 02:58:23.210: INFO: Created: latency-svc-k76pz
May 22 02:58:23.217: INFO: Got endpoints: latency-svc-hmdkx [556.404338ms]
May 22 02:58:23.232: INFO: Got endpoints: latency-svc-k76pz [571.758914ms]
May 22 02:58:23.244: INFO: Created: latency-svc-wxb6x
May 22 02:58:23.266: INFO: Got endpoints: latency-svc-wxb6x [605.644353ms]
May 22 02:58:23.276: INFO: Created: latency-svc-fl4vj
May 22 02:58:23.299: INFO: Got endpoints: latency-svc-fl4vj [638.376992ms]
May 22 02:58:23.310: INFO: Created: latency-svc-k8bg5
May 22 02:58:23.326: INFO: Got endpoints: latency-svc-k8bg5 [604.838987ms]
May 22 02:58:23.344: INFO: Created: latency-svc-dtxpn
May 22 02:58:23.366: INFO: Got endpoints: latency-svc-dtxpn [620.061526ms]
May 22 02:58:23.378: INFO: Created: latency-svc-7btzc
May 22 02:58:23.401: INFO: Got endpoints: latency-svc-7btzc [630.770823ms]
May 22 02:58:23.420: INFO: Created: latency-svc-d8ssz
May 22 02:58:23.438: INFO: Got endpoints: latency-svc-d8ssz [609.918033ms]
May 22 02:58:23.448: INFO: Created: latency-svc-fxh6v
May 22 02:58:23.464: INFO: Got endpoints: latency-svc-fxh6v [594.503481ms]
May 22 02:58:23.476: INFO: Created: latency-svc-nwr47
May 22 02:58:23.503: INFO: Got endpoints: latency-svc-nwr47 [562.087101ms]
May 22 02:58:23.511: INFO: Created: latency-svc-llqdd
May 22 02:58:23.540: INFO: Created: latency-svc-zq2k8
May 22 02:58:23.546: INFO: Got endpoints: latency-svc-llqdd [573.746887ms]
May 22 02:58:23.574: INFO: Got endpoints: latency-svc-zq2k8 [515.58918ms]
May 22 02:58:23.588: INFO: Created: latency-svc-4dn2j
May 22 02:58:23.596: INFO: Got endpoints: latency-svc-4dn2j [477.500246ms]
May 22 02:58:23.631: INFO: Created: latency-svc-mq5d2
May 22 02:58:23.647: INFO: Got endpoints: latency-svc-mq5d2 [495.625816ms]
May 22 02:58:23.662: INFO: Created: latency-svc-zpbwr
May 22 02:58:23.682: INFO: Got endpoints: latency-svc-zpbwr [502.574272ms]
May 22 02:58:23.694: INFO: Created: latency-svc-fg78m
May 22 02:58:23.731: INFO: Got endpoints: latency-svc-fg78m [513.874061ms]
May 22 02:58:23.741: INFO: Created: latency-svc-7sf6n
May 22 02:58:23.763: INFO: Got endpoints: latency-svc-7sf6n [530.999246ms]
May 22 02:58:23.778: INFO: Created: latency-svc-k2gvq
May 22 02:58:23.791: INFO: Got endpoints: latency-svc-k2gvq [524.372854ms]
May 22 02:58:23.816: INFO: Created: latency-svc-wpn8r
May 22 02:58:23.830: INFO: Created: latency-svc-zpnrt
May 22 02:58:23.835: INFO: Got endpoints: latency-svc-wpn8r [535.612712ms]
May 22 02:58:23.864: INFO: Got endpoints: latency-svc-zpnrt [537.715201ms]
May 22 02:58:23.883: INFO: Created: latency-svc-kp77k
May 22 02:58:23.894: INFO: Got endpoints: latency-svc-kp77k [527.498142ms]
May 22 02:58:23.908: INFO: Created: latency-svc-2stp2
May 22 02:58:23.927: INFO: Got endpoints: latency-svc-2stp2 [525.82763ms]
May 22 02:58:23.939: INFO: Created: latency-svc-f26q8
May 22 02:58:23.971: INFO: Got endpoints: latency-svc-f26q8 [533.043973ms]
May 22 02:58:23.974: INFO: Created: latency-svc-29rfd
May 22 02:58:23.997: INFO: Created: latency-svc-zgbft
May 22 02:58:24.020: INFO: Created: latency-svc-dnb5k
May 22 02:58:24.052: INFO: Got endpoints: latency-svc-29rfd [587.999223ms]
May 22 02:58:24.053: INFO: Got endpoints: latency-svc-zgbft [549.299616ms]
May 22 02:58:24.053: INFO: Got endpoints: latency-svc-dnb5k [507.13639ms]
May 22 02:58:24.075: INFO: Created: latency-svc-64b42
May 22 02:58:24.109: INFO: Created: latency-svc-z4hhn
May 22 02:58:24.109: INFO: Got endpoints: latency-svc-64b42 [535.452373ms]
May 22 02:58:24.121: INFO: Created: latency-svc-2zd5b
May 22 02:58:24.124: INFO: Got endpoints: latency-svc-z4hhn [528.451482ms]
May 22 02:58:24.140: INFO: Got endpoints: latency-svc-2zd5b [492.935703ms]
May 22 02:58:24.169: INFO: Created: latency-svc-pmttv
May 22 02:58:24.198: INFO: Got endpoints: latency-svc-pmttv [516.36155ms]
May 22 02:58:24.224: INFO: Created: latency-svc-cwzbr
May 22 02:58:24.243: INFO: Created: latency-svc-w4nkc
May 22 02:58:24.253: INFO: Got endpoints: latency-svc-cwzbr [522.409792ms]
May 22 02:58:24.273: INFO: Got endpoints: latency-svc-w4nkc [509.22711ms]
May 22 02:58:24.280: INFO: Created: latency-svc-jkc2g
May 22 02:58:24.300: INFO: Got endpoints: latency-svc-jkc2g [508.915943ms]
May 22 02:58:24.335: INFO: Created: latency-svc-zwgbq
May 22 02:58:24.339: INFO: Got endpoints: latency-svc-zwgbq [504.296606ms]
May 22 02:58:24.369: INFO: Created: latency-svc-sp8lc
May 22 02:58:24.382: INFO: Got endpoints: latency-svc-sp8lc [518.35048ms]
May 22 02:58:24.414: INFO: Created: latency-svc-dwgxs
May 22 02:58:24.435: INFO: Got endpoints: latency-svc-dwgxs [541.102971ms]
May 22 02:58:24.461: INFO: Created: latency-svc-gv9zw
May 22 02:58:24.479: INFO: Got endpoints: latency-svc-gv9zw [551.552534ms]
May 22 02:58:24.515: INFO: Created: latency-svc-2z8gx
May 22 02:58:24.550: INFO: Got endpoints: latency-svc-2z8gx [578.878325ms]
May 22 02:58:24.594: INFO: Created: latency-svc-qd4pd
May 22 02:58:24.612: INFO: Created: latency-svc-4dnf4
May 22 02:58:24.614: INFO: Got endpoints: latency-svc-qd4pd [561.942152ms]
May 22 02:58:24.632: INFO: Got endpoints: latency-svc-4dnf4 [579.473889ms]
May 22 02:58:24.663: INFO: Created: latency-svc-8822w
May 22 02:58:24.700: INFO: Got endpoints: latency-svc-8822w [647.697254ms]
May 22 02:58:24.724: INFO: Created: latency-svc-n8bml
May 22 02:58:24.737: INFO: Got endpoints: latency-svc-n8bml [627.94983ms]
May 22 02:58:24.756: INFO: Created: latency-svc-nvtzs
May 22 02:58:24.776: INFO: Got endpoints: latency-svc-nvtzs [651.177712ms]
May 22 02:58:24.811: INFO: Created: latency-svc-vlwlc
May 22 02:58:24.822: INFO: Got endpoints: latency-svc-vlwlc [681.747031ms]
May 22 02:58:24.853: INFO: Created: latency-svc-fz58k
May 22 02:58:24.870: INFO: Got endpoints: latency-svc-fz58k [671.92034ms]
May 22 02:58:24.895: INFO: Created: latency-svc-zfmgv
May 22 02:58:24.918: INFO: Got endpoints: latency-svc-zfmgv [664.483384ms]
May 22 02:58:24.952: INFO: Created: latency-svc-xxwhz
May 22 02:58:24.975: INFO: Created: latency-svc-zbhs5
May 22 02:58:24.979: INFO: Got endpoints: latency-svc-xxwhz [706.338838ms]
May 22 02:58:24.989: INFO: Got endpoints: latency-svc-zbhs5 [689.168174ms]
May 22 02:58:25.032: INFO: Created: latency-svc-5zj4w
May 22 02:58:25.045: INFO: Got endpoints: latency-svc-5zj4w [706.17599ms]
May 22 02:58:25.070: INFO: Created: latency-svc-kcj76
May 22 02:58:25.092: INFO: Got endpoints: latency-svc-kcj76 [710.131416ms]
May 22 02:58:25.115: INFO: Created: latency-svc-khpng
May 22 02:58:25.143: INFO: Got endpoints: latency-svc-khpng [708.238695ms]
May 22 02:58:25.165: INFO: Created: latency-svc-lgkwx
May 22 02:58:25.185: INFO: Got endpoints: latency-svc-lgkwx [706.264001ms]
May 22 02:58:25.228: INFO: Created: latency-svc-c4npq
May 22 02:58:25.249: INFO: Got endpoints: latency-svc-c4npq [699.159276ms]
May 22 02:58:25.275: INFO: Created: latency-svc-pzbwb
May 22 02:58:25.289: INFO: Got endpoints: latency-svc-pzbwb [675.287469ms]
May 22 02:58:25.313: INFO: Created: latency-svc-7hrgt
May 22 02:58:25.332: INFO: Got endpoints: latency-svc-7hrgt [700.185891ms]
May 22 02:58:25.347: INFO: Created: latency-svc-tk4hq
May 22 02:58:25.363: INFO: Got endpoints: latency-svc-tk4hq [662.298297ms]
May 22 02:58:25.395: INFO: Created: latency-svc-hvrjq
May 22 02:58:25.413: INFO: Got endpoints: latency-svc-hvrjq [675.751466ms]
May 22 02:58:25.446: INFO: Created: latency-svc-xslhc
May 22 02:58:25.485: INFO: Got endpoints: latency-svc-xslhc [709.263701ms]
May 22 02:58:25.572: INFO: Created: latency-svc-gbp4x
May 22 02:58:25.589: INFO: Got endpoints: latency-svc-gbp4x [766.822507ms]
May 22 02:58:25.611: INFO: Created: latency-svc-p6qsx
May 22 02:58:25.646: INFO: Got endpoints: latency-svc-p6qsx [775.518883ms]
May 22 02:58:25.675: INFO: Created: latency-svc-48f22
May 22 02:58:25.685: INFO: Got endpoints: latency-svc-48f22 [767.113248ms]
May 22 02:58:25.716: INFO: Created: latency-svc-hfntx
May 22 02:58:25.724: INFO: Got endpoints: latency-svc-hfntx [745.047612ms]
May 22 02:58:25.774: INFO: Created: latency-svc-nvsdd
May 22 02:58:25.803: INFO: Got endpoints: latency-svc-nvsdd [813.77366ms]
May 22 02:58:25.816: INFO: Created: latency-svc-dkpm9
May 22 02:58:25.822: INFO: Got endpoints: latency-svc-dkpm9 [777.139249ms]
May 22 02:58:25.879: INFO: Created: latency-svc-nj8l7
May 22 02:58:25.894: INFO: Got endpoints: latency-svc-nj8l7 [801.264969ms]
May 22 02:58:25.942: INFO: Created: latency-svc-mmj45
May 22 02:58:25.977: INFO: Got endpoints: latency-svc-mmj45 [834.028976ms]
May 22 02:58:25.994: INFO: Created: latency-svc-fzs69
May 22 02:58:26.023: INFO: Got endpoints: latency-svc-fzs69 [837.822325ms]
May 22 02:58:26.061: INFO: Created: latency-svc-5wvnr
May 22 02:58:26.069: INFO: Got endpoints: latency-svc-5wvnr [819.893678ms]
May 22 02:58:26.144: INFO: Created: latency-svc-qq2pb
May 22 02:58:26.157: INFO: Created: latency-svc-btkz8
May 22 02:58:26.181: INFO: Got endpoints: latency-svc-btkz8 [848.590555ms]
May 22 02:58:26.182: INFO: Got endpoints: latency-svc-qq2pb [892.225127ms]
May 22 02:58:26.207: INFO: Created: latency-svc-q64vq
May 22 02:58:26.227: INFO: Got endpoints: latency-svc-q64vq [864.323873ms]
May 22 02:58:26.236: INFO: Created: latency-svc-cpn6k
May 22 02:58:26.260: INFO: Got endpoints: latency-svc-cpn6k [846.705041ms]
May 22 02:58:26.270: INFO: Created: latency-svc-rk2rz
May 22 02:58:26.299: INFO: Got endpoints: latency-svc-rk2rz [814.196044ms]
May 22 02:58:26.305: INFO: Created: latency-svc-lqn46
May 22 02:58:26.336: INFO: Created: latency-svc-4ggsr
May 22 02:58:26.350: INFO: Got endpoints: latency-svc-lqn46 [761.542566ms]
May 22 02:58:26.361: INFO: Got endpoints: latency-svc-4ggsr [715.659943ms]
May 22 02:58:26.367: INFO: Created: latency-svc-rrmvg
May 22 02:58:26.393: INFO: Got endpoints: latency-svc-rrmvg [708.292078ms]
May 22 02:58:26.412: INFO: Created: latency-svc-qk22m
May 22 02:58:26.434: INFO: Got endpoints: latency-svc-qk22m [709.479085ms]
May 22 02:58:26.436: INFO: Created: latency-svc-b2lbc
May 22 02:58:26.456: INFO: Got endpoints: latency-svc-b2lbc [653.345955ms]
May 22 02:58:26.463: INFO: Created: latency-svc-9264c
May 22 02:58:26.487: INFO: Got endpoints: latency-svc-9264c [664.892204ms]
May 22 02:58:26.493: INFO: Created: latency-svc-kh44n
May 22 02:58:26.541: INFO: Created: latency-svc-6g2cf
May 22 02:58:26.544: INFO: Got endpoints: latency-svc-kh44n [650.534954ms]
May 22 02:58:26.561: INFO: Created: latency-svc-9m5zx
May 22 02:58:26.568: INFO: Got endpoints: latency-svc-6g2cf [590.702946ms]
May 22 02:58:26.591: INFO: Got endpoints: latency-svc-9m5zx [567.795964ms]
May 22 02:58:26.600: INFO: Created: latency-svc-jvzvr
May 22 02:58:26.630: INFO: Created: latency-svc-dzcgh
May 22 02:58:26.631: INFO: Got endpoints: latency-svc-jvzvr [561.800512ms]
May 22 02:58:26.654: INFO: Got endpoints: latency-svc-dzcgh [472.895183ms]
May 22 02:58:26.663: INFO: Created: latency-svc-p69sx
May 22 02:58:26.686: INFO: Created: latency-svc-zgpjs
May 22 02:58:26.693: INFO: Got endpoints: latency-svc-p69sx [511.269976ms]
May 22 02:58:26.708: INFO: Got endpoints: latency-svc-zgpjs [480.993541ms]
May 22 02:58:26.725: INFO: Created: latency-svc-b7twl
May 22 02:58:26.749: INFO: Created: latency-svc-mzkbj
May 22 02:58:26.790: INFO: Got endpoints: latency-svc-b7twl [530.353041ms]
May 22 02:58:26.790: INFO: Got endpoints: latency-svc-mzkbj [491.316057ms]
May 22 02:58:26.806: INFO: Created: latency-svc-l4lxh
May 22 02:58:26.827: INFO: Got endpoints: latency-svc-l4lxh [476.342368ms]
May 22 02:58:26.840: INFO: Created: latency-svc-fntx4
May 22 02:58:26.859: INFO: Got endpoints: latency-svc-fntx4 [497.880737ms]
May 22 02:58:26.868: INFO: Created: latency-svc-lrhdf
May 22 02:58:26.907: INFO: Created: latency-svc-tzfhf
May 22 02:58:26.910: INFO: Got endpoints: latency-svc-lrhdf [516.711439ms]
May 22 02:58:26.928: INFO: Got endpoints: latency-svc-tzfhf [494.403174ms]
May 22 02:58:26.934: INFO: Created: latency-svc-4gwdg
May 22 02:58:26.948: INFO: Got endpoints: latency-svc-4gwdg [491.652082ms]
May 22 02:58:26.966: INFO: Created: latency-svc-thp5t
May 22 02:58:26.989: INFO: Created: latency-svc-qknkp
May 22 02:58:26.991: INFO: Got endpoints: latency-svc-thp5t [503.391428ms]
May 22 02:58:27.021: INFO: Got endpoints: latency-svc-qknkp [477.24945ms]
May 22 02:58:27.029: INFO: Created: latency-svc-dfzkl
May 22 02:58:27.048: INFO: Created: latency-svc-xt8wr
May 22 02:58:27.059: INFO: Got endpoints: latency-svc-dfzkl [490.913429ms]
May 22 02:58:27.077: INFO: Got endpoints: latency-svc-xt8wr [486.080925ms]
May 22 02:58:27.099: INFO: Created: latency-svc-669cf
May 22 02:58:27.119: INFO: Created: latency-svc-k6tps
May 22 02:58:27.124: INFO: Got endpoints: latency-svc-669cf [493.109755ms]
May 22 02:58:27.149: INFO: Got endpoints: latency-svc-k6tps [495.070238ms]
May 22 02:58:27.160: INFO: Created: latency-svc-hkv2j
May 22 02:58:27.185: INFO: Got endpoints: latency-svc-hkv2j [492.318091ms]
May 22 02:58:27.211: INFO: Created: latency-svc-92fk2
May 22 02:58:27.235: INFO: Got endpoints: latency-svc-92fk2 [525.672027ms]
May 22 02:58:27.263: INFO: Created: latency-svc-csqvk
May 22 02:58:27.284: INFO: Got endpoints: latency-svc-csqvk [493.564431ms]
May 22 02:58:27.315: INFO: Created: latency-svc-9znkc
May 22 02:58:27.333: INFO: Got endpoints: latency-svc-9znkc [542.719844ms]
May 22 02:58:27.356: INFO: Created: latency-svc-p4b9s
May 22 02:58:27.361: INFO: Got endpoints: latency-svc-p4b9s [533.960266ms]
May 22 02:58:27.388: INFO: Created: latency-svc-f7682
May 22 02:58:27.409: INFO: Got endpoints: latency-svc-f7682 [550.20156ms]
May 22 02:58:27.440: INFO: Created: latency-svc-cmfvl
May 22 02:58:27.456: INFO: Got endpoints: latency-svc-cmfvl [545.241265ms]
May 22 02:58:27.482: INFO: Created: latency-svc-xcwrm
May 22 02:58:27.541: INFO: Got endpoints: latency-svc-xcwrm [613.225078ms]
May 22 02:58:27.542: INFO: Created: latency-svc-hls8m
May 22 02:58:27.549: INFO: Got endpoints: latency-svc-hls8m [600.99376ms]
May 22 02:58:27.575: INFO: Created: latency-svc-tccn4
May 22 02:58:27.603: INFO: Got endpoints: latency-svc-tccn4 [612.033518ms]
May 22 02:58:27.648: INFO: Created: latency-svc-gb2kt
May 22 02:58:27.669: INFO: Got endpoints: latency-svc-gb2kt [647.976103ms]
May 22 02:58:27.686: INFO: Created: latency-svc-fb8mn
May 22 02:58:27.699: INFO: Got endpoints: latency-svc-fb8mn [639.685506ms]
May 22 02:58:27.734: INFO: Created: latency-svc-4tztw
May 22 02:58:27.745: INFO: Got endpoints: latency-svc-4tztw [668.108582ms]
May 22 02:58:27.775: INFO: Created: latency-svc-w84mw
May 22 02:58:27.796: INFO: Got endpoints: latency-svc-w84mw [671.526988ms]
May 22 02:58:27.819: INFO: Created: latency-svc-7lp9b
May 22 02:58:27.844: INFO: Got endpoints: latency-svc-7lp9b [695.322653ms]
May 22 02:58:27.863: INFO: Created: latency-svc-b4mgj
May 22 02:58:27.886: INFO: Got endpoints: latency-svc-b4mgj [700.851323ms]
May 22 02:58:27.903: INFO: Created: latency-svc-wnn69
May 22 02:58:27.919: INFO: Got endpoints: latency-svc-wnn69 [684.576892ms]
May 22 02:58:27.961: INFO: Created: latency-svc-smqhw
May 22 02:58:27.972: INFO: Got endpoints: latency-svc-smqhw [688.001772ms]
May 22 02:58:28.007: INFO: Created: latency-svc-mmcgw
May 22 02:58:28.018: INFO: Got endpoints: latency-svc-mmcgw [684.575525ms]
May 22 02:58:28.059: INFO: Created: latency-svc-7p54w
May 22 02:58:28.071: INFO: Got endpoints: latency-svc-7p54w [710.342279ms]
May 22 02:58:28.108: INFO: Created: latency-svc-8cz7b
May 22 02:58:28.129: INFO: Got endpoints: latency-svc-8cz7b [719.567015ms]
May 22 02:58:28.152: INFO: Created: latency-svc-bcw2k
May 22 02:58:28.175: INFO: Got endpoints: latency-svc-bcw2k [719.254935ms]
May 22 02:58:28.218: INFO: Created: latency-svc-w8rpg
May 22 02:58:28.244: INFO: Got endpoints: latency-svc-w8rpg [702.456736ms]
May 22 02:58:28.268: INFO: Created: latency-svc-xr7cq
May 22 02:58:28.289: INFO: Got endpoints: latency-svc-xr7cq [740.716165ms]
May 22 02:58:28.311: INFO: Created: latency-svc-286vd
May 22 02:58:28.332: INFO: Got endpoints: latency-svc-286vd [729.164308ms]
May 22 02:58:28.345: INFO: Created: latency-svc-rr2cz
May 22 02:58:28.366: INFO: Got endpoints: latency-svc-rr2cz [696.674594ms]
May 22 02:58:28.380: INFO: Created: latency-svc-klkrz
May 22 02:58:28.404: INFO: Got endpoints: latency-svc-klkrz [705.285511ms]
May 22 02:58:28.424: INFO: Created: latency-svc-vjqt2
May 22 02:58:28.446: INFO: Created: latency-svc-nq5p5
May 22 02:58:28.456: INFO: Got endpoints: latency-svc-vjqt2 [711.411825ms]
May 22 02:58:28.483: INFO: Got endpoints: latency-svc-nq5p5 [687.153353ms]
May 22 02:58:28.495: INFO: Created: latency-svc-dswqw
May 22 02:58:28.522: INFO: Created: latency-svc-hszx8
May 22 02:58:28.526: INFO: Got endpoints: latency-svc-dswqw [681.387464ms]
May 22 02:58:28.531: INFO: Got endpoints: latency-svc-hszx8 [644.853835ms]
May 22 02:58:28.540: INFO: Created: latency-svc-jlhct
May 22 02:58:28.571: INFO: Got endpoints: latency-svc-jlhct [651.990334ms]
May 22 02:58:28.607: INFO: Created: latency-svc-kxkvq
May 22 02:58:28.632: INFO: Created: latency-svc-5h448
May 22 02:58:28.640: INFO: Got endpoints: latency-svc-kxkvq [667.962908ms]
May 22 02:58:28.661: INFO: Got endpoints: latency-svc-5h448 [642.746222ms]
May 22 02:58:28.674: INFO: Created: latency-svc-294rt
May 22 02:58:28.688: INFO: Got endpoints: latency-svc-294rt [616.352089ms]
May 22 02:58:28.694: INFO: Created: latency-svc-jvjkm
May 22 02:58:28.718: INFO: Got endpoints: latency-svc-jvjkm [589.066363ms]
May 22 02:58:28.759: INFO: Created: latency-svc-d2zjb
May 22 02:58:28.798: INFO: Got endpoints: latency-svc-d2zjb [622.986183ms]
May 22 02:58:28.823: INFO: Created: latency-svc-wdsb9
May 22 02:58:28.854: INFO: Got endpoints: latency-svc-wdsb9 [609.654313ms]
May 22 02:58:28.896: INFO: Created: latency-svc-rdtpf
May 22 02:58:28.908: INFO: Got endpoints: latency-svc-rdtpf [618.263258ms]
May 22 02:58:28.939: INFO: Created: latency-svc-vn6pq
May 22 02:58:28.958: INFO: Got endpoints: latency-svc-vn6pq [625.677558ms]
May 22 02:58:28.972: INFO: Created: latency-svc-6n68t
May 22 02:58:29.002: INFO: Got endpoints: latency-svc-6n68t [635.38699ms]
May 22 02:58:29.030: INFO: Created: latency-svc-6jjjs
May 22 02:58:29.062: INFO: Got endpoints: latency-svc-6jjjs [658.047538ms]
May 22 02:58:29.064: INFO: Created: latency-svc-mhtfb
May 22 02:58:29.092: INFO: Got endpoints: latency-svc-mhtfb [635.998676ms]
May 22 02:58:29.114: INFO: Created: latency-svc-dmz9w
May 22 02:58:29.140: INFO: Created: latency-svc-gnd5d
May 22 02:58:29.157: INFO: Got endpoints: latency-svc-dmz9w [673.629112ms]
May 22 02:58:29.169: INFO: Created: latency-svc-drftn
May 22 02:58:29.197: INFO: Got endpoints: latency-svc-gnd5d [671.452621ms]
May 22 02:58:29.198: INFO: Created: latency-svc-4jzlb
May 22 02:58:29.219: INFO: Got endpoints: latency-svc-drftn [688.056611ms]
May 22 02:58:29.238: INFO: Got endpoints: latency-svc-4jzlb [666.612011ms]
May 22 02:58:29.251: INFO: Created: latency-svc-2mfcz
May 22 02:58:29.278: INFO: Got endpoints: latency-svc-2mfcz [638.185285ms]
May 22 02:58:29.297: INFO: Created: latency-svc-62fjr
May 22 02:58:29.317: INFO: Created: latency-svc-5gpvp
May 22 02:58:29.354: INFO: Got endpoints: latency-svc-5gpvp [666.470758ms]
May 22 02:58:29.354: INFO: Got endpoints: latency-svc-62fjr [693.591985ms]
May 22 02:58:29.383: INFO: Created: latency-svc-2sskl
May 22 02:58:29.410: INFO: Created: latency-svc-x6c6s
May 22 02:58:29.413: INFO: Got endpoints: latency-svc-2sskl [694.59325ms]
May 22 02:58:29.436: INFO: Got endpoints: latency-svc-x6c6s [637.300337ms]
May 22 02:58:29.436: INFO: Created: latency-svc-gc7s7
May 22 02:58:29.455: INFO: Got endpoints: latency-svc-gc7s7 [601.008514ms]
May 22 02:58:29.462: INFO: Created: latency-svc-zrsw4
May 22 02:58:29.480: INFO: Got endpoints: latency-svc-zrsw4 [67.098199ms]
May 22 02:58:29.509: INFO: Created: latency-svc-f65fn
May 22 02:58:29.516: INFO: Got endpoints: latency-svc-f65fn [608.468739ms]
May 22 02:58:29.527: INFO: Created: latency-svc-crmzj
May 22 02:58:29.554: INFO: Got endpoints: latency-svc-crmzj [596.292945ms]
May 22 02:58:29.581: INFO: Created: latency-svc-jtgbs
May 22 02:58:29.594: INFO: Got endpoints: latency-svc-jtgbs [592.486575ms]
May 22 02:58:29.621: INFO: Created: latency-svc-sw4wh
May 22 02:58:29.634: INFO: Got endpoints: latency-svc-sw4wh [572.139086ms]
May 22 02:58:29.642: INFO: Created: latency-svc-spgh7
May 22 02:58:29.671: INFO: Got endpoints: latency-svc-spgh7 [579.017328ms]
May 22 02:58:29.681: INFO: Created: latency-svc-hknz7
May 22 02:58:29.717: INFO: Got endpoints: latency-svc-hknz7 [559.859525ms]
May 22 02:58:29.723: INFO: Created: latency-svc-c8vxj
May 22 02:58:29.747: INFO: Created: latency-svc-4xmj7
May 22 02:58:29.776: INFO: Got endpoints: latency-svc-c8vxj [578.200209ms]
May 22 02:58:29.778: INFO: Created: latency-svc-jccx8
May 22 02:58:29.798: INFO: Created: latency-svc-l7xzw
May 22 02:58:29.829: INFO: Got endpoints: latency-svc-4xmj7 [609.616247ms]
May 22 02:58:29.844: INFO: Created: latency-svc-d7tk7
May 22 02:58:29.862: INFO: Created: latency-svc-xzx4q
May 22 02:58:29.870: INFO: Got endpoints: latency-svc-jccx8 [631.642104ms]
May 22 02:58:29.912: INFO: Created: latency-svc-rdgfz
May 22 02:58:29.920: INFO: Created: latency-svc-s2c8v
May 22 02:58:29.931: INFO: Got endpoints: latency-svc-l7xzw [652.88984ms]
May 22 02:58:29.958: INFO: Created: latency-svc-hj9tw
May 22 02:58:29.970: INFO: Got endpoints: latency-svc-d7tk7 [615.913062ms]
May 22 02:58:29.997: INFO: Created: latency-svc-zctd8
May 22 02:58:30.028: INFO: Created: latency-svc-8w4q8
May 22 02:58:30.028: INFO: Got endpoints: latency-svc-xzx4q [673.790061ms]
May 22 02:58:30.054: INFO: Created: latency-svc-zwqjd
May 22 02:58:30.105: INFO: Got endpoints: latency-svc-rdgfz [669.650347ms]
May 22 02:58:30.106: INFO: Created: latency-svc-wg5zf
May 22 02:58:30.122: INFO: Got endpoints: latency-svc-s2c8v [667.283452ms]
May 22 02:58:30.132: INFO: Created: latency-svc-8pdcp
May 22 02:58:30.168: INFO: Created: latency-svc-rfscw
May 22 02:58:30.168: INFO: Got endpoints: latency-svc-hj9tw [688.497878ms]
May 22 02:58:30.193: INFO: Created: latency-svc-lt5qs
May 22 02:58:30.226: INFO: Got endpoints: latency-svc-zctd8 [709.619422ms]
May 22 02:58:30.227: INFO: Created: latency-svc-jjcxf
May 22 02:58:30.242: INFO: Created: latency-svc-thp75
May 22 02:58:30.270: INFO: Created: latency-svc-8c8s2
May 22 02:58:30.274: INFO: Got endpoints: latency-svc-8w4q8 [719.671226ms]
May 22 02:58:30.310: INFO: Got endpoints: latency-svc-zwqjd [716.117867ms]
May 22 02:58:30.311: INFO: Created: latency-svc-pjxl7
May 22 02:58:30.342: INFO: Created: latency-svc-tngg4
May 22 02:58:30.372: INFO: Created: latency-svc-5tnvm
May 22 02:58:30.376: INFO: Got endpoints: latency-svc-wg5zf [741.465675ms]
May 22 02:58:30.392: INFO: Created: latency-svc-n76kb
May 22 02:58:30.418: INFO: Created: latency-svc-s9rzk
May 22 02:58:30.423: INFO: Got endpoints: latency-svc-8pdcp [751.246774ms]
May 22 02:58:30.445: INFO: Created: latency-svc-25h5z
May 22 02:58:30.469: INFO: Got endpoints: latency-svc-rfscw [751.943713ms]
May 22 02:58:30.485: INFO: Created: latency-svc-gptjm
May 22 02:58:30.502: INFO: Created: latency-svc-nxvp4
May 22 02:58:30.521: INFO: Got endpoints: latency-svc-lt5qs [745.198878ms]
May 22 02:58:30.534: INFO: Created: latency-svc-hf8kb
May 22 02:58:30.568: INFO: Got endpoints: latency-svc-jjcxf [739.573381ms]
May 22 02:58:30.568: INFO: Created: latency-svc-26slc
May 22 02:58:30.591: INFO: Created: latency-svc-r248k
May 22 02:58:30.614: INFO: Got endpoints: latency-svc-thp75 [743.978467ms]
May 22 02:58:30.666: INFO: Got endpoints: latency-svc-8c8s2 [735.02083ms]
May 22 02:58:30.715: INFO: Got endpoints: latency-svc-pjxl7 [744.780329ms]
May 22 02:58:30.760: INFO: Got endpoints: latency-svc-tngg4 [732.315885ms]
May 22 02:58:30.812: INFO: Got endpoints: latency-svc-5tnvm [706.840948ms]
May 22 02:58:30.858: INFO: Got endpoints: latency-svc-n76kb [735.623765ms]
May 22 02:58:30.909: INFO: Got endpoints: latency-svc-s9rzk [740.966138ms]
May 22 02:58:30.958: INFO: Got endpoints: latency-svc-25h5z [731.9228ms]
May 22 02:58:31.007: INFO: Got endpoints: latency-svc-gptjm [733.588283ms]
May 22 02:58:31.061: INFO: Got endpoints: latency-svc-nxvp4 [750.916359ms]
May 22 02:58:31.107: INFO: Got endpoints: latency-svc-hf8kb [731.089357ms]
May 22 02:58:31.159: INFO: Got endpoints: latency-svc-26slc [735.936624ms]
May 22 02:58:31.207: INFO: Got endpoints: latency-svc-r248k [738.160389ms]
May 22 02:58:31.207: INFO: Latencies: [61.296395ms 67.098199ms 85.977242ms 110.211895ms 168.333973ms 209.23121ms 281.0014ms 311.634987ms 398.035697ms 457.792833ms 472.895183ms 476.342368ms 477.24945ms 477.500246ms 480.993541ms 486.080925ms 490.753142ms 490.913429ms 491.316057ms 491.652082ms 492.318091ms 492.935703ms 493.109755ms 493.564431ms 494.403174ms 495.070238ms 495.625816ms 497.880737ms 502.574272ms 503.391428ms 504.296606ms 507.13639ms 508.915943ms 509.22711ms 511.269976ms 513.874061ms 515.58918ms 516.36155ms 516.711439ms 518.35048ms 518.569058ms 522.409792ms 524.372854ms 525.672027ms 525.82763ms 527.498142ms 528.451482ms 530.353041ms 530.999246ms 533.043973ms 533.960266ms 535.452373ms 535.612712ms 537.715201ms 541.102971ms 542.719844ms 545.241265ms 549.299616ms 550.20156ms 551.552534ms 556.404338ms 559.859525ms 561.800512ms 561.942152ms 562.087101ms 567.795964ms 571.758914ms 572.139086ms 573.746887ms 578.200209ms 578.878325ms 579.017328ms 579.473889ms 587.999223ms 589.066363ms 590.702946ms 592.486575ms 594.503481ms 596.292945ms 600.99376ms 601.008514ms 604.838987ms 605.644353ms 608.468739ms 609.616247ms 609.654313ms 609.918033ms 612.033518ms 613.225078ms 615.913062ms 616.352089ms 618.263258ms 620.061526ms 622.986183ms 625.677558ms 627.94983ms 630.770823ms 631.642104ms 635.38699ms 635.998676ms 637.300337ms 638.185285ms 638.376992ms 639.685506ms 642.746222ms 644.853835ms 647.697254ms 647.976103ms 650.534954ms 651.177712ms 651.990334ms 652.88984ms 653.345955ms 658.047538ms 662.298297ms 664.483384ms 664.892204ms 666.470758ms 666.612011ms 667.283452ms 667.962908ms 668.108582ms 669.650347ms 671.452621ms 671.526988ms 671.92034ms 673.629112ms 673.790061ms 675.287469ms 675.751466ms 681.387464ms 681.747031ms 684.575525ms 684.576892ms 687.153353ms 688.001772ms 688.056611ms 688.497878ms 689.168174ms 693.591985ms 694.59325ms 695.322653ms 696.674594ms 699.159276ms 700.185891ms 700.851323ms 702.456736ms 705.285511ms 706.17599ms 706.264001ms 706.338838ms 706.840948ms 708.238695ms 708.292078ms 709.263701ms 709.479085ms 709.619422ms 710.131416ms 710.342279ms 711.411825ms 715.659943ms 716.117867ms 719.254935ms 719.567015ms 719.671226ms 729.164308ms 731.089357ms 731.9228ms 732.315885ms 733.588283ms 735.02083ms 735.623765ms 735.936624ms 738.160389ms 739.573381ms 740.716165ms 740.966138ms 741.465675ms 743.978467ms 744.780329ms 745.047612ms 745.198878ms 750.916359ms 751.246774ms 751.943713ms 761.542566ms 766.822507ms 767.113248ms 775.518883ms 777.139249ms 801.264969ms 813.77366ms 814.196044ms 819.893678ms 834.028976ms 837.822325ms 846.705041ms 848.590555ms 864.323873ms 892.225127ms]
May 22 02:58:31.207: INFO: 50 %ile: 637.300337ms
May 22 02:58:31.207: INFO: 90 %ile: 745.047612ms
May 22 02:58:31.207: INFO: 99 %ile: 864.323873ms
May 22 02:58:31.207: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:58:31.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4746" for this suite.
May 22 02:58:49.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:58:49.407: INFO: namespace svc-latency-4746 deletion completed in 18.188254641s

• [SLOW TEST:29.120 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:58:49.407: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-7296
May 22 02:58:51.600: INFO: Started pod liveness-exec in namespace container-probe-7296
STEP: checking the pod's current state and verifying that restartCount is present
May 22 02:58:51.604: INFO: Initial restart count of pod liveness-exec is 0
May 22 02:59:45.811: INFO: Restart count of pod container-probe-7296/liveness-exec is now 1 (54.206567327s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:59:45.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7296" for this suite.
May 22 02:59:51.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 02:59:52.078: INFO: namespace container-probe-7296 deletion completed in 6.220804113s

• [SLOW TEST:62.671 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 02:59:52.078: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 22 02:59:52.249: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 02:59:55.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3413" for this suite.
May 22 03:00:01.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:00:01.616: INFO: namespace init-container-3413 deletion completed in 6.162662862s

• [SLOW TEST:9.538 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:00:01.616: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-847
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 03:00:01.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 version'
May 22 03:00:01.871: INFO: stderr: ""
May 22 03:00:01.871: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.2\", GitCommit:\"66049e3b21efe110454d67df4fa62b08ea79a19b\", GitTreeState:\"clean\", BuildDate:\"2019-05-16T16:23:09Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.2\", GitCommit:\"66049e3b21efe110454d67df4fa62b08ea79a19b\", GitTreeState:\"clean\", BuildDate:\"2019-05-16T16:14:56Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:00:01.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-847" for this suite.
May 22 03:00:07.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:00:08.079: INFO: namespace kubectl-847 deletion completed in 6.1982739s

• [SLOW TEST:6.463 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:00:08.079: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9821
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 22 03:00:08.246: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 22 03:00:24.359: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.205:8080/dial?request=hostName&protocol=http&host=10.244.3.204&port=8080&tries=1'] Namespace:pod-network-test-9821 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:00:24.359: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:00:24.612: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:00:24.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9821" for this suite.
May 22 03:00:46.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:00:46.791: INFO: namespace pod-network-test-9821 deletion completed in 22.173516228s

• [SLOW TEST:38.712 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:00:46.791: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9993
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 22 03:00:46.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-9993'
May 22 03:00:47.166: INFO: stderr: ""
May 22 03:00:47.166: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 03:00:47.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9993'
May 22 03:00:47.270: INFO: stderr: ""
May 22 03:00:47.270: INFO: stdout: "update-demo-nautilus-2v8p8 update-demo-nautilus-tz69g "
May 22 03:00:47.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-2v8p8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9993'
May 22 03:00:47.343: INFO: stderr: ""
May 22 03:00:47.343: INFO: stdout: ""
May 22 03:00:47.343: INFO: update-demo-nautilus-2v8p8 is created but not running
May 22 03:00:52.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9993'
May 22 03:00:52.426: INFO: stderr: ""
May 22 03:00:52.426: INFO: stdout: "update-demo-nautilus-2v8p8 update-demo-nautilus-tz69g "
May 22 03:00:52.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-2v8p8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9993'
May 22 03:00:52.498: INFO: stderr: ""
May 22 03:00:52.498: INFO: stdout: "true"
May 22 03:00:52.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-2v8p8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9993'
May 22 03:00:52.570: INFO: stderr: ""
May 22 03:00:52.570: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 03:00:52.570: INFO: validating pod update-demo-nautilus-2v8p8
May 22 03:00:52.582: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 03:00:52.582: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 03:00:52.582: INFO: update-demo-nautilus-2v8p8 is verified up and running
May 22 03:00:52.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-tz69g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9993'
May 22 03:00:52.660: INFO: stderr: ""
May 22 03:00:52.660: INFO: stdout: "true"
May 22 03:00:52.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-tz69g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9993'
May 22 03:00:52.738: INFO: stderr: ""
May 22 03:00:52.738: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 03:00:52.738: INFO: validating pod update-demo-nautilus-tz69g
May 22 03:00:52.747: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 03:00:52.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 03:00:52.747: INFO: update-demo-nautilus-tz69g is verified up and running
STEP: using delete to clean up resources
May 22 03:00:52.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-9993'
May 22 03:00:52.833: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 03:00:52.833: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 22 03:00:52.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9993'
May 22 03:00:52.917: INFO: stderr: "No resources found.\n"
May 22 03:00:52.917: INFO: stdout: ""
May 22 03:00:52.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -l name=update-demo --namespace=kubectl-9993 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 03:00:52.987: INFO: stderr: ""
May 22 03:00:52.987: INFO: stdout: "update-demo-nautilus-2v8p8\nupdate-demo-nautilus-tz69g\n"
May 22 03:00:53.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9993'
May 22 03:00:53.588: INFO: stderr: "No resources found.\n"
May 22 03:00:53.588: INFO: stdout: ""
May 22 03:00:53.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -l name=update-demo --namespace=kubectl-9993 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 03:00:53.697: INFO: stderr: ""
May 22 03:00:53.697: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:00:53.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9993" for this suite.
May 22 03:01:15.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:01:15.879: INFO: namespace kubectl-9993 deletion completed in 22.174584083s

• [SLOW TEST:29.088 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:01:15.879: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6852
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6852
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 22 03:01:16.043: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 22 03:01:32.151: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.208:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6852 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:01:32.151: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:01:32.408: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:01:32.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6852" for this suite.
May 22 03:01:56.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:01:56.582: INFO: namespace pod-network-test-6852 deletion completed in 24.165902829s

• [SLOW TEST:40.703 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:01:56.582: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4754
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
May 22 03:01:56.767: INFO: Waiting up to 5m0s for pod "client-containers-f5a43401-7c3d-11e9-8c5e-b202ea6dae39" in namespace "containers-4754" to be "success or failure"
May 22 03:01:56.785: INFO: Pod "client-containers-f5a43401-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 18.191295ms
May 22 03:01:58.794: INFO: Pod "client-containers-f5a43401-7c3d-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027017535s
May 22 03:02:00.799: INFO: Pod "client-containers-f5a43401-7c3d-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031798053s
STEP: Saw pod success
May 22 03:02:00.799: INFO: Pod "client-containers-f5a43401-7c3d-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:02:00.803: INFO: Trying to get logs from node worker01 pod client-containers-f5a43401-7c3d-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 03:02:00.834: INFO: Waiting for pod client-containers-f5a43401-7c3d-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:02:00.844: INFO: Pod client-containers-f5a43401-7c3d-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:02:00.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4754" for this suite.
May 22 03:02:06.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:02:07.052: INFO: namespace containers-4754 deletion completed in 6.192933154s

• [SLOW TEST:10.470 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:02:07.052: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 22 03:02:11.790: INFO: Successfully updated pod "labelsupdatefbe1e058-7c3d-11e9-8c5e-b202ea6dae39"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:02:13.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2897" for this suite.
May 22 03:02:35.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:02:36.046: INFO: namespace downward-api-2897 deletion completed in 22.209887762s

• [SLOW TEST:28.994 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:02:36.046: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-0d28ca57-7c3e-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 03:02:36.239: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d2a55aa-7c3e-11e9-8c5e-b202ea6dae39" in namespace "configmap-1139" to be "success or failure"
May 22 03:02:36.253: INFO: Pod "pod-configmaps-0d2a55aa-7c3e-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.596433ms
May 22 03:02:38.258: INFO: Pod "pod-configmaps-0d2a55aa-7c3e-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019019336s
STEP: Saw pod success
May 22 03:02:38.258: INFO: Pod "pod-configmaps-0d2a55aa-7c3e-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:02:38.263: INFO: Trying to get logs from node worker01 pod pod-configmaps-0d2a55aa-7c3e-11e9-8c5e-b202ea6dae39 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 03:02:38.299: INFO: Waiting for pod pod-configmaps-0d2a55aa-7c3e-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:02:38.310: INFO: Pod pod-configmaps-0d2a55aa-7c3e-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:02:38.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1139" for this suite.
May 22 03:02:44.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:02:44.489: INFO: namespace configmap-1139 deletion completed in 6.169256178s

• [SLOW TEST:8.443 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:02:44.489: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 22 03:02:44.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-4550'
May 22 03:02:44.752: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 03:02:44.752: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
May 22 03:02:48.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4550'
May 22 03:02:48.878: INFO: stderr: ""
May 22 03:02:48.878: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:02:48.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4550" for this suite.
May 22 03:03:16.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:03:17.081: INFO: namespace kubectl-4550 deletion completed in 28.195567388s

• [SLOW TEST:32.592 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:03:17.082: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1153
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 03:03:17.256: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:03:23.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1153" for this suite.
May 22 03:03:29.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:03:29.557: INFO: namespace custom-resource-definition-1153 deletion completed in 6.179943795s

• [SLOW TEST:12.476 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:03:29.557: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 22 03:03:29.783: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:29.784: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:29.784: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:29.795: INFO: Number of nodes with available pods: 0
May 22 03:03:29.795: INFO: Node worker01 is running more than one daemon pod
May 22 03:03:30.800: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:30.800: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:30.800: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:30.806: INFO: Number of nodes with available pods: 0
May 22 03:03:30.806: INFO: Node worker01 is running more than one daemon pod
May 22 03:03:31.799: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:31.799: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:31.799: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:31.806: INFO: Number of nodes with available pods: 1
May 22 03:03:31.806: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 22 03:03:31.838: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:31.838: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:31.838: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:31.850: INFO: Number of nodes with available pods: 0
May 22 03:03:31.850: INFO: Node worker01 is running more than one daemon pod
May 22 03:03:32.857: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:32.857: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:32.857: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:32.861: INFO: Number of nodes with available pods: 0
May 22 03:03:32.861: INFO: Node worker01 is running more than one daemon pod
May 22 03:03:33.856: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:33.856: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:33.856: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:03:33.862: INFO: Number of nodes with available pods: 1
May 22 03:03:33.862: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7627, will wait for the garbage collector to delete the pods
May 22 03:03:33.941: INFO: Deleting DaemonSet.extensions daemon-set took: 9.345055ms
May 22 03:03:34.242: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.57171ms
May 22 03:03:42.347: INFO: Number of nodes with available pods: 0
May 22 03:03:42.347: INFO: Number of running nodes: 0, number of available pods: 0
May 22 03:03:42.351: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7627/daemonsets","resourceVersion":"145969"},"items":null}

May 22 03:03:42.358: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7627/pods","resourceVersion":"145969"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:03:42.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7627" for this suite.
May 22 03:03:48.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:03:48.558: INFO: namespace daemonsets-7627 deletion completed in 6.175287092s

• [SLOW TEST:19.001 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:03:48.558: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5734
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-143
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:03:55.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1480" for this suite.
May 22 03:04:01.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:04:01.334: INFO: namespace namespaces-1480 deletion completed in 6.194784457s
STEP: Destroying namespace "nsdeletetest-5734" for this suite.
May 22 03:04:01.338: INFO: Namespace nsdeletetest-5734 was already deleted
STEP: Destroying namespace "nsdeletetest-143" for this suite.
May 22 03:04:07.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:04:07.582: INFO: namespace nsdeletetest-143 deletion completed in 6.24425413s

• [SLOW TEST:19.024 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:04:07.582: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5873
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5873
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 22 03:04:07.757: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 22 03:04:27.895: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.218:8080/dial?request=hostName&protocol=udp&host=10.244.3.217&port=8081&tries=1'] Namespace:pod-network-test-5873 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:04:27.895: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:04:28.139: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:04:28.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5873" for this suite.
May 22 03:04:50.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:04:50.313: INFO: namespace pod-network-test-5873 deletion completed in 22.166004862s

• [SLOW TEST:42.731 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:04:50.313: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
May 22 03:04:50.484: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-734452489 proxy --unix-socket=/tmp/kubectl-proxy-unix016621896/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:04:50.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1669" for this suite.
May 22 03:04:56.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:04:56.714: INFO: namespace kubectl-1669 deletion completed in 6.170062097s

• [SLOW TEST:6.400 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:04:56.714: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
May 22 03:04:56.900: INFO: Waiting up to 5m0s for pod "client-containers-6101fa2f-7c3e-11e9-8c5e-b202ea6dae39" in namespace "containers-3971" to be "success or failure"
May 22 03:04:56.910: INFO: Pod "client-containers-6101fa2f-7c3e-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.330438ms
May 22 03:04:58.919: INFO: Pod "client-containers-6101fa2f-7c3e-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019286049s
May 22 03:05:00.924: INFO: Pod "client-containers-6101fa2f-7c3e-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024747822s
STEP: Saw pod success
May 22 03:05:00.924: INFO: Pod "client-containers-6101fa2f-7c3e-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:05:00.930: INFO: Trying to get logs from node worker01 pod client-containers-6101fa2f-7c3e-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 03:05:00.971: INFO: Waiting for pod client-containers-6101fa2f-7c3e-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:05:00.980: INFO: Pod client-containers-6101fa2f-7c3e-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:05:00.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3971" for this suite.
May 22 03:05:07.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:05:07.166: INFO: namespace containers-3971 deletion completed in 6.175871655s

• [SLOW TEST:10.452 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:05:07.166: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 22 03:05:07.339: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 22 03:05:07.358: INFO: Waiting for terminating namespaces to be deleted...
May 22 03:05:07.366: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
May 22 03:05:07.381: INFO: kube-proxy-l7wpq from kube-system started at 2019-05-20 03:44:11 +0000 UTC (1 container statuses recorded)
May 22 03:05:07.381: INFO: 	Container kube-proxy ready: true, restart count 5
May 22 03:05:07.381: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-22 01:41:43 +0000 UTC (1 container statuses recorded)
May 22 03:05:07.381: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 22 03:05:07.381: INFO: kube-flannel-ds-amd64-bd2jm from kube-system started at 2019-05-20 03:44:11 +0000 UTC (1 container statuses recorded)
May 22 03:05:07.381: INFO: 	Container kube-flannel ready: true, restart count 5
May 22 03:05:07.381: INFO: sonobuoy-e2e-job-858a03c85b4b4f46 from heptio-sonobuoy started at 2019-05-22 01:41:45 +0000 UTC (2 container statuses recorded)
May 22 03:05:07.381: INFO: 	Container e2e ready: true, restart count 0
May 22 03:05:07.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 03:05:07.381: INFO: tiller-deploy-6b9dd779b9-rlphv from kube-system started at 2019-05-20 03:44:51 +0000 UTC (1 container statuses recorded)
May 22 03:05:07.381: INFO: 	Container tiller ready: true, restart count 9
May 22 03:05:07.381: INFO: sonobuoy-systemd-logs-daemon-set-e3be60816e5648ce-qxw5h from heptio-sonobuoy started at 2019-05-22 01:41:46 +0000 UTC (2 container statuses recorded)
May 22 03:05:07.381: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 22 03:05:07.381: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-687d590f-7c3e-11e9-8c5e-b202ea6dae39 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-687d590f-7c3e-11e9-8c5e-b202ea6dae39 off the node worker01
STEP: verifying the node doesn't have the label kubernetes.io/e2e-687d590f-7c3e-11e9-8c5e-b202ea6dae39
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:05:11.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5655" for this suite.
May 22 03:05:23.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:05:23.698: INFO: namespace sched-pred-5655 deletion completed in 12.167952752s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:16.532 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:05:23.698: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-769
May 22 03:05:25.903: INFO: Started pod liveness-http in namespace container-probe-769
STEP: checking the pod's current state and verifying that restartCount is present
May 22 03:05:25.909: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:09:26.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-769" for this suite.
May 22 03:09:32.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:09:33.062: INFO: namespace container-probe-769 deletion completed in 6.192888238s

• [SLOW TEST:249.364 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:09:33.062: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-05b98540-7c3f-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume configMaps
May 22 03:09:33.262: INFO: Waiting up to 5m0s for pod "pod-configmaps-05bb17f9-7c3f-11e9-8c5e-b202ea6dae39" in namespace "configmap-8982" to be "success or failure"
May 22 03:09:33.274: INFO: Pod "pod-configmaps-05bb17f9-7c3f-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 11.72397ms
May 22 03:09:35.279: INFO: Pod "pod-configmaps-05bb17f9-7c3f-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016178164s
May 22 03:09:37.286: INFO: Pod "pod-configmaps-05bb17f9-7c3f-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023792593s
STEP: Saw pod success
May 22 03:09:37.286: INFO: Pod "pod-configmaps-05bb17f9-7c3f-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:09:37.296: INFO: Trying to get logs from node worker01 pod pod-configmaps-05bb17f9-7c3f-11e9-8c5e-b202ea6dae39 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 03:09:37.347: INFO: Waiting for pod pod-configmaps-05bb17f9-7c3f-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:09:37.355: INFO: Pod pod-configmaps-05bb17f9-7c3f-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:09:37.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8982" for this suite.
May 22 03:09:43.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:09:43.561: INFO: namespace configmap-8982 deletion completed in 6.197235592s

• [SLOW TEST:10.499 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:09:43.561: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-9239
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 22 03:09:47.793: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:47.793: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:48.054: INFO: Exec stderr: ""
May 22 03:09:48.054: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:48.054: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:48.369: INFO: Exec stderr: ""
May 22 03:09:48.369: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:48.369: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:48.651: INFO: Exec stderr: ""
May 22 03:09:48.652: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:48.652: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:48.981: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 22 03:09:48.981: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:48.981: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:49.251: INFO: Exec stderr: ""
May 22 03:09:49.251: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:49.580: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 22 03:09:49.580: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:49.580: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:49.870: INFO: Exec stderr: ""
May 22 03:09:49.870: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:49.870: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:50.176: INFO: Exec stderr: ""
May 22 03:09:50.176: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:50.176: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:50.464: INFO: Exec stderr: ""
May 22 03:09:50.464: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9239 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 03:09:50.464: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
May 22 03:09:50.789: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:09:50.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9239" for this suite.
May 22 03:10:34.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:10:34.996: INFO: namespace e2e-kubelet-etc-hosts-9239 deletion completed in 44.1993014s

• [SLOW TEST:51.435 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:10:34.997: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
May 22 03:10:35.163: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 22 03:10:35.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-7610'
May 22 03:10:35.439: INFO: stderr: ""
May 22 03:10:35.439: INFO: stdout: "service/redis-slave created\n"
May 22 03:10:35.439: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 22 03:10:35.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-7610'
May 22 03:10:35.736: INFO: stderr: ""
May 22 03:10:35.736: INFO: stdout: "service/redis-master created\n"
May 22 03:10:35.736: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 22 03:10:35.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-7610'
May 22 03:10:36.036: INFO: stderr: ""
May 22 03:10:36.036: INFO: stdout: "service/frontend created\n"
May 22 03:10:36.036: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 22 03:10:36.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-7610'
May 22 03:10:36.337: INFO: stderr: ""
May 22 03:10:36.338: INFO: stdout: "deployment.apps/frontend created\n"
May 22 03:10:36.338: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 22 03:10:36.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-7610'
May 22 03:10:36.586: INFO: stderr: ""
May 22 03:10:36.586: INFO: stdout: "deployment.apps/redis-master created\n"
May 22 03:10:36.586: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 22 03:10:36.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-7610'
May 22 03:10:36.829: INFO: stderr: ""
May 22 03:10:36.829: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 22 03:10:36.829: INFO: Waiting for all frontend pods to be Running.
May 22 03:10:41.880: INFO: Waiting for frontend to serve content.
May 22 03:10:41.915: INFO: Trying to add a new entry to the guestbook.
May 22 03:10:41.932: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 22 03:10:41.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-7610'
May 22 03:10:42.065: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 03:10:42.065: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 22 03:10:42.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-7610'
May 22 03:10:42.257: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 03:10:42.257: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 22 03:10:42.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-7610'
May 22 03:10:42.432: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 03:10:42.432: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 22 03:10:42.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-7610'
May 22 03:10:42.561: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 03:10:42.561: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 22 03:10:42.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-7610'
May 22 03:10:42.653: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 03:10:42.653: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 22 03:10:42.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-7610'
May 22 03:10:42.728: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 03:10:42.728: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:10:42.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7610" for this suite.
May 22 03:11:24.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:11:24.887: INFO: namespace kubectl-7610 deletion completed in 42.151723855s

• [SLOW TEST:49.890 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:11:24.887: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 03:11:25.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-485f89c4-7c3f-11e9-8c5e-b202ea6dae39" in namespace "downward-api-1422" to be "success or failure"
May 22 03:11:25.065: INFO: Pod "downwardapi-volume-485f89c4-7c3f-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.420256ms
May 22 03:11:27.070: INFO: Pod "downwardapi-volume-485f89c4-7c3f-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009208607s
May 22 03:11:29.080: INFO: Pod "downwardapi-volume-485f89c4-7c3f-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018541433s
STEP: Saw pod success
May 22 03:11:29.080: INFO: Pod "downwardapi-volume-485f89c4-7c3f-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:11:29.087: INFO: Trying to get logs from node worker01 pod downwardapi-volume-485f89c4-7c3f-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 03:11:29.128: INFO: Waiting for pod downwardapi-volume-485f89c4-7c3f-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:11:29.133: INFO: Pod downwardapi-volume-485f89c4-7c3f-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:11:29.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1422" for this suite.
May 22 03:11:35.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:11:35.331: INFO: namespace downward-api-1422 deletion completed in 6.191421202s

• [SLOW TEST:10.444 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:11:35.331: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 22 03:11:35.519: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:35.520: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:35.520: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:35.527: INFO: Number of nodes with available pods: 0
May 22 03:11:35.527: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:36.532: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:36.532: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:36.533: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:36.537: INFO: Number of nodes with available pods: 0
May 22 03:11:36.537: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:37.532: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:37.532: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:37.532: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:37.536: INFO: Number of nodes with available pods: 1
May 22 03:11:37.537: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 22 03:11:37.559: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:37.559: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:37.559: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:37.564: INFO: Number of nodes with available pods: 0
May 22 03:11:37.564: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:38.571: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:38.571: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:38.571: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:38.576: INFO: Number of nodes with available pods: 0
May 22 03:11:38.576: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:39.570: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:39.570: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:39.570: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:39.575: INFO: Number of nodes with available pods: 0
May 22 03:11:39.575: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:40.570: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:40.570: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:40.570: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:40.575: INFO: Number of nodes with available pods: 0
May 22 03:11:40.575: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:41.574: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:41.574: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:41.574: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:41.582: INFO: Number of nodes with available pods: 0
May 22 03:11:41.582: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:42.569: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:42.569: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:42.570: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:42.573: INFO: Number of nodes with available pods: 0
May 22 03:11:42.573: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:43.572: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:43.572: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:43.572: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:43.577: INFO: Number of nodes with available pods: 0
May 22 03:11:43.577: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:44.569: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:44.569: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:44.569: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:44.573: INFO: Number of nodes with available pods: 0
May 22 03:11:44.574: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:45.570: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:45.570: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:45.570: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:45.576: INFO: Number of nodes with available pods: 0
May 22 03:11:45.576: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:46.570: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:46.570: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:46.570: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:46.575: INFO: Number of nodes with available pods: 0
May 22 03:11:46.575: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:47.570: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:47.570: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:47.570: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:47.575: INFO: Number of nodes with available pods: 0
May 22 03:11:47.575: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:48.570: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:48.570: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:48.570: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:48.575: INFO: Number of nodes with available pods: 0
May 22 03:11:48.575: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:49.571: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:49.571: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:49.571: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:49.577: INFO: Number of nodes with available pods: 0
May 22 03:11:49.577: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:50.571: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:50.571: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:50.571: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:50.575: INFO: Number of nodes with available pods: 0
May 22 03:11:50.575: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:51.573: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:51.573: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:51.573: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:51.580: INFO: Number of nodes with available pods: 0
May 22 03:11:51.580: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:52.574: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:52.574: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:52.574: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:52.581: INFO: Number of nodes with available pods: 0
May 22 03:11:52.581: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:53.571: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:53.571: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:53.571: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:53.576: INFO: Number of nodes with available pods: 0
May 22 03:11:53.576: INFO: Node worker01 is running more than one daemon pod
May 22 03:11:54.572: INFO: DaemonSet pods can't tolerate node master01 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:54.572: INFO: DaemonSet pods can't tolerate node master02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:54.572: INFO: DaemonSet pods can't tolerate node master03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 03:11:54.577: INFO: Number of nodes with available pods: 1
May 22 03:11:54.577: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5802, will wait for the garbage collector to delete the pods
May 22 03:11:54.646: INFO: Deleting DaemonSet.extensions daemon-set took: 10.07991ms
May 22 03:11:54.947: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.476535ms
May 22 03:12:02.351: INFO: Number of nodes with available pods: 0
May 22 03:12:02.351: INFO: Number of running nodes: 0, number of available pods: 0
May 22 03:12:02.356: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5802/daemonsets","resourceVersion":"147426"},"items":null}

May 22 03:12:02.360: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5802/pods","resourceVersion":"147426"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:12:02.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5802" for this suite.
May 22 03:12:08.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:12:08.549: INFO: namespace daemonsets-5802 deletion completed in 6.170530624s

• [SLOW TEST:33.218 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:12:08.549: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 22 03:12:08.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 create -f - --namespace=kubectl-9934'
May 22 03:12:08.923: INFO: stderr: ""
May 22 03:12:08.923: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 03:12:08.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9934'
May 22 03:12:09.022: INFO: stderr: ""
May 22 03:12:09.022: INFO: stdout: "update-demo-nautilus-ntt8r update-demo-nautilus-rvcmp "
May 22 03:12:09.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-ntt8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:09.090: INFO: stderr: ""
May 22 03:12:09.090: INFO: stdout: ""
May 22 03:12:09.090: INFO: update-demo-nautilus-ntt8r is created but not running
May 22 03:12:14.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9934'
May 22 03:12:14.174: INFO: stderr: ""
May 22 03:12:14.174: INFO: stdout: "update-demo-nautilus-ntt8r update-demo-nautilus-rvcmp "
May 22 03:12:14.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-ntt8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:14.245: INFO: stderr: ""
May 22 03:12:14.245: INFO: stdout: "true"
May 22 03:12:14.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-ntt8r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:14.318: INFO: stderr: ""
May 22 03:12:14.318: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 03:12:14.318: INFO: validating pod update-demo-nautilus-ntt8r
May 22 03:12:14.325: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 03:12:14.325: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 03:12:14.325: INFO: update-demo-nautilus-ntt8r is verified up and running
May 22 03:12:14.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-rvcmp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:14.397: INFO: stderr: ""
May 22 03:12:14.398: INFO: stdout: "true"
May 22 03:12:14.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-rvcmp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:14.466: INFO: stderr: ""
May 22 03:12:14.466: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 03:12:14.466: INFO: validating pod update-demo-nautilus-rvcmp
May 22 03:12:14.473: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 03:12:14.473: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 03:12:14.473: INFO: update-demo-nautilus-rvcmp is verified up and running
STEP: scaling down the replication controller
May 22 03:12:14.475: INFO: scanned /root for discovery docs: <nil>
May 22 03:12:14.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9934'
May 22 03:12:15.577: INFO: stderr: ""
May 22 03:12:15.577: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 03:12:15.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9934'
May 22 03:12:15.648: INFO: stderr: ""
May 22 03:12:15.648: INFO: stdout: "update-demo-nautilus-ntt8r update-demo-nautilus-rvcmp "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 22 03:12:20.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9934'
May 22 03:12:20.725: INFO: stderr: ""
May 22 03:12:20.725: INFO: stdout: "update-demo-nautilus-ntt8r "
May 22 03:12:20.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-ntt8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:20.804: INFO: stderr: ""
May 22 03:12:20.804: INFO: stdout: "true"
May 22 03:12:20.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-ntt8r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:20.872: INFO: stderr: ""
May 22 03:12:20.872: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 03:12:20.872: INFO: validating pod update-demo-nautilus-ntt8r
May 22 03:12:20.878: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 03:12:20.878: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 03:12:20.878: INFO: update-demo-nautilus-ntt8r is verified up and running
STEP: scaling up the replication controller
May 22 03:12:20.880: INFO: scanned /root for discovery docs: <nil>
May 22 03:12:20.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9934'
May 22 03:12:21.994: INFO: stderr: ""
May 22 03:12:21.994: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 03:12:21.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9934'
May 22 03:12:22.076: INFO: stderr: ""
May 22 03:12:22.076: INFO: stdout: "update-demo-nautilus-ntt8r update-demo-nautilus-t6gc6 "
May 22 03:12:22.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-ntt8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:22.152: INFO: stderr: ""
May 22 03:12:22.152: INFO: stdout: "true"
May 22 03:12:22.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-ntt8r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:22.224: INFO: stderr: ""
May 22 03:12:22.224: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 03:12:22.224: INFO: validating pod update-demo-nautilus-ntt8r
May 22 03:12:22.229: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 03:12:22.229: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 03:12:22.229: INFO: update-demo-nautilus-ntt8r is verified up and running
May 22 03:12:22.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-t6gc6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:22.297: INFO: stderr: ""
May 22 03:12:22.297: INFO: stdout: ""
May 22 03:12:22.297: INFO: update-demo-nautilus-t6gc6 is created but not running
May 22 03:12:27.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9934'
May 22 03:12:27.372: INFO: stderr: ""
May 22 03:12:27.372: INFO: stdout: "update-demo-nautilus-ntt8r update-demo-nautilus-t6gc6 "
May 22 03:12:27.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-ntt8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:27.451: INFO: stderr: ""
May 22 03:12:27.451: INFO: stdout: "true"
May 22 03:12:27.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-ntt8r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:27.525: INFO: stderr: ""
May 22 03:12:27.525: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 03:12:27.525: INFO: validating pod update-demo-nautilus-ntt8r
May 22 03:12:27.532: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 03:12:27.532: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 03:12:27.532: INFO: update-demo-nautilus-ntt8r is verified up and running
May 22 03:12:27.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-t6gc6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:27.614: INFO: stderr: ""
May 22 03:12:27.614: INFO: stdout: "true"
May 22 03:12:27.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods update-demo-nautilus-t6gc6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9934'
May 22 03:12:27.679: INFO: stderr: ""
May 22 03:12:27.679: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 22 03:12:27.679: INFO: validating pod update-demo-nautilus-t6gc6
May 22 03:12:27.687: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 03:12:27.687: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 03:12:27.687: INFO: update-demo-nautilus-t6gc6 is verified up and running
STEP: using delete to clean up resources
May 22 03:12:27.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 delete --grace-period=0 --force -f - --namespace=kubectl-9934'
May 22 03:12:27.765: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 03:12:27.765: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 22 03:12:27.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9934'
May 22 03:12:27.843: INFO: stderr: "No resources found.\n"
May 22 03:12:27.843: INFO: stdout: ""
May 22 03:12:27.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -l name=update-demo --namespace=kubectl-9934 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 03:12:27.912: INFO: stderr: ""
May 22 03:12:27.913: INFO: stdout: "update-demo-nautilus-ntt8r\nupdate-demo-nautilus-t6gc6\n"
May 22 03:12:28.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9934'
May 22 03:12:28.506: INFO: stderr: "No resources found.\n"
May 22 03:12:28.506: INFO: stdout: ""
May 22 03:12:28.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-734452489 get pods -l name=update-demo --namespace=kubectl-9934 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 03:12:28.594: INFO: stderr: ""
May 22 03:12:28.594: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:12:28.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9934" for this suite.
May 22 03:12:34.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:12:34.800: INFO: namespace kubectl-9934 deletion completed in 6.199548678s

• [SLOW TEST:26.251 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:12:34.800: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-45
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-45
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-45
STEP: Deleting pre-stop pod
May 22 03:12:44.034: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:12:44.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-45" for this suite.
May 22 03:13:18.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:13:18.244: INFO: namespace prestop-45 deletion completed in 34.194861918s

• [SLOW TEST:43.444 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:13:18.244: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 03:13:18.406: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:13:20.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6612" for this suite.
May 22 03:14:04.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:14:04.840: INFO: namespace pods-6612 deletion completed in 44.185492795s

• [SLOW TEST:46.596 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:14:04.840: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1322
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 22 03:14:05.005: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:14:08.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1322" for this suite.
May 22 03:14:14.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:14:15.106: INFO: namespace init-container-1322 deletion completed in 6.207823653s

• [SLOW TEST:10.266 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:14:15.106: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 22 03:14:15.273: INFO: Waiting up to 5m0s for pod "pod-add3e8c0-7c3f-11e9-8c5e-b202ea6dae39" in namespace "emptydir-1381" to be "success or failure"
May 22 03:14:15.278: INFO: Pod "pod-add3e8c0-7c3f-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028482ms
May 22 03:14:17.283: INFO: Pod "pod-add3e8c0-7c3f-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010018191s
STEP: Saw pod success
May 22 03:14:17.283: INFO: Pod "pod-add3e8c0-7c3f-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:14:17.287: INFO: Trying to get logs from node worker01 pod pod-add3e8c0-7c3f-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 03:14:17.319: INFO: Waiting for pod pod-add3e8c0-7c3f-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:14:17.329: INFO: Pod pod-add3e8c0-7c3f-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:14:17.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1381" for this suite.
May 22 03:14:23.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:14:23.492: INFO: namespace emptydir-1381 deletion completed in 6.157933423s

• [SLOW TEST:8.386 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:14:23.493: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-9k8f
STEP: Creating a pod to test atomic-volume-subpath
May 22 03:14:23.685: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9k8f" in namespace "subpath-140" to be "success or failure"
May 22 03:14:23.693: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.876708ms
May 22 03:14:25.698: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013258842s
May 22 03:14:27.706: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.021144819s
May 22 03:14:29.712: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 6.027703561s
May 22 03:14:31.718: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 8.033435398s
May 22 03:14:33.726: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 10.041713578s
May 22 03:14:35.731: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 12.046498659s
May 22 03:14:37.737: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 14.051839017s
May 22 03:14:39.745: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 16.060110081s
May 22 03:14:41.752: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 18.067501243s
May 22 03:14:43.757: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 20.072400578s
May 22 03:14:45.765: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Running", Reason="", readiness=true. Elapsed: 22.080591099s
May 22 03:14:47.772: INFO: Pod "pod-subpath-test-secret-9k8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.08752736s
STEP: Saw pod success
May 22 03:14:47.772: INFO: Pod "pod-subpath-test-secret-9k8f" satisfied condition "success or failure"
May 22 03:14:47.780: INFO: Trying to get logs from node worker01 pod pod-subpath-test-secret-9k8f container test-container-subpath-secret-9k8f: <nil>
STEP: delete the pod
May 22 03:14:47.828: INFO: Waiting for pod pod-subpath-test-secret-9k8f to disappear
May 22 03:14:47.834: INFO: Pod pod-subpath-test-secret-9k8f no longer exists
STEP: Deleting pod pod-subpath-test-secret-9k8f
May 22 03:14:47.834: INFO: Deleting pod "pod-subpath-test-secret-9k8f" in namespace "subpath-140"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:14:47.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-140" for this suite.
May 22 03:14:53.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:14:54.022: INFO: namespace subpath-140 deletion completed in 6.172320008s

• [SLOW TEST:30.530 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:14:54.022: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4211
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 22 03:14:54.185: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-a,UID:c506079a-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148013,Generation:0,CreationTimestamp:2019-05-22 03:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 03:14:54.185: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-a,UID:c506079a-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148013,Generation:0,CreationTimestamp:2019-05-22 03:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 22 03:15:04.201: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-a,UID:c506079a-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148031,Generation:0,CreationTimestamp:2019-05-22 03:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 22 03:15:04.201: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-a,UID:c506079a-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148031,Generation:0,CreationTimestamp:2019-05-22 03:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 22 03:15:14.225: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-a,UID:c506079a-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148049,Generation:0,CreationTimestamp:2019-05-22 03:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 03:15:14.225: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-a,UID:c506079a-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148049,Generation:0,CreationTimestamp:2019-05-22 03:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 22 03:15:24.241: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-a,UID:c506079a-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148070,Generation:0,CreationTimestamp:2019-05-22 03:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 03:15:24.241: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-a,UID:c506079a-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148070,Generation:0,CreationTimestamp:2019-05-22 03:14:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 22 03:15:34.254: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-b,UID:dce77a33-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148086,Generation:0,CreationTimestamp:2019-05-22 03:15:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 03:15:34.254: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-b,UID:dce77a33-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148086,Generation:0,CreationTimestamp:2019-05-22 03:15:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 22 03:15:44.266: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-b,UID:dce77a33-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148105,Generation:0,CreationTimestamp:2019-05-22 03:15:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 03:15:44.266: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4211,SelfLink:/api/v1/namespaces/watch-4211/configmaps/e2e-watch-test-configmap-b,UID:dce77a33-7c3f-11e9-b253-000c290a0f16,ResourceVersion:148105,Generation:0,CreationTimestamp:2019-05-22 03:15:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:15:54.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4211" for this suite.
May 22 03:16:00.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:16:00.443: INFO: namespace watch-4211 deletion completed in 6.166535117s

• [SLOW TEST:66.421 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:16:00.443: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 03:16:00.609: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec9cde40-7c3f-11e9-8c5e-b202ea6dae39" in namespace "projected-7778" to be "success or failure"
May 22 03:16:00.617: INFO: Pod "downwardapi-volume-ec9cde40-7c3f-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 7.159379ms
May 22 03:16:02.623: INFO: Pod "downwardapi-volume-ec9cde40-7c3f-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013390489s
May 22 03:16:04.628: INFO: Pod "downwardapi-volume-ec9cde40-7c3f-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018499984s
STEP: Saw pod success
May 22 03:16:04.628: INFO: Pod "downwardapi-volume-ec9cde40-7c3f-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:16:04.632: INFO: Trying to get logs from node worker01 pod downwardapi-volume-ec9cde40-7c3f-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 03:16:04.660: INFO: Waiting for pod downwardapi-volume-ec9cde40-7c3f-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:16:04.668: INFO: Pod downwardapi-volume-ec9cde40-7c3f-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:16:04.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7778" for this suite.
May 22 03:16:10.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:16:10.849: INFO: namespace projected-7778 deletion completed in 6.175734796s

• [SLOW TEST:10.405 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:16:10.849: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0522 03:16:51.045818      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 03:16:51.045: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:16:51.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8411" for this suite.
May 22 03:16:59.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:16:59.228: INFO: namespace gc-8411 deletion completed in 8.176680863s

• [SLOW TEST:48.379 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:16:59.228: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 22 03:16:59.404: INFO: Waiting up to 5m0s for pod "pod-0fa740b6-7c40-11e9-8c5e-b202ea6dae39" in namespace "emptydir-863" to be "success or failure"
May 22 03:16:59.413: INFO: Pod "pod-0fa740b6-7c40-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.727723ms
May 22 03:17:01.419: INFO: Pod "pod-0fa740b6-7c40-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014008629s
May 22 03:17:03.423: INFO: Pod "pod-0fa740b6-7c40-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018512042s
STEP: Saw pod success
May 22 03:17:03.423: INFO: Pod "pod-0fa740b6-7c40-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:17:03.427: INFO: Trying to get logs from node worker01 pod pod-0fa740b6-7c40-11e9-8c5e-b202ea6dae39 container test-container: <nil>
STEP: delete the pod
May 22 03:17:03.453: INFO: Waiting for pod pod-0fa740b6-7c40-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:17:03.457: INFO: Pod pod-0fa740b6-7c40-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:17:03.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-863" for this suite.
May 22 03:17:09.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:17:09.616: INFO: namespace emptydir-863 deletion completed in 6.152739883s

• [SLOW TEST:10.388 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:17:09.616: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 22 03:17:14.339: INFO: Successfully updated pod "pod-update-activedeadlineseconds-15d8c20e-7c40-11e9-8c5e-b202ea6dae39"
May 22 03:17:14.339: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-15d8c20e-7c40-11e9-8c5e-b202ea6dae39" in namespace "pods-9148" to be "terminated due to deadline exceeded"
May 22 03:17:14.343: INFO: Pod "pod-update-activedeadlineseconds-15d8c20e-7c40-11e9-8c5e-b202ea6dae39": Phase="Running", Reason="", readiness=true. Elapsed: 4.261012ms
May 22 03:17:16.348: INFO: Pod "pod-update-activedeadlineseconds-15d8c20e-7c40-11e9-8c5e-b202ea6dae39": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.008946923s
May 22 03:17:16.348: INFO: Pod "pod-update-activedeadlineseconds-15d8c20e-7c40-11e9-8c5e-b202ea6dae39" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:17:16.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9148" for this suite.
May 22 03:17:22.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:17:22.505: INFO: namespace pods-9148 deletion completed in 6.151306826s

• [SLOW TEST:12.888 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:17:22.505: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 03:17:22.673: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d86ce25-7c40-11e9-8c5e-b202ea6dae39" in namespace "downward-api-6107" to be "success or failure"
May 22 03:17:22.679: INFO: Pod "downwardapi-volume-1d86ce25-7c40-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 5.583347ms
May 22 03:17:24.684: INFO: Pod "downwardapi-volume-1d86ce25-7c40-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010715809s
STEP: Saw pod success
May 22 03:17:24.684: INFO: Pod "downwardapi-volume-1d86ce25-7c40-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:17:24.689: INFO: Trying to get logs from node worker01 pod downwardapi-volume-1d86ce25-7c40-11e9-8c5e-b202ea6dae39 container client-container: <nil>
STEP: delete the pod
May 22 03:17:24.719: INFO: Waiting for pod downwardapi-volume-1d86ce25-7c40-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:17:24.725: INFO: Pod downwardapi-volume-1d86ce25-7c40-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:17:24.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6107" for this suite.
May 22 03:17:30.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:17:30.928: INFO: namespace downward-api-6107 deletion completed in 6.196053388s

• [SLOW TEST:8.423 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:17:30.928: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 03:17:31.110: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 22 03:17:31.121: INFO: Number of nodes with available pods: 0
May 22 03:17:31.121: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 22 03:17:31.153: INFO: Number of nodes with available pods: 0
May 22 03:17:31.153: INFO: Node worker01 is running more than one daemon pod
May 22 03:17:32.158: INFO: Number of nodes with available pods: 0
May 22 03:17:32.158: INFO: Node worker01 is running more than one daemon pod
May 22 03:17:33.165: INFO: Number of nodes with available pods: 1
May 22 03:17:33.165: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 22 03:17:33.221: INFO: Number of nodes with available pods: 0
May 22 03:17:33.221: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 22 03:17:33.251: INFO: Number of nodes with available pods: 0
May 22 03:17:33.251: INFO: Node worker01 is running more than one daemon pod
May 22 03:17:34.258: INFO: Number of nodes with available pods: 0
May 22 03:17:34.258: INFO: Node worker01 is running more than one daemon pod
May 22 03:17:35.256: INFO: Number of nodes with available pods: 0
May 22 03:17:35.256: INFO: Node worker01 is running more than one daemon pod
May 22 03:17:36.256: INFO: Number of nodes with available pods: 0
May 22 03:17:36.256: INFO: Node worker01 is running more than one daemon pod
May 22 03:17:37.263: INFO: Number of nodes with available pods: 0
May 22 03:17:37.263: INFO: Node worker01 is running more than one daemon pod
May 22 03:17:38.256: INFO: Number of nodes with available pods: 0
May 22 03:17:38.256: INFO: Node worker01 is running more than one daemon pod
May 22 03:17:39.257: INFO: Number of nodes with available pods: 0
May 22 03:17:39.257: INFO: Node worker01 is running more than one daemon pod
May 22 03:17:40.256: INFO: Number of nodes with available pods: 1
May 22 03:17:40.256: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5685, will wait for the garbage collector to delete the pods
May 22 03:17:40.334: INFO: Deleting DaemonSet.extensions daemon-set took: 9.746109ms
May 22 03:17:40.634: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.544995ms
May 22 03:17:52.341: INFO: Number of nodes with available pods: 0
May 22 03:17:52.341: INFO: Number of running nodes: 0, number of available pods: 0
May 22 03:17:52.346: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5685/daemonsets","resourceVersion":"148696"},"items":null}

May 22 03:17:52.350: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5685/pods","resourceVersion":"148696"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:17:52.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5685" for this suite.
May 22 03:17:58.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:17:58.543: INFO: namespace daemonsets-5685 deletion completed in 6.162840795s

• [SLOW TEST:27.615 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 03:17:58.543: INFO: >>> kubeConfig: /tmp/kubeconfig-734452489
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-3302cf54-7c40-11e9-8c5e-b202ea6dae39
STEP: Creating a pod to test consume secrets
May 22 03:17:58.724: INFO: Waiting up to 5m0s for pod "pod-secrets-3303ab2c-7c40-11e9-8c5e-b202ea6dae39" in namespace "secrets-9946" to be "success or failure"
May 22 03:17:58.731: INFO: Pod "pod-secrets-3303ab2c-7c40-11e9-8c5e-b202ea6dae39": Phase="Pending", Reason="", readiness=false. Elapsed: 7.212572ms
May 22 03:18:00.736: INFO: Pod "pod-secrets-3303ab2c-7c40-11e9-8c5e-b202ea6dae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012750555s
STEP: Saw pod success
May 22 03:18:00.737: INFO: Pod "pod-secrets-3303ab2c-7c40-11e9-8c5e-b202ea6dae39" satisfied condition "success or failure"
May 22 03:18:00.741: INFO: Trying to get logs from node worker01 pod pod-secrets-3303ab2c-7c40-11e9-8c5e-b202ea6dae39 container secret-volume-test: <nil>
STEP: delete the pod
May 22 03:18:00.773: INFO: Waiting for pod pod-secrets-3303ab2c-7c40-11e9-8c5e-b202ea6dae39 to disappear
May 22 03:18:00.779: INFO: Pod pod-secrets-3303ab2c-7c40-11e9-8c5e-b202ea6dae39 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 03:18:00.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9946" for this suite.
May 22 03:18:06.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 03:18:06.964: INFO: namespace secrets-9946 deletion completed in 6.179218088s

• [SLOW TEST:8.421 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSMay 22 03:18:06.964: INFO: Running AfterSuite actions on all nodes
May 22 03:18:06.965: INFO: Running AfterSuite actions on node 1
May 22 03:18:06.965: INFO: Skipping dumping logs from cluster

Ran 204 of 3585 Specs in 5770.645 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3381 Skipped PASS

Ginkgo ran 1 suite in 1h36m12.043725993s
Test Suite Passed
