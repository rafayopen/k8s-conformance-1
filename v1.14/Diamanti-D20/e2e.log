I0905 20:39:36.245286      18 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-156594675
I0905 20:39:36.246067      18 e2e.go:240] Starting e2e run "458df513-d01d-11e9-a0ad-8693e9898db7" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1567715974 - Will randomize all specs
Will run 204 of 3585 specs

Sep  5 20:39:36.462: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:39:36.466: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  5 20:39:36.494: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  5 20:39:36.526: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  5 20:39:36.526: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep  5 20:39:36.526: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  5 20:39:36.535: INFO: e2e test version: v1.14.3
Sep  5 20:39:36.536: INFO: kube-apiserver version: v1.14.3
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:39:36.536: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename aggregator
Sep  5 20:39:36.567: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep  5 20:39:36.577: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1795
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Sep  5 20:39:37.741: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  5 20:39:39.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703312777, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703312777, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703312777, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703312777, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:39:42.616: INFO: Waited 830.992055ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:39:43.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1795" for this suite.
Sep  5 20:39:49.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:39:49.445: INFO: namespace aggregator-1795 deletion completed in 6.199746273s

• [SLOW TEST:12.909 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:39:49.445: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:39:51.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5391" for this suite.
Sep  5 20:39:57.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:39:57.739: INFO: namespace emptydir-wrapper-5391 deletion completed in 6.103590777s

• [SLOW TEST:8.294 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:39:57.739: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9217
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 20:39:57.876: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  5 20:40:19.950: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=http&host=172.16.141.11&port=8080&tries=1'] Namespace:pod-network-test-9217 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:40:19.950: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:40:20.074: INFO: Waiting for endpoints: map[]
Sep  5 20:40:20.078: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=http&host=172.16.141.12&port=8080&tries=1'] Namespace:pod-network-test-9217 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:40:20.078: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:40:20.182: INFO: Waiting for endpoints: map[]
Sep  5 20:40:20.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=http&host=172.16.141.13&port=8080&tries=1'] Namespace:pod-network-test-9217 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:40:20.186: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:40:20.283: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:40:20.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9217" for this suite.
Sep  5 20:40:42.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:40:42.437: INFO: namespace pod-network-test-9217 deletion completed in 22.149329033s

• [SLOW TEST:44.697 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:40:42.437: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 20:40:42.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6dc59dc4-d01d-11e9-a0ad-8693e9898db7" in namespace "downward-api-2412" to be "success or failure"
Sep  5 20:40:42.593: INFO: Pod "downwardapi-volume-6dc59dc4-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.415229ms
Sep  5 20:40:44.598: INFO: Pod "downwardapi-volume-6dc59dc4-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007562617s
STEP: Saw pod success
Sep  5 20:40:44.598: INFO: Pod "downwardapi-volume-6dc59dc4-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:40:44.601: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-6dc59dc4-d01d-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 20:40:44.632: INFO: Waiting for pod downwardapi-volume-6dc59dc4-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:40:44.635: INFO: Pod downwardapi-volume-6dc59dc4-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:40:44.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2412" for this suite.
Sep  5 20:40:50.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:40:50.754: INFO: namespace downward-api-2412 deletion completed in 6.114089852s

• [SLOW TEST:8.317 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:40:50.754: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3360
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 20:40:50.902: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72ba059c-d01d-11e9-a0ad-8693e9898db7" in namespace "projected-3360" to be "success or failure"
Sep  5 20:40:50.906: INFO: Pod "downwardapi-volume-72ba059c-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.558261ms
Sep  5 20:40:52.481: INFO: Pod "downwardapi-volume-72ba059c-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008371289s
STEP: Saw pod success
Sep  5 20:40:52.481: INFO: Pod "downwardapi-volume-72ba059c-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:40:52.485: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-72ba059c-d01d-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 20:40:52.513: INFO: Waiting for pod downwardapi-volume-72ba059c-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:40:52.516: INFO: Pod downwardapi-volume-72ba059c-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:40:52.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3360" for this suite.
Sep  5 20:40:58.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:40:58.632: INFO: namespace projected-3360 deletion completed in 6.111863415s

• [SLOW TEST:8.308 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:40:58.632: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5034
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  5 20:40:58.781: INFO: Waiting up to 5m0s for pod "downward-api-776c657c-d01d-11e9-a0ad-8693e9898db7" in namespace "downward-api-5034" to be "success or failure"
Sep  5 20:40:58.784: INFO: Pod "downward-api-776c657c-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084524ms
Sep  5 20:41:00.788: INFO: Pod "downward-api-776c657c-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007433694s
STEP: Saw pod success
Sep  5 20:41:00.788: INFO: Pod "downward-api-776c657c-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:41:00.792: INFO: Trying to get logs from node appserv10 pod downward-api-776c657c-d01d-11e9-a0ad-8693e9898db7 container dapi-container: <nil>
STEP: delete the pod
Sep  5 20:41:00.815: INFO: Waiting for pod downward-api-776c657c-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:41:00.817: INFO: Pod downward-api-776c657c-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:41:00.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5034" for this suite.
Sep  5 20:41:06.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:41:06.928: INFO: namespace downward-api-5034 deletion completed in 6.106509317s

• [SLOW TEST:8.296 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:41:06.928: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9065
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 20:41:07.094: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  5 20:41:07.101: INFO: Number of nodes with available pods: 0
Sep  5 20:41:07.101: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  5 20:41:07.117: INFO: Number of nodes with available pods: 0
Sep  5 20:41:07.117: INFO: Node appserv10 is running more than one daemon pod
Sep  5 20:41:08.121: INFO: Number of nodes with available pods: 0
Sep  5 20:41:08.121: INFO: Node appserv10 is running more than one daemon pod
Sep  5 20:41:09.121: INFO: Number of nodes with available pods: 1
Sep  5 20:41:09.121: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  5 20:41:09.139: INFO: Number of nodes with available pods: 1
Sep  5 20:41:09.139: INFO: Number of running nodes: 0, number of available pods: 1
Sep  5 20:41:10.143: INFO: Number of nodes with available pods: 0
Sep  5 20:41:10.143: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  5 20:41:10.156: INFO: Number of nodes with available pods: 0
Sep  5 20:41:10.156: INFO: Node appserv10 is running more than one daemon pod
Sep  5 20:41:11.161: INFO: Number of nodes with available pods: 0
Sep  5 20:41:11.161: INFO: Node appserv10 is running more than one daemon pod
Sep  5 20:41:12.160: INFO: Number of nodes with available pods: 0
Sep  5 20:41:12.160: INFO: Node appserv10 is running more than one daemon pod
Sep  5 20:41:13.160: INFO: Number of nodes with available pods: 0
Sep  5 20:41:13.160: INFO: Node appserv10 is running more than one daemon pod
Sep  5 20:41:14.160: INFO: Number of nodes with available pods: 0
Sep  5 20:41:14.160: INFO: Node appserv10 is running more than one daemon pod
Sep  5 20:41:15.161: INFO: Number of nodes with available pods: 0
Sep  5 20:41:15.161: INFO: Node appserv10 is running more than one daemon pod
Sep  5 20:41:16.160: INFO: Number of nodes with available pods: 0
Sep  5 20:41:16.160: INFO: Node appserv10 is running more than one daemon pod
Sep  5 20:41:17.160: INFO: Number of nodes with available pods: 1
Sep  5 20:41:17.160: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9065, will wait for the garbage collector to delete the pods
Sep  5 20:41:17.225: INFO: Deleting DaemonSet.extensions daemon-set took: 6.286615ms
Sep  5 20:41:17.725: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.217705ms
Sep  5 20:41:28.429: INFO: Number of nodes with available pods: 0
Sep  5 20:41:28.429: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 20:41:28.435: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9065/daemonsets","resourceVersion":"1702"},"items":null}

Sep  5 20:41:28.438: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9065/pods","resourceVersion":"1702"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:41:28.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9065" for this suite.
Sep  5 20:41:34.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:41:34.575: INFO: namespace daemonsets-9065 deletion completed in 6.11146981s

• [SLOW TEST:27.647 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:41:34.576: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1500
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-8cd8498e-d01d-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 20:41:34.724: INFO: Waiting up to 5m0s for pod "pod-configmaps-8cd8e38f-d01d-11e9-a0ad-8693e9898db7" in namespace "configmap-1500" to be "success or failure"
Sep  5 20:41:34.728: INFO: Pod "pod-configmaps-8cd8e38f-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.909293ms
Sep  5 20:41:36.732: INFO: Pod "pod-configmaps-8cd8e38f-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007903689s
STEP: Saw pod success
Sep  5 20:41:36.732: INFO: Pod "pod-configmaps-8cd8e38f-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:41:36.735: INFO: Trying to get logs from node appserv9 pod pod-configmaps-8cd8e38f-d01d-11e9-a0ad-8693e9898db7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 20:41:36.753: INFO: Waiting for pod pod-configmaps-8cd8e38f-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:41:36.756: INFO: Pod pod-configmaps-8cd8e38f-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:41:36.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1500" for this suite.
Sep  5 20:41:42.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:41:42.871: INFO: namespace configmap-1500 deletion completed in 6.111426016s

• [SLOW TEST:8.295 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:41:42.872: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  5 20:41:43.019: INFO: Waiting up to 5m0s for pod "pod-91ca8791-d01d-11e9-a0ad-8693e9898db7" in namespace "emptydir-5458" to be "success or failure"
Sep  5 20:41:43.022: INFO: Pod "pod-91ca8791-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.008171ms
Sep  5 20:41:45.026: INFO: Pod "pod-91ca8791-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007260528s
STEP: Saw pod success
Sep  5 20:41:45.026: INFO: Pod "pod-91ca8791-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:41:45.029: INFO: Trying to get logs from node appserv10 pod pod-91ca8791-d01d-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 20:41:45.049: INFO: Waiting for pod pod-91ca8791-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:41:45.054: INFO: Pod pod-91ca8791-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:41:45.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5458" for this suite.
Sep  5 20:41:51.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:41:51.174: INFO: namespace emptydir-5458 deletion completed in 6.115528161s

• [SLOW TEST:8.302 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:41:51.174: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  5 20:41:51.320: INFO: Waiting up to 5m0s for pod "pod-96bd5c91-d01d-11e9-a0ad-8693e9898db7" in namespace "emptydir-9831" to be "success or failure"
Sep  5 20:41:51.323: INFO: Pod "pod-96bd5c91-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.023991ms
Sep  5 20:41:53.328: INFO: Pod "pod-96bd5c91-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00717862s
STEP: Saw pod success
Sep  5 20:41:53.328: INFO: Pod "pod-96bd5c91-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:41:53.331: INFO: Trying to get logs from node appserv9 pod pod-96bd5c91-d01d-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 20:41:53.351: INFO: Waiting for pod pod-96bd5c91-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:41:53.353: INFO: Pod pod-96bd5c91-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:41:53.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9831" for this suite.
Sep  5 20:41:59.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:41:59.471: INFO: namespace emptydir-9831 deletion completed in 6.112343836s

• [SLOW TEST:8.297 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:41:59.471: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4129
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-9baf4cdd-d01d-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 20:41:59.621: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9bafd291-d01d-11e9-a0ad-8693e9898db7" in namespace "projected-4129" to be "success or failure"
Sep  5 20:41:59.624: INFO: Pod "pod-projected-secrets-9bafd291-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.998526ms
Sep  5 20:42:01.628: INFO: Pod "pod-projected-secrets-9bafd291-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007020502s
STEP: Saw pod success
Sep  5 20:42:01.628: INFO: Pod "pod-projected-secrets-9bafd291-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:42:01.631: INFO: Trying to get logs from node appserv10 pod pod-projected-secrets-9bafd291-d01d-11e9-a0ad-8693e9898db7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 20:42:01.651: INFO: Waiting for pod pod-projected-secrets-9bafd291-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:42:01.654: INFO: Pod pod-projected-secrets-9bafd291-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:42:01.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4129" for this suite.
Sep  5 20:42:07.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:42:07.764: INFO: namespace projected-4129 deletion completed in 6.106019711s

• [SLOW TEST:8.293 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:42:07.764: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 20:42:07.907: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  5 20:42:12.911: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  5 20:42:12.911: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  5 20:42:12.931: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-7970,SelfLink:/apis/apps/v1/namespaces/deployment-7970/deployments/test-cleanup-deployment,UID:a3df3bcd-d01d-11e9-9b5e-2c600c82ec72,ResourceVersion:1958,Generation:1,CreationTimestamp:2019-09-05 20:42:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Sep  5 20:42:12.935: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Sep  5 20:42:12.935: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  5 20:42:12.935: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-7970,SelfLink:/apis/apps/v1/namespaces/deployment-7970/replicasets/test-cleanup-controller,UID:a0e1d3fb-d01d-11e9-9b5e-2c600c82ec72,ResourceVersion:1959,Generation:1,CreationTimestamp:2019-09-05 20:42:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a3df3bcd-d01d-11e9-9b5e-2c600c82ec72 0xc002fad71f 0xc002fad730}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  5 20:42:12.939: INFO: Pod "test-cleanup-controller-gdh7z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-gdh7z,GenerateName:test-cleanup-controller-,Namespace:deployment-7970,SelfLink:/api/v1/namespaces/deployment-7970/pods/test-cleanup-controller-gdh7z,UID:a0e2f4fd-d01d-11e9-9b5e-2c600c82ec72,ResourceVersion:1953,Generation:0,CreationTimestamp:2019-09-05 20:42:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller a0e1d3fb-d01d-11e9-9b5e-2c600c82ec72 0xc00295f3af 0xc00295f3d0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mh7h8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mh7h8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mh7h8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00295f440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00295f460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:42:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:42:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:42:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:42:08 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.11,StartTime:2019-09-05 20:42:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 20:42:09 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6ad02b3fda27721f409c88f61d0e3e0295e2fc330fc42b51655ebac0b966dc6c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:42:12.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7970" for this suite.
Sep  5 20:42:18.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:42:19.055: INFO: namespace deployment-7970 deletion completed in 6.110261985s

• [SLOW TEST:11.291 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:42:19.055: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-a75cc620-d01d-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 20:42:19.213: INFO: Waiting up to 5m0s for pod "pod-secrets-a75d60e4-d01d-11e9-a0ad-8693e9898db7" in namespace "secrets-7787" to be "success or failure"
Sep  5 20:42:19.216: INFO: Pod "pod-secrets-a75d60e4-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021279ms
Sep  5 20:42:21.220: INFO: Pod "pod-secrets-a75d60e4-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006804185s
STEP: Saw pod success
Sep  5 20:42:21.220: INFO: Pod "pod-secrets-a75d60e4-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:42:21.223: INFO: Trying to get logs from node appserv10 pod pod-secrets-a75d60e4-d01d-11e9-a0ad-8693e9898db7 container secret-env-test: <nil>
STEP: delete the pod
Sep  5 20:42:21.243: INFO: Waiting for pod pod-secrets-a75d60e4-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:42:21.245: INFO: Pod pod-secrets-a75d60e4-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:42:21.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7787" for this suite.
Sep  5 20:42:27.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:42:27.357: INFO: namespace secrets-7787 deletion completed in 6.107671563s

• [SLOW TEST:8.302 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:42:27.357: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Sep  5 20:42:27.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-1908'
Sep  5 20:42:27.970: INFO: stderr: ""
Sep  5 20:42:27.970: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 20:42:27.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1908'
Sep  5 20:42:28.086: INFO: stderr: ""
Sep  5 20:42:28.086: INFO: stdout: "update-demo-nautilus-gnj8b update-demo-nautilus-l67sf "
Sep  5 20:42:28.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-gnj8b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1908'
Sep  5 20:42:28.187: INFO: stderr: ""
Sep  5 20:42:28.187: INFO: stdout: ""
Sep  5 20:42:28.187: INFO: update-demo-nautilus-gnj8b is created but not running
Sep  5 20:42:33.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1908'
Sep  5 20:42:33.311: INFO: stderr: ""
Sep  5 20:42:33.311: INFO: stdout: "update-demo-nautilus-gnj8b update-demo-nautilus-l67sf "
Sep  5 20:42:33.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-gnj8b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1908'
Sep  5 20:42:33.422: INFO: stderr: ""
Sep  5 20:42:33.422: INFO: stdout: "true"
Sep  5 20:42:33.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-gnj8b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1908'
Sep  5 20:42:33.532: INFO: stderr: ""
Sep  5 20:42:33.532: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 20:42:33.532: INFO: validating pod update-demo-nautilus-gnj8b
Sep  5 20:42:33.541: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 20:42:33.541: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 20:42:33.541: INFO: update-demo-nautilus-gnj8b is verified up and running
Sep  5 20:42:33.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-l67sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1908'
Sep  5 20:42:33.648: INFO: stderr: ""
Sep  5 20:42:33.648: INFO: stdout: "true"
Sep  5 20:42:33.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-l67sf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1908'
Sep  5 20:42:33.757: INFO: stderr: ""
Sep  5 20:42:33.757: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 20:42:33.757: INFO: validating pod update-demo-nautilus-l67sf
Sep  5 20:42:33.766: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 20:42:33.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 20:42:33.766: INFO: update-demo-nautilus-l67sf is verified up and running
STEP: rolling-update to new replication controller
Sep  5 20:42:33.768: INFO: scanned /root for discovery docs: <nil>
Sep  5 20:42:33.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-1908'
Sep  5 20:42:56.196: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  5 20:42:56.196: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 20:42:56.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1908'
Sep  5 20:42:56.315: INFO: stderr: ""
Sep  5 20:42:56.315: INFO: stdout: "update-demo-kitten-r6fcl update-demo-kitten-tc6tf update-demo-nautilus-gnj8b "
STEP: Replicas for name=update-demo: expected=2 actual=3
Sep  5 20:43:01.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1908'
Sep  5 20:43:01.444: INFO: stderr: ""
Sep  5 20:43:01.444: INFO: stdout: "update-demo-kitten-r6fcl update-demo-kitten-tc6tf "
Sep  5 20:43:01.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-kitten-r6fcl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1908'
Sep  5 20:43:01.556: INFO: stderr: ""
Sep  5 20:43:01.556: INFO: stdout: "true"
Sep  5 20:43:01.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-kitten-r6fcl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1908'
Sep  5 20:43:01.668: INFO: stderr: ""
Sep  5 20:43:01.668: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  5 20:43:01.668: INFO: validating pod update-demo-kitten-r6fcl
Sep  5 20:43:01.677: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  5 20:43:01.678: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  5 20:43:01.678: INFO: update-demo-kitten-r6fcl is verified up and running
Sep  5 20:43:01.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-kitten-tc6tf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1908'
Sep  5 20:43:01.791: INFO: stderr: ""
Sep  5 20:43:01.791: INFO: stdout: "true"
Sep  5 20:43:01.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-kitten-tc6tf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1908'
Sep  5 20:43:01.905: INFO: stderr: ""
Sep  5 20:43:01.905: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  5 20:43:01.905: INFO: validating pod update-demo-kitten-tc6tf
Sep  5 20:43:01.911: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  5 20:43:01.911: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  5 20:43:01.911: INFO: update-demo-kitten-tc6tf is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:43:01.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1908" for this suite.
Sep  5 20:43:23.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:43:24.032: INFO: namespace kubectl-1908 deletion completed in 22.11636088s

• [SLOW TEST:56.675 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:43:24.033: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3953
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3953
STEP: Creating statefulset with conflicting port in namespace statefulset-3953
STEP: Waiting until pod test-pod will start running in namespace statefulset-3953
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3953
Sep  5 20:43:26.202: INFO: Observed stateful pod in namespace: statefulset-3953, name: ss-0, uid: cf327f1d-d01d-11e9-9b5e-2c600c82ec72, status phase: Pending. Waiting for statefulset controller to delete.
Sep  5 20:43:26.596: INFO: Observed stateful pod in namespace: statefulset-3953, name: ss-0, uid: cf327f1d-d01d-11e9-9b5e-2c600c82ec72, status phase: Failed. Waiting for statefulset controller to delete.
Sep  5 20:43:26.601: INFO: Observed stateful pod in namespace: statefulset-3953, name: ss-0, uid: cf327f1d-d01d-11e9-9b5e-2c600c82ec72, status phase: Failed. Waiting for statefulset controller to delete.
Sep  5 20:43:26.604: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3953
STEP: Removing pod with conflicting port in namespace statefulset-3953
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3953 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  5 20:43:30.624: INFO: Deleting all statefulset in ns statefulset-3953
Sep  5 20:43:30.627: INFO: Scaling statefulset ss to 0
Sep  5 20:43:40.642: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 20:43:40.645: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:43:40.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3953" for this suite.
Sep  5 20:43:46.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:43:46.762: INFO: namespace statefulset-3953 deletion completed in 6.100740101s

• [SLOW TEST:22.729 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:43:46.763: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-dba264e6-d01d-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 20:43:46.910: INFO: Waiting up to 5m0s for pod "pod-secrets-dba2f49a-d01d-11e9-a0ad-8693e9898db7" in namespace "secrets-4112" to be "success or failure"
Sep  5 20:43:46.912: INFO: Pod "pod-secrets-dba2f49a-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.598871ms
Sep  5 20:43:48.916: INFO: Pod "pod-secrets-dba2f49a-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006513095s
STEP: Saw pod success
Sep  5 20:43:48.916: INFO: Pod "pod-secrets-dba2f49a-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:43:48.920: INFO: Trying to get logs from node appserv10 pod pod-secrets-dba2f49a-d01d-11e9-a0ad-8693e9898db7 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 20:43:48.940: INFO: Waiting for pod pod-secrets-dba2f49a-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:43:48.942: INFO: Pod pod-secrets-dba2f49a-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:43:48.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4112" for this suite.
Sep  5 20:43:54.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:43:55.073: INFO: namespace secrets-4112 deletion completed in 6.126532245s

• [SLOW TEST:8.310 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:43:55.074: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 20:43:55.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e096ebcb-d01d-11e9-a0ad-8693e9898db7" in namespace "downward-api-691" to be "success or failure"
Sep  5 20:43:55.223: INFO: Pod "downwardapi-volume-e096ebcb-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.057188ms
Sep  5 20:43:57.228: INFO: Pod "downwardapi-volume-e096ebcb-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007315835s
STEP: Saw pod success
Sep  5 20:43:57.228: INFO: Pod "downwardapi-volume-e096ebcb-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:43:57.231: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-e096ebcb-d01d-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 20:43:57.249: INFO: Waiting for pod downwardapi-volume-e096ebcb-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:43:57.251: INFO: Pod downwardapi-volume-e096ebcb-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:43:57.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-691" for this suite.
Sep  5 20:44:03.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:44:03.360: INFO: namespace downward-api-691 deletion completed in 6.105962507s

• [SLOW TEST:8.287 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:44:03.361: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-5gf7
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 20:44:03.514: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5gf7" in namespace "subpath-869" to be "success or failure"
Sep  5 20:44:03.517: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.082368ms
Sep  5 20:44:05.521: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007502922s
Sep  5 20:44:07.526: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 4.011907583s
Sep  5 20:44:09.530: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 6.016152817s
Sep  5 20:44:11.534: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 8.020246395s
Sep  5 20:44:13.538: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 10.024461189s
Sep  5 20:44:15.542: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 12.028302914s
Sep  5 20:44:17.546: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 14.032185907s
Sep  5 20:44:19.550: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 16.036153251s
Sep  5 20:44:21.554: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 18.040307949s
Sep  5 20:44:23.558: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Running", Reason="", readiness=true. Elapsed: 20.044443604s
Sep  5 20:44:25.562: INFO: Pod "pod-subpath-test-configmap-5gf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.04767216s
STEP: Saw pod success
Sep  5 20:44:25.562: INFO: Pod "pod-subpath-test-configmap-5gf7" satisfied condition "success or failure"
Sep  5 20:44:25.564: INFO: Trying to get logs from node appserv10 pod pod-subpath-test-configmap-5gf7 container test-container-subpath-configmap-5gf7: <nil>
STEP: delete the pod
Sep  5 20:44:25.583: INFO: Waiting for pod pod-subpath-test-configmap-5gf7 to disappear
Sep  5 20:44:25.587: INFO: Pod pod-subpath-test-configmap-5gf7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5gf7
Sep  5 20:44:25.587: INFO: Deleting pod "pod-subpath-test-configmap-5gf7" in namespace "subpath-869"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:44:25.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-869" for this suite.
Sep  5 20:44:31.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:44:31.703: INFO: namespace subpath-869 deletion completed in 6.108639428s

• [SLOW TEST:28.342 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:44:31.703: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Sep  5 20:44:31.841: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-156594675 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:44:31.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6414" for this suite.
Sep  5 20:44:37.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:44:38.054: INFO: namespace kubectl-6414 deletion completed in 6.112156882s

• [SLOW TEST:6.351 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:44:38.054: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  5 20:44:38.200: INFO: Waiting up to 5m0s for pod "pod-fa351737-d01d-11e9-a0ad-8693e9898db7" in namespace "emptydir-8586" to be "success or failure"
Sep  5 20:44:38.203: INFO: Pod "pod-fa351737-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915618ms
Sep  5 20:44:40.207: INFO: Pod "pod-fa351737-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006951206s
STEP: Saw pod success
Sep  5 20:44:40.207: INFO: Pod "pod-fa351737-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:44:40.210: INFO: Trying to get logs from node appserv9 pod pod-fa351737-d01d-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 20:44:40.230: INFO: Waiting for pod pod-fa351737-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:44:40.233: INFO: Pod pod-fa351737-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:44:40.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8586" for this suite.
Sep  5 20:44:46.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:44:46.333: INFO: namespace emptydir-8586 deletion completed in 6.095150647s

• [SLOW TEST:8.279 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:44:46.333: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 20:44:46.475: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff23d698-d01d-11e9-a0ad-8693e9898db7" in namespace "projected-5341" to be "success or failure"
Sep  5 20:44:46.477: INFO: Pod "downwardapi-volume-ff23d698-d01d-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238162ms
Sep  5 20:44:48.481: INFO: Pod "downwardapi-volume-ff23d698-d01d-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006323361s
STEP: Saw pod success
Sep  5 20:44:48.481: INFO: Pod "downwardapi-volume-ff23d698-d01d-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:44:48.484: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-ff23d698-d01d-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 20:44:48.506: INFO: Waiting for pod downwardapi-volume-ff23d698-d01d-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:44:48.508: INFO: Pod downwardapi-volume-ff23d698-d01d-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:44:48.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5341" for this suite.
Sep  5 20:44:54.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:44:54.625: INFO: namespace projected-5341 deletion completed in 6.111787877s

• [SLOW TEST:8.292 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:44:54.625: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-7lzc
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 20:44:54.780: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7lzc" in namespace "subpath-445" to be "success or failure"
Sep  5 20:44:54.783: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896814ms
Sep  5 20:44:56.787: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 2.00655797s
Sep  5 20:44:58.791: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 4.011292107s
Sep  5 20:45:00.795: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 6.015370777s
Sep  5 20:45:02.801: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 8.020576272s
Sep  5 20:45:04.805: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 10.024699051s
Sep  5 20:45:06.809: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 12.028519293s
Sep  5 20:45:08.813: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 14.032742951s
Sep  5 20:45:10.817: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 16.036777541s
Sep  5 20:45:12.821: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 18.040968271s
Sep  5 20:45:14.826: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Running", Reason="", readiness=true. Elapsed: 20.045781005s
Sep  5 20:45:16.830: INFO: Pod "pod-subpath-test-configmap-7lzc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.04961268s
STEP: Saw pod success
Sep  5 20:45:16.830: INFO: Pod "pod-subpath-test-configmap-7lzc" satisfied condition "success or failure"
Sep  5 20:45:16.833: INFO: Trying to get logs from node appserv9 pod pod-subpath-test-configmap-7lzc container test-container-subpath-configmap-7lzc: <nil>
STEP: delete the pod
Sep  5 20:45:16.852: INFO: Waiting for pod pod-subpath-test-configmap-7lzc to disappear
Sep  5 20:45:16.854: INFO: Pod pod-subpath-test-configmap-7lzc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7lzc
Sep  5 20:45:16.854: INFO: Deleting pod "pod-subpath-test-configmap-7lzc" in namespace "subpath-445"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:45:16.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-445" for this suite.
Sep  5 20:45:22.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:45:22.969: INFO: namespace subpath-445 deletion completed in 6.108407669s

• [SLOW TEST:28.344 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:45:22.969: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  5 20:45:25.641: INFO: Successfully updated pod "labelsupdate14fa6ffe-d01e-11e9-a0ad-8693e9898db7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:45:29.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1059" for this suite.
Sep  5 20:45:51.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:45:51.785: INFO: namespace downward-api-1059 deletion completed in 22.111195995s

• [SLOW TEST:28.816 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:45:51.785: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7719
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0905 20:46:21.957696      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 20:46:21.957: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:46:21.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7719" for this suite.
Sep  5 20:46:27.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:46:28.077: INFO: namespace gc-7719 deletion completed in 6.115335926s

• [SLOW TEST:36.291 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:46:28.077: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  5 20:46:34.255: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 20:46:34.259: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 20:46:36.259: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 20:46:36.263: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 20:46:38.259: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 20:46:38.263: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 20:46:40.259: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 20:46:40.263: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 20:46:42.259: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 20:46:42.263: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 20:46:44.259: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 20:46:44.263: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 20:46:46.259: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 20:46:46.263: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 20:46:48.259: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 20:46:48.263: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 20:46:50.259: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 20:46:50.263: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:46:50.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1767" for this suite.
Sep  5 20:47:12.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:47:12.387: INFO: namespace container-lifecycle-hook-1767 deletion completed in 22.110433088s

• [SLOW TEST:44.311 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:47:12.388: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7993
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 20:47:12.535: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5632c6a0-d01e-11e9-a0ad-8693e9898db7" in namespace "downward-api-7993" to be "success or failure"
Sep  5 20:47:12.538: INFO: Pod "downwardapi-volume-5632c6a0-d01e-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.053019ms
Sep  5 20:47:14.542: INFO: Pod "downwardapi-volume-5632c6a0-d01e-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007156928s
STEP: Saw pod success
Sep  5 20:47:14.542: INFO: Pod "downwardapi-volume-5632c6a0-d01e-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:47:14.545: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-5632c6a0-d01e-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 20:47:14.567: INFO: Waiting for pod downwardapi-volume-5632c6a0-d01e-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:47:14.569: INFO: Pod downwardapi-volume-5632c6a0-d01e-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:47:14.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7993" for this suite.
Sep  5 20:47:20.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:47:20.683: INFO: namespace downward-api-7993 deletion completed in 6.109168745s

• [SLOW TEST:8.295 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:47:20.683: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  5 20:47:20.822: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:47:24.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-330" for this suite.
Sep  5 20:47:30.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:47:30.389: INFO: namespace init-container-330 deletion completed in 6.114987608s

• [SLOW TEST:9.706 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:47:30.389: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  5 20:47:30.538: INFO: Waiting up to 5m0s for pod "pod-60edb0d3-d01e-11e9-a0ad-8693e9898db7" in namespace "emptydir-7863" to be "success or failure"
Sep  5 20:47:30.541: INFO: Pod "pod-60edb0d3-d01e-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12036ms
Sep  5 20:47:32.545: INFO: Pod "pod-60edb0d3-d01e-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007341773s
STEP: Saw pod success
Sep  5 20:47:32.545: INFO: Pod "pod-60edb0d3-d01e-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:47:32.548: INFO: Trying to get logs from node appserv9 pod pod-60edb0d3-d01e-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 20:47:32.575: INFO: Waiting for pod pod-60edb0d3-d01e-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:47:32.578: INFO: Pod pod-60edb0d3-d01e-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:47:32.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7863" for this suite.
Sep  5 20:47:38.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:47:38.691: INFO: namespace emptydir-7863 deletion completed in 6.107793426s

• [SLOW TEST:8.302 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:47:38.691: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-wjxq
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 20:47:38.846: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wjxq" in namespace "subpath-4878" to be "success or failure"
Sep  5 20:47:38.849: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.913861ms
Sep  5 20:47:40.853: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 2.007193461s
Sep  5 20:47:42.858: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 4.011452774s
Sep  5 20:47:44.862: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 6.015407248s
Sep  5 20:47:46.865: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 8.018811325s
Sep  5 20:47:48.869: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 10.023169836s
Sep  5 20:47:50.874: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 12.027651916s
Sep  5 20:47:52.878: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 14.031910915s
Sep  5 20:47:54.882: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 16.036166095s
Sep  5 20:47:56.887: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 18.040667769s
Sep  5 20:47:58.891: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Running", Reason="", readiness=true. Elapsed: 20.045157249s
Sep  5 20:48:00.896: INFO: Pod "pod-subpath-test-projected-wjxq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.049450421s
STEP: Saw pod success
Sep  5 20:48:00.896: INFO: Pod "pod-subpath-test-projected-wjxq" satisfied condition "success or failure"
Sep  5 20:48:00.899: INFO: Trying to get logs from node appserv9 pod pod-subpath-test-projected-wjxq container test-container-subpath-projected-wjxq: <nil>
STEP: delete the pod
Sep  5 20:48:00.916: INFO: Waiting for pod pod-subpath-test-projected-wjxq to disappear
Sep  5 20:48:00.918: INFO: Pod pod-subpath-test-projected-wjxq no longer exists
STEP: Deleting pod pod-subpath-test-projected-wjxq
Sep  5 20:48:00.919: INFO: Deleting pod "pod-subpath-test-projected-wjxq" in namespace "subpath-4878"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:48:00.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4878" for this suite.
Sep  5 20:48:06.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:48:07.032: INFO: namespace subpath-4878 deletion completed in 6.107818158s

• [SLOW TEST:28.341 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:48:07.033: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8383
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 20:48:07.196: INFO: (0) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.562195ms)
Sep  5 20:48:07.200: INFO: (1) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.626869ms)
Sep  5 20:48:07.203: INFO: (2) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.400885ms)
Sep  5 20:48:07.207: INFO: (3) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.60291ms)
Sep  5 20:48:07.211: INFO: (4) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.568131ms)
Sep  5 20:48:07.214: INFO: (5) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.686057ms)
Sep  5 20:48:07.218: INFO: (6) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.553983ms)
Sep  5 20:48:07.222: INFO: (7) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.683623ms)
Sep  5 20:48:07.225: INFO: (8) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.660899ms)
Sep  5 20:48:07.229: INFO: (9) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.842038ms)
Sep  5 20:48:07.233: INFO: (10) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.700993ms)
Sep  5 20:48:07.237: INFO: (11) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.592143ms)
Sep  5 20:48:07.240: INFO: (12) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.526531ms)
Sep  5 20:48:07.244: INFO: (13) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.597795ms)
Sep  5 20:48:07.247: INFO: (14) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.590991ms)
Sep  5 20:48:07.251: INFO: (15) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.695831ms)
Sep  5 20:48:07.255: INFO: (16) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.647196ms)
Sep  5 20:48:07.258: INFO: (17) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.593974ms)
Sep  5 20:48:07.262: INFO: (18) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.785909ms)
Sep  5 20:48:07.266: INFO: (19) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.608594ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:48:07.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8383" for this suite.
Sep  5 20:48:13.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:48:13.383: INFO: namespace proxy-8383 deletion completed in 6.112985054s

• [SLOW TEST:6.350 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:48:13.383: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 20:48:13.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a8d9ddb-d01e-11e9-a0ad-8693e9898db7" in namespace "downward-api-8105" to be "success or failure"
Sep  5 20:48:13.532: INFO: Pod "downwardapi-volume-7a8d9ddb-d01e-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076637ms
Sep  5 20:48:15.536: INFO: Pod "downwardapi-volume-7a8d9ddb-d01e-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006748671s
STEP: Saw pod success
Sep  5 20:48:15.536: INFO: Pod "downwardapi-volume-7a8d9ddb-d01e-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:48:15.539: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-7a8d9ddb-d01e-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 20:48:15.555: INFO: Waiting for pod downwardapi-volume-7a8d9ddb-d01e-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:48:15.558: INFO: Pod downwardapi-volume-7a8d9ddb-d01e-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:48:15.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8105" for this suite.
Sep  5 20:48:21.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:48:21.668: INFO: namespace downward-api-8105 deletion completed in 6.107447924s

• [SLOW TEST:8.286 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:48:21.668: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  5 20:48:21.819: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  5 20:48:26.823: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:48:27.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7954" for this suite.
Sep  5 20:48:33.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:48:33.950: INFO: namespace replication-controller-7954 deletion completed in 6.109404803s

• [SLOW TEST:12.282 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:48:33.951: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6570
I0905 20:48:34.093578      18 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6570, replica count: 1
I0905 20:48:35.144081      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0905 20:48:36.144318      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0905 20:48:37.144571      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  5 20:48:37.252: INFO: Created: latency-svc-wtfzx
Sep  5 20:48:37.256: INFO: Got endpoints: latency-svc-wtfzx [11.584404ms]
Sep  5 20:48:37.263: INFO: Created: latency-svc-4c4sk
Sep  5 20:48:37.266: INFO: Got endpoints: latency-svc-4c4sk [10.461548ms]
Sep  5 20:48:37.267: INFO: Created: latency-svc-kncsh
Sep  5 20:48:37.270: INFO: Got endpoints: latency-svc-kncsh [13.798121ms]
Sep  5 20:48:37.270: INFO: Created: latency-svc-4xbl9
Sep  5 20:48:37.272: INFO: Got endpoints: latency-svc-4xbl9 [16.298226ms]
Sep  5 20:48:37.274: INFO: Created: latency-svc-m44ml
Sep  5 20:48:37.277: INFO: Got endpoints: latency-svc-m44ml [20.694078ms]
Sep  5 20:48:37.277: INFO: Created: latency-svc-fjpwx
Sep  5 20:48:37.279: INFO: Got endpoints: latency-svc-fjpwx [23.249991ms]
Sep  5 20:48:37.280: INFO: Created: latency-svc-cf4qv
Sep  5 20:48:37.283: INFO: Got endpoints: latency-svc-cf4qv [26.410171ms]
Sep  5 20:48:37.283: INFO: Created: latency-svc-wmmg8
Sep  5 20:48:37.286: INFO: Got endpoints: latency-svc-wmmg8 [29.460409ms]
Sep  5 20:48:37.286: INFO: Created: latency-svc-pkkp6
Sep  5 20:48:37.288: INFO: Got endpoints: latency-svc-pkkp6 [32.166944ms]
Sep  5 20:48:37.289: INFO: Created: latency-svc-8kbgr
Sep  5 20:48:37.292: INFO: Got endpoints: latency-svc-8kbgr [35.471627ms]
Sep  5 20:48:37.292: INFO: Created: latency-svc-r9ksm
Sep  5 20:48:37.295: INFO: Got endpoints: latency-svc-r9ksm [38.658415ms]
Sep  5 20:48:37.295: INFO: Created: latency-svc-gff8v
Sep  5 20:48:37.298: INFO: Got endpoints: latency-svc-gff8v [41.443269ms]
Sep  5 20:48:37.298: INFO: Created: latency-svc-rltf2
Sep  5 20:48:37.301: INFO: Got endpoints: latency-svc-rltf2 [44.846753ms]
Sep  5 20:48:37.302: INFO: Created: latency-svc-4w9ws
Sep  5 20:48:37.304: INFO: Got endpoints: latency-svc-4w9ws [47.514721ms]
Sep  5 20:48:37.304: INFO: Created: latency-svc-qx67z
Sep  5 20:48:37.307: INFO: Got endpoints: latency-svc-qx67z [50.595432ms]
Sep  5 20:48:37.307: INFO: Created: latency-svc-q5lx7
Sep  5 20:48:37.310: INFO: Got endpoints: latency-svc-q5lx7 [53.505293ms]
Sep  5 20:48:37.310: INFO: Created: latency-svc-pwvbk
Sep  5 20:48:37.313: INFO: Got endpoints: latency-svc-pwvbk [46.186997ms]
Sep  5 20:48:37.313: INFO: Created: latency-svc-9pprf
Sep  5 20:48:37.315: INFO: Got endpoints: latency-svc-9pprf [45.291124ms]
Sep  5 20:48:37.316: INFO: Created: latency-svc-24tsl
Sep  5 20:48:37.318: INFO: Got endpoints: latency-svc-24tsl [45.989096ms]
Sep  5 20:48:37.319: INFO: Created: latency-svc-cqrbg
Sep  5 20:48:37.321: INFO: Got endpoints: latency-svc-cqrbg [44.160254ms]
Sep  5 20:48:37.322: INFO: Created: latency-svc-vb77n
Sep  5 20:48:37.324: INFO: Got endpoints: latency-svc-vb77n [44.221485ms]
Sep  5 20:48:37.325: INFO: Created: latency-svc-mbdv2
Sep  5 20:48:37.327: INFO: Got endpoints: latency-svc-mbdv2 [44.047299ms]
Sep  5 20:48:37.327: INFO: Created: latency-svc-j996b
Sep  5 20:48:37.329: INFO: Got endpoints: latency-svc-j996b [43.563293ms]
Sep  5 20:48:37.330: INFO: Created: latency-svc-htcb9
Sep  5 20:48:37.332: INFO: Got endpoints: latency-svc-htcb9 [43.960056ms]
Sep  5 20:48:37.333: INFO: Created: latency-svc-qgzbb
Sep  5 20:48:37.335: INFO: Got endpoints: latency-svc-qgzbb [42.998823ms]
Sep  5 20:48:37.335: INFO: Created: latency-svc-hwbgd
Sep  5 20:48:37.337: INFO: Got endpoints: latency-svc-hwbgd [42.407532ms]
Sep  5 20:48:37.338: INFO: Created: latency-svc-2twp4
Sep  5 20:48:37.340: INFO: Got endpoints: latency-svc-2twp4 [42.644395ms]
Sep  5 20:48:37.341: INFO: Created: latency-svc-gzh4l
Sep  5 20:48:37.343: INFO: Got endpoints: latency-svc-gzh4l [41.81996ms]
Sep  5 20:48:37.344: INFO: Created: latency-svc-llwb7
Sep  5 20:48:37.346: INFO: Got endpoints: latency-svc-llwb7 [42.178359ms]
Sep  5 20:48:37.346: INFO: Created: latency-svc-s2v85
Sep  5 20:48:37.349: INFO: Got endpoints: latency-svc-s2v85 [41.745637ms]
Sep  5 20:48:37.349: INFO: Created: latency-svc-fnsjt
Sep  5 20:48:37.351: INFO: Got endpoints: latency-svc-fnsjt [41.19557ms]
Sep  5 20:48:37.351: INFO: Created: latency-svc-mszjb
Sep  5 20:48:37.354: INFO: Created: latency-svc-jj5zm
Sep  5 20:48:37.355: INFO: Got endpoints: latency-svc-mszjb [42.070561ms]
Sep  5 20:48:37.356: INFO: Created: latency-svc-hfq5f
Sep  5 20:48:37.359: INFO: Created: latency-svc-r7jps
Sep  5 20:48:37.361: INFO: Created: latency-svc-zl62g
Sep  5 20:48:37.363: INFO: Created: latency-svc-qdwsr
Sep  5 20:48:37.366: INFO: Created: latency-svc-6x4zf
Sep  5 20:48:37.368: INFO: Created: latency-svc-7g7z4
Sep  5 20:48:37.371: INFO: Created: latency-svc-nwrhw
Sep  5 20:48:37.374: INFO: Created: latency-svc-q4bst
Sep  5 20:48:37.376: INFO: Created: latency-svc-tjz82
Sep  5 20:48:37.378: INFO: Created: latency-svc-2jpfh
Sep  5 20:48:37.380: INFO: Created: latency-svc-j8vzx
Sep  5 20:48:37.383: INFO: Created: latency-svc-xv8m2
Sep  5 20:48:37.385: INFO: Created: latency-svc-pbzzl
Sep  5 20:48:37.388: INFO: Created: latency-svc-4xg5b
Sep  5 20:48:37.405: INFO: Got endpoints: latency-svc-jj5zm [89.802481ms]
Sep  5 20:48:37.410: INFO: Created: latency-svc-7jgv6
Sep  5 20:48:37.456: INFO: Got endpoints: latency-svc-hfq5f [137.385083ms]
Sep  5 20:48:37.461: INFO: Created: latency-svc-h2bcp
Sep  5 20:48:37.506: INFO: Got endpoints: latency-svc-r7jps [184.707325ms]
Sep  5 20:48:37.511: INFO: Created: latency-svc-bcg7n
Sep  5 20:48:37.555: INFO: Got endpoints: latency-svc-zl62g [231.416552ms]
Sep  5 20:48:37.561: INFO: Created: latency-svc-nw2bd
Sep  5 20:48:37.606: INFO: Got endpoints: latency-svc-qdwsr [278.878077ms]
Sep  5 20:48:37.611: INFO: Created: latency-svc-qm6dz
Sep  5 20:48:37.656: INFO: Got endpoints: latency-svc-6x4zf [326.429275ms]
Sep  5 20:48:37.662: INFO: Created: latency-svc-qz9n5
Sep  5 20:48:37.706: INFO: Got endpoints: latency-svc-7g7z4 [373.461175ms]
Sep  5 20:48:37.711: INFO: Created: latency-svc-pnshl
Sep  5 20:48:37.756: INFO: Got endpoints: latency-svc-nwrhw [421.166863ms]
Sep  5 20:48:37.762: INFO: Created: latency-svc-wjkz8
Sep  5 20:48:37.805: INFO: Got endpoints: latency-svc-q4bst [467.990677ms]
Sep  5 20:48:37.811: INFO: Created: latency-svc-4znsl
Sep  5 20:48:37.856: INFO: Got endpoints: latency-svc-tjz82 [515.824037ms]
Sep  5 20:48:37.862: INFO: Created: latency-svc-gbhd2
Sep  5 20:48:37.905: INFO: Got endpoints: latency-svc-2jpfh [562.553975ms]
Sep  5 20:48:37.912: INFO: Created: latency-svc-g9t6r
Sep  5 20:48:37.955: INFO: Got endpoints: latency-svc-j8vzx [609.183095ms]
Sep  5 20:48:37.961: INFO: Created: latency-svc-2bcq8
Sep  5 20:48:38.006: INFO: Got endpoints: latency-svc-xv8m2 [656.967936ms]
Sep  5 20:48:38.011: INFO: Created: latency-svc-qdr8r
Sep  5 20:48:38.055: INFO: Got endpoints: latency-svc-pbzzl [704.463185ms]
Sep  5 20:48:38.061: INFO: Created: latency-svc-42zz8
Sep  5 20:48:38.106: INFO: Got endpoints: latency-svc-4xg5b [750.984961ms]
Sep  5 20:48:38.112: INFO: Created: latency-svc-vdct8
Sep  5 20:48:38.156: INFO: Got endpoints: latency-svc-7jgv6 [750.749799ms]
Sep  5 20:48:38.162: INFO: Created: latency-svc-sfzgq
Sep  5 20:48:38.206: INFO: Got endpoints: latency-svc-h2bcp [750.167985ms]
Sep  5 20:48:38.213: INFO: Created: latency-svc-nn4tr
Sep  5 20:48:38.256: INFO: Got endpoints: latency-svc-bcg7n [750.450123ms]
Sep  5 20:48:38.263: INFO: Created: latency-svc-znnfn
Sep  5 20:48:38.306: INFO: Got endpoints: latency-svc-nw2bd [750.913802ms]
Sep  5 20:48:38.313: INFO: Created: latency-svc-rtgwv
Sep  5 20:48:38.356: INFO: Got endpoints: latency-svc-qm6dz [750.26738ms]
Sep  5 20:48:38.363: INFO: Created: latency-svc-9n572
Sep  5 20:48:38.406: INFO: Got endpoints: latency-svc-qz9n5 [750.652097ms]
Sep  5 20:48:38.413: INFO: Created: latency-svc-m22p2
Sep  5 20:48:38.456: INFO: Got endpoints: latency-svc-pnshl [750.299591ms]
Sep  5 20:48:38.463: INFO: Created: latency-svc-59m4f
Sep  5 20:48:38.506: INFO: Got endpoints: latency-svc-wjkz8 [750.283062ms]
Sep  5 20:48:38.514: INFO: Created: latency-svc-nxhz8
Sep  5 20:48:38.556: INFO: Got endpoints: latency-svc-4znsl [750.494ms]
Sep  5 20:48:38.563: INFO: Created: latency-svc-zccqd
Sep  5 20:48:38.606: INFO: Got endpoints: latency-svc-gbhd2 [749.966545ms]
Sep  5 20:48:38.614: INFO: Created: latency-svc-c5vcg
Sep  5 20:48:38.656: INFO: Got endpoints: latency-svc-g9t6r [750.848073ms]
Sep  5 20:48:38.663: INFO: Created: latency-svc-798jr
Sep  5 20:48:38.706: INFO: Got endpoints: latency-svc-2bcq8 [750.792093ms]
Sep  5 20:48:38.713: INFO: Created: latency-svc-jc8ct
Sep  5 20:48:38.756: INFO: Got endpoints: latency-svc-qdr8r [750.56166ms]
Sep  5 20:48:38.763: INFO: Created: latency-svc-rp2zj
Sep  5 20:48:38.806: INFO: Got endpoints: latency-svc-42zz8 [750.31392ms]
Sep  5 20:48:38.813: INFO: Created: latency-svc-5xmrx
Sep  5 20:48:38.856: INFO: Got endpoints: latency-svc-vdct8 [750.194458ms]
Sep  5 20:48:38.863: INFO: Created: latency-svc-mwvq7
Sep  5 20:48:38.906: INFO: Got endpoints: latency-svc-sfzgq [749.962111ms]
Sep  5 20:48:38.912: INFO: Created: latency-svc-8nmt2
Sep  5 20:48:38.956: INFO: Got endpoints: latency-svc-nn4tr [749.966767ms]
Sep  5 20:48:38.964: INFO: Created: latency-svc-p2kgm
Sep  5 20:48:39.006: INFO: Got endpoints: latency-svc-znnfn [749.906799ms]
Sep  5 20:48:39.013: INFO: Created: latency-svc-7sqbd
Sep  5 20:48:39.067: INFO: Got endpoints: latency-svc-rtgwv [760.745678ms]
Sep  5 20:48:39.073: INFO: Created: latency-svc-lnh77
Sep  5 20:48:39.106: INFO: Got endpoints: latency-svc-9n572 [750.286205ms]
Sep  5 20:48:39.114: INFO: Created: latency-svc-m9b8t
Sep  5 20:48:39.156: INFO: Got endpoints: latency-svc-m22p2 [749.518441ms]
Sep  5 20:48:39.163: INFO: Created: latency-svc-cnpp7
Sep  5 20:48:39.206: INFO: Got endpoints: latency-svc-59m4f [749.957882ms]
Sep  5 20:48:39.213: INFO: Created: latency-svc-nxp5z
Sep  5 20:48:39.256: INFO: Got endpoints: latency-svc-nxhz8 [750.209175ms]
Sep  5 20:48:39.264: INFO: Created: latency-svc-xqc2s
Sep  5 20:48:39.306: INFO: Got endpoints: latency-svc-zccqd [749.952838ms]
Sep  5 20:48:39.313: INFO: Created: latency-svc-hdkd4
Sep  5 20:48:39.356: INFO: Got endpoints: latency-svc-c5vcg [749.917439ms]
Sep  5 20:48:39.363: INFO: Created: latency-svc-zwg2k
Sep  5 20:48:39.406: INFO: Got endpoints: latency-svc-798jr [749.706885ms]
Sep  5 20:48:39.414: INFO: Created: latency-svc-qfmgh
Sep  5 20:48:39.456: INFO: Got endpoints: latency-svc-jc8ct [750.353704ms]
Sep  5 20:48:39.463: INFO: Created: latency-svc-5d5cx
Sep  5 20:48:39.506: INFO: Got endpoints: latency-svc-rp2zj [749.997867ms]
Sep  5 20:48:39.514: INFO: Created: latency-svc-n4fbw
Sep  5 20:48:39.556: INFO: Got endpoints: latency-svc-5xmrx [750.11136ms]
Sep  5 20:48:39.563: INFO: Created: latency-svc-hxw9h
Sep  5 20:48:39.606: INFO: Got endpoints: latency-svc-mwvq7 [750.154505ms]
Sep  5 20:48:39.613: INFO: Created: latency-svc-gsgw2
Sep  5 20:48:39.656: INFO: Got endpoints: latency-svc-8nmt2 [750.369699ms]
Sep  5 20:48:39.664: INFO: Created: latency-svc-6bnfp
Sep  5 20:48:39.706: INFO: Got endpoints: latency-svc-p2kgm [749.978851ms]
Sep  5 20:48:39.712: INFO: Created: latency-svc-rgc2w
Sep  5 20:48:39.756: INFO: Got endpoints: latency-svc-7sqbd [749.813604ms]
Sep  5 20:48:39.763: INFO: Created: latency-svc-jqlzl
Sep  5 20:48:39.806: INFO: Got endpoints: latency-svc-lnh77 [739.476082ms]
Sep  5 20:48:39.813: INFO: Created: latency-svc-2rwk5
Sep  5 20:48:39.856: INFO: Got endpoints: latency-svc-m9b8t [750.107684ms]
Sep  5 20:48:39.863: INFO: Created: latency-svc-w9m9d
Sep  5 20:48:39.906: INFO: Got endpoints: latency-svc-cnpp7 [750.407099ms]
Sep  5 20:48:39.913: INFO: Created: latency-svc-8x4gv
Sep  5 20:48:39.956: INFO: Got endpoints: latency-svc-nxp5z [749.813921ms]
Sep  5 20:48:39.963: INFO: Created: latency-svc-5hwk5
Sep  5 20:48:40.006: INFO: Got endpoints: latency-svc-xqc2s [749.837268ms]
Sep  5 20:48:40.013: INFO: Created: latency-svc-ghfk8
Sep  5 20:48:40.056: INFO: Got endpoints: latency-svc-hdkd4 [749.979914ms]
Sep  5 20:48:40.063: INFO: Created: latency-svc-bvdqk
Sep  5 20:48:40.106: INFO: Got endpoints: latency-svc-zwg2k [750.217817ms]
Sep  5 20:48:40.114: INFO: Created: latency-svc-j4mqd
Sep  5 20:48:40.156: INFO: Got endpoints: latency-svc-qfmgh [750.300338ms]
Sep  5 20:48:40.164: INFO: Created: latency-svc-sbnjj
Sep  5 20:48:40.206: INFO: Got endpoints: latency-svc-5d5cx [749.549149ms]
Sep  5 20:48:40.213: INFO: Created: latency-svc-jpnjj
Sep  5 20:48:40.256: INFO: Got endpoints: latency-svc-n4fbw [749.84004ms]
Sep  5 20:48:40.263: INFO: Created: latency-svc-rqlq5
Sep  5 20:48:40.306: INFO: Got endpoints: latency-svc-hxw9h [750.198158ms]
Sep  5 20:48:40.314: INFO: Created: latency-svc-x5dwn
Sep  5 20:48:40.356: INFO: Got endpoints: latency-svc-gsgw2 [749.827108ms]
Sep  5 20:48:40.363: INFO: Created: latency-svc-mzbbz
Sep  5 20:48:40.406: INFO: Got endpoints: latency-svc-6bnfp [749.827894ms]
Sep  5 20:48:40.413: INFO: Created: latency-svc-q9pwq
Sep  5 20:48:40.458: INFO: Got endpoints: latency-svc-rgc2w [752.079095ms]
Sep  5 20:48:40.466: INFO: Created: latency-svc-wbhfv
Sep  5 20:48:40.506: INFO: Got endpoints: latency-svc-jqlzl [750.21896ms]
Sep  5 20:48:40.514: INFO: Created: latency-svc-p26h4
Sep  5 20:48:40.556: INFO: Got endpoints: latency-svc-2rwk5 [749.763085ms]
Sep  5 20:48:40.564: INFO: Created: latency-svc-2sl55
Sep  5 20:48:40.606: INFO: Got endpoints: latency-svc-w9m9d [750.04972ms]
Sep  5 20:48:40.614: INFO: Created: latency-svc-rn6fk
Sep  5 20:48:40.656: INFO: Got endpoints: latency-svc-8x4gv [749.639979ms]
Sep  5 20:48:40.663: INFO: Created: latency-svc-sf79d
Sep  5 20:48:40.706: INFO: Got endpoints: latency-svc-5hwk5 [750.204494ms]
Sep  5 20:48:40.714: INFO: Created: latency-svc-n5k79
Sep  5 20:48:40.757: INFO: Got endpoints: latency-svc-ghfk8 [750.308399ms]
Sep  5 20:48:40.764: INFO: Created: latency-svc-hhsm7
Sep  5 20:48:40.806: INFO: Got endpoints: latency-svc-bvdqk [750.187273ms]
Sep  5 20:48:40.813: INFO: Created: latency-svc-kbwtj
Sep  5 20:48:40.856: INFO: Got endpoints: latency-svc-j4mqd [749.753401ms]
Sep  5 20:48:40.862: INFO: Created: latency-svc-qrrqj
Sep  5 20:48:40.906: INFO: Got endpoints: latency-svc-sbnjj [749.867767ms]
Sep  5 20:48:40.913: INFO: Created: latency-svc-kld6k
Sep  5 20:48:40.956: INFO: Got endpoints: latency-svc-jpnjj [749.892801ms]
Sep  5 20:48:40.962: INFO: Created: latency-svc-x4vq7
Sep  5 20:48:41.006: INFO: Got endpoints: latency-svc-rqlq5 [749.707755ms]
Sep  5 20:48:41.012: INFO: Created: latency-svc-882bv
Sep  5 20:48:41.056: INFO: Got endpoints: latency-svc-x5dwn [749.853789ms]
Sep  5 20:48:41.062: INFO: Created: latency-svc-7m6f8
Sep  5 20:48:41.106: INFO: Got endpoints: latency-svc-mzbbz [749.544729ms]
Sep  5 20:48:41.112: INFO: Created: latency-svc-mjvqg
Sep  5 20:48:41.156: INFO: Got endpoints: latency-svc-q9pwq [749.584577ms]
Sep  5 20:48:41.162: INFO: Created: latency-svc-zm9qk
Sep  5 20:48:41.206: INFO: Got endpoints: latency-svc-wbhfv [748.023458ms]
Sep  5 20:48:41.213: INFO: Created: latency-svc-7955h
Sep  5 20:48:41.256: INFO: Got endpoints: latency-svc-p26h4 [749.906397ms]
Sep  5 20:48:41.263: INFO: Created: latency-svc-nvstq
Sep  5 20:48:41.306: INFO: Got endpoints: latency-svc-2sl55 [749.9235ms]
Sep  5 20:48:41.313: INFO: Created: latency-svc-hgzc8
Sep  5 20:48:41.356: INFO: Got endpoints: latency-svc-rn6fk [750.016469ms]
Sep  5 20:48:41.363: INFO: Created: latency-svc-788t9
Sep  5 20:48:41.406: INFO: Got endpoints: latency-svc-sf79d [749.948137ms]
Sep  5 20:48:41.413: INFO: Created: latency-svc-f4xv2
Sep  5 20:48:41.456: INFO: Got endpoints: latency-svc-n5k79 [749.791927ms]
Sep  5 20:48:41.463: INFO: Created: latency-svc-g8l8r
Sep  5 20:48:41.506: INFO: Got endpoints: latency-svc-hhsm7 [749.666279ms]
Sep  5 20:48:41.514: INFO: Created: latency-svc-gk5kz
Sep  5 20:48:41.556: INFO: Got endpoints: latency-svc-kbwtj [749.98003ms]
Sep  5 20:48:41.563: INFO: Created: latency-svc-5mtpv
Sep  5 20:48:41.606: INFO: Got endpoints: latency-svc-qrrqj [749.784705ms]
Sep  5 20:48:41.613: INFO: Created: latency-svc-khgdp
Sep  5 20:48:41.656: INFO: Got endpoints: latency-svc-kld6k [750.126853ms]
Sep  5 20:48:41.664: INFO: Created: latency-svc-v9t98
Sep  5 20:48:41.707: INFO: Got endpoints: latency-svc-x4vq7 [750.807348ms]
Sep  5 20:48:41.714: INFO: Created: latency-svc-566s6
Sep  5 20:48:41.756: INFO: Got endpoints: latency-svc-882bv [749.952548ms]
Sep  5 20:48:41.763: INFO: Created: latency-svc-8jc7l
Sep  5 20:48:41.806: INFO: Got endpoints: latency-svc-7m6f8 [750.255544ms]
Sep  5 20:48:41.813: INFO: Created: latency-svc-cwvgx
Sep  5 20:48:41.857: INFO: Got endpoints: latency-svc-mjvqg [750.948314ms]
Sep  5 20:48:41.864: INFO: Created: latency-svc-c4xqf
Sep  5 20:48:41.906: INFO: Got endpoints: latency-svc-zm9qk [750.289894ms]
Sep  5 20:48:41.912: INFO: Created: latency-svc-ngh5z
Sep  5 20:48:41.956: INFO: Got endpoints: latency-svc-7955h [750.000742ms]
Sep  5 20:48:41.964: INFO: Created: latency-svc-qgm5c
Sep  5 20:48:42.006: INFO: Got endpoints: latency-svc-nvstq [749.858989ms]
Sep  5 20:48:42.014: INFO: Created: latency-svc-5r4xr
Sep  5 20:48:42.056: INFO: Got endpoints: latency-svc-hgzc8 [749.929236ms]
Sep  5 20:48:42.062: INFO: Created: latency-svc-lxx8j
Sep  5 20:48:42.106: INFO: Got endpoints: latency-svc-788t9 [749.534093ms]
Sep  5 20:48:42.112: INFO: Created: latency-svc-gb4nh
Sep  5 20:48:42.156: INFO: Got endpoints: latency-svc-f4xv2 [749.699011ms]
Sep  5 20:48:42.162: INFO: Created: latency-svc-btvv5
Sep  5 20:48:42.206: INFO: Got endpoints: latency-svc-g8l8r [749.883164ms]
Sep  5 20:48:42.213: INFO: Created: latency-svc-wn742
Sep  5 20:48:42.257: INFO: Got endpoints: latency-svc-gk5kz [750.20925ms]
Sep  5 20:48:42.264: INFO: Created: latency-svc-nhrlz
Sep  5 20:48:42.307: INFO: Got endpoints: latency-svc-5mtpv [750.352193ms]
Sep  5 20:48:42.314: INFO: Created: latency-svc-8tdft
Sep  5 20:48:42.356: INFO: Got endpoints: latency-svc-khgdp [750.301959ms]
Sep  5 20:48:42.363: INFO: Created: latency-svc-5pjfd
Sep  5 20:48:42.406: INFO: Got endpoints: latency-svc-v9t98 [749.54971ms]
Sep  5 20:48:42.414: INFO: Created: latency-svc-zctg8
Sep  5 20:48:42.456: INFO: Got endpoints: latency-svc-566s6 [749.551647ms]
Sep  5 20:48:42.464: INFO: Created: latency-svc-vchj9
Sep  5 20:48:42.506: INFO: Got endpoints: latency-svc-8jc7l [750.14912ms]
Sep  5 20:48:42.513: INFO: Created: latency-svc-bvjmh
Sep  5 20:48:42.556: INFO: Got endpoints: latency-svc-cwvgx [749.852918ms]
Sep  5 20:48:42.563: INFO: Created: latency-svc-kj2n2
Sep  5 20:48:42.606: INFO: Got endpoints: latency-svc-c4xqf [749.307988ms]
Sep  5 20:48:42.620: INFO: Created: latency-svc-pt6c6
Sep  5 20:48:42.657: INFO: Got endpoints: latency-svc-ngh5z [750.602322ms]
Sep  5 20:48:42.664: INFO: Created: latency-svc-nf8pd
Sep  5 20:48:42.706: INFO: Got endpoints: latency-svc-qgm5c [750.201541ms]
Sep  5 20:48:42.714: INFO: Created: latency-svc-txsrf
Sep  5 20:48:42.756: INFO: Got endpoints: latency-svc-5r4xr [750.253571ms]
Sep  5 20:48:42.764: INFO: Created: latency-svc-qklqd
Sep  5 20:48:42.806: INFO: Got endpoints: latency-svc-lxx8j [749.968949ms]
Sep  5 20:48:42.814: INFO: Created: latency-svc-mdjjd
Sep  5 20:48:42.856: INFO: Got endpoints: latency-svc-gb4nh [750.36831ms]
Sep  5 20:48:42.864: INFO: Created: latency-svc-lrwdz
Sep  5 20:48:42.906: INFO: Got endpoints: latency-svc-btvv5 [750.423933ms]
Sep  5 20:48:42.914: INFO: Created: latency-svc-dthqm
Sep  5 20:48:42.956: INFO: Got endpoints: latency-svc-wn742 [750.271822ms]
Sep  5 20:48:42.964: INFO: Created: latency-svc-z45lg
Sep  5 20:48:43.006: INFO: Got endpoints: latency-svc-nhrlz [749.549094ms]
Sep  5 20:48:43.014: INFO: Created: latency-svc-kcwrh
Sep  5 20:48:43.065: INFO: Got endpoints: latency-svc-8tdft [758.836949ms]
Sep  5 20:48:43.073: INFO: Created: latency-svc-p6llq
Sep  5 20:48:43.106: INFO: Got endpoints: latency-svc-5pjfd [749.848114ms]
Sep  5 20:48:43.114: INFO: Created: latency-svc-hgg9m
Sep  5 20:48:43.156: INFO: Got endpoints: latency-svc-zctg8 [750.351122ms]
Sep  5 20:48:43.164: INFO: Created: latency-svc-kb57j
Sep  5 20:48:43.206: INFO: Got endpoints: latency-svc-vchj9 [750.04926ms]
Sep  5 20:48:43.214: INFO: Created: latency-svc-6559m
Sep  5 20:48:43.256: INFO: Got endpoints: latency-svc-bvjmh [750.093109ms]
Sep  5 20:48:43.263: INFO: Created: latency-svc-s94sc
Sep  5 20:48:43.306: INFO: Got endpoints: latency-svc-kj2n2 [749.800917ms]
Sep  5 20:48:43.313: INFO: Created: latency-svc-c2p7x
Sep  5 20:48:43.356: INFO: Got endpoints: latency-svc-pt6c6 [750.438319ms]
Sep  5 20:48:43.364: INFO: Created: latency-svc-wh8nq
Sep  5 20:48:43.406: INFO: Got endpoints: latency-svc-nf8pd [749.542984ms]
Sep  5 20:48:43.413: INFO: Created: latency-svc-4hg6r
Sep  5 20:48:43.456: INFO: Got endpoints: latency-svc-txsrf [750.029176ms]
Sep  5 20:48:43.464: INFO: Created: latency-svc-bwt9z
Sep  5 20:48:43.506: INFO: Got endpoints: latency-svc-qklqd [749.943553ms]
Sep  5 20:48:43.514: INFO: Created: latency-svc-2cjmb
Sep  5 20:48:43.556: INFO: Got endpoints: latency-svc-mdjjd [750.302774ms]
Sep  5 20:48:43.564: INFO: Created: latency-svc-46m8r
Sep  5 20:48:43.606: INFO: Got endpoints: latency-svc-lrwdz [749.764821ms]
Sep  5 20:48:43.614: INFO: Created: latency-svc-6t4nn
Sep  5 20:48:43.656: INFO: Got endpoints: latency-svc-dthqm [750.028852ms]
Sep  5 20:48:43.663: INFO: Created: latency-svc-psfxk
Sep  5 20:48:43.706: INFO: Got endpoints: latency-svc-z45lg [750.040365ms]
Sep  5 20:48:43.713: INFO: Created: latency-svc-gt2vf
Sep  5 20:48:43.756: INFO: Got endpoints: latency-svc-kcwrh [749.679741ms]
Sep  5 20:48:43.763: INFO: Created: latency-svc-stcj7
Sep  5 20:48:43.806: INFO: Got endpoints: latency-svc-p6llq [740.670215ms]
Sep  5 20:48:43.813: INFO: Created: latency-svc-kltqp
Sep  5 20:48:43.856: INFO: Got endpoints: latency-svc-hgg9m [750.155409ms]
Sep  5 20:48:43.864: INFO: Created: latency-svc-lhv29
Sep  5 20:48:43.906: INFO: Got endpoints: latency-svc-kb57j [749.776086ms]
Sep  5 20:48:43.913: INFO: Created: latency-svc-f8t4k
Sep  5 20:48:43.956: INFO: Got endpoints: latency-svc-6559m [749.663395ms]
Sep  5 20:48:43.963: INFO: Created: latency-svc-84z5l
Sep  5 20:48:44.006: INFO: Got endpoints: latency-svc-s94sc [749.802892ms]
Sep  5 20:48:44.013: INFO: Created: latency-svc-6z5ps
Sep  5 20:48:44.056: INFO: Got endpoints: latency-svc-c2p7x [749.944883ms]
Sep  5 20:48:44.063: INFO: Created: latency-svc-wfzdj
Sep  5 20:48:44.106: INFO: Got endpoints: latency-svc-wh8nq [749.743049ms]
Sep  5 20:48:44.113: INFO: Created: latency-svc-q6lnr
Sep  5 20:48:44.156: INFO: Got endpoints: latency-svc-4hg6r [749.765671ms]
Sep  5 20:48:44.163: INFO: Created: latency-svc-72qrv
Sep  5 20:48:44.206: INFO: Got endpoints: latency-svc-bwt9z [749.635681ms]
Sep  5 20:48:44.214: INFO: Created: latency-svc-74xqd
Sep  5 20:48:44.256: INFO: Got endpoints: latency-svc-2cjmb [750.018566ms]
Sep  5 20:48:44.263: INFO: Created: latency-svc-vnftk
Sep  5 20:48:44.306: INFO: Got endpoints: latency-svc-46m8r [749.689127ms]
Sep  5 20:48:44.313: INFO: Created: latency-svc-tgwz2
Sep  5 20:48:44.356: INFO: Got endpoints: latency-svc-6t4nn [749.773736ms]
Sep  5 20:48:44.363: INFO: Created: latency-svc-sc4rp
Sep  5 20:48:44.406: INFO: Got endpoints: latency-svc-psfxk [749.600905ms]
Sep  5 20:48:44.416: INFO: Created: latency-svc-88qcw
Sep  5 20:48:44.456: INFO: Got endpoints: latency-svc-gt2vf [749.647491ms]
Sep  5 20:48:44.462: INFO: Created: latency-svc-zxr62
Sep  5 20:48:44.506: INFO: Got endpoints: latency-svc-stcj7 [749.871541ms]
Sep  5 20:48:44.512: INFO: Created: latency-svc-5q8v5
Sep  5 20:48:44.556: INFO: Got endpoints: latency-svc-kltqp [749.599257ms]
Sep  5 20:48:44.562: INFO: Created: latency-svc-scqc6
Sep  5 20:48:44.606: INFO: Got endpoints: latency-svc-lhv29 [749.569241ms]
Sep  5 20:48:44.614: INFO: Created: latency-svc-9x9q5
Sep  5 20:48:44.656: INFO: Got endpoints: latency-svc-f8t4k [749.970401ms]
Sep  5 20:48:44.663: INFO: Created: latency-svc-tdldh
Sep  5 20:48:44.707: INFO: Got endpoints: latency-svc-84z5l [750.66665ms]
Sep  5 20:48:44.714: INFO: Created: latency-svc-6g4qw
Sep  5 20:48:44.756: INFO: Got endpoints: latency-svc-6z5ps [750.189423ms]
Sep  5 20:48:44.763: INFO: Created: latency-svc-67pxk
Sep  5 20:48:44.806: INFO: Got endpoints: latency-svc-wfzdj [749.900812ms]
Sep  5 20:48:44.813: INFO: Created: latency-svc-2g6qp
Sep  5 20:48:44.856: INFO: Got endpoints: latency-svc-q6lnr [749.630757ms]
Sep  5 20:48:44.862: INFO: Created: latency-svc-jxr8p
Sep  5 20:48:44.906: INFO: Got endpoints: latency-svc-72qrv [749.802001ms]
Sep  5 20:48:44.912: INFO: Created: latency-svc-rz92t
Sep  5 20:48:44.956: INFO: Got endpoints: latency-svc-74xqd [749.83656ms]
Sep  5 20:48:44.963: INFO: Created: latency-svc-wfgvg
Sep  5 20:48:45.006: INFO: Got endpoints: latency-svc-vnftk [749.403207ms]
Sep  5 20:48:45.012: INFO: Created: latency-svc-xctqk
Sep  5 20:48:45.056: INFO: Got endpoints: latency-svc-tgwz2 [749.607621ms]
Sep  5 20:48:45.063: INFO: Created: latency-svc-fppms
Sep  5 20:48:45.106: INFO: Got endpoints: latency-svc-sc4rp [749.873573ms]
Sep  5 20:48:45.156: INFO: Got endpoints: latency-svc-88qcw [750.37269ms]
Sep  5 20:48:45.206: INFO: Got endpoints: latency-svc-zxr62 [749.861084ms]
Sep  5 20:48:45.270: INFO: Got endpoints: latency-svc-5q8v5 [763.952682ms]
Sep  5 20:48:45.306: INFO: Got endpoints: latency-svc-scqc6 [749.958638ms]
Sep  5 20:48:45.356: INFO: Got endpoints: latency-svc-9x9q5 [750.239412ms]
Sep  5 20:48:45.406: INFO: Got endpoints: latency-svc-tdldh [749.633761ms]
Sep  5 20:48:45.456: INFO: Got endpoints: latency-svc-6g4qw [749.624051ms]
Sep  5 20:48:45.506: INFO: Got endpoints: latency-svc-67pxk [750.007446ms]
Sep  5 20:48:45.556: INFO: Got endpoints: latency-svc-2g6qp [749.500171ms]
Sep  5 20:48:45.606: INFO: Got endpoints: latency-svc-jxr8p [749.71587ms]
Sep  5 20:48:45.655: INFO: Got endpoints: latency-svc-rz92t [749.546002ms]
Sep  5 20:48:45.706: INFO: Got endpoints: latency-svc-wfgvg [749.541633ms]
Sep  5 20:48:45.756: INFO: Got endpoints: latency-svc-xctqk [750.296904ms]
Sep  5 20:48:45.806: INFO: Got endpoints: latency-svc-fppms [750.406305ms]
Sep  5 20:48:45.806: INFO: Latencies: [10.461548ms 13.798121ms 16.298226ms 20.694078ms 23.249991ms 26.410171ms 29.460409ms 32.166944ms 35.471627ms 38.658415ms 41.19557ms 41.443269ms 41.745637ms 41.81996ms 42.070561ms 42.178359ms 42.407532ms 42.644395ms 42.998823ms 43.563293ms 43.960056ms 44.047299ms 44.160254ms 44.221485ms 44.846753ms 45.291124ms 45.989096ms 46.186997ms 47.514721ms 50.595432ms 53.505293ms 89.802481ms 137.385083ms 184.707325ms 231.416552ms 278.878077ms 326.429275ms 373.461175ms 421.166863ms 467.990677ms 515.824037ms 562.553975ms 609.183095ms 656.967936ms 704.463185ms 739.476082ms 740.670215ms 748.023458ms 749.307988ms 749.403207ms 749.500171ms 749.518441ms 749.534093ms 749.541633ms 749.542984ms 749.544729ms 749.546002ms 749.549094ms 749.549149ms 749.54971ms 749.551647ms 749.569241ms 749.584577ms 749.599257ms 749.600905ms 749.607621ms 749.624051ms 749.630757ms 749.633761ms 749.635681ms 749.639979ms 749.647491ms 749.663395ms 749.666279ms 749.679741ms 749.689127ms 749.699011ms 749.706885ms 749.707755ms 749.71587ms 749.743049ms 749.753401ms 749.763085ms 749.764821ms 749.765671ms 749.773736ms 749.776086ms 749.784705ms 749.791927ms 749.800917ms 749.802001ms 749.802892ms 749.813604ms 749.813921ms 749.827108ms 749.827894ms 749.83656ms 749.837268ms 749.84004ms 749.848114ms 749.852918ms 749.853789ms 749.858989ms 749.861084ms 749.867767ms 749.871541ms 749.873573ms 749.883164ms 749.892801ms 749.900812ms 749.906397ms 749.906799ms 749.917439ms 749.9235ms 749.929236ms 749.943553ms 749.944883ms 749.948137ms 749.952548ms 749.952838ms 749.957882ms 749.958638ms 749.962111ms 749.966545ms 749.966767ms 749.968949ms 749.970401ms 749.978851ms 749.979914ms 749.98003ms 749.997867ms 750.000742ms 750.007446ms 750.016469ms 750.018566ms 750.028852ms 750.029176ms 750.040365ms 750.04926ms 750.04972ms 750.093109ms 750.107684ms 750.11136ms 750.126853ms 750.14912ms 750.154505ms 750.155409ms 750.167985ms 750.187273ms 750.189423ms 750.194458ms 750.198158ms 750.201541ms 750.204494ms 750.209175ms 750.20925ms 750.217817ms 750.21896ms 750.239412ms 750.253571ms 750.255544ms 750.26738ms 750.271822ms 750.283062ms 750.286205ms 750.289894ms 750.296904ms 750.299591ms 750.300338ms 750.301959ms 750.302774ms 750.308399ms 750.31392ms 750.351122ms 750.352193ms 750.353704ms 750.36831ms 750.369699ms 750.37269ms 750.406305ms 750.407099ms 750.423933ms 750.438319ms 750.450123ms 750.494ms 750.56166ms 750.602322ms 750.652097ms 750.66665ms 750.749799ms 750.792093ms 750.807348ms 750.848073ms 750.913802ms 750.948314ms 750.984961ms 752.079095ms 758.836949ms 760.745678ms 763.952682ms]
Sep  5 20:48:45.806: INFO: 50 %ile: 749.852918ms
Sep  5 20:48:45.806: INFO: 90 %ile: 750.407099ms
Sep  5 20:48:45.806: INFO: 99 %ile: 760.745678ms
Sep  5 20:48:45.806: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:48:45.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6570" for this suite.
Sep  5 20:49:05.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:49:05.910: INFO: namespace svc-latency-6570 deletion completed in 20.099981032s

• [SLOW TEST:31.959 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:49:05.911: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7789
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  5 20:49:08.573: INFO: Successfully updated pod "pod-update-99dbdb90-d01e-11e9-a0ad-8693e9898db7"
STEP: verifying the updated pod is in kubernetes
Sep  5 20:49:08.579: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:49:08.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7789" for this suite.
Sep  5 20:49:30.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:49:30.697: INFO: namespace pods-7789 deletion completed in 22.113579542s

• [SLOW TEST:24.786 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:49:30.697: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 20:49:30.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-505'
Sep  5 20:49:30.973: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 20:49:30.973: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Sep  5 20:49:30.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete jobs e2e-test-nginx-job --namespace=kubectl-505'
Sep  5 20:49:31.092: INFO: stderr: ""
Sep  5 20:49:31.092: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:49:31.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-505" for this suite.
Sep  5 20:49:37.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:49:37.204: INFO: namespace kubectl-505 deletion completed in 6.108262027s

• [SLOW TEST:6.507 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:49:37.204: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-8972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep  5 20:49:41.375: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:41.375: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:41.484: INFO: Exec stderr: ""
Sep  5 20:49:41.484: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:41.484: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:41.587: INFO: Exec stderr: ""
Sep  5 20:49:41.587: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:41.587: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:41.678: INFO: Exec stderr: ""
Sep  5 20:49:41.678: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:41.678: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:41.769: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep  5 20:49:41.769: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:41.769: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:41.858: INFO: Exec stderr: ""
Sep  5 20:49:41.858: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:41.858: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:41.949: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep  5 20:49:41.949: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:41.949: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:42.041: INFO: Exec stderr: ""
Sep  5 20:49:42.041: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:42.041: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:42.143: INFO: Exec stderr: ""
Sep  5 20:49:42.143: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:42.143: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:42.235: INFO: Exec stderr: ""
Sep  5 20:49:42.235: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8972 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:49:42.235: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:49:42.339: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:49:42.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8972" for this suite.
Sep  5 20:50:44.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:50:44.459: INFO: namespace e2e-kubelet-etc-hosts-8972 deletion completed in 1m2.115280543s

• [SLOW TEST:67.255 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:50:44.459: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5339
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 20:50:44.598: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  5 20:51:06.674: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.141.11:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5339 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:51:06.675: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:51:06.772: INFO: Found all expected endpoints: [netserver-0]
Sep  5 20:51:06.775: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.141.12:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5339 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:51:06.775: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:51:06.858: INFO: Found all expected endpoints: [netserver-1]
Sep  5 20:51:06.860: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.141.13:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5339 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:51:06.860: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:51:06.940: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:51:06.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5339" for this suite.
Sep  5 20:51:28.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:51:29.061: INFO: namespace pod-network-test-5339 deletion completed in 22.116899344s

• [SLOW TEST:44.602 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:51:29.062: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-ef301246-d01e-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 20:51:29.213: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ef30a339-d01e-11e9-a0ad-8693e9898db7" in namespace "projected-339" to be "success or failure"
Sep  5 20:51:29.216: INFO: Pod "pod-projected-configmaps-ef30a339-d01e-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.051167ms
Sep  5 20:51:31.220: INFO: Pod "pod-projected-configmaps-ef30a339-d01e-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007193697s
STEP: Saw pod success
Sep  5 20:51:31.220: INFO: Pod "pod-projected-configmaps-ef30a339-d01e-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:51:31.223: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-ef30a339-d01e-11e9-a0ad-8693e9898db7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 20:51:31.242: INFO: Waiting for pod pod-projected-configmaps-ef30a339-d01e-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:51:31.245: INFO: Pod pod-projected-configmaps-ef30a339-d01e-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:51:31.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-339" for this suite.
Sep  5 20:51:37.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:51:37.352: INFO: namespace projected-339 deletion completed in 6.102070671s

• [SLOW TEST:8.290 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:51:37.352: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-f4201dd8-d01e-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 20:51:37.495: INFO: Waiting up to 5m0s for pod "pod-configmaps-f420abec-d01e-11e9-a0ad-8693e9898db7" in namespace "configmap-9856" to be "success or failure"
Sep  5 20:51:37.498: INFO: Pod "pod-configmaps-f420abec-d01e-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.498384ms
Sep  5 20:51:39.502: INFO: Pod "pod-configmaps-f420abec-d01e-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006460402s
Sep  5 20:51:41.506: INFO: Pod "pod-configmaps-f420abec-d01e-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01049051s
STEP: Saw pod success
Sep  5 20:51:41.506: INFO: Pod "pod-configmaps-f420abec-d01e-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:51:41.509: INFO: Trying to get logs from node appserv10 pod pod-configmaps-f420abec-d01e-11e9-a0ad-8693e9898db7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 20:51:41.530: INFO: Waiting for pod pod-configmaps-f420abec-d01e-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:51:41.534: INFO: Pod pod-configmaps-f420abec-d01e-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:51:41.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9856" for this suite.
Sep  5 20:51:47.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:51:47.640: INFO: namespace configmap-9856 deletion completed in 6.100930881s

• [SLOW TEST:10.288 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:51:47.640: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:51:49.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6662" for this suite.
Sep  5 20:52:35.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:52:35.904: INFO: namespace kubelet-test-6662 deletion completed in 46.098830552s

• [SLOW TEST:48.264 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:52:35.904: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3942
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 20:52:36.040: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:52:38.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3942" for this suite.
Sep  5 20:53:24.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:53:24.189: INFO: namespace pods-3942 deletion completed in 46.109119284s

• [SLOW TEST:48.285 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:53:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-86
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 20:53:24.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33cf356a-d01f-11e9-a0ad-8693e9898db7" in namespace "projected-86" to be "success or failure"
Sep  5 20:53:24.340: INFO: Pod "downwardapi-volume-33cf356a-d01f-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.184019ms
Sep  5 20:53:26.345: INFO: Pod "downwardapi-volume-33cf356a-d01f-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007310694s
STEP: Saw pod success
Sep  5 20:53:26.345: INFO: Pod "downwardapi-volume-33cf356a-d01f-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:53:26.348: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-33cf356a-d01f-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 20:53:26.367: INFO: Waiting for pod downwardapi-volume-33cf356a-d01f-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:53:26.370: INFO: Pod downwardapi-volume-33cf356a-d01f-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:53:26.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-86" for this suite.
Sep  5 20:53:32.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:53:32.484: INFO: namespace projected-86 deletion completed in 6.11064642s

• [SLOW TEST:8.295 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:53:32.485: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Sep  5 20:53:32.630: INFO: Waiting up to 5m0s for pod "var-expansion-38c0ad85-d01f-11e9-a0ad-8693e9898db7" in namespace "var-expansion-471" to be "success or failure"
Sep  5 20:53:32.633: INFO: Pod "var-expansion-38c0ad85-d01f-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814288ms
Sep  5 20:53:34.637: INFO: Pod "var-expansion-38c0ad85-d01f-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006930435s
Sep  5 20:53:36.641: INFO: Pod "var-expansion-38c0ad85-d01f-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011247751s
STEP: Saw pod success
Sep  5 20:53:36.641: INFO: Pod "var-expansion-38c0ad85-d01f-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:53:36.645: INFO: Trying to get logs from node appserv10 pod var-expansion-38c0ad85-d01f-11e9-a0ad-8693e9898db7 container dapi-container: <nil>
STEP: delete the pod
Sep  5 20:53:36.664: INFO: Waiting for pod var-expansion-38c0ad85-d01f-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:53:36.666: INFO: Pod var-expansion-38c0ad85-d01f-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:53:36.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-471" for this suite.
Sep  5 20:53:42.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:53:42.782: INFO: namespace var-expansion-471 deletion completed in 6.111077259s

• [SLOW TEST:10.297 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:53:42.782: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-3ee44c63-d01f-11e9-a0ad-8693e9898db7
Sep  5 20:53:42.929: INFO: Pod name my-hostname-basic-3ee44c63-d01f-11e9-a0ad-8693e9898db7: Found 0 pods out of 1
Sep  5 20:53:47.933: INFO: Pod name my-hostname-basic-3ee44c63-d01f-11e9-a0ad-8693e9898db7: Found 1 pods out of 1
Sep  5 20:53:47.933: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3ee44c63-d01f-11e9-a0ad-8693e9898db7" are running
Sep  5 20:53:47.936: INFO: Pod "my-hostname-basic-3ee44c63-d01f-11e9-a0ad-8693e9898db7-pmz68" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 20:53:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 20:53:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 20:53:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 20:53:43 +0000 UTC Reason: Message:}])
Sep  5 20:53:47.936: INFO: Trying to dial the pod
Sep  5 20:53:52.951: INFO: Controller my-hostname-basic-3ee44c63-d01f-11e9-a0ad-8693e9898db7: Got expected result from replica 1 [my-hostname-basic-3ee44c63-d01f-11e9-a0ad-8693e9898db7-pmz68]: "my-hostname-basic-3ee44c63-d01f-11e9-a0ad-8693e9898db7-pmz68", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:53:52.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1555" for this suite.
Sep  5 20:53:58.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:53:59.075: INFO: namespace replication-controller-1555 deletion completed in 6.119722993s

• [SLOW TEST:16.293 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:53:59.076: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 20:53:59.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 version'
Sep  5 20:53:59.327: INFO: stderr: ""
Sep  5 20:53:59.327: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:36:19Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:53:59.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3597" for this suite.
Sep  5 20:54:05.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:54:05.439: INFO: namespace kubectl-3597 deletion completed in 6.108024602s

• [SLOW TEST:6.363 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:54:05.439: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7003
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 20:54:05.576: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  5 20:54:27.652: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=udp&host=172.16.141.11&port=8081&tries=1'] Namespace:pod-network-test-7003 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:54:27.652: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:54:27.750: INFO: Waiting for endpoints: map[]
Sep  5 20:54:27.753: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=udp&host=172.16.141.12&port=8081&tries=1'] Namespace:pod-network-test-7003 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:54:27.753: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:54:27.835: INFO: Waiting for endpoints: map[]
Sep  5 20:54:27.838: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=udp&host=172.16.141.13&port=8081&tries=1'] Namespace:pod-network-test-7003 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:54:27.838: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 20:54:27.918: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:54:27.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7003" for this suite.
Sep  5 20:54:49.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:54:50.032: INFO: namespace pod-network-test-7003 deletion completed in 22.109567445s

• [SLOW TEST:44.592 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:54:50.032: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 20:54:50.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8293'
Sep  5 20:54:50.377: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 20:54:50.377: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep  5 20:54:50.383: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-hlmv7]
Sep  5 20:54:50.383: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-hlmv7" in namespace "kubectl-8293" to be "running and ready"
Sep  5 20:54:50.386: INFO: Pod "e2e-test-nginx-rc-hlmv7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.264254ms
Sep  5 20:54:52.390: INFO: Pod "e2e-test-nginx-rc-hlmv7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007416591s
Sep  5 20:54:52.390: INFO: Pod "e2e-test-nginx-rc-hlmv7" satisfied condition "running and ready"
Sep  5 20:54:52.390: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-hlmv7]
Sep  5 20:54:52.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 logs rc/e2e-test-nginx-rc --namespace=kubectl-8293'
Sep  5 20:54:52.536: INFO: stderr: ""
Sep  5 20:54:52.536: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Sep  5 20:54:52.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete rc e2e-test-nginx-rc --namespace=kubectl-8293'
Sep  5 20:54:52.657: INFO: stderr: ""
Sep  5 20:54:52.657: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:54:52.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8293" for this suite.
Sep  5 20:55:14.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:55:14.769: INFO: namespace kubectl-8293 deletion completed in 22.107599353s

• [SLOW TEST:24.737 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:55:14.769: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Sep  5 20:55:14.912: INFO: Waiting up to 5m0s for pod "client-containers-75b7b4e9-d01f-11e9-a0ad-8693e9898db7" in namespace "containers-7826" to be "success or failure"
Sep  5 20:55:14.915: INFO: Pod "client-containers-75b7b4e9-d01f-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907157ms
Sep  5 20:55:16.918: INFO: Pod "client-containers-75b7b4e9-d01f-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006714896s
Sep  5 20:55:18.923: INFO: Pod "client-containers-75b7b4e9-d01f-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011013253s
STEP: Saw pod success
Sep  5 20:55:18.923: INFO: Pod "client-containers-75b7b4e9-d01f-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:55:18.926: INFO: Trying to get logs from node appserv9 pod client-containers-75b7b4e9-d01f-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 20:55:18.953: INFO: Waiting for pod client-containers-75b7b4e9-d01f-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:55:18.956: INFO: Pod client-containers-75b7b4e9-d01f-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:55:18.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7826" for this suite.
Sep  5 20:55:24.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:55:25.072: INFO: namespace containers-7826 deletion completed in 6.110270643s

• [SLOW TEST:10.302 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:55:25.072: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2441
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  5 20:55:25.217: INFO: Waiting up to 5m0s for pod "pod-7bdc30a6-d01f-11e9-a0ad-8693e9898db7" in namespace "emptydir-2441" to be "success or failure"
Sep  5 20:55:25.220: INFO: Pod "pod-7bdc30a6-d01f-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907092ms
Sep  5 20:55:27.224: INFO: Pod "pod-7bdc30a6-d01f-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006721696s
STEP: Saw pod success
Sep  5 20:55:27.224: INFO: Pod "pod-7bdc30a6-d01f-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:55:27.227: INFO: Trying to get logs from node appserv10 pod pod-7bdc30a6-d01f-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 20:55:27.247: INFO: Waiting for pod pod-7bdc30a6-d01f-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:55:27.249: INFO: Pod pod-7bdc30a6-d01f-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:55:27.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2441" for this suite.
Sep  5 20:55:33.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:55:33.365: INFO: namespace emptydir-2441 deletion completed in 6.111269293s

• [SLOW TEST:8.292 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:55:33.365: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2699
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 20:55:49.522: INFO: Container started at 2019-09-05 20:55:34 +0000 UTC, pod became ready at 2019-09-05 20:55:49 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:55:49.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2699" for this suite.
Sep  5 20:56:11.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:56:11.637: INFO: namespace container-probe-2699 deletion completed in 22.111206033s

• [SLOW TEST:38.272 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:56:11.638: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-7208
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7208 to expose endpoints map[]
Sep  5 20:56:11.788: INFO: Get endpoints failed (3.25623ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep  5 20:56:12.792: INFO: successfully validated that service endpoint-test2 in namespace services-7208 exposes endpoints map[] (1.007035542s elapsed)
STEP: Creating pod pod1 in namespace services-7208
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7208 to expose endpoints map[pod1:[80]]
Sep  5 20:56:14.820: INFO: successfully validated that service endpoint-test2 in namespace services-7208 exposes endpoints map[pod1:[80]] (2.019847164s elapsed)
STEP: Creating pod pod2 in namespace services-7208
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7208 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  5 20:56:16.852: INFO: successfully validated that service endpoint-test2 in namespace services-7208 exposes endpoints map[pod1:[80] pod2:[80]] (2.026571246s elapsed)
STEP: Deleting pod pod1 in namespace services-7208
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7208 to expose endpoints map[pod2:[80]]
Sep  5 20:56:16.864: INFO: successfully validated that service endpoint-test2 in namespace services-7208 exposes endpoints map[pod2:[80]] (6.453472ms elapsed)
STEP: Deleting pod pod2 in namespace services-7208
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7208 to expose endpoints map[]
Sep  5 20:56:16.875: INFO: successfully validated that service endpoint-test2 in namespace services-7208 exposes endpoints map[] (4.528743ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:56:16.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7208" for this suite.
Sep  5 20:56:38.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:56:39.011: INFO: namespace services-7208 deletion completed in 22.119105498s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:27.373 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:56:39.011: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1653
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 20:56:39.149: INFO: Creating deployment "nginx-deployment"
Sep  5 20:56:39.154: INFO: Waiting for observed generation 1
Sep  5 20:56:41.162: INFO: Waiting for all required pods to come up
Sep  5 20:56:41.167: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  5 20:56:43.175: INFO: Waiting for deployment "nginx-deployment" to complete
Sep  5 20:56:43.182: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep  5 20:56:43.190: INFO: Updating deployment nginx-deployment
Sep  5 20:56:43.190: INFO: Waiting for observed generation 2
Sep  5 20:56:45.197: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  5 20:56:45.200: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  5 20:56:45.203: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  5 20:56:45.212: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  5 20:56:45.212: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  5 20:56:45.215: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  5 20:56:45.220: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep  5 20:56:45.221: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep  5 20:56:45.228: INFO: Updating deployment nginx-deployment
Sep  5 20:56:45.228: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep  5 20:56:45.233: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  5 20:56:45.237: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  5 20:56:45.244: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1653,SelfLink:/apis/apps/v1/namespaces/deployment-1653/deployments/nginx-deployment,UID:a7eec126-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6935,Generation:3,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-09-05 20:56:43 +0000 UTC 2019-09-05 20:56:39 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-09-05 20:56:45 +0000 UTC 2019-09-05 20:56:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep  5 20:56:45.250: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-1653,SelfLink:/apis/apps/v1/namespaces/deployment-1653/replicasets/nginx-deployment-5f9595f595,UID:aa575603-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6932,Generation:3,CreationTimestamp:2019-09-05 20:56:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a7eec126-d01f-11e9-9b5e-2c600c82ec72 0xc0004b8557 0xc0004b8558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 20:56:45.250: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep  5 20:56:45.250: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-1653,SelfLink:/apis/apps/v1/namespaces/deployment-1653/replicasets/nginx-deployment-6f478d8d8,UID:a7ef67fe-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6931,Generation:3,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a7eec126-d01f-11e9-9b5e-2c600c82ec72 0xc0004b87f7 0xc0004b87f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep  5 20:56:45.257: INFO: Pod "nginx-deployment-5f9595f595-29cbs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-29cbs,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-29cbs,UID:aa5c79ea-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6912,Generation:0,CreationTimestamp:2019-09-05 20:56:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d94470 0xc001d94471}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d944f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d94510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:,StartTime:2019-09-05 20:56:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.257: INFO: Pod "nginx-deployment-5f9595f595-2qnc4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-2qnc4,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-2qnc4,UID:ab9126ec-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6952,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d945f0 0xc001d945f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d94660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d94680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.258: INFO: Pod "nginx-deployment-5f9595f595-524zw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-524zw,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-524zw,UID:ab9042be-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6942,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d946f7 0xc001d946f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d94760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d94780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.258: INFO: Pod "nginx-deployment-5f9595f595-b8vd8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-b8vd8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-b8vd8,UID:ab911476-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6950,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d947f7 0xc001d947f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d94860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d94880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.258: INFO: Pod "nginx-deployment-5f9595f595-f5zpc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-f5zpc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-f5zpc,UID:ab9107ea-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6949,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d948f7 0xc001d948f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d94960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d94980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.258: INFO: Pod "nginx-deployment-5f9595f595-gjlbs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-gjlbs,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-gjlbs,UID:aa5d0505-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6917,Generation:0,CreationTimestamp:2019-09-05 20:56:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d949f7 0xc001d949f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d94a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d94a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.111,PodIP:,StartTime:2019-09-05 20:56:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.258: INFO: Pod "nginx-deployment-5f9595f595-n7vkm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-n7vkm,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-n7vkm,UID:ab910a48-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6951,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d94b70 0xc001d94b71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d94be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d94c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.259: INFO: Pod "nginx-deployment-5f9595f595-p9l8c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-p9l8c,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-p9l8c,UID:aa580b38-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6895,Generation:0,CreationTimestamp:2019-09-05 20:56:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d94c77 0xc001d94c78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d94cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d94d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:,StartTime:2019-09-05 20:56:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.259: INFO: Pod "nginx-deployment-5f9595f595-rzh9t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-rzh9t,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-rzh9t,UID:ab8f8e63-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6937,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d94df0 0xc001d94df1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d94e80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d94eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.260: INFO: Pod "nginx-deployment-5f9595f595-sspgl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-sspgl,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-sspgl,UID:aa58bc8e-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6900,Generation:0,CreationTimestamp:2019-09-05 20:56:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d94f67 0xc001d94f68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d95170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d95190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:,StartTime:2019-09-05 20:56:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.260: INFO: Pod "nginx-deployment-5f9595f595-t2fdh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-t2fdh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-t2fdh,UID:ab9044f2-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6943,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d95390 0xc001d95391}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d95410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d95430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.260: INFO: Pod "nginx-deployment-5f9595f595-vnfvz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-vnfvz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-5f9595f595-vnfvz,UID:aa58c304-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6905,Generation:0,CreationTimestamp:2019-09-05 20:56:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aa575603-d01f-11e9-9b5e-2c600c82ec72 0xc001d954a7 0xc001d954a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d95520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d955c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.111,PodIP:,StartTime:2019-09-05 20:56:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.260: INFO: Pod "nginx-deployment-6f478d8d8-26vrf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-26vrf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-26vrf,UID:a7f08194-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6790,Generation:0,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc001d956a0 0xc001d956a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d95710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d95790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.11,StartTime:2019-09-05 20:56:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 20:56:40 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://578bdf85cbfb8d119327d9e1ca2a6e39d9a7552f69be067539cc1c548a377826}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.260: INFO: Pod "nginx-deployment-6f478d8d8-2tfqf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2tfqf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-2tfqf,UID:ab90939d-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6946,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc001d959c0 0xc001d959c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d95a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d95ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.261: INFO: Pod "nginx-deployment-6f478d8d8-4sh9z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4sh9z,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-4sh9z,UID:a7f2a946-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6843,Generation:0,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc001d95b67 0xc001d95b68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d95ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d95d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.18,StartTime:2019-09-05 20:56:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 20:56:40 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a6c617bd69774d2bd5628e6447406b548e7e578e95e64e13e4590a17cbf0d54a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.261: INFO: Pod "nginx-deployment-6f478d8d8-8fsb2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8fsb2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-8fsb2,UID:a7f13bf2-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6824,Generation:0,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc001d95ee0 0xc001d95ee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d95fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d95fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.111,PodIP:172.16.141.13,StartTime:2019-09-05 20:56:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 20:56:40 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7aaee3561b39f4115af8624318a06e64ce9d6a8a149614304722db21419e8a12}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.261: INFO: Pod "nginx-deployment-6f478d8d8-c5kt7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-c5kt7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-c5kt7,UID:ab8ee9dc-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6934,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061e1e0 0xc00061e1e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00061e2b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00061e300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.261: INFO: Pod "nginx-deployment-6f478d8d8-fczxr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fczxr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-fczxr,UID:a7f1eab6-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6840,Generation:0,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061e3b7 0xc00061e3b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00061e430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00061e450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.14,StartTime:2019-09-05 20:56:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 20:56:40 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c6115135007b473625e1235d03cd7366d03868918d185328576c6b4a45876552}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.261: INFO: Pod "nginx-deployment-6f478d8d8-fgrqq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fgrqq,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-fgrqq,UID:ab8fa3fc-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6938,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061e610 0xc00061e611}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00061e670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00061e690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.262: INFO: Pod "nginx-deployment-6f478d8d8-fmt6k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fmt6k,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-fmt6k,UID:ab8faf7e-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6939,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061e7b7 0xc00061e7b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00061e940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00061e960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.262: INFO: Pod "nginx-deployment-6f478d8d8-ftmk5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ftmk5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-ftmk5,UID:ab91732a-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6953,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061eab7 0xc00061eab8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00061ebb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00061ebe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.262: INFO: Pod "nginx-deployment-6f478d8d8-h4hvw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h4hvw,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-h4hvw,UID:a7f1f9c4-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6837,Generation:0,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061ed27 0xc00061ed28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00061ee40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00061ee60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.17,StartTime:2019-09-05 20:56:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 20:56:40 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9553bf5ee63a673bca3d73236036f403119bca241094eb1e6ed2260be31f025c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.262: INFO: Pod "nginx-deployment-6f478d8d8-pqscd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-pqscd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-pqscd,UID:a7f1f2ba-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6812,Generation:0,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061efa0 0xc00061efa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00061f0f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00061f120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.15,StartTime:2019-09-05 20:56:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 20:56:40 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://51ea025e47ca930504770a4010ded487bb69da6be50454d44cda0fe8bdb65ce1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.262: INFO: Pod "nginx-deployment-6f478d8d8-q9x56" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-q9x56,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-q9x56,UID:ab9096ee-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6947,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061f3f0 0xc00061f3f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00061f560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00061f580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.263: INFO: Pod "nginx-deployment-6f478d8d8-qrm5r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qrm5r,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-qrm5r,UID:ab9177eb-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6956,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061f607 0xc00061f608}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00061f670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00061f690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.263: INFO: Pod "nginx-deployment-6f478d8d8-rflk4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rflk4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-rflk4,UID:ab908554-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6944,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00061ff87 0xc00061ff88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00099e090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00099e0b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.263: INFO: Pod "nginx-deployment-6f478d8d8-rgnrm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rgnrm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-rgnrm,UID:a7f1fbbb-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6821,Generation:0,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00099e707 0xc00099e708}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00099e9e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00099ea00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.111,PodIP:172.16.141.16,StartTime:2019-09-05 20:56:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 20:56:40 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://13dc301b03bcde19c93248fab592aa1ccae88cd061e3e36a3a3c6fc78d5d4a6e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.263: INFO: Pod "nginx-deployment-6f478d8d8-rh9wj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rh9wj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-rh9wj,UID:ab917e91-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6955,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00099ee80 0xc00099ee81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00099f0f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00099f450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.263: INFO: Pod "nginx-deployment-6f478d8d8-sqggs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-sqggs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-sqggs,UID:ab909c05-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6948,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00099f877 0xc00099f878}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00099f910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00099f930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.263: INFO: Pod "nginx-deployment-6f478d8d8-vtdhj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vtdhj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-vtdhj,UID:a7f134fe-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6809,Generation:0,CreationTimestamp:2019-09-05 20:56:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00099fa37 0xc00099fa38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00099fb50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00099fb70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 20:56:39 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.12,StartTime:2019-09-05 20:56:39 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 20:56:40 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://993883e209cfb763b14bae8f82fbc92427b6d691d50eef77653a92c768ae5f1b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 20:56:45.264: INFO: Pod "nginx-deployment-6f478d8d8-wpcnz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wpcnz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1653,SelfLink:/api/v1/namespaces/deployment-1653/pods/nginx-deployment-6f478d8d8-wpcnz,UID:ab9178f5-d01f-11e9-9b5e-2c600c82ec72,ResourceVersion:6954,Generation:0,CreationTimestamp:2019-09-05 20:56:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a7ef67fe-d01f-11e9-9b5e-2c600c82ec72 0xc00099fda0 0xc00099fda1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l9nd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l9nd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-8l9nd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00099fed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00099fef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:56:45.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1653" for this suite.
Sep  5 20:56:51.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:56:51.378: INFO: namespace deployment-1653 deletion completed in 6.110864845s

• [SLOW TEST:12.367 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:56:51.378: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3453
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5555
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5990
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:57:19.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3453" for this suite.
Sep  5 20:57:25.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:57:25.910: INFO: namespace namespaces-3453 deletion completed in 6.096779539s
STEP: Destroying namespace "nsdeletetest-5555" for this suite.
Sep  5 20:57:25.913: INFO: Namespace nsdeletetest-5555 was already deleted
STEP: Destroying namespace "nsdeletetest-5990" for this suite.
Sep  5 20:57:31.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:57:32.021: INFO: namespace nsdeletetest-5990 deletion completed in 6.108656292s

• [SLOW TEST:40.643 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:57:32.022: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  5 20:57:36.209: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:36.212: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:38.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:38.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:40.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:40.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:42.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:42.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:44.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:44.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:46.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:46.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:48.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:48.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:50.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:50.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:52.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:52.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:54.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:54.217: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:56.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:56.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:57:58.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:57:58.216: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 20:58:00.212: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 20:58:00.217: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:58:00.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8982" for this suite.
Sep  5 20:58:22.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:58:22.333: INFO: namespace container-lifecycle-hook-8982 deletion completed in 22.111676524s

• [SLOW TEST:50.311 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:58:22.334: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:58:26.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8412" for this suite.
Sep  5 20:58:32.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:58:32.605: INFO: namespace kubelet-test-8412 deletion completed in 6.114375685s

• [SLOW TEST:10.271 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:58:32.605: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  5 20:58:37.282: INFO: Successfully updated pod "annotationupdateeba38139-d01f-11e9-a0ad-8693e9898db7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:58:39.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4497" for this suite.
Sep  5 20:59:01.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:59:01.418: INFO: namespace downward-api-4497 deletion completed in 22.111497575s

• [SLOW TEST:28.813 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:59:01.418: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8086
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  5 20:59:01.564: INFO: Waiting up to 5m0s for pod "pod-fcd014d5-d01f-11e9-a0ad-8693e9898db7" in namespace "emptydir-8086" to be "success or failure"
Sep  5 20:59:01.567: INFO: Pod "pod-fcd014d5-d01f-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921499ms
Sep  5 20:59:03.572: INFO: Pod "pod-fcd014d5-d01f-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007225867s
STEP: Saw pod success
Sep  5 20:59:03.572: INFO: Pod "pod-fcd014d5-d01f-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:59:03.575: INFO: Trying to get logs from node appserv10 pod pod-fcd014d5-d01f-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 20:59:03.596: INFO: Waiting for pod pod-fcd014d5-d01f-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:59:03.599: INFO: Pod pod-fcd014d5-d01f-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:59:03.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8086" for this suite.
Sep  5 20:59:09.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:59:09.717: INFO: namespace emptydir-8086 deletion completed in 6.11390208s

• [SLOW TEST:8.300 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:59:09.718: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5695
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 20:59:09.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01c28366-d020-11e9-a0ad-8693e9898db7" in namespace "projected-5695" to be "success or failure"
Sep  5 20:59:09.868: INFO: Pod "downwardapi-volume-01c28366-d020-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.118516ms
Sep  5 20:59:11.872: INFO: Pod "downwardapi-volume-01c28366-d020-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007038204s
STEP: Saw pod success
Sep  5 20:59:11.872: INFO: Pod "downwardapi-volume-01c28366-d020-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:59:11.875: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-01c28366-d020-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 20:59:11.895: INFO: Waiting for pod downwardapi-volume-01c28366-d020-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:59:11.898: INFO: Pod downwardapi-volume-01c28366-d020-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:59:11.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5695" for this suite.
Sep  5 20:59:17.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:59:18.003: INFO: namespace projected-5695 deletion completed in 6.100179554s

• [SLOW TEST:8.285 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:59:18.003: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-149
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Sep  5 20:59:18.147: INFO: Waiting up to 5m0s for pod "client-containers-06b271ec-d020-11e9-a0ad-8693e9898db7" in namespace "containers-149" to be "success or failure"
Sep  5 20:59:18.150: INFO: Pod "client-containers-06b271ec-d020-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.012207ms
Sep  5 20:59:20.155: INFO: Pod "client-containers-06b271ec-d020-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007418745s
STEP: Saw pod success
Sep  5 20:59:20.155: INFO: Pod "client-containers-06b271ec-d020-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:59:20.158: INFO: Trying to get logs from node appserv9 pod client-containers-06b271ec-d020-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 20:59:20.178: INFO: Waiting for pod client-containers-06b271ec-d020-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:59:20.180: INFO: Pod client-containers-06b271ec-d020-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:59:20.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-149" for this suite.
Sep  5 20:59:26.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:59:26.282: INFO: namespace containers-149 deletion completed in 6.097657376s

• [SLOW TEST:8.279 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:59:26.283: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep  5 20:59:26.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-2331'
Sep  5 20:59:26.626: INFO: stderr: ""
Sep  5 20:59:26.626: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  5 20:59:27.630: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 20:59:27.630: INFO: Found 0 / 1
Sep  5 20:59:28.630: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 20:59:28.631: INFO: Found 1 / 1
Sep  5 20:59:28.631: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  5 20:59:28.634: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 20:59:28.634: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 20:59:28.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 patch pod redis-master-8grrl --namespace=kubectl-2331 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  5 20:59:28.764: INFO: stderr: ""
Sep  5 20:59:28.764: INFO: stdout: "pod/redis-master-8grrl patched\n"
STEP: checking annotations
Sep  5 20:59:28.767: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 20:59:28.767: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:59:28.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2331" for this suite.
Sep  5 20:59:50.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:59:50.876: INFO: namespace kubectl-2331 deletion completed in 22.104930258s

• [SLOW TEST:24.594 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:59:50.877: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-1a4a10bf-d020-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 20:59:51.022: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1a4aac9f-d020-11e9-a0ad-8693e9898db7" in namespace "projected-9257" to be "success or failure"
Sep  5 20:59:51.025: INFO: Pod "pod-projected-configmaps-1a4aac9f-d020-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.283613ms
Sep  5 20:59:53.030: INFO: Pod "pod-projected-configmaps-1a4aac9f-d020-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007472584s
STEP: Saw pod success
Sep  5 20:59:53.030: INFO: Pod "pod-projected-configmaps-1a4aac9f-d020-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 20:59:53.033: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-1a4aac9f-d020-11e9-a0ad-8693e9898db7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 20:59:53.052: INFO: Waiting for pod pod-projected-configmaps-1a4aac9f-d020-11e9-a0ad-8693e9898db7 to disappear
Sep  5 20:59:53.054: INFO: Pod pod-projected-configmaps-1a4aac9f-d020-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 20:59:53.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9257" for this suite.
Sep  5 20:59:59.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 20:59:59.169: INFO: namespace projected-9257 deletion completed in 6.110678355s

• [SLOW TEST:8.293 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 20:59:59.169: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-824
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 20:59:59.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 version --client'
Sep  5 20:59:59.402: INFO: stderr: ""
Sep  5 20:59:59.402: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Sep  5 20:59:59.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-824'
Sep  5 20:59:59.608: INFO: stderr: ""
Sep  5 20:59:59.608: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep  5 20:59:59.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-824'
Sep  5 20:59:59.813: INFO: stderr: ""
Sep  5 20:59:59.813: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  5 21:00:00.818: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 21:00:00.818: INFO: Found 0 / 1
Sep  5 21:00:01.818: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 21:00:01.818: INFO: Found 1 / 1
Sep  5 21:00:01.818: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  5 21:00:01.822: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 21:00:01.822: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 21:00:01.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 describe pod redis-master-zs986 --namespace=kubectl-824'
Sep  5 21:00:01.965: INFO: stderr: ""
Sep  5 21:00:01.965: INFO: stdout: "Name:               redis-master-zs986\nNamespace:          kubectl-824\nPriority:           0\nPriorityClassName:  <none>\nNode:               appserv10/172.16.6.110\nStart Time:         Thu, 05 Sep 2019 20:59:59 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: collecting\nStatus:             Running\nIP:                 172.16.141.11\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://ae42370d8006f47ed718b23ec432e6210d06b39941d6867dfdcb05d9e403864b\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Sep 2019 21:00:00 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tx8cl (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tx8cl:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tx8cl\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                Message\n  ----    ------     ----  ----                -------\n  Normal  Scheduled  2s    default-scheduler   Successfully assigned kubectl-824/redis-master-zs986 to appserv10\n  Normal  Pulled     1s    kubelet, appserv10  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, appserv10  Created container redis-master\n  Normal  Started    1s    kubelet, appserv10  Started container redis-master\n"
Sep  5 21:00:01.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 describe rc redis-master --namespace=kubectl-824'
Sep  5 21:00:02.110: INFO: stderr: ""
Sep  5 21:00:02.110: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-824\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-zs986\n"
Sep  5 21:00:02.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 describe service redis-master --namespace=kubectl-824'
Sep  5 21:00:02.227: INFO: stderr: ""
Sep  5 21:00:02.227: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-824\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.0.46\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.141.11:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  5 21:00:02.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 describe node appserv10'
Sep  5 21:00:02.384: INFO: stderr: ""
Sep  5 21:00:02.384: INFO: stdout: "Name:               appserv10\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=appserv10\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\nCreationTimestamp:  Thu, 05 Sep 2019 20:37:46 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 05 Sep 2019 20:59:47 +0000   Thu, 05 Sep 2019 20:37:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 05 Sep 2019 20:59:47 +0000   Thu, 05 Sep 2019 20:37:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 05 Sep 2019 20:59:47 +0000   Thu, 05 Sep 2019 20:37:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 05 Sep 2019 20:59:47 +0000   Thu, 05 Sep 2019 20:37:44 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.16.6.110\n  Hostname:    appserv10\nCapacity:\n cpu:                32\n ephemeral-storage:  65504Mi\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65698416Ki\n pods:               110\nAllocatable:\n cpu:                32\n ephemeral-storage:  61817329972\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65596016Ki\n pods:               110\nSystem Info:\n Machine ID:                 7b92f3dcc670423e9daa144ee7f0a6e3\n System UUID:                8D3B0B1C-7BF9-1000-AE0C-54AB3A29191F\n Boot ID:                    d3bc2d29-9c47-46e7-9cea-bd1c4ca5a0a8\n Kernel Version:             3.10.0-693.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.14.3\n Kube-Proxy Version:         v1.14.3\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  diamanti-system            collectd-v0.8-dflpk                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\n  diamanti-system            prometheus-v1-0                                            0 (0%)        0 (0%)      1Gi (1%)         1Gi (1%)       22m\n  diamanti-system            snapshot-controller-7cbc6c6455-2jwnr                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-5x8tc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  kube-system                helm-chart-547cc86f54-kdx6m                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\n  kubectl-824                redis-master-zs986                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             1Gi (1%)  1Gi (1%)\n  ephemeral-storage  0 (0%)    0 (0%)\nEvents:\n  Type    Reason                   Age                From                   Message\n  ----    ------                   ----               ----                   -------\n  Normal  Starting                 22m                kube-proxy, appserv10  Starting kube-proxy.\n  Normal  NodeAllocatableEnforced  22m                kubelet, appserv10     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  22m (x7 over 22m)  kubelet, appserv10     Node appserv10 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    22m (x7 over 22m)  kubelet, appserv10     Node appserv10 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     22m (x7 over 22m)  kubelet, appserv10     Node appserv10 status is now: NodeHasSufficientPID\n"
Sep  5 21:00:02.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 describe namespace kubectl-824'
Sep  5 21:00:02.532: INFO: stderr: ""
Sep  5 21:00:02.532: INFO: stdout: "Name:         kubectl-824\nLabels:       e2e-framework=kubectl\n              e2e-run=458df513-d01d-11e9-a0ad-8693e9898db7\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:00:02.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-824" for this suite.
Sep  5 21:00:24.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:00:24.646: INFO: namespace kubectl-824 deletion completed in 22.110404997s

• [SLOW TEST:25.477 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:00:24.647: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9733
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-tnkf
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 21:00:24.801: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tnkf" in namespace "subpath-9733" to be "success or failure"
Sep  5 21:00:24.804: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.938178ms
Sep  5 21:00:26.808: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006507657s
Sep  5 21:00:28.812: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 4.010782629s
Sep  5 21:00:30.817: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015201241s
Sep  5 21:00:32.821: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 8.019822564s
Sep  5 21:00:34.825: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 10.023908334s
Sep  5 21:00:36.829: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 12.027914565s
Sep  5 21:00:38.834: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 14.03236002s
Sep  5 21:00:40.838: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 16.036582024s
Sep  5 21:00:42.842: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 18.040855324s
Sep  5 21:00:44.846: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Running", Reason="", readiness=true. Elapsed: 20.044754135s
Sep  5 21:00:46.850: INFO: Pod "pod-subpath-test-downwardapi-tnkf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.048582846s
STEP: Saw pod success
Sep  5 21:00:46.850: INFO: Pod "pod-subpath-test-downwardapi-tnkf" satisfied condition "success or failure"
Sep  5 21:00:46.853: INFO: Trying to get logs from node appserv9 pod pod-subpath-test-downwardapi-tnkf container test-container-subpath-downwardapi-tnkf: <nil>
STEP: delete the pod
Sep  5 21:00:46.872: INFO: Waiting for pod pod-subpath-test-downwardapi-tnkf to disappear
Sep  5 21:00:46.875: INFO: Pod pod-subpath-test-downwardapi-tnkf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-tnkf
Sep  5 21:00:46.875: INFO: Deleting pod "pod-subpath-test-downwardapi-tnkf" in namespace "subpath-9733"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:00:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9733" for this suite.
Sep  5 21:00:52.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:00:52.994: INFO: namespace subpath-9733 deletion completed in 6.113012024s

• [SLOW TEST:28.348 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:00:52.995: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3358
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-3f513d44-d020-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 21:00:53.144: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3f51c4bd-d020-11e9-a0ad-8693e9898db7" in namespace "projected-3358" to be "success or failure"
Sep  5 21:00:53.147: INFO: Pod "pod-projected-secrets-3f51c4bd-d020-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822757ms
Sep  5 21:00:55.150: INFO: Pod "pod-projected-secrets-3f51c4bd-d020-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006713683s
Sep  5 21:00:57.154: INFO: Pod "pod-projected-secrets-3f51c4bd-d020-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010377375s
STEP: Saw pod success
Sep  5 21:00:57.154: INFO: Pod "pod-projected-secrets-3f51c4bd-d020-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:00:57.157: INFO: Trying to get logs from node appserv10 pod pod-projected-secrets-3f51c4bd-d020-11e9-a0ad-8693e9898db7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:00:57.175: INFO: Waiting for pod pod-projected-secrets-3f51c4bd-d020-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:00:57.178: INFO: Pod pod-projected-secrets-3f51c4bd-d020-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:00:57.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3358" for this suite.
Sep  5 21:01:03.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:01:03.292: INFO: namespace projected-3358 deletion completed in 6.110628257s

• [SLOW TEST:10.297 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:01:03.292: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Sep  5 21:01:03.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 cluster-info'
Sep  5 21:01:03.551: INFO: stderr: ""
Sep  5 21:01:03.551: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:01:03.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6819" for this suite.
Sep  5 21:01:09.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:01:09.667: INFO: namespace kubectl-6819 deletion completed in 6.11206551s

• [SLOW TEST:6.375 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:01:09.668: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1985
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  5 21:01:12.347: INFO: Successfully updated pod "annotationupdate49413035-d020-11e9-a0ad-8693e9898db7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:01:14.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1985" for this suite.
Sep  5 21:01:36.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:01:36.468: INFO: namespace projected-1985 deletion completed in 22.097288899s

• [SLOW TEST:26.800 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:01:36.468: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7379
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 21:01:36.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7379'
Sep  5 21:01:36.723: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 21:01:36.723: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Sep  5 21:01:36.729: INFO: scanned /root for discovery docs: <nil>
Sep  5 21:01:36.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7379'
Sep  5 21:01:52.527: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  5 21:01:52.527: INFO: stdout: "Created e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301\nScaling up e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep  5 21:01:52.527: INFO: stdout: "Created e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301\nScaling up e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep  5 21:01:52.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7379'
Sep  5 21:01:52.642: INFO: stderr: ""
Sep  5 21:01:52.642: INFO: stdout: "e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301-zmqrx "
Sep  5 21:01:52.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301-zmqrx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7379'
Sep  5 21:01:52.753: INFO: stderr: ""
Sep  5 21:01:52.753: INFO: stdout: "true"
Sep  5 21:01:52.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301-zmqrx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7379'
Sep  5 21:01:52.871: INFO: stderr: ""
Sep  5 21:01:52.871: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep  5 21:01:52.871: INFO: e2e-test-nginx-rc-4ade215c1d1cfc675a46455afb75d301-zmqrx is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Sep  5 21:01:52.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete rc e2e-test-nginx-rc --namespace=kubectl-7379'
Sep  5 21:01:52.976: INFO: stderr: ""
Sep  5 21:01:52.976: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:01:52.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7379" for this suite.
Sep  5 21:02:14.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:02:15.099: INFO: namespace kubectl-7379 deletion completed in 22.118186558s

• [SLOW TEST:38.631 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:02:15.099: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:02:15.245: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70414d19-d020-11e9-a0ad-8693e9898db7" in namespace "projected-9972" to be "success or failure"
Sep  5 21:02:15.248: INFO: Pod "downwardapi-volume-70414d19-d020-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.864797ms
Sep  5 21:02:17.252: INFO: Pod "downwardapi-volume-70414d19-d020-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006539925s
STEP: Saw pod success
Sep  5 21:02:17.252: INFO: Pod "downwardapi-volume-70414d19-d020-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:02:17.255: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-70414d19-d020-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:02:17.274: INFO: Waiting for pod downwardapi-volume-70414d19-d020-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:02:17.277: INFO: Pod downwardapi-volume-70414d19-d020-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:02:17.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9972" for this suite.
Sep  5 21:02:23.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:02:23.391: INFO: namespace projected-9972 deletion completed in 6.109695072s

• [SLOW TEST:8.292 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:02:23.391: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9664
Sep  5 21:02:25.545: INFO: Started pod liveness-http in namespace container-probe-9664
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:02:25.548: INFO: Initial restart count of pod liveness-http is 0
Sep  5 21:02:43.586: INFO: Restart count of pod container-probe-9664/liveness-http is now 1 (18.038084187s elapsed)
Sep  5 21:03:01.620: INFO: Restart count of pod container-probe-9664/liveness-http is now 2 (36.071970747s elapsed)
Sep  5 21:03:21.661: INFO: Restart count of pod container-probe-9664/liveness-http is now 3 (56.1127539s elapsed)
Sep  5 21:03:43.703: INFO: Restart count of pod container-probe-9664/liveness-http is now 4 (1m18.154971048s elapsed)
Sep  5 21:04:49.834: INFO: Restart count of pod container-probe-9664/liveness-http is now 5 (2m24.286228014s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:04:49.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9664" for this suite.
Sep  5 21:04:55.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:04:55.949: INFO: namespace container-probe-9664 deletion completed in 6.100297477s

• [SLOW TEST:152.558 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:04:55.950: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2463.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2463.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2463.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2463.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2463.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2463.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2463.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2463.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 134.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.134_udp@PTR;check="$$(dig +tcp +noall +answer +search 134.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.134_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2463.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2463.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2463.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2463.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2463.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2463.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2463.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2463.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 134.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.134_udp@PTR;check="$$(dig +tcp +noall +answer +search 134.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.134_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 21:05:00.183: INFO: DNS probes using dns-2463/dns-test-d02208e1-d020-11e9-a0ad-8693e9898db7 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:05:00.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2463" for this suite.
Sep  5 21:05:06.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:05:06.317: INFO: namespace dns-2463 deletion completed in 6.092447274s

• [SLOW TEST:10.367 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:05:06.317: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7990
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1995
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:05:12.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7504" for this suite.
Sep  5 21:05:18.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:05:18.856: INFO: namespace namespaces-7504 deletion completed in 6.115375281s
STEP: Destroying namespace "nsdeletetest-7990" for this suite.
Sep  5 21:05:18.858: INFO: Namespace nsdeletetest-7990 was already deleted
STEP: Destroying namespace "nsdeletetest-1995" for this suite.
Sep  5 21:05:24.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:05:24.967: INFO: namespace nsdeletetest-1995 deletion completed in 6.108817944s

• [SLOW TEST:18.650 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:05:24.968: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:05:25.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2927" for this suite.
Sep  5 21:05:47.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:05:47.224: INFO: namespace pods-2927 deletion completed in 22.101570011s

• [SLOW TEST:22.256 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:05:47.224: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:05:47.359: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  5 21:05:47.365: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  5 21:05:52.369: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  5 21:05:52.369: INFO: Creating deployment "test-rolling-update-deployment"
Sep  5 21:05:52.374: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  5 21:05:52.381: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  5 21:05:54.388: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  5 21:05:54.391: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  5 21:05:54.401: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9957,SelfLink:/apis/apps/v1/namespaces/deployment-9957/deployments/test-rolling-update-deployment,UID:f1ad855b-d020-11e9-9b5e-2c600c82ec72,ResourceVersion:9210,Generation:1,CreationTimestamp:2019-09-05 21:05:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-05 21:05:52 +0000 UTC 2019-09-05 21:05:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-05 21:05:53 +0000 UTC 2019-09-05 21:05:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  5 21:05:54.405: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-9957,SelfLink:/apis/apps/v1/namespaces/deployment-9957/replicasets/test-rolling-update-deployment-67599b4d9,UID:f1af474b-d020-11e9-9b5e-2c600c82ec72,ResourceVersion:9199,Generation:1,CreationTimestamp:2019-09-05 21:05:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f1ad855b-d020-11e9-9b5e-2c600c82ec72 0xc00295e0e0 0xc00295e0e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  5 21:05:54.405: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  5 21:05:54.405: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9957,SelfLink:/apis/apps/v1/namespaces/deployment-9957/replicasets/test-rolling-update-controller,UID:eeb0f7c8-d020-11e9-9b5e-2c600c82ec72,ResourceVersion:9208,Generation:2,CreationTimestamp:2019-09-05 21:05:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f1ad855b-d020-11e9-9b5e-2c600c82ec72 0xc00295e00f 0xc00295e020}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 21:05:54.409: INFO: Pod "test-rolling-update-deployment-67599b4d9-jhshf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-jhshf,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-9957,SelfLink:/api/v1/namespaces/deployment-9957/pods/test-rolling-update-deployment-67599b4d9-jhshf,UID:f1afe5ac-d020-11e9-9b5e-2c600c82ec72,ResourceVersion:9198,Generation:0,CreationTimestamp:2019-09-05 21:05:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 f1af474b-d020-11e9-9b5e-2c600c82ec72 0xc00295e960 0xc00295e961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-w9j6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w9j6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-w9j6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00295e9d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00295e9f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:05:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:05:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:05:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:05:52 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.12,StartTime:2019-09-05 21:05:52 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-05 21:05:53 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://8d69e2cd616ca877521fd244e743e56f698de8271500bd25425845bdde7aac75}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:05:54.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9957" for this suite.
Sep  5 21:06:00.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:06:00.526: INFO: namespace deployment-9957 deletion completed in 6.112713183s

• [SLOW TEST:13.302 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:06:00.526: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0905 21:06:10.721222      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 21:06:10.721: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:06:10.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9206" for this suite.
Sep  5 21:06:16.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:06:16.825: INFO: namespace gc-9206 deletion completed in 6.099822695s

• [SLOW TEST:16.299 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:06:16.825: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6673
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6673
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6673
Sep  5 21:06:16.971: INFO: Found 0 stateful pods, waiting for 1
Sep  5 21:06:26.975: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  5 21:06:26.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-6673 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:06:27.205: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:06:27.205: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:06:27.205: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 21:06:27.208: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  5 21:06:37.212: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:06:37.212: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:06:37.224: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999546s
Sep  5 21:06:38.228: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996965452s
Sep  5 21:06:39.232: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993102168s
Sep  5 21:06:40.236: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988951096s
Sep  5 21:06:41.240: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985039094s
Sep  5 21:06:42.244: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.981286579s
Sep  5 21:06:43.248: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977754177s
Sep  5 21:06:44.252: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973507257s
Sep  5 21:06:45.256: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969174573s
Sep  5 21:06:46.260: INFO: Verifying statefulset ss doesn't scale past 1 for another 965.090146ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6673
Sep  5 21:06:47.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-6673 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:06:47.477: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 21:06:47.477: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 21:06:47.477: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 21:06:47.480: INFO: Found 1 stateful pods, waiting for 3
Sep  5 21:06:57.484: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:06:57.485: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:06:57.485: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  5 21:06:57.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-6673 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:06:57.704: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:06:57.704: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:06:57.704: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 21:06:57.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-6673 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:06:57.917: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:06:57.917: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:06:57.917: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 21:06:57.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-6673 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:06:58.152: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:06:58.152: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:06:58.152: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 21:06:58.152: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:06:58.155: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  5 21:07:08.163: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:07:08.163: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:07:08.163: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:07:08.173: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99996438s
Sep  5 21:07:09.177: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996600135s
Sep  5 21:07:10.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99227076s
Sep  5 21:07:11.186: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98804565s
Sep  5 21:07:12.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98357472s
Sep  5 21:07:13.194: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979592588s
Sep  5 21:07:14.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975219803s
Sep  5 21:07:15.203: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970781938s
Sep  5 21:07:16.207: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966408087s
Sep  5 21:07:17.211: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.627506ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6673
Sep  5 21:07:18.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-6673 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:07:18.440: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 21:07:18.440: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 21:07:18.440: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 21:07:18.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-6673 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:07:18.657: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 21:07:18.657: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 21:07:18.657: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 21:07:18.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-6673 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:07:18.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 21:07:18.882: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 21:07:18.882: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 21:07:18.882: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  5 21:07:48.899: INFO: Deleting all statefulset in ns statefulset-6673
Sep  5 21:07:48.902: INFO: Scaling statefulset ss to 0
Sep  5 21:07:48.913: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:07:48.916: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:07:48.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6673" for this suite.
Sep  5 21:07:54.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:07:55.034: INFO: namespace statefulset-6673 deletion completed in 6.103017484s

• [SLOW TEST:98.209 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:07:55.035: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:07:55.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ae31d44-d021-11e9-a0ad-8693e9898db7" in namespace "downward-api-607" to be "success or failure"
Sep  5 21:07:55.208: INFO: Pod "downwardapi-volume-3ae31d44-d021-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901366ms
Sep  5 21:07:57.212: INFO: Pod "downwardapi-volume-3ae31d44-d021-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007109341s
STEP: Saw pod success
Sep  5 21:07:57.212: INFO: Pod "downwardapi-volume-3ae31d44-d021-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:07:57.215: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-3ae31d44-d021-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:07:57.233: INFO: Waiting for pod downwardapi-volume-3ae31d44-d021-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:07:57.235: INFO: Pod downwardapi-volume-3ae31d44-d021-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:07:57.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-607" for this suite.
Sep  5 21:08:03.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:08:03.346: INFO: namespace downward-api-607 deletion completed in 6.106818293s

• [SLOW TEST:8.311 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:08:03.346: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8296
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep  5 21:08:03.496: INFO: Found 0 stateful pods, waiting for 3
Sep  5 21:08:13.501: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:08:13.502: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:08:13.502: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:08:13.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-8296 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:08:13.742: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:08:13.742: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:08:13.742: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  5 21:08:23.776: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  5 21:08:33.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-8296 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:08:34.021: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 21:08:34.021: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 21:08:34.021: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 21:09:04.044: INFO: Waiting for StatefulSet statefulset-8296/ss2 to complete update
Sep  5 21:09:04.044: INFO: Waiting for Pod statefulset-8296/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Sep  5 21:09:14.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-8296 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:09:14.291: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:09:14.291: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:09:14.291: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 21:09:24.325: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  5 21:09:34.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-8296 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:09:34.566: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 21:09:34.566: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 21:09:34.566: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 21:09:44.590: INFO: Waiting for StatefulSet statefulset-8296/ss2 to complete update
Sep  5 21:09:44.591: INFO: Waiting for Pod statefulset-8296/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Sep  5 21:09:44.591: INFO: Waiting for Pod statefulset-8296/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Sep  5 21:09:44.591: INFO: Waiting for Pod statefulset-8296/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Sep  5 21:09:54.599: INFO: Waiting for StatefulSet statefulset-8296/ss2 to complete update
Sep  5 21:09:54.600: INFO: Waiting for Pod statefulset-8296/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Sep  5 21:09:54.600: INFO: Waiting for Pod statefulset-8296/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Sep  5 21:10:04.599: INFO: Waiting for StatefulSet statefulset-8296/ss2 to complete update
Sep  5 21:10:04.599: INFO: Waiting for Pod statefulset-8296/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Sep  5 21:10:14.599: INFO: Waiting for StatefulSet statefulset-8296/ss2 to complete update
Sep  5 21:10:14.599: INFO: Waiting for Pod statefulset-8296/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  5 21:10:24.599: INFO: Deleting all statefulset in ns statefulset-8296
Sep  5 21:10:24.602: INFO: Scaling statefulset ss2 to 0
Sep  5 21:10:54.618: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:10:54.622: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:10:54.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8296" for this suite.
Sep  5 21:11:00.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:11:00.755: INFO: namespace statefulset-8296 deletion completed in 6.115837699s

• [SLOW TEST:177.408 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:11:00.755: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Sep  5 21:11:02.914: INFO: Pod pod-hostip-a99246b2-d021-11e9-a0ad-8693e9898db7 has hostIP: 172.16.6.109
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:11:02.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4335" for this suite.
Sep  5 21:11:24.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:11:25.027: INFO: namespace pods-4335 deletion completed in 22.108726704s

• [SLOW TEST:24.272 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:11:25.028: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-cn997 in namespace proxy-3285
I0905 21:11:25.176837      18 runners.go:184] Created replication controller with name: proxy-service-cn997, namespace: proxy-3285, replica count: 1
I0905 21:11:26.227273      18 runners.go:184] proxy-service-cn997 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0905 21:11:27.227471      18 runners.go:184] proxy-service-cn997 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 21:11:28.227702      18 runners.go:184] proxy-service-cn997 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  5 21:11:28.231: INFO: setup took 3.065831801s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  5 21:11:28.238: INFO: (0) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 7.011789ms)
Sep  5 21:11:28.238: INFO: (0) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 7.219112ms)
Sep  5 21:11:28.238: INFO: (0) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 7.207867ms)
Sep  5 21:11:28.238: INFO: (0) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 7.381834ms)
Sep  5 21:11:28.238: INFO: (0) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 7.260223ms)
Sep  5 21:11:28.238: INFO: (0) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 7.434666ms)
Sep  5 21:11:28.238: INFO: (0) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 7.382076ms)
Sep  5 21:11:28.239: INFO: (0) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 7.528216ms)
Sep  5 21:11:28.239: INFO: (0) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 7.79522ms)
Sep  5 21:11:28.240: INFO: (0) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 8.795511ms)
Sep  5 21:11:28.240: INFO: (0) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 8.750693ms)
Sep  5 21:11:28.249: INFO: (0) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 17.806454ms)
Sep  5 21:11:28.249: INFO: (0) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 17.97401ms)
Sep  5 21:11:28.250: INFO: (0) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 18.377442ms)
Sep  5 21:11:28.254: INFO: (0) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 22.864104ms)
Sep  5 21:11:28.254: INFO: (0) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 23.101785ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.707084ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.695099ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.772119ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.719099ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.851432ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.695402ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.771134ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.77025ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.71503ms)
Sep  5 21:11:28.259: INFO: (1) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 4.737993ms)
Sep  5 21:11:28.260: INFO: (1) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.166834ms)
Sep  5 21:11:28.260: INFO: (1) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.412634ms)
Sep  5 21:11:28.260: INFO: (1) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.914152ms)
Sep  5 21:11:28.260: INFO: (1) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 6.087483ms)
Sep  5 21:11:28.261: INFO: (1) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 6.202834ms)
Sep  5 21:11:28.261: INFO: (1) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 6.071524ms)
Sep  5 21:11:28.264: INFO: (2) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 3.105078ms)
Sep  5 21:11:28.265: INFO: (2) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 3.91355ms)
Sep  5 21:11:28.265: INFO: (2) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.160081ms)
Sep  5 21:11:28.265: INFO: (2) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.201285ms)
Sep  5 21:11:28.265: INFO: (2) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.203718ms)
Sep  5 21:11:28.265: INFO: (2) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.261986ms)
Sep  5 21:11:28.265: INFO: (2) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.202246ms)
Sep  5 21:11:28.265: INFO: (2) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.169411ms)
Sep  5 21:11:28.265: INFO: (2) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.267206ms)
Sep  5 21:11:28.265: INFO: (2) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.298309ms)
Sep  5 21:11:28.266: INFO: (2) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 5.509232ms)
Sep  5 21:11:28.266: INFO: (2) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 5.49663ms)
Sep  5 21:11:28.267: INFO: (2) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 6.084174ms)
Sep  5 21:11:28.267: INFO: (2) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 6.298391ms)
Sep  5 21:11:28.267: INFO: (2) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 6.311601ms)
Sep  5 21:11:28.267: INFO: (2) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 6.422777ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.79458ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.727943ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.583025ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.74408ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.784782ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 4.765131ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.856005ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.735354ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.759638ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.824628ms)
Sep  5 21:11:28.272: INFO: (3) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.138672ms)
Sep  5 21:11:28.273: INFO: (3) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 5.93181ms)
Sep  5 21:11:28.273: INFO: (3) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 6.033339ms)
Sep  5 21:11:28.273: INFO: (3) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 6.08026ms)
Sep  5 21:11:28.273: INFO: (3) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.964911ms)
Sep  5 21:11:28.273: INFO: (3) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 6.098796ms)
Sep  5 21:11:28.277: INFO: (4) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 3.545746ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.272951ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.257934ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 4.303123ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.292235ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.399491ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.386045ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.455325ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.362488ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.374205ms)
Sep  5 21:11:28.278: INFO: (4) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 4.810711ms)
Sep  5 21:11:28.279: INFO: (4) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 6.098135ms)
Sep  5 21:11:28.279: INFO: (4) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 6.102969ms)
Sep  5 21:11:28.280: INFO: (4) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 6.118862ms)
Sep  5 21:11:28.280: INFO: (4) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 6.093559ms)
Sep  5 21:11:28.280: INFO: (4) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 6.149733ms)
Sep  5 21:11:28.283: INFO: (5) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 3.339451ms)
Sep  5 21:11:28.284: INFO: (5) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 4.025311ms)
Sep  5 21:11:28.284: INFO: (5) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.499038ms)
Sep  5 21:11:28.284: INFO: (5) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.587779ms)
Sep  5 21:11:28.284: INFO: (5) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.669786ms)
Sep  5 21:11:28.284: INFO: (5) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.674757ms)
Sep  5 21:11:28.284: INFO: (5) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.654054ms)
Sep  5 21:11:28.284: INFO: (5) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.637917ms)
Sep  5 21:11:28.284: INFO: (5) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.651414ms)
Sep  5 21:11:28.284: INFO: (5) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.694348ms)
Sep  5 21:11:28.285: INFO: (5) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.584636ms)
Sep  5 21:11:28.286: INFO: (5) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 6.456376ms)
Sep  5 21:11:28.286: INFO: (5) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 6.306667ms)
Sep  5 21:11:28.286: INFO: (5) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 6.456683ms)
Sep  5 21:11:28.286: INFO: (5) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 6.4647ms)
Sep  5 21:11:28.286: INFO: (5) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 6.801582ms)
Sep  5 21:11:28.290: INFO: (6) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 3.420442ms)
Sep  5 21:11:28.290: INFO: (6) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 3.792053ms)
Sep  5 21:11:28.291: INFO: (6) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.120701ms)
Sep  5 21:11:28.291: INFO: (6) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.028906ms)
Sep  5 21:11:28.291: INFO: (6) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 4.073919ms)
Sep  5 21:11:28.291: INFO: (6) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.314167ms)
Sep  5 21:11:28.291: INFO: (6) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.264078ms)
Sep  5 21:11:28.291: INFO: (6) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.318514ms)
Sep  5 21:11:28.291: INFO: (6) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.348456ms)
Sep  5 21:11:28.291: INFO: (6) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.332177ms)
Sep  5 21:11:28.292: INFO: (6) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.00574ms)
Sep  5 21:11:28.293: INFO: (6) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 6.147436ms)
Sep  5 21:11:28.293: INFO: (6) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.861743ms)
Sep  5 21:11:28.293: INFO: (6) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 6.316741ms)
Sep  5 21:11:28.293: INFO: (6) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 6.252102ms)
Sep  5 21:11:28.293: INFO: (6) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 6.341934ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.446119ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.402172ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.363497ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.416048ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 4.492743ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.399839ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.507474ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.45059ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.501229ms)
Sep  5 21:11:28.297: INFO: (7) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.402297ms)
Sep  5 21:11:28.298: INFO: (7) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.250631ms)
Sep  5 21:11:28.299: INFO: (7) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.587816ms)
Sep  5 21:11:28.299: INFO: (7) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.730807ms)
Sep  5 21:11:28.299: INFO: (7) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 6.399235ms)
Sep  5 21:11:28.299: INFO: (7) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 6.414839ms)
Sep  5 21:11:28.299: INFO: (7) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 6.403128ms)
Sep  5 21:11:28.303: INFO: (8) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 3.808435ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.334155ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.365826ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.419051ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.393882ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.466268ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.442969ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.508134ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.449516ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.479344ms)
Sep  5 21:11:28.304: INFO: (8) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 4.814136ms)
Sep  5 21:11:28.305: INFO: (8) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.60174ms)
Sep  5 21:11:28.305: INFO: (8) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.638636ms)
Sep  5 21:11:28.305: INFO: (8) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 5.797463ms)
Sep  5 21:11:28.305: INFO: (8) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.760296ms)
Sep  5 21:11:28.306: INFO: (8) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 6.242098ms)
Sep  5 21:11:28.310: INFO: (9) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.494119ms)
Sep  5 21:11:28.310: INFO: (9) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.532588ms)
Sep  5 21:11:28.310: INFO: (9) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.420625ms)
Sep  5 21:11:28.310: INFO: (9) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.532269ms)
Sep  5 21:11:28.310: INFO: (9) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.566009ms)
Sep  5 21:11:28.310: INFO: (9) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.526852ms)
Sep  5 21:11:28.310: INFO: (9) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 4.542199ms)
Sep  5 21:11:28.310: INFO: (9) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.526824ms)
Sep  5 21:11:28.311: INFO: (9) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.684457ms)
Sep  5 21:11:28.311: INFO: (9) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.585824ms)
Sep  5 21:11:28.311: INFO: (9) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.049867ms)
Sep  5 21:11:28.312: INFO: (9) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 5.686952ms)
Sep  5 21:11:28.312: INFO: (9) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.746701ms)
Sep  5 21:11:28.312: INFO: (9) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.90573ms)
Sep  5 21:11:28.312: INFO: (9) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.891822ms)
Sep  5 21:11:28.312: INFO: (9) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 5.944102ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.091424ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 4.294459ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.249066ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.320512ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.297198ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.330924ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.409617ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.43404ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.347072ms)
Sep  5 21:11:28.316: INFO: (10) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.3839ms)
Sep  5 21:11:28.317: INFO: (10) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 4.884049ms)
Sep  5 21:11:28.318: INFO: (10) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.633007ms)
Sep  5 21:11:28.318: INFO: (10) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.801646ms)
Sep  5 21:11:28.318: INFO: (10) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 5.910533ms)
Sep  5 21:11:28.318: INFO: (10) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.90143ms)
Sep  5 21:11:28.318: INFO: (10) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.94991ms)
Sep  5 21:11:28.321: INFO: (11) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 3.070402ms)
Sep  5 21:11:28.322: INFO: (11) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 3.824603ms)
Sep  5 21:11:28.322: INFO: (11) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 3.892954ms)
Sep  5 21:11:28.322: INFO: (11) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 3.952368ms)
Sep  5 21:11:28.322: INFO: (11) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 3.983534ms)
Sep  5 21:11:28.322: INFO: (11) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 3.98595ms)
Sep  5 21:11:28.322: INFO: (11) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 3.917057ms)
Sep  5 21:11:28.323: INFO: (11) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.595772ms)
Sep  5 21:11:28.323: INFO: (11) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.612276ms)
Sep  5 21:11:28.323: INFO: (11) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.617034ms)
Sep  5 21:11:28.323: INFO: (11) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.471067ms)
Sep  5 21:11:28.323: INFO: (11) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.424585ms)
Sep  5 21:11:28.324: INFO: (11) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.599331ms)
Sep  5 21:11:28.324: INFO: (11) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 5.529127ms)
Sep  5 21:11:28.324: INFO: (11) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 5.627755ms)
Sep  5 21:11:28.324: INFO: (11) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.595072ms)
Sep  5 21:11:28.327: INFO: (12) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 3.068418ms)
Sep  5 21:11:28.328: INFO: (12) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 3.747896ms)
Sep  5 21:11:28.328: INFO: (12) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 3.991099ms)
Sep  5 21:11:28.328: INFO: (12) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.004678ms)
Sep  5 21:11:28.328: INFO: (12) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.075968ms)
Sep  5 21:11:28.328: INFO: (12) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.160578ms)
Sep  5 21:11:28.328: INFO: (12) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.042043ms)
Sep  5 21:11:28.328: INFO: (12) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.029148ms)
Sep  5 21:11:28.328: INFO: (12) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.1762ms)
Sep  5 21:11:28.328: INFO: (12) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.171622ms)
Sep  5 21:11:28.329: INFO: (12) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.129951ms)
Sep  5 21:11:28.330: INFO: (12) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 6.135827ms)
Sep  5 21:11:28.330: INFO: (12) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 6.179869ms)
Sep  5 21:11:28.330: INFO: (12) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 6.183688ms)
Sep  5 21:11:28.330: INFO: (12) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 6.211635ms)
Sep  5 21:11:28.330: INFO: (12) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 6.275927ms)
Sep  5 21:11:28.333: INFO: (13) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 3.178857ms)
Sep  5 21:11:28.334: INFO: (13) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 3.612454ms)
Sep  5 21:11:28.334: INFO: (13) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 3.995991ms)
Sep  5 21:11:28.334: INFO: (13) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 3.954334ms)
Sep  5 21:11:28.334: INFO: (13) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 3.989657ms)
Sep  5 21:11:28.334: INFO: (13) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.00621ms)
Sep  5 21:11:28.334: INFO: (13) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.017506ms)
Sep  5 21:11:28.334: INFO: (13) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.150047ms)
Sep  5 21:11:28.334: INFO: (13) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.023334ms)
Sep  5 21:11:28.334: INFO: (13) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.175502ms)
Sep  5 21:11:28.335: INFO: (13) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 5.284713ms)
Sep  5 21:11:28.336: INFO: (13) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.916304ms)
Sep  5 21:11:28.336: INFO: (13) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 6.03285ms)
Sep  5 21:11:28.336: INFO: (13) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.968523ms)
Sep  5 21:11:28.336: INFO: (13) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.981828ms)
Sep  5 21:11:28.336: INFO: (13) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 6.005346ms)
Sep  5 21:11:28.339: INFO: (14) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 3.086303ms)
Sep  5 21:11:28.340: INFO: (14) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 3.828024ms)
Sep  5 21:11:28.340: INFO: (14) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.106152ms)
Sep  5 21:11:28.340: INFO: (14) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 4.060561ms)
Sep  5 21:11:28.340: INFO: (14) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 4.147971ms)
Sep  5 21:11:28.340: INFO: (14) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.219828ms)
Sep  5 21:11:28.341: INFO: (14) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.186872ms)
Sep  5 21:11:28.341: INFO: (14) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.202881ms)
Sep  5 21:11:28.341: INFO: (14) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.352441ms)
Sep  5 21:11:28.341: INFO: (14) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.362537ms)
Sep  5 21:11:28.342: INFO: (14) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.367599ms)
Sep  5 21:11:28.343: INFO: (14) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 6.211609ms)
Sep  5 21:11:28.343: INFO: (14) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 6.325482ms)
Sep  5 21:11:28.343: INFO: (14) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 6.338799ms)
Sep  5 21:11:28.343: INFO: (14) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 6.394072ms)
Sep  5 21:11:28.343: INFO: (14) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 6.388461ms)
Sep  5 21:11:28.346: INFO: (15) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 3.397639ms)
Sep  5 21:11:28.347: INFO: (15) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 4.12881ms)
Sep  5 21:11:28.347: INFO: (15) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 4.225375ms)
Sep  5 21:11:28.347: INFO: (15) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 4.341901ms)
Sep  5 21:11:28.348: INFO: (15) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 4.647572ms)
Sep  5 21:11:28.348: INFO: (15) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.862762ms)
Sep  5 21:11:28.348: INFO: (15) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 5.15726ms)
Sep  5 21:11:28.348: INFO: (15) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 5.19824ms)
Sep  5 21:11:28.348: INFO: (15) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 5.355519ms)
Sep  5 21:11:28.348: INFO: (15) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 5.434492ms)
Sep  5 21:11:28.348: INFO: (15) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.592664ms)
Sep  5 21:11:28.349: INFO: (15) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 5.991058ms)
Sep  5 21:11:28.349: INFO: (15) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 6.087105ms)
Sep  5 21:11:28.349: INFO: (15) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 6.211778ms)
Sep  5 21:11:28.349: INFO: (15) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 6.183765ms)
Sep  5 21:11:28.349: INFO: (15) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 6.142165ms)
Sep  5 21:11:28.352: INFO: (16) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 2.946959ms)
Sep  5 21:11:28.353: INFO: (16) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 3.765054ms)
Sep  5 21:11:28.353: INFO: (16) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 3.903118ms)
Sep  5 21:11:28.353: INFO: (16) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 3.927909ms)
Sep  5 21:11:28.353: INFO: (16) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 3.963404ms)
Sep  5 21:11:28.353: INFO: (16) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 3.929159ms)
Sep  5 21:11:28.353: INFO: (16) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 3.93396ms)
Sep  5 21:11:28.353: INFO: (16) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 4.035821ms)
Sep  5 21:11:28.353: INFO: (16) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 3.941869ms)
Sep  5 21:11:28.353: INFO: (16) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 3.908255ms)
Sep  5 21:11:28.354: INFO: (16) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 4.725689ms)
Sep  5 21:11:28.355: INFO: (16) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 5.613572ms)
Sep  5 21:11:28.355: INFO: (16) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 5.714724ms)
Sep  5 21:11:28.355: INFO: (16) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.63051ms)
Sep  5 21:11:28.355: INFO: (16) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.70281ms)
Sep  5 21:11:28.355: INFO: (16) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.717363ms)
Sep  5 21:11:28.358: INFO: (17) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 3.014674ms)
Sep  5 21:11:28.359: INFO: (17) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 3.743804ms)
Sep  5 21:11:28.359: INFO: (17) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 3.939809ms)
Sep  5 21:11:28.359: INFO: (17) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 3.864307ms)
Sep  5 21:11:28.359: INFO: (17) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 3.968645ms)
Sep  5 21:11:28.359: INFO: (17) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 3.903467ms)
Sep  5 21:11:28.359: INFO: (17) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 3.879963ms)
Sep  5 21:11:28.359: INFO: (17) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 3.893391ms)
Sep  5 21:11:28.359: INFO: (17) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 3.868897ms)
Sep  5 21:11:28.359: INFO: (17) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 3.877539ms)
Sep  5 21:11:28.360: INFO: (17) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 4.773924ms)
Sep  5 21:11:28.361: INFO: (17) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.437106ms)
Sep  5 21:11:28.361: INFO: (17) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.505947ms)
Sep  5 21:11:28.361: INFO: (17) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 5.619948ms)
Sep  5 21:11:28.361: INFO: (17) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.593019ms)
Sep  5 21:11:28.361: INFO: (17) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 5.683241ms)
Sep  5 21:11:28.364: INFO: (18) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 2.828368ms)
Sep  5 21:11:28.364: INFO: (18) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 3.606783ms)
Sep  5 21:11:28.364: INFO: (18) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 3.630722ms)
Sep  5 21:11:28.364: INFO: (18) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 3.743487ms)
Sep  5 21:11:28.364: INFO: (18) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 3.682424ms)
Sep  5 21:11:28.365: INFO: (18) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 3.824654ms)
Sep  5 21:11:28.365: INFO: (18) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 3.776199ms)
Sep  5 21:11:28.365: INFO: (18) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 3.871741ms)
Sep  5 21:11:28.365: INFO: (18) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 3.863714ms)
Sep  5 21:11:28.365: INFO: (18) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 3.835607ms)
Sep  5 21:11:28.366: INFO: (18) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 4.747668ms)
Sep  5 21:11:28.366: INFO: (18) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 5.616089ms)
Sep  5 21:11:28.367: INFO: (18) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 5.673978ms)
Sep  5 21:11:28.367: INFO: (18) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 5.682529ms)
Sep  5 21:11:28.367: INFO: (18) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 5.682549ms)
Sep  5 21:11:28.367: INFO: (18) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 5.665533ms)
Sep  5 21:11:28.369: INFO: (19) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:160/proxy/: foo (200; 2.663406ms)
Sep  5 21:11:28.370: INFO: (19) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:162/proxy/: bar (200; 3.269402ms)
Sep  5 21:11:28.370: INFO: (19) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:162/proxy/: bar (200; 3.621555ms)
Sep  5 21:11:28.370: INFO: (19) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:462/proxy/: tls qux (200; 3.742616ms)
Sep  5 21:11:28.370: INFO: (19) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:160/proxy/: foo (200; 3.752875ms)
Sep  5 21:11:28.370: INFO: (19) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl:1080/proxy/rewriteme">test<... (200; 3.741131ms)
Sep  5 21:11:28.371: INFO: (19) /api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/http:proxy-service-cn997-86twl:1080/proxy/rewriteme">... (200; 3.919579ms)
Sep  5 21:11:28.371: INFO: (19) /api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/proxy-service-cn997-86twl/proxy/rewriteme">test</a> (200; 3.970858ms)
Sep  5 21:11:28.371: INFO: (19) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:460/proxy/: tls baz (200; 4.122439ms)
Sep  5 21:11:28.371: INFO: (19) /api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/: <a href="/api/v1/namespaces/proxy-3285/pods/https:proxy-service-cn997-86twl:443/proxy/tlsrewritem... (200; 4.263299ms)
Sep  5 21:11:28.372: INFO: (19) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname2/proxy/: bar (200; 4.865839ms)
Sep  5 21:11:28.372: INFO: (19) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname2/proxy/: tls qux (200; 4.90144ms)
Sep  5 21:11:28.372: INFO: (19) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname1/proxy/: foo (200; 4.81292ms)
Sep  5 21:11:28.372: INFO: (19) /api/v1/namespaces/proxy-3285/services/http:proxy-service-cn997:portname2/proxy/: bar (200; 4.85242ms)
Sep  5 21:11:28.372: INFO: (19) /api/v1/namespaces/proxy-3285/services/proxy-service-cn997:portname1/proxy/: foo (200; 4.828854ms)
Sep  5 21:11:28.372: INFO: (19) /api/v1/namespaces/proxy-3285/services/https:proxy-service-cn997:tlsportname1/proxy/: tls baz (200; 4.935537ms)
STEP: deleting ReplicationController proxy-service-cn997 in namespace proxy-3285, will wait for the garbage collector to delete the pods
Sep  5 21:11:28.431: INFO: Deleting ReplicationController proxy-service-cn997 took: 6.29531ms
Sep  5 21:11:28.931: INFO: Terminating ReplicationController proxy-service-cn997 pods took: 500.229848ms
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:11:38.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3285" for this suite.
Sep  5 21:11:44.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:11:44.560: INFO: namespace proxy-3285 deletion completed in 6.124398387s

• [SLOW TEST:19.532 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:11:44.560: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:11:44.706: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3ae4c58-d021-11e9-a0ad-8693e9898db7" in namespace "downward-api-1302" to be "success or failure"
Sep  5 21:11:44.709: INFO: Pod "downwardapi-volume-c3ae4c58-d021-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.998194ms
Sep  5 21:11:46.713: INFO: Pod "downwardapi-volume-c3ae4c58-d021-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006499909s
STEP: Saw pod success
Sep  5 21:11:46.713: INFO: Pod "downwardapi-volume-c3ae4c58-d021-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:11:46.715: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-c3ae4c58-d021-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:11:46.731: INFO: Waiting for pod downwardapi-volume-c3ae4c58-d021-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:11:46.733: INFO: Pod downwardapi-volume-c3ae4c58-d021-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:11:46.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1302" for this suite.
Sep  5 21:11:52.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:11:52.849: INFO: namespace downward-api-1302 deletion completed in 6.112714794s

• [SLOW TEST:8.290 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:11:52.850: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7874
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-c89fde63-d021-11e9-a0ad-8693e9898db7
STEP: Creating configMap with name cm-test-opt-upd-c89fdebc-d021-11e9-a0ad-8693e9898db7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c89fde63-d021-11e9-a0ad-8693e9898db7
STEP: Updating configmap cm-test-opt-upd-c89fdebc-d021-11e9-a0ad-8693e9898db7
STEP: Creating configMap with name cm-test-opt-create-c89fdee9-d021-11e9-a0ad-8693e9898db7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:13:03.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7874" for this suite.
Sep  5 21:13:25.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:13:25.538: INFO: namespace configmap-7874 deletion completed in 22.102197587s

• [SLOW TEST:92.688 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:13:25.538: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  5 21:13:27.694: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-ffde064f-d021-11e9-a0ad-8693e9898db7,GenerateName:,Namespace:events-3637,SelfLink:/api/v1/namespaces/events-3637/pods/send-events-ffde064f-d021-11e9-a0ad-8693e9898db7,UID:ffdeb5ee-d021-11e9-9b5e-2c600c82ec72,ResourceVersion:11220,Generation:0,CreationTimestamp:2019-09-05 21:13:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 674969137,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-667ss {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-667ss,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-667ss true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028a7420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028a7440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:13:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:13:27 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:13:27 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:13:25 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.11,StartTime:2019-09-05 21:13:25 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-05 21:13:26 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://99e26908072a271ec7d33c966ca3902dcc18fc317b3b57365335b1e70e21315e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep  5 21:13:29.698: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  5 21:13:31.702: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:13:31.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3637" for this suite.
Sep  5 21:14:13.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:14:13.825: INFO: namespace events-3637 deletion completed in 42.111399442s

• [SLOW TEST:48.287 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:14:13.826: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:14:13.989: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1ca8d910-d022-11e9-9b5e-2c600c82ec72", Controller:(*bool)(0xc0024aa756), BlockOwnerDeletion:(*bool)(0xc0024aa757)}}
Sep  5 21:14:13.993: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1ca75106-d022-11e9-9b5e-2c600c82ec72", Controller:(*bool)(0xc002c1d186), BlockOwnerDeletion:(*bool)(0xc002c1d187)}}
Sep  5 21:14:13.998: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1ca8116b-d022-11e9-9b5e-2c600c82ec72", Controller:(*bool)(0xc002fac3f6), BlockOwnerDeletion:(*bool)(0xc002fac3f7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:14:19.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2589" for this suite.
Sep  5 21:14:25.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:14:25.118: INFO: namespace gc-2589 deletion completed in 6.10726508s

• [SLOW TEST:11.293 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:14:25.119: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5413
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  5 21:14:25.265: INFO: Waiting up to 5m0s for pod "downward-api-236188ca-d022-11e9-a0ad-8693e9898db7" in namespace "downward-api-5413" to be "success or failure"
Sep  5 21:14:25.268: INFO: Pod "downward-api-236188ca-d022-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.219656ms
Sep  5 21:14:27.273: INFO: Pod "downward-api-236188ca-d022-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008271655s
STEP: Saw pod success
Sep  5 21:14:27.274: INFO: Pod "downward-api-236188ca-d022-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:14:27.277: INFO: Trying to get logs from node appserv11 pod downward-api-236188ca-d022-11e9-a0ad-8693e9898db7 container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:14:27.303: INFO: Waiting for pod downward-api-236188ca-d022-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:14:27.305: INFO: Pod downward-api-236188ca-d022-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:14:27.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5413" for this suite.
Sep  5 21:14:33.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:14:33.419: INFO: namespace downward-api-5413 deletion completed in 6.109889061s

• [SLOW TEST:8.300 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:14:33.419: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:14:35.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8316" for this suite.
Sep  5 21:15:21.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:15:21.703: INFO: namespace kubelet-test-8316 deletion completed in 46.113524075s

• [SLOW TEST:48.284 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:15:21.703: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Sep  5 21:15:21.843: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-156594675 proxy --unix-socket=/tmp/kubectl-proxy-unix098915914/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:15:21.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1840" for this suite.
Sep  5 21:15:27.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:15:28.050: INFO: namespace kubectl-1840 deletion completed in 6.103634847s

• [SLOW TEST:6.347 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:15:28.050: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 21:15:28.216: INFO: Number of nodes with available pods: 0
Sep  5 21:15:28.216: INFO: Node appserv10 is running more than one daemon pod
Sep  5 21:15:29.225: INFO: Number of nodes with available pods: 0
Sep  5 21:15:29.225: INFO: Node appserv10 is running more than one daemon pod
Sep  5 21:15:30.224: INFO: Number of nodes with available pods: 3
Sep  5 21:15:30.224: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  5 21:15:30.244: INFO: Number of nodes with available pods: 2
Sep  5 21:15:30.244: INFO: Node appserv9 is running more than one daemon pod
Sep  5 21:15:31.252: INFO: Number of nodes with available pods: 2
Sep  5 21:15:31.252: INFO: Node appserv9 is running more than one daemon pod
Sep  5 21:15:32.252: INFO: Number of nodes with available pods: 3
Sep  5 21:15:32.252: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7524, will wait for the garbage collector to delete the pods
Sep  5 21:15:32.319: INFO: Deleting DaemonSet.extensions daemon-set took: 7.173374ms
Sep  5 21:15:32.819: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.255801ms
Sep  5 21:15:42.422: INFO: Number of nodes with available pods: 0
Sep  5 21:15:42.423: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 21:15:42.426: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7524/daemonsets","resourceVersion":"11720"},"items":null}

Sep  5 21:15:42.428: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7524/pods","resourceVersion":"11720"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:15:42.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7524" for this suite.
Sep  5 21:15:48.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:15:48.560: INFO: namespace daemonsets-7524 deletion completed in 6.112025748s

• [SLOW TEST:20.511 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:15:48.561: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-44
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:15:48.722: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 21:15:48.734: INFO: Number of nodes with available pods: 0
Sep  5 21:15:48.734: INFO: Node appserv10 is running more than one daemon pod
Sep  5 21:15:49.742: INFO: Number of nodes with available pods: 0
Sep  5 21:15:49.742: INFO: Node appserv10 is running more than one daemon pod
Sep  5 21:15:50.742: INFO: Number of nodes with available pods: 3
Sep  5 21:15:50.742: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  5 21:15:50.768: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:50.768: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:50.768: INFO: Wrong image for pod: daemon-set-x8w7n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:51.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:51.776: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:51.776: INFO: Wrong image for pod: daemon-set-x8w7n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:52.777: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:52.777: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:52.777: INFO: Wrong image for pod: daemon-set-x8w7n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:53.778: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:53.778: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:53.778: INFO: Wrong image for pod: daemon-set-x8w7n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:54.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:54.776: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:54.776: INFO: Wrong image for pod: daemon-set-x8w7n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:55.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:55.776: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:55.776: INFO: Wrong image for pod: daemon-set-x8w7n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:56.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:56.776: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:56.776: INFO: Wrong image for pod: daemon-set-x8w7n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:56.776: INFO: Pod daemon-set-x8w7n is not available
Sep  5 21:15:57.775: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:57.775: INFO: Pod daemon-set-85qkn is not available
Sep  5 21:15:57.775: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:58.777: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:58.777: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:59.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:15:59.776: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:00.777: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:00.777: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:01.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:01.776: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:01.776: INFO: Pod daemon-set-k6gdg is not available
Sep  5 21:16:02.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:02.776: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:02.776: INFO: Pod daemon-set-k6gdg is not available
Sep  5 21:16:03.777: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:03.777: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:03.777: INFO: Pod daemon-set-k6gdg is not available
Sep  5 21:16:04.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:04.777: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:04.777: INFO: Pod daemon-set-k6gdg is not available
Sep  5 21:16:05.778: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:05.778: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:05.778: INFO: Pod daemon-set-k6gdg is not available
Sep  5 21:16:06.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:06.776: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:06.776: INFO: Pod daemon-set-k6gdg is not available
Sep  5 21:16:07.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:07.776: INFO: Wrong image for pod: daemon-set-k6gdg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:07.776: INFO: Pod daemon-set-k6gdg is not available
Sep  5 21:16:08.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:08.776: INFO: Pod daemon-set-69skp is not available
Sep  5 21:16:09.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:09.776: INFO: Pod daemon-set-69skp is not available
Sep  5 21:16:10.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:11.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:12.777: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:13.776: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:14.777: INFO: Wrong image for pod: daemon-set-2l5zg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 21:16:14.777: INFO: Pod daemon-set-2l5zg is not available
Sep  5 21:16:15.776: INFO: Pod daemon-set-c6lb6 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  5 21:16:15.786: INFO: Number of nodes with available pods: 2
Sep  5 21:16:15.786: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:16:16.794: INFO: Number of nodes with available pods: 2
Sep  5 21:16:16.794: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:16:17.794: INFO: Number of nodes with available pods: 3
Sep  5 21:16:17.794: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-44, will wait for the garbage collector to delete the pods
Sep  5 21:16:17.869: INFO: Deleting DaemonSet.extensions daemon-set took: 6.430603ms
Sep  5 21:16:18.369: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.232519ms
Sep  5 21:16:28.473: INFO: Number of nodes with available pods: 0
Sep  5 21:16:28.474: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 21:16:28.477: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-44/daemonsets","resourceVersion":"11995"},"items":null}

Sep  5 21:16:28.480: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-44/pods","resourceVersion":"11995"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:16:28.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-44" for this suite.
Sep  5 21:16:34.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:16:34.607: INFO: namespace daemonsets-44 deletion completed in 6.107680924s

• [SLOW TEST:46.046 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:16:34.607: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Sep  5 21:16:34.753: INFO: Waiting up to 5m0s for pod "client-containers-708fed8e-d022-11e9-a0ad-8693e9898db7" in namespace "containers-7590" to be "success or failure"
Sep  5 21:16:34.756: INFO: Pod "client-containers-708fed8e-d022-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.945329ms
Sep  5 21:16:36.760: INFO: Pod "client-containers-708fed8e-d022-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006649589s
Sep  5 21:16:38.764: INFO: Pod "client-containers-708fed8e-d022-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010736845s
STEP: Saw pod success
Sep  5 21:16:38.764: INFO: Pod "client-containers-708fed8e-d022-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:16:38.767: INFO: Trying to get logs from node appserv9 pod client-containers-708fed8e-d022-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 21:16:38.786: INFO: Waiting for pod client-containers-708fed8e-d022-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:16:38.789: INFO: Pod client-containers-708fed8e-d022-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:16:38.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7590" for this suite.
Sep  5 21:16:44.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:16:44.910: INFO: namespace containers-7590 deletion completed in 6.116383321s

• [SLOW TEST:10.303 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:16:44.910: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-76b401b3-d022-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 21:16:45.060: INFO: Waiting up to 5m0s for pod "pod-secrets-76b48eb2-d022-11e9-a0ad-8693e9898db7" in namespace "secrets-8055" to be "success or failure"
Sep  5 21:16:45.063: INFO: Pod "pod-secrets-76b48eb2-d022-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.058817ms
Sep  5 21:16:47.067: INFO: Pod "pod-secrets-76b48eb2-d022-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007560901s
STEP: Saw pod success
Sep  5 21:16:47.067: INFO: Pod "pod-secrets-76b48eb2-d022-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:16:47.070: INFO: Trying to get logs from node appserv10 pod pod-secrets-76b48eb2-d022-11e9-a0ad-8693e9898db7 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:16:47.088: INFO: Waiting for pod pod-secrets-76b48eb2-d022-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:16:47.091: INFO: Pod pod-secrets-76b48eb2-d022-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:16:47.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8055" for this suite.
Sep  5 21:16:53.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:16:53.202: INFO: namespace secrets-8055 deletion completed in 6.107987567s

• [SLOW TEST:8.292 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:16:53.203: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-d8ds
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 21:16:53.355: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-d8ds" in namespace "subpath-4254" to be "success or failure"
Sep  5 21:16:53.358: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Pending", Reason="", readiness=false. Elapsed: 2.952927ms
Sep  5 21:16:55.362: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007670125s
Sep  5 21:16:57.366: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 4.011776746s
Sep  5 21:16:59.371: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 6.016156581s
Sep  5 21:17:01.375: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 8.020497586s
Sep  5 21:17:03.379: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 10.024455235s
Sep  5 21:17:05.383: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 12.028816521s
Sep  5 21:17:07.387: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 14.032779818s
Sep  5 21:17:09.392: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 16.036928679s
Sep  5 21:17:11.396: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 18.041291323s
Sep  5 21:17:13.400: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 20.045444184s
Sep  5 21:17:15.404: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Running", Reason="", readiness=true. Elapsed: 22.049615091s
Sep  5 21:17:17.408: INFO: Pod "pod-subpath-test-secret-d8ds": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.053434301s
STEP: Saw pod success
Sep  5 21:17:17.408: INFO: Pod "pod-subpath-test-secret-d8ds" satisfied condition "success or failure"
Sep  5 21:17:17.411: INFO: Trying to get logs from node appserv9 pod pod-subpath-test-secret-d8ds container test-container-subpath-secret-d8ds: <nil>
STEP: delete the pod
Sep  5 21:17:17.431: INFO: Waiting for pod pod-subpath-test-secret-d8ds to disappear
Sep  5 21:17:17.434: INFO: Pod pod-subpath-test-secret-d8ds no longer exists
STEP: Deleting pod pod-subpath-test-secret-d8ds
Sep  5 21:17:17.434: INFO: Deleting pod "pod-subpath-test-secret-d8ds" in namespace "subpath-4254"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:17:17.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4254" for this suite.
Sep  5 21:17:23.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:17:23.554: INFO: namespace subpath-4254 deletion completed in 6.113664321s

• [SLOW TEST:30.351 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:17:23.554: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-8dbcb97e-d022-11e9-a0ad-8693e9898db7
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:17:23.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2713" for this suite.
Sep  5 21:17:29.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:17:29.807: INFO: namespace configmap-2713 deletion completed in 6.108799053s

• [SLOW TEST:6.253 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:17:29.808: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3591
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep  5 21:17:29.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-3591'
Sep  5 21:17:30.232: INFO: stderr: ""
Sep  5 21:17:30.232: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 21:17:30.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3591'
Sep  5 21:17:30.317: INFO: stderr: ""
Sep  5 21:17:30.317: INFO: stdout: "update-demo-nautilus-hpc5m update-demo-nautilus-x2qcq "
Sep  5 21:17:30.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-hpc5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:30.424: INFO: stderr: ""
Sep  5 21:17:30.424: INFO: stdout: ""
Sep  5 21:17:30.424: INFO: update-demo-nautilus-hpc5m is created but not running
Sep  5 21:17:35.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3591'
Sep  5 21:17:35.542: INFO: stderr: ""
Sep  5 21:17:35.542: INFO: stdout: "update-demo-nautilus-hpc5m update-demo-nautilus-x2qcq "
Sep  5 21:17:35.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-hpc5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:35.646: INFO: stderr: ""
Sep  5 21:17:35.646: INFO: stdout: "true"
Sep  5 21:17:35.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-hpc5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:35.754: INFO: stderr: ""
Sep  5 21:17:35.754: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:17:35.754: INFO: validating pod update-demo-nautilus-hpc5m
Sep  5 21:17:35.760: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:17:35.760: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:17:35.760: INFO: update-demo-nautilus-hpc5m is verified up and running
Sep  5 21:17:35.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-x2qcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:35.840: INFO: stderr: ""
Sep  5 21:17:35.840: INFO: stdout: "true"
Sep  5 21:17:35.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-x2qcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:35.919: INFO: stderr: ""
Sep  5 21:17:35.919: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:17:35.920: INFO: validating pod update-demo-nautilus-x2qcq
Sep  5 21:17:35.925: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:17:35.925: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:17:35.925: INFO: update-demo-nautilus-x2qcq is verified up and running
STEP: scaling down the replication controller
Sep  5 21:17:35.927: INFO: scanned /root for discovery docs: <nil>
Sep  5 21:17:35.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3591'
Sep  5 21:17:37.041: INFO: stderr: ""
Sep  5 21:17:37.041: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 21:17:37.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3591'
Sep  5 21:17:37.160: INFO: stderr: ""
Sep  5 21:17:37.160: INFO: stdout: "update-demo-nautilus-hpc5m update-demo-nautilus-x2qcq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  5 21:17:42.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3591'
Sep  5 21:17:42.287: INFO: stderr: ""
Sep  5 21:17:42.287: INFO: stdout: "update-demo-nautilus-hpc5m update-demo-nautilus-x2qcq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  5 21:17:47.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3591'
Sep  5 21:17:47.408: INFO: stderr: ""
Sep  5 21:17:47.408: INFO: stdout: "update-demo-nautilus-x2qcq "
Sep  5 21:17:47.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-x2qcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:47.516: INFO: stderr: ""
Sep  5 21:17:47.517: INFO: stdout: "true"
Sep  5 21:17:47.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-x2qcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:47.619: INFO: stderr: ""
Sep  5 21:17:47.619: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:17:47.619: INFO: validating pod update-demo-nautilus-x2qcq
Sep  5 21:17:47.623: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:17:47.623: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:17:47.623: INFO: update-demo-nautilus-x2qcq is verified up and running
STEP: scaling up the replication controller
Sep  5 21:17:47.626: INFO: scanned /root for discovery docs: <nil>
Sep  5 21:17:47.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3591'
Sep  5 21:17:48.755: INFO: stderr: ""
Sep  5 21:17:48.755: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 21:17:48.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3591'
Sep  5 21:17:48.881: INFO: stderr: ""
Sep  5 21:17:48.881: INFO: stdout: "update-demo-nautilus-f4dm8 update-demo-nautilus-x2qcq "
Sep  5 21:17:48.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-f4dm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:48.995: INFO: stderr: ""
Sep  5 21:17:48.995: INFO: stdout: ""
Sep  5 21:17:48.995: INFO: update-demo-nautilus-f4dm8 is created but not running
Sep  5 21:17:53.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3591'
Sep  5 21:17:54.117: INFO: stderr: ""
Sep  5 21:17:54.118: INFO: stdout: "update-demo-nautilus-f4dm8 update-demo-nautilus-x2qcq "
Sep  5 21:17:54.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-f4dm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:54.232: INFO: stderr: ""
Sep  5 21:17:54.232: INFO: stdout: "true"
Sep  5 21:17:54.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-f4dm8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:54.335: INFO: stderr: ""
Sep  5 21:17:54.335: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:17:54.335: INFO: validating pod update-demo-nautilus-f4dm8
Sep  5 21:17:54.341: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:17:54.341: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:17:54.341: INFO: update-demo-nautilus-f4dm8 is verified up and running
Sep  5 21:17:54.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-x2qcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:54.445: INFO: stderr: ""
Sep  5 21:17:54.445: INFO: stdout: "true"
Sep  5 21:17:54.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-x2qcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3591'
Sep  5 21:17:54.552: INFO: stderr: ""
Sep  5 21:17:54.552: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:17:54.552: INFO: validating pod update-demo-nautilus-x2qcq
Sep  5 21:17:54.556: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:17:54.556: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:17:54.556: INFO: update-demo-nautilus-x2qcq is verified up and running
STEP: using delete to clean up resources
Sep  5 21:17:54.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-3591'
Sep  5 21:17:54.669: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:17:54.669: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  5 21:17:54.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3591'
Sep  5 21:17:54.783: INFO: stderr: "No resources found.\n"
Sep  5 21:17:54.783: INFO: stdout: ""
Sep  5 21:17:54.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -l name=update-demo --namespace=kubectl-3591 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:17:54.891: INFO: stderr: ""
Sep  5 21:17:54.891: INFO: stdout: "update-demo-nautilus-f4dm8\nupdate-demo-nautilus-x2qcq\n"
Sep  5 21:17:55.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3591'
Sep  5 21:17:55.509: INFO: stderr: "No resources found.\n"
Sep  5 21:17:55.509: INFO: stdout: ""
Sep  5 21:17:55.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -l name=update-demo --namespace=kubectl-3591 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:17:55.609: INFO: stderr: ""
Sep  5 21:17:55.609: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:17:55.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3591" for this suite.
Sep  5 21:18:17.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:18:17.717: INFO: namespace kubectl-3591 deletion completed in 22.104189402s

• [SLOW TEST:47.909 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:18:17.717: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9928
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  5 21:18:17.860: INFO: Waiting up to 5m0s for pod "pod-ae04ce07-d022-11e9-a0ad-8693e9898db7" in namespace "emptydir-9928" to be "success or failure"
Sep  5 21:18:17.862: INFO: Pod "pod-ae04ce07-d022-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718397ms
Sep  5 21:18:19.866: INFO: Pod "pod-ae04ce07-d022-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006672075s
STEP: Saw pod success
Sep  5 21:18:19.866: INFO: Pod "pod-ae04ce07-d022-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:18:19.869: INFO: Trying to get logs from node appserv9 pod pod-ae04ce07-d022-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 21:18:19.889: INFO: Waiting for pod pod-ae04ce07-d022-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:18:19.892: INFO: Pod pod-ae04ce07-d022-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:18:19.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9928" for this suite.
Sep  5 21:18:25.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:18:25.994: INFO: namespace emptydir-9928 deletion completed in 6.098593481s

• [SLOW TEST:8.278 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:18:25.994: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:18:26.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2f3f1c0-d022-11e9-a0ad-8693e9898db7" in namespace "downward-api-3541" to be "success or failure"
Sep  5 21:18:26.140: INFO: Pod "downwardapi-volume-b2f3f1c0-d022-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.720431ms
Sep  5 21:18:28.144: INFO: Pod "downwardapi-volume-b2f3f1c0-d022-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006259423s
STEP: Saw pod success
Sep  5 21:18:28.144: INFO: Pod "downwardapi-volume-b2f3f1c0-d022-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:18:28.147: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-b2f3f1c0-d022-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:18:28.162: INFO: Waiting for pod downwardapi-volume-b2f3f1c0-d022-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:18:28.164: INFO: Pod downwardapi-volume-b2f3f1c0-d022-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:18:28.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3541" for this suite.
Sep  5 21:18:34.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:18:34.276: INFO: namespace downward-api-3541 deletion completed in 6.108230561s

• [SLOW TEST:8.282 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:18:34.276: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-115
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  5 21:18:34.638: INFO: Pod name wrapped-volume-race-b7fe6225-d022-11e9-a0ad-8693e9898db7: Found 2 pods out of 5
Sep  5 21:18:39.645: INFO: Pod name wrapped-volume-race-b7fe6225-d022-11e9-a0ad-8693e9898db7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b7fe6225-d022-11e9-a0ad-8693e9898db7 in namespace emptydir-wrapper-115, will wait for the garbage collector to delete the pods
Sep  5 21:18:49.742: INFO: Deleting ReplicationController wrapped-volume-race-b7fe6225-d022-11e9-a0ad-8693e9898db7 took: 7.694222ms
Sep  5 21:18:50.242: INFO: Terminating ReplicationController wrapped-volume-race-b7fe6225-d022-11e9-a0ad-8693e9898db7 pods took: 500.246548ms
STEP: Creating RC which spawns configmap-volume pods
Sep  5 21:19:38.858: INFO: Pod name wrapped-volume-race-de4ae21a-d022-11e9-a0ad-8693e9898db7: Found 0 pods out of 5
Sep  5 21:19:43.865: INFO: Pod name wrapped-volume-race-de4ae21a-d022-11e9-a0ad-8693e9898db7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-de4ae21a-d022-11e9-a0ad-8693e9898db7 in namespace emptydir-wrapper-115, will wait for the garbage collector to delete the pods
Sep  5 21:19:53.957: INFO: Deleting ReplicationController wrapped-volume-race-de4ae21a-d022-11e9-a0ad-8693e9898db7 took: 7.525064ms
Sep  5 21:19:54.458: INFO: Terminating ReplicationController wrapped-volume-race-de4ae21a-d022-11e9-a0ad-8693e9898db7 pods took: 500.238072ms
STEP: Creating RC which spawns configmap-volume pods
Sep  5 21:20:48.476: INFO: Pod name wrapped-volume-race-07c967ab-d023-11e9-a0ad-8693e9898db7: Found 0 pods out of 5
Sep  5 21:20:53.483: INFO: Pod name wrapped-volume-race-07c967ab-d023-11e9-a0ad-8693e9898db7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-07c967ab-d023-11e9-a0ad-8693e9898db7 in namespace emptydir-wrapper-115, will wait for the garbage collector to delete the pods
Sep  5 21:21:03.576: INFO: Deleting ReplicationController wrapped-volume-race-07c967ab-d023-11e9-a0ad-8693e9898db7 took: 7.853871ms
Sep  5 21:21:04.076: INFO: Terminating ReplicationController wrapped-volume-race-07c967ab-d023-11e9-a0ad-8693e9898db7 pods took: 500.184696ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:21:58.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-115" for this suite.
Sep  5 21:22:04.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:22:04.871: INFO: namespace emptydir-wrapper-115 deletion completed in 6.103934685s

• [SLOW TEST:210.595 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:22:04.872: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4845
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-356b1cb0-d023-11e9-a0ad-8693e9898db7
STEP: Creating configMap with name cm-test-opt-upd-356b1d14-d023-11e9-a0ad-8693e9898db7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-356b1cb0-d023-11e9-a0ad-8693e9898db7
STEP: Updating configmap cm-test-opt-upd-356b1d14-d023-11e9-a0ad-8693e9898db7
STEP: Creating configMap with name cm-test-opt-create-356b1d42-d023-11e9-a0ad-8693e9898db7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:22:09.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4845" for this suite.
Sep  5 21:22:31.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:22:31.246: INFO: namespace projected-4845 deletion completed in 22.115568207s

• [SLOW TEST:26.374 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:22:31.246: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7940
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  5 21:22:31.392: INFO: Waiting up to 5m0s for pod "pod-45229cbc-d023-11e9-a0ad-8693e9898db7" in namespace "emptydir-7940" to be "success or failure"
Sep  5 21:22:31.395: INFO: Pod "pod-45229cbc-d023-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.842746ms
Sep  5 21:22:33.398: INFO: Pod "pod-45229cbc-d023-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006583721s
STEP: Saw pod success
Sep  5 21:22:33.398: INFO: Pod "pod-45229cbc-d023-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:22:33.401: INFO: Trying to get logs from node appserv10 pod pod-45229cbc-d023-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 21:22:33.421: INFO: Waiting for pod pod-45229cbc-d023-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:22:33.423: INFO: Pod pod-45229cbc-d023-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:22:33.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7940" for this suite.
Sep  5 21:22:39.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:22:39.540: INFO: namespace emptydir-7940 deletion completed in 6.112493103s

• [SLOW TEST:8.294 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:22:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8531
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-4a15fca0-d023-11e9-a0ad-8693e9898db7
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:22:43.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8531" for this suite.
Sep  5 21:23:05.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:23:05.843: INFO: namespace configmap-8531 deletion completed in 22.092742053s

• [SLOW TEST:26.303 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:23:05.843: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  5 21:23:05.985: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-a,UID:59c1eaa4-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14149,Generation:0,CreationTimestamp:2019-09-05 21:23:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 21:23:05.985: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-a,UID:59c1eaa4-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14149,Generation:0,CreationTimestamp:2019-09-05 21:23:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  5 21:23:15.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-a,UID:59c1eaa4-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14168,Generation:0,CreationTimestamp:2019-09-05 21:23:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  5 21:23:15.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-a,UID:59c1eaa4-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14168,Generation:0,CreationTimestamp:2019-09-05 21:23:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  5 21:23:25.999: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-a,UID:59c1eaa4-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14188,Generation:0,CreationTimestamp:2019-09-05 21:23:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 21:23:25.999: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-a,UID:59c1eaa4-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14188,Generation:0,CreationTimestamp:2019-09-05 21:23:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  5 21:23:36.006: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-a,UID:59c1eaa4-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14206,Generation:0,CreationTimestamp:2019-09-05 21:23:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 21:23:36.006: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-a,UID:59c1eaa4-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14206,Generation:0,CreationTimestamp:2019-09-05 21:23:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  5 21:23:46.012: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-b,UID:719d44e5-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14225,Generation:0,CreationTimestamp:2019-09-05 21:23:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 21:23:46.013: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-b,UID:719d44e5-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14225,Generation:0,CreationTimestamp:2019-09-05 21:23:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  5 21:23:56.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-b,UID:719d44e5-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14246,Generation:0,CreationTimestamp:2019-09-05 21:23:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 21:23:56.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5356,SelfLink:/api/v1/namespaces/watch-5356/configmaps/e2e-watch-test-configmap-b,UID:719d44e5-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14246,Generation:0,CreationTimestamp:2019-09-05 21:23:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:24:06.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5356" for this suite.
Sep  5 21:24:12.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:24:12.133: INFO: namespace watch-5356 deletion completed in 6.109586209s

• [SLOW TEST:66.290 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:24:12.134: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-4816/configmap-test-8145073e-d023-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 21:24:12.284: INFO: Waiting up to 5m0s for pod "pod-configmaps-81459fcd-d023-11e9-a0ad-8693e9898db7" in namespace "configmap-4816" to be "success or failure"
Sep  5 21:24:12.287: INFO: Pod "pod-configmaps-81459fcd-d023-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.000439ms
Sep  5 21:24:14.291: INFO: Pod "pod-configmaps-81459fcd-d023-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007000942s
Sep  5 21:24:16.295: INFO: Pod "pod-configmaps-81459fcd-d023-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011072909s
STEP: Saw pod success
Sep  5 21:24:16.295: INFO: Pod "pod-configmaps-81459fcd-d023-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:24:16.298: INFO: Trying to get logs from node appserv10 pod pod-configmaps-81459fcd-d023-11e9-a0ad-8693e9898db7 container env-test: <nil>
STEP: delete the pod
Sep  5 21:24:16.317: INFO: Waiting for pod pod-configmaps-81459fcd-d023-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:24:16.319: INFO: Pod pod-configmaps-81459fcd-d023-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:24:16.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4816" for this suite.
Sep  5 21:24:22.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:24:22.436: INFO: namespace configmap-4816 deletion completed in 6.1123395s

• [SLOW TEST:10.302 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:24:22.436: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:24:22.583: INFO: (0) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.123221ms)
Sep  5 21:24:22.587: INFO: (1) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.989285ms)
Sep  5 21:24:22.591: INFO: (2) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.995397ms)
Sep  5 21:24:22.595: INFO: (3) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.854268ms)
Sep  5 21:24:22.599: INFO: (4) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.811546ms)
Sep  5 21:24:22.603: INFO: (5) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.8714ms)
Sep  5 21:24:22.607: INFO: (6) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.943023ms)
Sep  5 21:24:22.611: INFO: (7) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 4.020031ms)
Sep  5 21:24:22.615: INFO: (8) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.750186ms)
Sep  5 21:24:22.619: INFO: (9) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.926766ms)
Sep  5 21:24:22.623: INFO: (10) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.856501ms)
Sep  5 21:24:22.627: INFO: (11) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.994385ms)
Sep  5 21:24:22.631: INFO: (12) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.979801ms)
Sep  5 21:24:22.635: INFO: (13) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 4.111484ms)
Sep  5 21:24:22.639: INFO: (14) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 4.085448ms)
Sep  5 21:24:22.643: INFO: (15) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.871909ms)
Sep  5 21:24:22.647: INFO: (16) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.986166ms)
Sep  5 21:24:22.651: INFO: (17) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 4.025527ms)
Sep  5 21:24:22.655: INFO: (18) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.992166ms)
Sep  5 21:24:22.659: INFO: (19) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.935787ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:24:22.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2535" for this suite.
Sep  5 21:24:28.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:24:28.771: INFO: namespace proxy-2535 deletion completed in 6.10806688s

• [SLOW TEST:6.335 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:24:28.771: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  5 21:24:34.971: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 21:24:34.975: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 21:24:36.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 21:24:36.979: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 21:24:38.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 21:24:38.979: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 21:24:40.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 21:24:40.979: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 21:24:42.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 21:24:42.979: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 21:24:44.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 21:24:44.979: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 21:24:46.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 21:24:46.979: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 21:24:48.975: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 21:24:48.978: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:24:48.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4147" for this suite.
Sep  5 21:25:10.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:25:11.108: INFO: namespace container-lifecycle-hook-4147 deletion completed in 22.125544125s

• [SLOW TEST:42.337 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:25:11.108: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  5 21:25:11.261: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-801,SelfLink:/api/v1/namespaces/watch-801/configmaps/e2e-watch-test-watch-closed,UID:a46c5efc-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14504,Generation:0,CreationTimestamp:2019-09-05 21:25:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 21:25:11.261: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-801,SelfLink:/api/v1/namespaces/watch-801/configmaps/e2e-watch-test-watch-closed,UID:a46c5efc-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14505,Generation:0,CreationTimestamp:2019-09-05 21:25:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  5 21:25:11.275: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-801,SelfLink:/api/v1/namespaces/watch-801/configmaps/e2e-watch-test-watch-closed,UID:a46c5efc-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14506,Generation:0,CreationTimestamp:2019-09-05 21:25:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 21:25:11.276: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-801,SelfLink:/api/v1/namespaces/watch-801/configmaps/e2e-watch-test-watch-closed,UID:a46c5efc-d023-11e9-9b5e-2c600c82ec72,ResourceVersion:14507,Generation:0,CreationTimestamp:2019-09-05 21:25:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:25:11.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-801" for this suite.
Sep  5 21:25:17.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:25:17.387: INFO: namespace watch-801 deletion completed in 6.107078939s

• [SLOW TEST:6.278 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:25:17.387: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 21:25:17.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-770'
Sep  5 21:25:17.661: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 21:25:17.661: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Sep  5 21:25:17.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete deployment e2e-test-nginx-deployment --namespace=kubectl-770'
Sep  5 21:25:17.771: INFO: stderr: ""
Sep  5 21:25:17.771: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:25:17.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-770" for this suite.
Sep  5 21:25:23.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:25:23.888: INFO: namespace kubectl-770 deletion completed in 6.113418249s

• [SLOW TEST:6.501 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:25:23.888: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-1891
STEP: Creating secret with name secret-test-ac09bdbb-d023-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 21:25:24.175: INFO: Waiting up to 5m0s for pod "pod-secrets-ac1f3d07-d023-11e9-a0ad-8693e9898db7" in namespace "secrets-1701" to be "success or failure"
Sep  5 21:25:24.178: INFO: Pod "pod-secrets-ac1f3d07-d023-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91835ms
Sep  5 21:25:26.181: INFO: Pod "pod-secrets-ac1f3d07-d023-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.00659712s
Sep  5 21:25:28.185: INFO: Pod "pod-secrets-ac1f3d07-d023-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010314416s
STEP: Saw pod success
Sep  5 21:25:28.185: INFO: Pod "pod-secrets-ac1f3d07-d023-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:25:28.188: INFO: Trying to get logs from node appserv10 pod pod-secrets-ac1f3d07-d023-11e9-a0ad-8693e9898db7 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:25:28.207: INFO: Waiting for pod pod-secrets-ac1f3d07-d023-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:25:28.209: INFO: Pod pod-secrets-ac1f3d07-d023-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:25:28.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1701" for this suite.
Sep  5 21:25:34.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:25:34.324: INFO: namespace secrets-1701 deletion completed in 6.110569104s
STEP: Destroying namespace "secret-namespace-1891" for this suite.
Sep  5 21:25:40.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:25:40.434: INFO: namespace secret-namespace-1891 deletion completed in 6.109806974s

• [SLOW TEST:16.546 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:25:40.434: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4278
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-b5e67ece-d023-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 21:25:40.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-b5e70e77-d023-11e9-a0ad-8693e9898db7" in namespace "configmap-4278" to be "success or failure"
Sep  5 21:25:40.587: INFO: Pod "pod-configmaps-b5e70e77-d023-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.676406ms
Sep  5 21:25:42.591: INFO: Pod "pod-configmaps-b5e70e77-d023-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0077052s
STEP: Saw pod success
Sep  5 21:25:42.591: INFO: Pod "pod-configmaps-b5e70e77-d023-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:25:42.594: INFO: Trying to get logs from node appserv10 pod pod-configmaps-b5e70e77-d023-11e9-a0ad-8693e9898db7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:25:42.613: INFO: Waiting for pod pod-configmaps-b5e70e77-d023-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:25:42.615: INFO: Pod pod-configmaps-b5e70e77-d023-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:25:42.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4278" for this suite.
Sep  5 21:25:48.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:25:48.730: INFO: namespace configmap-4278 deletion completed in 6.110822161s

• [SLOW TEST:8.296 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:25:48.731: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Sep  5 21:25:49.397: INFO: created pod pod-service-account-defaultsa
Sep  5 21:25:49.397: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  5 21:25:49.402: INFO: created pod pod-service-account-mountsa
Sep  5 21:25:49.402: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  5 21:25:49.407: INFO: created pod pod-service-account-nomountsa
Sep  5 21:25:49.407: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  5 21:25:49.412: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  5 21:25:49.412: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  5 21:25:49.417: INFO: created pod pod-service-account-mountsa-mountspec
Sep  5 21:25:49.417: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  5 21:25:49.422: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  5 21:25:49.422: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  5 21:25:49.426: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  5 21:25:49.426: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  5 21:25:49.431: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  5 21:25:49.431: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  5 21:25:49.435: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  5 21:25:49.435: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:25:49.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-600" for this suite.
Sep  5 21:26:11.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:26:11.553: INFO: namespace svcaccounts-600 deletion completed in 22.113572421s

• [SLOW TEST:22.822 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:26:11.553: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1761
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-c872fd05-d023-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 21:26:11.703: INFO: Waiting up to 5m0s for pod "pod-configmaps-c8738a2b-d023-11e9-a0ad-8693e9898db7" in namespace "configmap-1761" to be "success or failure"
Sep  5 21:26:11.707: INFO: Pod "pod-configmaps-c8738a2b-d023-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.385062ms
Sep  5 21:26:13.711: INFO: Pod "pod-configmaps-c8738a2b-d023-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007456509s
STEP: Saw pod success
Sep  5 21:26:13.711: INFO: Pod "pod-configmaps-c8738a2b-d023-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:26:13.714: INFO: Trying to get logs from node appserv10 pod pod-configmaps-c8738a2b-d023-11e9-a0ad-8693e9898db7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:26:13.734: INFO: Waiting for pod pod-configmaps-c8738a2b-d023-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:26:13.737: INFO: Pod pod-configmaps-c8738a2b-d023-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:26:13.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1761" for this suite.
Sep  5 21:26:19.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:26:19.854: INFO: namespace configmap-1761 deletion completed in 6.113511753s

• [SLOW TEST:8.301 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:26:19.855: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  5 21:26:19.994: INFO: PodSpec: initContainers in spec.initContainers
Sep  5 21:27:05.611: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cd65c3c4-d023-11e9-a0ad-8693e9898db7", GenerateName:"", Namespace:"init-container-3279", SelfLink:"/api/v1/namespaces/init-container-3279/pods/pod-init-cd65c3c4-d023-11e9-a0ad-8693e9898db7", UID:"cd665a1e-d023-11e9-9b5e-2c600c82ec72", ResourceVersion:"15008", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63703315579, loc:(*time.Location)(0x8a1a0e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"994215533"}, Annotations:map[string]string{"kubernetes.io/psp":"collecting"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-f6hx5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002ddbf40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f6hx5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f6hx5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f6hx5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002d03cf8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"appserv11", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0033bd920), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d03d80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d03da0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002d03da8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002d03dac)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315580, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315580, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315580, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315580, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.6.111", PodIP:"172.16.141.20", StartTime:(*v1.Time)(0xc001247d00), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001247d40), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0028160e0)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://a715e3c8ac818a3654309f5d1e59e45e689c4ef500ceb435f1a4fcd27233b632"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001247d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001247d20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:27:05.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3279" for this suite.
Sep  5 21:27:27.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:27:27.713: INFO: namespace init-container-3279 deletion completed in 22.097267872s

• [SLOW TEST:67.858 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:27:27.713: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6737
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f5d7696b-d023-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 21:27:27.857: INFO: Waiting up to 5m0s for pod "pod-secrets-f5d7f312-d023-11e9-a0ad-8693e9898db7" in namespace "secrets-6737" to be "success or failure"
Sep  5 21:27:27.860: INFO: Pod "pod-secrets-f5d7f312-d023-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.623785ms
Sep  5 21:27:29.864: INFO: Pod "pod-secrets-f5d7f312-d023-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006686025s
STEP: Saw pod success
Sep  5 21:27:29.864: INFO: Pod "pod-secrets-f5d7f312-d023-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:27:29.867: INFO: Trying to get logs from node appserv9 pod pod-secrets-f5d7f312-d023-11e9-a0ad-8693e9898db7 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:27:29.887: INFO: Waiting for pod pod-secrets-f5d7f312-d023-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:27:29.890: INFO: Pod pod-secrets-f5d7f312-d023-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:27:29.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6737" for this suite.
Sep  5 21:27:35.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:27:35.989: INFO: namespace secrets-6737 deletion completed in 6.095014192s

• [SLOW TEST:8.276 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:27:35.990: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-9900
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9900 to expose endpoints map[]
Sep  5 21:27:36.130: INFO: Get endpoints failed (2.431728ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep  5 21:27:37.134: INFO: successfully validated that service multi-endpoint-test in namespace services-9900 exposes endpoints map[] (1.005797946s elapsed)
STEP: Creating pod pod1 in namespace services-9900
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9900 to expose endpoints map[pod1:[100]]
Sep  5 21:27:39.160: INFO: successfully validated that service multi-endpoint-test in namespace services-9900 exposes endpoints map[pod1:[100]] (2.019016934s elapsed)
STEP: Creating pod pod2 in namespace services-9900
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9900 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  5 21:27:41.195: INFO: successfully validated that service multi-endpoint-test in namespace services-9900 exposes endpoints map[pod1:[100] pod2:[101]] (2.029321268s elapsed)
STEP: Deleting pod pod1 in namespace services-9900
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9900 to expose endpoints map[pod2:[101]]
Sep  5 21:27:41.208: INFO: successfully validated that service multi-endpoint-test in namespace services-9900 exposes endpoints map[pod2:[101]] (6.603112ms elapsed)
STEP: Deleting pod pod2 in namespace services-9900
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9900 to expose endpoints map[]
Sep  5 21:27:42.222: INFO: successfully validated that service multi-endpoint-test in namespace services-9900 exposes endpoints map[] (1.007420179s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:27:42.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9900" for this suite.
Sep  5 21:28:04.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:28:04.355: INFO: namespace services-9900 deletion completed in 22.11244417s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:28.366 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:28:04.356: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:28:04.502: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  5 21:28:09.507: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  5 21:28:09.507: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  5 21:28:11.511: INFO: Creating deployment "test-rollover-deployment"
Sep  5 21:28:11.519: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  5 21:28:13.527: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  5 21:28:13.533: INFO: Ensure that both replica sets have 1 created replica
Sep  5 21:28:13.539: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  5 21:28:13.547: INFO: Updating deployment test-rollover-deployment
Sep  5 21:28:13.547: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  5 21:28:15.555: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  5 21:28:15.562: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  5 21:28:15.569: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 21:28:15.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315695, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:28:17.576: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 21:28:17.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315695, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:28:19.576: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 21:28:19.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315695, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:28:21.577: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 21:28:21.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315695, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:28:23.577: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 21:28:23.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315695, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703315691, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:28:25.576: INFO: 
Sep  5 21:28:25.576: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  5 21:28:25.584: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9775,SelfLink:/apis/apps/v1/namespaces/deployment-9775/deployments/test-rollover-deployment,UID:0fde0116-d024-11e9-9b5e-2c600c82ec72,ResourceVersion:15372,Generation:2,CreationTimestamp:2019-09-05 21:28:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-05 21:28:11 +0000 UTC 2019-09-05 21:28:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-05 21:28:25 +0000 UTC 2019-09-05 21:28:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  5 21:28:25.587: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-9775,SelfLink:/apis/apps/v1/namespaces/deployment-9775/replicasets/test-rollover-deployment-766b4d6c9d,UID:1114bca2-d024-11e9-9b5e-2c600c82ec72,ResourceVersion:15361,Generation:2,CreationTimestamp:2019-09-05 21:28:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0fde0116-d024-11e9-9b5e-2c600c82ec72 0xc002ee6037 0xc002ee6038}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  5 21:28:25.587: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  5 21:28:25.587: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9775,SelfLink:/apis/apps/v1/namespaces/deployment-9775/replicasets/test-rollover-controller,UID:0baf5009-d024-11e9-9b5e-2c600c82ec72,ResourceVersion:15370,Generation:2,CreationTimestamp:2019-09-05 21:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0fde0116-d024-11e9-9b5e-2c600c82ec72 0xc002747e87 0xc002747e88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 21:28:25.587: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-9775,SelfLink:/apis/apps/v1/namespaces/deployment-9775/replicasets/test-rollover-deployment-6455657675,UID:0fdfd6fb-d024-11e9-9b5e-2c600c82ec72,ResourceVersion:15317,Generation:2,CreationTimestamp:2019-09-05 21:28:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0fde0116-d024-11e9-9b5e-2c600c82ec72 0xc002747f57 0xc002747f58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 21:28:25.590: INFO: Pod "test-rollover-deployment-766b4d6c9d-rgjlm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-rgjlm,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-9775,SelfLink:/api/v1/namespaces/deployment-9775/pods/test-rollover-deployment-766b4d6c9d-rgjlm,UID:1117dc8a-d024-11e9-9b5e-2c600c82ec72,ResourceVersion:15339,Generation:0,CreationTimestamp:2019-09-05 21:28:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 1114bca2-d024-11e9-9b5e-2c600c82ec72 0xc0004b9dd7 0xc0004b9dd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8l4f5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8l4f5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8l4f5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00246c090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00246c0b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:28:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:28:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:28:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:28:13 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.111,PodIP:172.16.141.13,StartTime:2019-09-05 21:28:13 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-05 21:28:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://6196845c3db7e569a70806480d45a01e0add84afccc9641cd1ec9ef44c435369}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:28:25.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9775" for this suite.
Sep  5 21:28:31.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:28:31.708: INFO: namespace deployment-9775 deletion completed in 6.113920102s

• [SLOW TEST:27.352 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:28:31.708: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0905 21:28:41.873138      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 21:28:41.873: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:28:41.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3943" for this suite.
Sep  5 21:28:47.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:28:47.983: INFO: namespace gc-3943 deletion completed in 6.106261919s

• [SLOW TEST:16.275 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:28:47.983: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep  5 21:28:48.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-4685'
Sep  5 21:28:48.406: INFO: stderr: ""
Sep  5 21:28:48.406: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 21:28:48.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4685'
Sep  5 21:28:48.518: INFO: stderr: ""
Sep  5 21:28:48.518: INFO: stdout: "update-demo-nautilus-2bcff update-demo-nautilus-56hw7 "
Sep  5 21:28:48.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-2bcff -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4685'
Sep  5 21:28:48.620: INFO: stderr: ""
Sep  5 21:28:48.620: INFO: stdout: ""
Sep  5 21:28:48.620: INFO: update-demo-nautilus-2bcff is created but not running
Sep  5 21:28:53.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4685'
Sep  5 21:28:53.748: INFO: stderr: ""
Sep  5 21:28:53.748: INFO: stdout: "update-demo-nautilus-2bcff update-demo-nautilus-56hw7 "
Sep  5 21:28:53.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-2bcff -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4685'
Sep  5 21:28:53.853: INFO: stderr: ""
Sep  5 21:28:53.853: INFO: stdout: "true"
Sep  5 21:28:53.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-2bcff -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4685'
Sep  5 21:28:53.964: INFO: stderr: ""
Sep  5 21:28:53.964: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:28:53.964: INFO: validating pod update-demo-nautilus-2bcff
Sep  5 21:28:53.970: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:28:53.970: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:28:53.970: INFO: update-demo-nautilus-2bcff is verified up and running
Sep  5 21:28:53.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-56hw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4685'
Sep  5 21:28:54.072: INFO: stderr: ""
Sep  5 21:28:54.072: INFO: stdout: "true"
Sep  5 21:28:54.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods update-demo-nautilus-56hw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4685'
Sep  5 21:28:54.188: INFO: stderr: ""
Sep  5 21:28:54.188: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:28:54.188: INFO: validating pod update-demo-nautilus-56hw7
Sep  5 21:28:54.193: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:28:54.193: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:28:54.193: INFO: update-demo-nautilus-56hw7 is verified up and running
STEP: using delete to clean up resources
Sep  5 21:28:54.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-4685'
Sep  5 21:28:54.305: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:28:54.305: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  5 21:28:54.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4685'
Sep  5 21:28:54.414: INFO: stderr: "No resources found.\n"
Sep  5 21:28:54.414: INFO: stdout: ""
Sep  5 21:28:54.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -l name=update-demo --namespace=kubectl-4685 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:28:54.523: INFO: stderr: ""
Sep  5 21:28:54.523: INFO: stdout: "update-demo-nautilus-2bcff\nupdate-demo-nautilus-56hw7\n"
Sep  5 21:28:55.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4685'
Sep  5 21:28:55.144: INFO: stderr: "No resources found.\n"
Sep  5 21:28:55.144: INFO: stdout: ""
Sep  5 21:28:55.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -l name=update-demo --namespace=kubectl-4685 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:28:55.263: INFO: stderr: ""
Sep  5 21:28:55.263: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:28:55.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4685" for this suite.
Sep  5 21:29:17.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:29:17.368: INFO: namespace kubectl-4685 deletion completed in 22.100204801s

• [SLOW TEST:29.385 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:29:17.368: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-373371a0-d024-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 21:29:17.512: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3733ecba-d024-11e9-a0ad-8693e9898db7" in namespace "projected-5909" to be "success or failure"
Sep  5 21:29:17.515: INFO: Pod "pod-projected-secrets-3733ecba-d024-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612112ms
Sep  5 21:29:19.519: INFO: Pod "pod-projected-secrets-3733ecba-d024-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006962792s
STEP: Saw pod success
Sep  5 21:29:19.519: INFO: Pod "pod-projected-secrets-3733ecba-d024-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:29:19.523: INFO: Trying to get logs from node appserv9 pod pod-projected-secrets-3733ecba-d024-11e9-a0ad-8693e9898db7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:29:19.542: INFO: Waiting for pod pod-projected-secrets-3733ecba-d024-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:29:19.545: INFO: Pod pod-projected-secrets-3733ecba-d024-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:29:19.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5909" for this suite.
Sep  5 21:29:25.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:29:25.653: INFO: namespace projected-5909 deletion completed in 6.103938267s

• [SLOW TEST:8.285 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:29:25.653: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2909
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 21:29:25.789: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  5 21:29:41.870: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.141.11 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2909 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 21:29:41.870: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 21:29:42.976: INFO: Found all expected endpoints: [netserver-0]
Sep  5 21:29:42.979: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.141.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2909 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 21:29:42.979: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 21:29:44.078: INFO: Found all expected endpoints: [netserver-1]
Sep  5 21:29:44.082: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.141.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2909 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 21:29:44.082: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
Sep  5 21:29:45.182: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:29:45.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2909" for this suite.
Sep  5 21:30:07.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:30:07.290: INFO: namespace pod-network-test-2909 deletion completed in 22.103964866s

• [SLOW TEST:41.637 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:30:07.291: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-5537
Sep  5 21:30:09.440: INFO: Started pod liveness-http in namespace container-probe-5537
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:30:09.443: INFO: Initial restart count of pod liveness-http is 0
Sep  5 21:30:29.487: INFO: Restart count of pod container-probe-5537/liveness-http is now 1 (20.043434659s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:30:29.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5537" for this suite.
Sep  5 21:30:35.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:30:35.608: INFO: namespace container-probe-5537 deletion completed in 6.105498408s

• [SLOW TEST:28.318 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:30:35.609: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:30:35.764: INFO: Create a RollingUpdate DaemonSet
Sep  5 21:30:35.767: INFO: Check that daemon pods launch on every node of the cluster
Sep  5 21:30:35.774: INFO: Number of nodes with available pods: 0
Sep  5 21:30:35.774: INFO: Node appserv10 is running more than one daemon pod
Sep  5 21:30:36.781: INFO: Number of nodes with available pods: 0
Sep  5 21:30:36.781: INFO: Node appserv10 is running more than one daemon pod
Sep  5 21:30:37.782: INFO: Number of nodes with available pods: 3
Sep  5 21:30:37.782: INFO: Number of running nodes: 3, number of available pods: 3
Sep  5 21:30:37.782: INFO: Update the DaemonSet to trigger a rollout
Sep  5 21:30:37.789: INFO: Updating DaemonSet daemon-set
Sep  5 21:30:48.801: INFO: Roll back the DaemonSet before rollout is complete
Sep  5 21:30:48.808: INFO: Updating DaemonSet daemon-set
Sep  5 21:30:48.808: INFO: Make sure DaemonSet rollback is complete
Sep  5 21:30:48.811: INFO: Wrong image for pod: daemon-set-t9p4z. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  5 21:30:48.812: INFO: Pod daemon-set-t9p4z is not available
Sep  5 21:30:49.819: INFO: Wrong image for pod: daemon-set-t9p4z. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  5 21:30:49.819: INFO: Pod daemon-set-t9p4z is not available
Sep  5 21:30:50.819: INFO: Wrong image for pod: daemon-set-t9p4z. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  5 21:30:50.819: INFO: Pod daemon-set-t9p4z is not available
Sep  5 21:30:51.820: INFO: Pod daemon-set-xhf8q is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4874, will wait for the garbage collector to delete the pods
Sep  5 21:30:51.891: INFO: Deleting DaemonSet.extensions daemon-set took: 6.942879ms
Sep  5 21:30:52.392: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.213375ms
Sep  5 21:31:02.395: INFO: Number of nodes with available pods: 0
Sep  5 21:31:02.396: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 21:31:02.398: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4874/daemonsets","resourceVersion":"16167"},"items":null}

Sep  5 21:31:02.401: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4874/pods","resourceVersion":"16167"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:31:02.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4874" for this suite.
Sep  5 21:31:08.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:31:08.533: INFO: namespace daemonsets-4874 deletion completed in 6.112974494s

• [SLOW TEST:32.925 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:31:08.533: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Sep  5 21:31:08.672: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep  5 21:31:08.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-2760'
Sep  5 21:31:08.891: INFO: stderr: ""
Sep  5 21:31:08.891: INFO: stdout: "service/redis-slave created\n"
Sep  5 21:31:08.891: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep  5 21:31:08.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-2760'
Sep  5 21:31:09.110: INFO: stderr: ""
Sep  5 21:31:09.110: INFO: stdout: "service/redis-master created\n"
Sep  5 21:31:09.111: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  5 21:31:09.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-2760'
Sep  5 21:31:09.325: INFO: stderr: ""
Sep  5 21:31:09.325: INFO: stdout: "service/frontend created\n"
Sep  5 21:31:09.325: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep  5 21:31:09.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-2760'
Sep  5 21:31:09.503: INFO: stderr: ""
Sep  5 21:31:09.503: INFO: stdout: "deployment.apps/frontend created\n"
Sep  5 21:31:09.503: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  5 21:31:09.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-2760'
Sep  5 21:31:09.691: INFO: stderr: ""
Sep  5 21:31:09.691: INFO: stdout: "deployment.apps/redis-master created\n"
Sep  5 21:31:09.691: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep  5 21:31:09.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-2760'
Sep  5 21:31:09.869: INFO: stderr: ""
Sep  5 21:31:09.869: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep  5 21:31:09.869: INFO: Waiting for all frontend pods to be Running.
Sep  5 21:31:14.920: INFO: Waiting for frontend to serve content.
Sep  5 21:31:14.953: INFO: Trying to add a new entry to the guestbook.
Sep  5 21:31:14.981: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep  5 21:31:14.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-2760'
Sep  5 21:31:15.097: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:31:15.097: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:31:15.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-2760'
Sep  5 21:31:15.215: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:31:15.215: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:31:15.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-2760'
Sep  5 21:31:15.317: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:31:15.317: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:31:15.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-2760'
Sep  5 21:31:15.420: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:31:15.420: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:31:15.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-2760'
Sep  5 21:31:15.502: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:31:15.502: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:31:15.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-2760'
Sep  5 21:31:15.577: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:31:15.577: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:31:15.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2760" for this suite.
Sep  5 21:32:01.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:32:01.708: INFO: namespace kubectl-2760 deletion completed in 46.128499414s

• [SLOW TEST:53.175 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:32:01.709: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  5 21:32:01.855: INFO: Waiting up to 5m0s for pod "downward-api-992864d1-d024-11e9-a0ad-8693e9898db7" in namespace "downward-api-1103" to be "success or failure"
Sep  5 21:32:01.858: INFO: Pod "downward-api-992864d1-d024-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.956739ms
Sep  5 21:32:03.862: INFO: Pod "downward-api-992864d1-d024-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006749737s
Sep  5 21:32:05.865: INFO: Pod "downward-api-992864d1-d024-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010410504s
STEP: Saw pod success
Sep  5 21:32:05.865: INFO: Pod "downward-api-992864d1-d024-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:32:05.868: INFO: Trying to get logs from node appserv9 pod downward-api-992864d1-d024-11e9-a0ad-8693e9898db7 container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:32:05.888: INFO: Waiting for pod downward-api-992864d1-d024-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:32:05.890: INFO: Pod downward-api-992864d1-d024-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:32:05.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1103" for this suite.
Sep  5 21:32:11.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:32:12.006: INFO: namespace downward-api-1103 deletion completed in 6.112303758s

• [SLOW TEST:10.298 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:32:12.006: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Sep  5 21:32:12.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 api-versions'
Sep  5 21:32:12.257: INFO: stderr: ""
Sep  5 21:32:12.257: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvolumesnapshot.external-storage.k8s.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:32:12.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2691" for this suite.
Sep  5 21:32:18.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:32:18.369: INFO: namespace kubectl-2691 deletion completed in 6.10705876s

• [SLOW TEST:6.363 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:32:18.369: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9733
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  5 21:32:21.048: INFO: Successfully updated pod "labelsupdatea316c27f-d024-11e9-a0ad-8693e9898db7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:32:23.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9733" for this suite.
Sep  5 21:32:45.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:32:45.224: INFO: namespace projected-9733 deletion completed in 22.14458057s

• [SLOW TEST:26.855 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:32:45.225: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:32:45.359: INFO: Creating ReplicaSet my-hostname-basic-b317e3d1-d024-11e9-a0ad-8693e9898db7
Sep  5 21:32:45.379: INFO: Pod name my-hostname-basic-b317e3d1-d024-11e9-a0ad-8693e9898db7: Found 0 pods out of 1
Sep  5 21:32:50.383: INFO: Pod name my-hostname-basic-b317e3d1-d024-11e9-a0ad-8693e9898db7: Found 1 pods out of 1
Sep  5 21:32:50.383: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b317e3d1-d024-11e9-a0ad-8693e9898db7" is running
Sep  5 21:32:50.387: INFO: Pod "my-hostname-basic-b317e3d1-d024-11e9-a0ad-8693e9898db7-gqh8q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 21:32:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 21:32:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 21:32:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 21:32:45 +0000 UTC Reason: Message:}])
Sep  5 21:32:50.387: INFO: Trying to dial the pod
Sep  5 21:32:55.400: INFO: Controller my-hostname-basic-b317e3d1-d024-11e9-a0ad-8693e9898db7: Got expected result from replica 1 [my-hostname-basic-b317e3d1-d024-11e9-a0ad-8693e9898db7-gqh8q]: "my-hostname-basic-b317e3d1-d024-11e9-a0ad-8693e9898db7-gqh8q", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:32:55.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5702" for this suite.
Sep  5 21:33:01.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:33:01.532: INFO: namespace replicaset-5702 deletion completed in 6.126623827s

• [SLOW TEST:16.308 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:33:01.532: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  5 21:33:04.203: INFO: Successfully updated pod "pod-update-activedeadlineseconds-bcd0df6f-d024-11e9-a0ad-8693e9898db7"
Sep  5 21:33:04.203: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-bcd0df6f-d024-11e9-a0ad-8693e9898db7" in namespace "pods-6564" to be "terminated due to deadline exceeded"
Sep  5 21:33:04.207: INFO: Pod "pod-update-activedeadlineseconds-bcd0df6f-d024-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 3.344121ms
Sep  5 21:33:06.210: INFO: Pod "pod-update-activedeadlineseconds-bcd0df6f-d024-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006988707s
Sep  5 21:33:08.214: INFO: Pod "pod-update-activedeadlineseconds-bcd0df6f-d024-11e9-a0ad-8693e9898db7": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.010524594s
Sep  5 21:33:08.214: INFO: Pod "pod-update-activedeadlineseconds-bcd0df6f-d024-11e9-a0ad-8693e9898db7" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:33:08.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6564" for this suite.
Sep  5 21:33:14.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:33:14.331: INFO: namespace pods-6564 deletion completed in 6.112873876s

• [SLOW TEST:12.799 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:33:14.331: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-2037
Sep  5 21:33:16.483: INFO: Started pod liveness-http in namespace container-probe-2037
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:33:16.486: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:37:16.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2037" for this suite.
Sep  5 21:37:23.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:37:23.114: INFO: namespace container-probe-2037 deletion completed in 6.123965423s

• [SLOW TEST:248.783 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:37:23.115: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-8377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-8377
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8377
STEP: Deleting pre-stop pod
Sep  5 21:37:32.296: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:37:32.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8377" for this suite.
Sep  5 21:38:06.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:38:06.401: INFO: namespace prestop-8377 deletion completed in 34.094103101s

• [SLOW TEST:43.286 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:38:06.401: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0905 21:38:46.563837      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 21:38:46.563: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:38:46.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6630" for this suite.
Sep  5 21:38:52.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:38:52.679: INFO: namespace gc-6630 deletion completed in 6.111983948s

• [SLOW TEST:46.278 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:38:52.679: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Sep  5 21:38:52.826: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6735" to be "success or failure"
Sep  5 21:38:52.830: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.299593ms
Sep  5 21:38:54.834: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007216428s
Sep  5 21:38:56.837: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011064351s
Sep  5 21:38:58.843: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016612853s
STEP: Saw pod success
Sep  5 21:38:58.843: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep  5 21:38:58.846: INFO: Trying to get logs from node appserv10 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep  5 21:38:58.866: INFO: Waiting for pod pod-host-path-test to disappear
Sep  5 21:38:58.869: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:38:58.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6735" for this suite.
Sep  5 21:39:04.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:39:04.983: INFO: namespace hostpath-6735 deletion completed in 6.109770063s

• [SLOW TEST:12.304 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:39:04.983: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  5 21:39:05.130: INFO: Waiting up to 5m0s for pod "pod-95731a2b-d025-11e9-a0ad-8693e9898db7" in namespace "emptydir-5660" to be "success or failure"
Sep  5 21:39:05.133: INFO: Pod "pod-95731a2b-d025-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.974918ms
Sep  5 21:39:07.137: INFO: Pod "pod-95731a2b-d025-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006767603s
STEP: Saw pod success
Sep  5 21:39:07.137: INFO: Pod "pod-95731a2b-d025-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:39:07.140: INFO: Trying to get logs from node appserv10 pod pod-95731a2b-d025-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 21:39:07.159: INFO: Waiting for pod pod-95731a2b-d025-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:39:07.161: INFO: Pod pod-95731a2b-d025-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:39:07.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5660" for this suite.
Sep  5 21:39:19.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:39:19.279: INFO: namespace emptydir-5660 deletion completed in 12.113380219s

• [SLOW TEST:14.296 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:39:19.280: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-9df8af2d-d025-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 21:39:19.432: INFO: Waiting up to 5m0s for pod "pod-configmaps-9df94e64-d025-11e9-a0ad-8693e9898db7" in namespace "configmap-4511" to be "success or failure"
Sep  5 21:39:19.435: INFO: Pod "pod-configmaps-9df94e64-d025-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.916533ms
Sep  5 21:39:21.439: INFO: Pod "pod-configmaps-9df94e64-d025-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006884233s
STEP: Saw pod success
Sep  5 21:39:21.439: INFO: Pod "pod-configmaps-9df94e64-d025-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:39:21.443: INFO: Trying to get logs from node appserv10 pod pod-configmaps-9df94e64-d025-11e9-a0ad-8693e9898db7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:39:21.464: INFO: Waiting for pod pod-configmaps-9df94e64-d025-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:39:21.467: INFO: Pod pod-configmaps-9df94e64-d025-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:39:21.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4511" for this suite.
Sep  5 21:39:27.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:39:27.578: INFO: namespace configmap-4511 deletion completed in 6.107229264s

• [SLOW TEST:8.299 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:39:27.579: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:39:27.721: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2ea2b42-d025-11e9-a0ad-8693e9898db7" in namespace "projected-6121" to be "success or failure"
Sep  5 21:39:27.723: INFO: Pod "downwardapi-volume-a2ea2b42-d025-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893326ms
Sep  5 21:39:29.728: INFO: Pod "downwardapi-volume-a2ea2b42-d025-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00710695s
Sep  5 21:39:31.732: INFO: Pod "downwardapi-volume-a2ea2b42-d025-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011560294s
STEP: Saw pod success
Sep  5 21:39:31.732: INFO: Pod "downwardapi-volume-a2ea2b42-d025-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:39:31.736: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-a2ea2b42-d025-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:39:31.755: INFO: Waiting for pod downwardapi-volume-a2ea2b42-d025-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:39:31.758: INFO: Pod downwardapi-volume-a2ea2b42-d025-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:39:31.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6121" for this suite.
Sep  5 21:39:37.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:39:37.861: INFO: namespace projected-6121 deletion completed in 6.09777681s

• [SLOW TEST:10.282 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:39:37.862: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:40:01.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4159" for this suite.
Sep  5 21:40:07.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:40:07.344: INFO: namespace container-runtime-4159 deletion completed in 6.108679731s

• [SLOW TEST:29.482 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:40:07.344: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5596
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep  5 21:40:07.490: INFO: Found 0 stateful pods, waiting for 3
Sep  5 21:40:17.495: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:40:17.496: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:40:17.496: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  5 21:40:17.524: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  5 21:40:27.556: INFO: Updating stateful set ss2
Sep  5 21:40:27.563: INFO: Waiting for Pod statefulset-5596/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  5 21:40:37.571: INFO: Waiting for Pod statefulset-5596/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Sep  5 21:40:47.596: INFO: Found 2 stateful pods, waiting for 3
Sep  5 21:40:57.602: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:40:57.602: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:40:57.602: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  5 21:40:57.627: INFO: Updating stateful set ss2
Sep  5 21:40:57.633: INFO: Waiting for Pod statefulset-5596/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  5 21:41:07.642: INFO: Waiting for Pod statefulset-5596/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  5 21:41:17.659: INFO: Updating stateful set ss2
Sep  5 21:41:17.665: INFO: Waiting for StatefulSet statefulset-5596/ss2 to complete update
Sep  5 21:41:17.665: INFO: Waiting for Pod statefulset-5596/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  5 21:41:27.673: INFO: Waiting for StatefulSet statefulset-5596/ss2 to complete update
Sep  5 21:41:27.673: INFO: Waiting for Pod statefulset-5596/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  5 21:41:37.674: INFO: Deleting all statefulset in ns statefulset-5596
Sep  5 21:41:37.677: INFO: Scaling statefulset ss2 to 0
Sep  5 21:42:17.692: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:42:17.696: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:42:17.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5596" for this suite.
Sep  5 21:42:23.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:42:23.823: INFO: namespace statefulset-5596 deletion completed in 6.11086989s

• [SLOW TEST:136.479 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:42:23.823: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7580
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:42:26.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7580" for this suite.
Sep  5 21:42:49.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:42:49.107: INFO: namespace replication-controller-7580 deletion completed in 22.112700539s

• [SLOW TEST:25.284 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:42:49.108: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:42:49.254: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b098dbb-d026-11e9-a0ad-8693e9898db7" in namespace "projected-9276" to be "success or failure"
Sep  5 21:42:49.257: INFO: Pod "downwardapi-volume-1b098dbb-d026-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.105566ms
Sep  5 21:42:51.262: INFO: Pod "downwardapi-volume-1b098dbb-d026-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007644988s
STEP: Saw pod success
Sep  5 21:42:51.262: INFO: Pod "downwardapi-volume-1b098dbb-d026-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:42:51.265: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-1b098dbb-d026-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:42:51.285: INFO: Waiting for pod downwardapi-volume-1b098dbb-d026-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:42:51.288: INFO: Pod downwardapi-volume-1b098dbb-d026-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:42:51.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9276" for this suite.
Sep  5 21:42:57.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:42:57.397: INFO: namespace projected-9276 deletion completed in 6.104474523s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:42:57.397: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  5 21:42:57.532: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:43:00.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8374" for this suite.
Sep  5 21:43:06.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:43:06.479: INFO: namespace init-container-8374 deletion completed in 6.099979964s

• [SLOW TEST:9.082 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:43:06.479: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9938
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:43:06.615: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:43:07.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9938" for this suite.
Sep  5 21:43:13.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:43:13.764: INFO: namespace custom-resource-definition-9938 deletion completed in 6.109914594s

• [SLOW TEST:7.284 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:43:13.764: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-29bb9a62-d026-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 21:43:13.912: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-29bc3620-d026-11e9-a0ad-8693e9898db7" in namespace "projected-9306" to be "success or failure"
Sep  5 21:43:13.915: INFO: Pod "pod-projected-configmaps-29bc3620-d026-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999225ms
Sep  5 21:43:15.919: INFO: Pod "pod-projected-configmaps-29bc3620-d026-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00682019s
STEP: Saw pod success
Sep  5 21:43:15.919: INFO: Pod "pod-projected-configmaps-29bc3620-d026-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:43:15.922: INFO: Trying to get logs from node appserv10 pod pod-projected-configmaps-29bc3620-d026-11e9-a0ad-8693e9898db7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:43:15.941: INFO: Waiting for pod pod-projected-configmaps-29bc3620-d026-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:43:15.943: INFO: Pod pod-projected-configmaps-29bc3620-d026-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:43:15.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9306" for this suite.
Sep  5 21:43:21.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:43:22.059: INFO: namespace projected-9306 deletion completed in 6.111986482s

• [SLOW TEST:8.295 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:43:22.059: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep  5 21:43:22.198: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  5 21:43:22.206: INFO: Waiting for terminating namespaces to be deleted...
Sep  5 21:43:22.209: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Sep  5 21:43:22.216: INFO: snapshot-controller-7cbc6c6455-2jwnr from diamanti-system started at 2019-09-05 20:39:12 +0000 UTC (2 container statuses recorded)
Sep  5 21:43:22.216: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep  5 21:43:22.216: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Sep  5 21:43:22.216: INFO: prometheus-v1-0 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:43:22.216: INFO: 	Container prometheus ready: true, restart count 0
Sep  5 21:43:22.216: INFO: sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-5x8tc from heptio-sonobuoy started at 2019-09-05 20:39:30 +0000 UTC (2 container statuses recorded)
Sep  5 21:43:22.216: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  5 21:43:22.216: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  5 21:43:22.216: INFO: collectd-v0.8-dflpk from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (4 container statuses recorded)
Sep  5 21:43:22.216: INFO: 	Container cadvisor ready: true, restart count 0
Sep  5 21:43:22.216: INFO: 	Container collectd-es ready: true, restart count 0
Sep  5 21:43:22.216: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  5 21:43:22.216: INFO: 	Container node-exporter ready: true, restart count 0
Sep  5 21:43:22.216: INFO: helm-chart-547cc86f54-kdx6m from kube-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:43:22.216: INFO: 	Container helm-chart ready: true, restart count 0
Sep  5 21:43:22.216: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Sep  5 21:43:22.224: INFO: provisioner-677d65965-9ld29 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:43:22.224: INFO: 	Container provisioner ready: true, restart count 0
Sep  5 21:43:22.224: INFO: sonobuoy-e2e-job-f5a25811b59246f1 from heptio-sonobuoy started at 2019-09-05 20:39:31 +0000 UTC (2 container statuses recorded)
Sep  5 21:43:22.224: INFO: 	Container e2e ready: true, restart count 0
Sep  5 21:43:22.224: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  5 21:43:22.224: INFO: collectd-v0.8-8b4p5 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (4 container statuses recorded)
Sep  5 21:43:22.224: INFO: 	Container cadvisor ready: true, restart count 0
Sep  5 21:43:22.224: INFO: 	Container collectd-es ready: true, restart count 0
Sep  5 21:43:22.224: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  5 21:43:22.224: INFO: 	Container node-exporter ready: true, restart count 0
Sep  5 21:43:22.224: INFO: prometheus-v1-2 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:43:22.224: INFO: 	Container prometheus ready: true, restart count 0
Sep  5 21:43:22.224: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-05 20:39:30 +0000 UTC (1 container statuses recorded)
Sep  5 21:43:22.224: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  5 21:43:22.224: INFO: sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-8248f from heptio-sonobuoy started at 2019-09-05 20:39:31 +0000 UTC (2 container statuses recorded)
Sep  5 21:43:22.224: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  5 21:43:22.224: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  5 21:43:22.224: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Sep  5 21:43:22.231: INFO: alertmanager-0 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:43:22.231: INFO: 	Container alertmanager ready: true, restart count 0
Sep  5 21:43:22.231: INFO: sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-75kqn from heptio-sonobuoy started at 2019-09-05 20:39:31 +0000 UTC (2 container statuses recorded)
Sep  5 21:43:22.231: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  5 21:43:22.231: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  5 21:43:22.231: INFO: tiller-deploy-7ddbd797c7-kxxhl from kube-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:43:22.231: INFO: 	Container tiller ready: true, restart count 0
Sep  5 21:43:22.231: INFO: collectd-v0.8-7bjvl from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (4 container statuses recorded)
Sep  5 21:43:22.231: INFO: 	Container cadvisor ready: true, restart count 0
Sep  5 21:43:22.231: INFO: 	Container collectd-es ready: true, restart count 0
Sep  5 21:43:22.231: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  5 21:43:22.231: INFO: 	Container node-exporter ready: true, restart count 0
Sep  5 21:43:22.231: INFO: prometheus-v1-1 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:43:22.231: INFO: 	Container prometheus ready: true, restart count 0
Sep  5 21:43:22.231: INFO: metrics-server-v1-f8c95fb9c-jv47r from kube-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:43:22.231: INFO: 	Container metrics-server ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2fe81b52-d026-11e9-a0ad-8693e9898db7 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2fe81b52-d026-11e9-a0ad-8693e9898db7 off the node appserv10
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2fe81b52-d026-11e9-a0ad-8693e9898db7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:43:28.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8564" for this suite.
Sep  5 21:43:56.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:43:56.414: INFO: namespace sched-pred-8564 deletion completed in 28.107635351s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:34.354 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:43:56.414: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:43:56.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4327055d-d026-11e9-a0ad-8693e9898db7" in namespace "downward-api-8633" to be "success or failure"
Sep  5 21:43:56.558: INFO: Pod "downwardapi-volume-4327055d-d026-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.085348ms
Sep  5 21:43:58.563: INFO: Pod "downwardapi-volume-4327055d-d026-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007710182s
STEP: Saw pod success
Sep  5 21:43:58.563: INFO: Pod "downwardapi-volume-4327055d-d026-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:43:58.566: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-4327055d-d026-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:43:58.587: INFO: Waiting for pod downwardapi-volume-4327055d-d026-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:43:58.590: INFO: Pod downwardapi-volume-4327055d-d026-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:43:58.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8633" for this suite.
Sep  5 21:44:04.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:44:04.708: INFO: namespace downward-api-8633 deletion completed in 6.113955916s

• [SLOW TEST:8.294 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:44:04.708: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-48197fd2-d026-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 21:44:04.859: INFO: Waiting up to 5m0s for pod "pod-secrets-481a149e-d026-11e9-a0ad-8693e9898db7" in namespace "secrets-6365" to be "success or failure"
Sep  5 21:44:04.862: INFO: Pod "pod-secrets-481a149e-d026-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.019311ms
Sep  5 21:44:06.866: INFO: Pod "pod-secrets-481a149e-d026-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006938143s
Sep  5 21:44:08.870: INFO: Pod "pod-secrets-481a149e-d026-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011007396s
STEP: Saw pod success
Sep  5 21:44:08.870: INFO: Pod "pod-secrets-481a149e-d026-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:44:08.874: INFO: Trying to get logs from node appserv9 pod pod-secrets-481a149e-d026-11e9-a0ad-8693e9898db7 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:44:08.894: INFO: Waiting for pod pod-secrets-481a149e-d026-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:44:08.897: INFO: Pod pod-secrets-481a149e-d026-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:44:08.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6365" for this suite.
Sep  5 21:44:14.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:44:15.016: INFO: namespace secrets-6365 deletion completed in 6.115062306s

• [SLOW TEST:10.308 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:44:15.016: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-4e3d99c0-d026-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 21:44:15.159: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e3e107c-d026-11e9-a0ad-8693e9898db7" in namespace "projected-9959" to be "success or failure"
Sep  5 21:44:15.162: INFO: Pod "pod-projected-configmaps-4e3e107c-d026-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.702647ms
Sep  5 21:44:17.166: INFO: Pod "pod-projected-configmaps-4e3e107c-d026-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006919018s
STEP: Saw pod success
Sep  5 21:44:17.166: INFO: Pod "pod-projected-configmaps-4e3e107c-d026-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:44:17.169: INFO: Trying to get logs from node appserv10 pod pod-projected-configmaps-4e3e107c-d026-11e9-a0ad-8693e9898db7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:44:17.189: INFO: Waiting for pod pod-projected-configmaps-4e3e107c-d026-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:44:17.191: INFO: Pod pod-projected-configmaps-4e3e107c-d026-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:44:17.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9959" for this suite.
Sep  5 21:44:23.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:44:23.307: INFO: namespace projected-9959 deletion completed in 6.111738097s

• [SLOW TEST:8.291 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:44:23.307: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 21:44:23.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-1602'
Sep  5 21:44:23.652: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 21:44:23.652: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Sep  5 21:44:25.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1602'
Sep  5 21:44:25.746: INFO: stderr: ""
Sep  5 21:44:25.746: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:44:25.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1602" for this suite.
Sep  5 21:44:47.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:44:47.852: INFO: namespace kubectl-1602 deletion completed in 22.099709796s

• [SLOW TEST:24.545 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:44:47.853: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Sep  5 21:44:47.996: INFO: Waiting up to 5m0s for pod "var-expansion-61d05db1-d026-11e9-a0ad-8693e9898db7" in namespace "var-expansion-6442" to be "success or failure"
Sep  5 21:44:47.999: INFO: Pod "var-expansion-61d05db1-d026-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778809ms
Sep  5 21:44:50.003: INFO: Pod "var-expansion-61d05db1-d026-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006970853s
STEP: Saw pod success
Sep  5 21:44:50.003: INFO: Pod "var-expansion-61d05db1-d026-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:44:50.006: INFO: Trying to get logs from node appserv10 pod var-expansion-61d05db1-d026-11e9-a0ad-8693e9898db7 container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:44:50.025: INFO: Waiting for pod var-expansion-61d05db1-d026-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:44:50.028: INFO: Pod var-expansion-61d05db1-d026-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:44:50.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6442" for this suite.
Sep  5 21:44:56.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:44:56.122: INFO: namespace var-expansion-6442 deletion completed in 6.090025577s

• [SLOW TEST:8.270 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:44:56.123: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-66bdb0f4-d026-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 21:44:56.279: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-66c06b6f-d026-11e9-a0ad-8693e9898db7" in namespace "projected-2097" to be "success or failure"
Sep  5 21:44:56.282: INFO: Pod "pod-projected-configmaps-66c06b6f-d026-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527042ms
Sep  5 21:44:58.286: INFO: Pod "pod-projected-configmaps-66c06b6f-d026-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006293679s
STEP: Saw pod success
Sep  5 21:44:58.286: INFO: Pod "pod-projected-configmaps-66c06b6f-d026-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:44:58.289: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-66c06b6f-d026-11e9-a0ad-8693e9898db7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:44:58.307: INFO: Waiting for pod pod-projected-configmaps-66c06b6f-d026-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:44:58.310: INFO: Pod pod-projected-configmaps-66c06b6f-d026-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:44:58.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2097" for this suite.
Sep  5 21:45:04.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:45:04.426: INFO: namespace projected-2097 deletion completed in 6.112756668s

• [SLOW TEST:8.304 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:45:04.427: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:45:04.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bb1de95-d026-11e9-a0ad-8693e9898db7" in namespace "projected-4302" to be "success or failure"
Sep  5 21:45:04.577: INFO: Pod "downwardapi-volume-6bb1de95-d026-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.909763ms
Sep  5 21:45:06.581: INFO: Pod "downwardapi-volume-6bb1de95-d026-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006886214s
STEP: Saw pod success
Sep  5 21:45:06.581: INFO: Pod "downwardapi-volume-6bb1de95-d026-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:45:06.584: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-6bb1de95-d026-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:45:06.602: INFO: Waiting for pod downwardapi-volume-6bb1de95-d026-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:45:06.604: INFO: Pod downwardapi-volume-6bb1de95-d026-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:45:06.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4302" for this suite.
Sep  5 21:45:12.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:45:12.721: INFO: namespace projected-4302 deletion completed in 6.112438402s

• [SLOW TEST:8.294 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:45:12.722: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1856
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  5 21:45:16.899: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:16.902: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:18.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:18.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:20.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:20.907: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:22.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:22.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:24.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:24.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:26.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:26.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:28.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:28.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:30.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:30.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:32.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:32.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:34.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:34.907: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:36.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:36.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:38.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:38.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:40.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:40.907: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:42.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:42.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:44.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:44.907: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:46.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:46.906: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 21:45:48.902: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 21:45:48.906: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:45:48.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1856" for this suite.
Sep  5 21:46:10.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:46:11.037: INFO: namespace container-lifecycle-hook-1856 deletion completed in 22.117151149s

• [SLOW TEST:58.315 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:46:11.037: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Sep  5 21:46:11.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-3252'
Sep  5 21:46:11.413: INFO: stderr: ""
Sep  5 21:46:11.413: INFO: stdout: "pod/pause created\n"
Sep  5 21:46:11.413: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  5 21:46:11.413: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3252" to be "running and ready"
Sep  5 21:46:11.417: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.46731ms
Sep  5 21:46:13.421: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007577816s
Sep  5 21:46:13.421: INFO: Pod "pause" satisfied condition "running and ready"
Sep  5 21:46:13.421: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  5 21:46:13.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 label pods pause testing-label=testing-label-value --namespace=kubectl-3252'
Sep  5 21:46:13.547: INFO: stderr: ""
Sep  5 21:46:13.547: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  5 21:46:13.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pod pause -L testing-label --namespace=kubectl-3252'
Sep  5 21:46:13.655: INFO: stderr: ""
Sep  5 21:46:13.655: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  5 21:46:13.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 label pods pause testing-label- --namespace=kubectl-3252'
Sep  5 21:46:13.765: INFO: stderr: ""
Sep  5 21:46:13.765: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  5 21:46:13.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pod pause -L testing-label --namespace=kubectl-3252'
Sep  5 21:46:13.877: INFO: stderr: ""
Sep  5 21:46:13.877: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Sep  5 21:46:13.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-3252'
Sep  5 21:46:13.978: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:46:13.978: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  5 21:46:13.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get rc,svc -l name=pause --no-headers --namespace=kubectl-3252'
Sep  5 21:46:14.082: INFO: stderr: "No resources found.\n"
Sep  5 21:46:14.082: INFO: stdout: ""
Sep  5 21:46:14.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -l name=pause --namespace=kubectl-3252 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:46:14.176: INFO: stderr: ""
Sep  5 21:46:14.176: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:46:14.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3252" for this suite.
Sep  5 21:46:20.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:46:20.291: INFO: namespace kubectl-3252 deletion completed in 6.111507832s

• [SLOW TEST:9.254 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:46:20.292: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-909
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-909
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-909
Sep  5 21:46:20.444: INFO: Found 0 stateful pods, waiting for 1
Sep  5 21:46:30.449: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  5 21:46:30.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:46:30.684: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:46:30.684: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:46:30.684: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 21:46:30.687: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  5 21:46:40.691: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:46:40.691: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:46:40.707: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  5 21:46:40.707: INFO: ss-0  appserv10  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  }]
Sep  5 21:46:40.707: INFO: 
Sep  5 21:46:40.707: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  5 21:46:41.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996276226s
Sep  5 21:46:42.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99049175s
Sep  5 21:46:43.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986132589s
Sep  5 21:46:44.726: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981802182s
Sep  5 21:46:45.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977290784s
Sep  5 21:46:46.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972946628s
Sep  5 21:46:47.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968967487s
Sep  5 21:46:48.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965041499s
Sep  5 21:46:49.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.679961ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-909
Sep  5 21:46:50.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:46:50.980: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 21:46:50.980: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 21:46:50.980: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 21:46:50.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:46:51.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  5 21:46:51.203: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 21:46:51.203: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 21:46:51.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:46:51.431: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  5 21:46:51.431: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 21:46:51.431: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 21:46:51.435: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep  5 21:47:01.441: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:47:01.441: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:47:01.441: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  5 21:47:01.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:47:01.680: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:47:01.680: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:47:01.680: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 21:47:01.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:47:01.906: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:47:01.906: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:47:01.906: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 21:47:01.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 21:47:02.132: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 21:47:02.132: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 21:47:02.132: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 21:47:02.132: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:47:02.136: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  5 21:47:12.144: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:47:12.144: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:47:12.144: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:47:12.155: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  5 21:47:12.155: INFO: ss-0  appserv10  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  }]
Sep  5 21:47:12.155: INFO: ss-1  appserv9   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:12.155: INFO: ss-2  appserv11  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:12.155: INFO: 
Sep  5 21:47:12.155: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 21:47:13.161: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  5 21:47:13.161: INFO: ss-0  appserv10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  }]
Sep  5 21:47:13.161: INFO: ss-1  appserv9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:13.161: INFO: ss-2  appserv11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:13.161: INFO: 
Sep  5 21:47:13.161: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 21:47:14.166: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  5 21:47:14.166: INFO: ss-0  appserv10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  }]
Sep  5 21:47:14.166: INFO: ss-1  appserv9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:14.166: INFO: ss-2  appserv11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:14.166: INFO: 
Sep  5 21:47:14.166: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 21:47:15.171: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  5 21:47:15.171: INFO: ss-0  appserv10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  }]
Sep  5 21:47:15.171: INFO: ss-1  appserv9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:15.171: INFO: ss-2  appserv11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:15.171: INFO: 
Sep  5 21:47:15.171: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 21:47:16.175: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  5 21:47:16.175: INFO: ss-0  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  }]
Sep  5 21:47:16.175: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:16.175: INFO: ss-2  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:16.175: INFO: 
Sep  5 21:47:16.175: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 21:47:17.179: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  5 21:47:17.179: INFO: ss-0  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  }]
Sep  5 21:47:17.179: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:17.179: INFO: ss-2  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:17.179: INFO: 
Sep  5 21:47:17.179: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 21:47:18.184: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  5 21:47:18.184: INFO: ss-0  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:20 +0000 UTC  }]
Sep  5 21:47:18.184: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:18.184: INFO: ss-2  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:18.184: INFO: 
Sep  5 21:47:18.184: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 21:47:19.188: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  5 21:47:19.188: INFO: ss-1  appserv9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:19.188: INFO: 
Sep  5 21:47:19.188: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  5 21:47:20.192: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  5 21:47:20.192: INFO: ss-1  appserv9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:20.192: INFO: 
Sep  5 21:47:20.192: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  5 21:47:21.197: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  5 21:47:21.197: INFO: ss-1  appserv9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:47:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 21:46:40 +0000 UTC  }]
Sep  5 21:47:21.197: INFO: 
Sep  5 21:47:21.197: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-909
Sep  5 21:47:22.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:47:22.352: INFO: rc: 1
Sep  5 21:47:22.352: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0029badb0 exit status 1 <nil> <nil> true [0xc0032fa570 0xc0032fa588 0xc0032fa5a0] [0xc0032fa570 0xc0032fa588 0xc0032fa5a0] [0xc0032fa580 0xc0032fa598] [0x9c00a0 0x9c00a0] 0xc003693da0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Sep  5 21:47:32.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:47:32.463: INFO: rc: 1
Sep  5 21:47:32.463: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003128a80 exit status 1 <nil> <nil> true [0xc0030b2010 0xc0030b2028 0xc0030b2040] [0xc0030b2010 0xc0030b2028 0xc0030b2040] [0xc0030b2020 0xc0030b2038] [0x9c00a0 0x9c00a0] 0xc0028b5c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:47:42.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:47:42.572: INFO: rc: 1
Sep  5 21:47:42.573: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003128ed0 exit status 1 <nil> <nil> true [0xc0030b2048 0xc0030b2060 0xc0030b2078] [0xc0030b2048 0xc0030b2060 0xc0030b2078] [0xc0030b2058 0xc0030b2070] [0x9c00a0 0x9c00a0] 0xc0024c40c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:47:52.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:47:52.683: INFO: rc: 1
Sep  5 21:47:52.683: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003129260 exit status 1 <nil> <nil> true [0xc0030b2080 0xc0030b2098 0xc0030b20b0] [0xc0030b2080 0xc0030b2098 0xc0030b20b0] [0xc0030b2090 0xc0030b20a8] [0x9c00a0 0x9c00a0] 0xc0024c4a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:48:02.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:48:02.788: INFO: rc: 1
Sep  5 21:48:02.788: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00211c360 exit status 1 <nil> <nil> true [0xc0022f4088 0xc0022f4148 0xc0022f4190] [0xc0022f4088 0xc0022f4148 0xc0022f4190] [0xc0022f4110 0xc0022f4180] [0x9c00a0 0x9c00a0] 0xc0028b4300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:48:12.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:48:12.897: INFO: rc: 1
Sep  5 21:48:12.897: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002890330 exit status 1 <nil> <nil> true [0xc0001ac320 0xc0001ad310 0xc0001adaa8] [0xc0001ac320 0xc0001ad310 0xc0001adaa8] [0xc0001ad278 0xc0001ad890] [0x9c00a0 0x9c00a0] 0xc0027ac600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:48:22.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:48:23.007: INFO: rc: 1
Sep  5 21:48:23.007: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0028906c0 exit status 1 <nil> <nil> true [0xc0001adb38 0xc0001adda0 0xc0001adf78] [0xc0001adb38 0xc0001adda0 0xc0001adf78] [0xc0001adc70 0xc0001adee0] [0x9c00a0 0x9c00a0] 0xc0027acfc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:48:33.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:48:33.124: INFO: rc: 1
Sep  5 21:48:33.124: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00211c720 exit status 1 <nil> <nil> true [0xc0022f41b0 0xc0022f4220 0xc0022f4288] [0xc0022f41b0 0xc0022f4220 0xc0022f4288] [0xc0022f4210 0xc0022f4268] [0x9c00a0 0x9c00a0] 0xc0028b4720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:48:43.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:48:43.232: INFO: rc: 1
Sep  5 21:48:43.232: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00350a330 exit status 1 <nil> <nil> true [0xc0006f5648 0xc0006f5900 0xc0006f5b68] [0xc0006f5648 0xc0006f5900 0xc0006f5b68] [0xc0006f5878 0xc0006f5b40] [0x9c00a0 0x9c00a0] 0xc002c768a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:48:53.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:48:53.345: INFO: rc: 1
Sep  5 21:48:53.345: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c62360 exit status 1 <nil> <nil> true [0xc002804000 0xc002804018 0xc002804030] [0xc002804000 0xc002804018 0xc002804030] [0xc002804010 0xc002804028] [0x9c00a0 0x9c00a0] 0xc00265fce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:49:03.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:49:03.461: INFO: rc: 1
Sep  5 21:49:03.461: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00211cab0 exit status 1 <nil> <nil> true [0xc0022f4298 0xc0022f42b0 0xc0022f42e0] [0xc0022f4298 0xc0022f42b0 0xc0022f42e0] [0xc0022f42a8 0xc0022f42d0] [0x9c00a0 0x9c00a0] 0xc0028b4cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:49:13.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:49:13.572: INFO: rc: 1
Sep  5 21:49:13.573: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00211ce40 exit status 1 <nil> <nil> true [0xc0022f42e8 0xc0022f4300 0xc0022f4330] [0xc0022f42e8 0xc0022f4300 0xc0022f4330] [0xc0022f42f8 0xc0022f4318] [0x9c00a0 0x9c00a0] 0xc0028b52c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:49:23.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:49:23.679: INFO: rc: 1
Sep  5 21:49:23.679: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002890a20 exit status 1 <nil> <nil> true [0xc0001adfa8 0xc002836008 0xc002836020] [0xc0001adfa8 0xc002836008 0xc002836020] [0xc002836000 0xc002836018] [0x9c00a0 0x9c00a0] 0xc0027ad6e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:49:33.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:49:33.782: INFO: rc: 1
Sep  5 21:49:33.782: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00211d320 exit status 1 <nil> <nil> true [0xc0022f4358 0xc0022f4388 0xc0022f43b8] [0xc0022f4358 0xc0022f4388 0xc0022f43b8] [0xc0022f4380 0xc0022f43a8] [0x9c00a0 0x9c00a0] 0xc0028b58c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:49:43.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:49:43.889: INFO: rc: 1
Sep  5 21:49:43.889: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00211d6b0 exit status 1 <nil> <nil> true [0xc0022f43c0 0xc0022f43d8 0xc0022f44b8] [0xc0022f43c0 0xc0022f43d8 0xc0022f44b8] [0xc0022f43d0 0xc0022f4490] [0x9c00a0 0x9c00a0] 0xc0028b5e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:49:53.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:49:54.004: INFO: rc: 1
Sep  5 21:49:54.004: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002890db0 exit status 1 <nil> <nil> true [0xc002836028 0xc002836040 0xc002836058] [0xc002836028 0xc002836040 0xc002836058] [0xc002836038 0xc002836050] [0x9c00a0 0x9c00a0] 0xc0027adb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:50:04.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:50:04.109: INFO: rc: 1
Sep  5 21:50:04.109: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002890360 exit status 1 <nil> <nil> true [0xc0001ad028 0xc0001ad810 0xc0001adb38] [0xc0001ad028 0xc0001ad810 0xc0001adb38] [0xc0001ad310 0xc0001adaa8] [0x9c00a0 0x9c00a0] 0xc0027ac600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:50:14.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:50:14.212: INFO: rc: 1
Sep  5 21:50:14.212: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002890720 exit status 1 <nil> <nil> true [0xc0001adc10 0xc0001adec8 0xc0001adfa8] [0xc0001adc10 0xc0001adec8 0xc0001adfa8] [0xc0001adda0 0xc0001adf78] [0x9c00a0 0x9c00a0] 0xc0027acfc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:50:24.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:50:24.325: INFO: rc: 1
Sep  5 21:50:24.325: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c62330 exit status 1 <nil> <nil> true [0xc002836000 0xc002836018 0xc002836030] [0xc002836000 0xc002836018 0xc002836030] [0xc002836010 0xc002836028] [0x9c00a0 0x9c00a0] 0xc002c768a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:50:34.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:50:34.426: INFO: rc: 1
Sep  5 21:50:34.427: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c626c0 exit status 1 <nil> <nil> true [0xc002836038 0xc002836050 0xc002836068] [0xc002836038 0xc002836050 0xc002836068] [0xc002836048 0xc002836060] [0x9c00a0 0x9c00a0] 0xc002c77920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:50:44.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:50:44.528: INFO: rc: 1
Sep  5 21:50:44.528: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c62a20 exit status 1 <nil> <nil> true [0xc002836070 0xc002836088 0xc0028360a0] [0xc002836070 0xc002836088 0xc0028360a0] [0xc002836080 0xc002836098] [0x9c00a0 0x9c00a0] 0xc00265f3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:50:54.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:50:54.629: INFO: rc: 1
Sep  5 21:50:54.629: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c62de0 exit status 1 <nil> <nil> true [0xc0028360a8 0xc0028360c0 0xc0028360d8] [0xc0028360a8 0xc0028360c0 0xc0028360d8] [0xc0028360b8 0xc0028360d0] [0x9c00a0 0x9c00a0] 0xc0028b41e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:51:04.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:51:04.738: INFO: rc: 1
Sep  5 21:51:04.738: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002890b10 exit status 1 <nil> <nil> true [0xc0001adfe8 0xc0006f5878 0xc0006f5b40] [0xc0001adfe8 0xc0006f5878 0xc0006f5b40] [0xc0006f5658 0xc0006f5960] [0x9c00a0 0x9c00a0] 0xc0027ad6e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:51:14.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:51:14.848: INFO: rc: 1
Sep  5 21:51:14.849: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002890ea0 exit status 1 <nil> <nil> true [0xc0006f5b68 0xc0006f5d10 0xc0006f5f20] [0xc0006f5b68 0xc0006f5d10 0xc0006f5f20] [0xc0006f5bc0 0xc0006f5e48] [0x9c00a0 0x9c00a0] 0xc0027adb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:51:24.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:51:24.952: INFO: rc: 1
Sep  5 21:51:24.952: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002891350 exit status 1 <nil> <nil> true [0xc0006f5fb8 0xc002804010 0xc002804028] [0xc0006f5fb8 0xc002804010 0xc002804028] [0xc002804008 0xc002804020] [0x9c00a0 0x9c00a0] 0xc0027adf20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:51:34.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:51:35.068: INFO: rc: 1
Sep  5 21:51:35.068: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0028916b0 exit status 1 <nil> <nil> true [0xc002804030 0xc002804048 0xc002804060] [0xc002804030 0xc002804048 0xc002804060] [0xc002804040 0xc002804058] [0x9c00a0 0x9c00a0] 0xc003010420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:51:45.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:51:45.176: INFO: rc: 1
Sep  5 21:51:45.176: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00350a3c0 exit status 1 <nil> <nil> true [0xc0022f4030 0xc0022f4110 0xc0022f4180] [0xc0022f4030 0xc0022f4110 0xc0022f4180] [0xc0022f4090 0xc0022f4150] [0x9c00a0 0x9c00a0] 0xc0033bcf60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:51:55.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:51:55.276: INFO: rc: 1
Sep  5 21:51:55.276: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00211c3f0 exit status 1 <nil> <nil> true [0xc002fbe000 0xc002fbe018 0xc002fbe030] [0xc002fbe000 0xc002fbe018 0xc002fbe030] [0xc002fbe010 0xc002fbe028] [0x9c00a0 0x9c00a0] 0xc00248d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:52:05.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:52:05.379: INFO: rc: 1
Sep  5 21:52:05.379: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00211c330 exit status 1 <nil> <nil> true [0xc0006f5658 0xc0006f5960 0xc0006f5b98] [0xc0006f5658 0xc0006f5960 0xc0006f5b98] [0xc0006f5900 0xc0006f5b68] [0x9c00a0 0x9c00a0] 0xc00265fce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:52:15.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:52:15.479: INFO: rc: 1
Sep  5 21:52:15.479: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c62360 exit status 1 <nil> <nil> true [0xc0001ac320 0xc0001ad310 0xc0001adaa8] [0xc0001ac320 0xc0001ad310 0xc0001adaa8] [0xc0001ad278 0xc0001ad890] [0x9c00a0 0x9c00a0] 0xc002c768a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  5 21:52:25.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 exec --namespace=statefulset-909 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 21:52:25.575: INFO: rc: 1
Sep  5 21:52:25.575: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Sep  5 21:52:25.575: INFO: Scaling statefulset ss to 0
Sep  5 21:52:25.585: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  5 21:52:25.588: INFO: Deleting all statefulset in ns statefulset-909
Sep  5 21:52:25.591: INFO: Scaling statefulset ss to 0
Sep  5 21:52:25.600: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:52:25.603: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:52:25.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-909" for this suite.
Sep  5 21:52:31.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:52:31.730: INFO: namespace statefulset-909 deletion completed in 6.11256094s

• [SLOW TEST:371.438 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:52:31.730: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-31
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0905 21:52:37.900462      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 21:52:37.900: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:52:37.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-31" for this suite.
Sep  5 21:52:43.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:52:44.018: INFO: namespace gc-31 deletion completed in 6.114280426s

• [SLOW TEST:12.288 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:52:44.019: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7097
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  5 21:52:44.166: INFO: Waiting up to 5m0s for pod "pod-7da20588-d027-11e9-a0ad-8693e9898db7" in namespace "emptydir-7097" to be "success or failure"
Sep  5 21:52:44.169: INFO: Pod "pod-7da20588-d027-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949964ms
Sep  5 21:52:46.172: INFO: Pod "pod-7da20588-d027-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006551635s
STEP: Saw pod success
Sep  5 21:52:46.173: INFO: Pod "pod-7da20588-d027-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:52:46.175: INFO: Trying to get logs from node appserv10 pod pod-7da20588-d027-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 21:52:46.193: INFO: Waiting for pod pod-7da20588-d027-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:52:46.195: INFO: Pod pod-7da20588-d027-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:52:46.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7097" for this suite.
Sep  5 21:52:52.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:52:52.306: INFO: namespace emptydir-7097 deletion completed in 6.106608844s

• [SLOW TEST:8.287 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:52:52.306: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:52:54.476: INFO: Waiting up to 5m0s for pod "client-envvars-83c7a6d3-d027-11e9-a0ad-8693e9898db7" in namespace "pods-3626" to be "success or failure"
Sep  5 21:52:54.478: INFO: Pod "client-envvars-83c7a6d3-d027-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.825369ms
Sep  5 21:52:56.482: INFO: Pod "client-envvars-83c7a6d3-d027-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006779967s
Sep  5 21:52:58.487: INFO: Pod "client-envvars-83c7a6d3-d027-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011341016s
STEP: Saw pod success
Sep  5 21:52:58.487: INFO: Pod "client-envvars-83c7a6d3-d027-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:52:58.490: INFO: Trying to get logs from node appserv9 pod client-envvars-83c7a6d3-d027-11e9-a0ad-8693e9898db7 container env3cont: <nil>
STEP: delete the pod
Sep  5 21:52:58.508: INFO: Waiting for pod client-envvars-83c7a6d3-d027-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:52:58.511: INFO: Pod client-envvars-83c7a6d3-d027-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:52:58.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3626" for this suite.
Sep  5 21:53:58.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:53:58.631: INFO: namespace pods-3626 deletion completed in 1m0.115660832s

• [SLOW TEST:66.325 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:53:58.631: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2371
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-aa1bac49-d027-11e9-a0ad-8693e9898db7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-aa1bac49-d027-11e9-a0ad-8693e9898db7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:54:02.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2371" for this suite.
Sep  5 21:54:24.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:54:24.944: INFO: namespace projected-2371 deletion completed in 22.109176494s

• [SLOW TEST:26.313 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:54:24.944: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 21:54:25.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6944'
Sep  5 21:54:25.278: INFO: stderr: ""
Sep  5 21:54:25.278: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep  5 21:54:30.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pod e2e-test-nginx-pod --namespace=kubectl-6944 -o json'
Sep  5 21:54:30.454: INFO: stderr: ""
Sep  5 21:54:30.454: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"collecting\"\n        },\n        \"creationTimestamp\": \"2019-09-05T21:54:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6944\",\n        \"resourceVersion\": \"21337\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6944/pods/e2e-test-nginx-pod\",\n        \"uid\": \"b9e54e1e-d027-11e9-9b5e-2c600c82ec72\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-txn6c\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"appserv9\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-txn6c\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-txn6c\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-05T21:54:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-05T21:54:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-05T21:54:26Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-05T21:54:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://7ecd40cbc35b9b61f2bee441b15bbe47ab540223ff13dd69665ae78a15dab04d\",\n                \"image\": \"docker.io/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-05T21:54:26Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.6.109\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.141.11\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-05T21:54:25Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  5 21:54:30.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 replace -f - --namespace=kubectl-6944'
Sep  5 21:54:30.667: INFO: stderr: ""
Sep  5 21:54:30.667: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Sep  5 21:54:30.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete pods e2e-test-nginx-pod --namespace=kubectl-6944'
Sep  5 21:54:34.846: INFO: stderr: ""
Sep  5 21:54:34.846: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:54:34.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6944" for this suite.
Sep  5 21:54:40.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:54:40.962: INFO: namespace kubectl-6944 deletion completed in 6.110576964s

• [SLOW TEST:16.018 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:54:40.962: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-c355d4e7-d027-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 21:54:41.111: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c356620d-d027-11e9-a0ad-8693e9898db7" in namespace "projected-7155" to be "success or failure"
Sep  5 21:54:41.114: INFO: Pod "pod-projected-secrets-c356620d-d027-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.547141ms
Sep  5 21:54:43.118: INFO: Pod "pod-projected-secrets-c356620d-d027-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007365072s
STEP: Saw pod success
Sep  5 21:54:43.118: INFO: Pod "pod-projected-secrets-c356620d-d027-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:54:43.122: INFO: Trying to get logs from node appserv10 pod pod-projected-secrets-c356620d-d027-11e9-a0ad-8693e9898db7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:54:43.141: INFO: Waiting for pod pod-projected-secrets-c356620d-d027-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:54:43.144: INFO: Pod pod-projected-secrets-c356620d-d027-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:54:43.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7155" for this suite.
Sep  5 21:54:49.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:54:49.260: INFO: namespace projected-7155 deletion completed in 6.111685122s

• [SLOW TEST:8.298 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:54:49.260: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep  5 21:54:49.400: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  5 21:54:49.408: INFO: Waiting for terminating namespaces to be deleted...
Sep  5 21:54:49.411: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Sep  5 21:54:49.418: INFO: snapshot-controller-7cbc6c6455-2jwnr from diamanti-system started at 2019-09-05 20:39:12 +0000 UTC (2 container statuses recorded)
Sep  5 21:54:49.418: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep  5 21:54:49.418: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Sep  5 21:54:49.418: INFO: prometheus-v1-0 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:54:49.418: INFO: 	Container prometheus ready: true, restart count 0
Sep  5 21:54:49.418: INFO: sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-5x8tc from heptio-sonobuoy started at 2019-09-05 20:39:30 +0000 UTC (2 container statuses recorded)
Sep  5 21:54:49.418: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  5 21:54:49.418: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  5 21:54:49.418: INFO: collectd-v0.8-dflpk from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (4 container statuses recorded)
Sep  5 21:54:49.418: INFO: 	Container cadvisor ready: true, restart count 0
Sep  5 21:54:49.418: INFO: 	Container collectd-es ready: true, restart count 0
Sep  5 21:54:49.418: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  5 21:54:49.418: INFO: 	Container node-exporter ready: true, restart count 0
Sep  5 21:54:49.418: INFO: helm-chart-547cc86f54-kdx6m from kube-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:54:49.418: INFO: 	Container helm-chart ready: true, restart count 0
Sep  5 21:54:49.418: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Sep  5 21:54:49.425: INFO: provisioner-677d65965-9ld29 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:54:49.425: INFO: 	Container provisioner ready: true, restart count 0
Sep  5 21:54:49.425: INFO: sonobuoy-e2e-job-f5a25811b59246f1 from heptio-sonobuoy started at 2019-09-05 20:39:31 +0000 UTC (2 container statuses recorded)
Sep  5 21:54:49.425: INFO: 	Container e2e ready: true, restart count 0
Sep  5 21:54:49.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  5 21:54:49.425: INFO: sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-8248f from heptio-sonobuoy started at 2019-09-05 20:39:31 +0000 UTC (2 container statuses recorded)
Sep  5 21:54:49.425: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  5 21:54:49.425: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  5 21:54:49.425: INFO: collectd-v0.8-8b4p5 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (4 container statuses recorded)
Sep  5 21:54:49.425: INFO: 	Container cadvisor ready: true, restart count 0
Sep  5 21:54:49.425: INFO: 	Container collectd-es ready: true, restart count 0
Sep  5 21:54:49.425: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  5 21:54:49.425: INFO: 	Container node-exporter ready: true, restart count 0
Sep  5 21:54:49.425: INFO: prometheus-v1-2 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:54:49.425: INFO: 	Container prometheus ready: true, restart count 0
Sep  5 21:54:49.425: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-05 20:39:30 +0000 UTC (1 container statuses recorded)
Sep  5 21:54:49.425: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  5 21:54:49.425: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Sep  5 21:54:49.432: INFO: metrics-server-v1-f8c95fb9c-jv47r from kube-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:54:49.432: INFO: 	Container metrics-server ready: true, restart count 0
Sep  5 21:54:49.432: INFO: collectd-v0.8-7bjvl from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (4 container statuses recorded)
Sep  5 21:54:49.432: INFO: 	Container cadvisor ready: true, restart count 0
Sep  5 21:54:49.432: INFO: 	Container collectd-es ready: true, restart count 0
Sep  5 21:54:49.432: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  5 21:54:49.432: INFO: 	Container node-exporter ready: true, restart count 0
Sep  5 21:54:49.432: INFO: prometheus-v1-1 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:54:49.432: INFO: 	Container prometheus ready: true, restart count 0
Sep  5 21:54:49.432: INFO: tiller-deploy-7ddbd797c7-kxxhl from kube-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:54:49.432: INFO: 	Container tiller ready: true, restart count 0
Sep  5 21:54:49.432: INFO: alertmanager-0 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:54:49.432: INFO: 	Container alertmanager ready: true, restart count 0
Sep  5 21:54:49.432: INFO: sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-75kqn from heptio-sonobuoy started at 2019-09-05 20:39:31 +0000 UTC (2 container statuses recorded)
Sep  5 21:54:49.432: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  5 21:54:49.432: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c1a97aa0d090a5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:54:50.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3137" for this suite.
Sep  5 21:54:56.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:54:56.559: INFO: namespace sched-pred-3137 deletion completed in 6.10025724s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.299 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:54:56.559: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-cca15fca-d027-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 21:54:56.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cca2154c-d027-11e9-a0ad-8693e9898db7" in namespace "projected-3470" to be "success or failure"
Sep  5 21:54:56.709: INFO: Pod "pod-projected-configmaps-cca2154c-d027-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.696426ms
Sep  5 21:54:58.713: INFO: Pod "pod-projected-configmaps-cca2154c-d027-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006791848s
STEP: Saw pod success
Sep  5 21:54:58.713: INFO: Pod "pod-projected-configmaps-cca2154c-d027-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:54:58.716: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-cca2154c-d027-11e9-a0ad-8693e9898db7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:54:58.736: INFO: Waiting for pod pod-projected-configmaps-cca2154c-d027-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:54:58.739: INFO: Pod pod-projected-configmaps-cca2154c-d027-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:54:58.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3470" for this suite.
Sep  5 21:55:04.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:55:04.857: INFO: namespace projected-3470 deletion completed in 6.113783956s

• [SLOW TEST:8.298 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:55:04.857: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-8678/configmap-test-d19407d7-d027-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 21:55:05.006: INFO: Waiting up to 5m0s for pod "pod-configmaps-d194a0d8-d027-11e9-a0ad-8693e9898db7" in namespace "configmap-8678" to be "success or failure"
Sep  5 21:55:05.009: INFO: Pod "pod-configmaps-d194a0d8-d027-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.975676ms
Sep  5 21:55:07.013: INFO: Pod "pod-configmaps-d194a0d8-d027-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006816981s
Sep  5 21:55:09.020: INFO: Pod "pod-configmaps-d194a0d8-d027-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013038149s
STEP: Saw pod success
Sep  5 21:55:09.020: INFO: Pod "pod-configmaps-d194a0d8-d027-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:55:09.023: INFO: Trying to get logs from node appserv10 pod pod-configmaps-d194a0d8-d027-11e9-a0ad-8693e9898db7 container env-test: <nil>
STEP: delete the pod
Sep  5 21:55:09.043: INFO: Waiting for pod pod-configmaps-d194a0d8-d027-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:55:09.046: INFO: Pod pod-configmaps-d194a0d8-d027-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:55:09.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8678" for this suite.
Sep  5 21:55:15.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:55:15.165: INFO: namespace configmap-8678 deletion completed in 6.1145024s

• [SLOW TEST:10.308 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:55:15.165: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3607
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3607.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3607.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 21:55:19.362: INFO: DNS probes using dns-3607/dns-test-d7b8efb6-d027-11e9-a0ad-8693e9898db7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:55:19.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3607" for this suite.
Sep  5 21:55:25.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:55:25.489: INFO: namespace dns-3607 deletion completed in 6.111077863s

• [SLOW TEST:10.324 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:55:25.490: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 21:55:25.651: INFO: Number of nodes with available pods: 0
Sep  5 21:55:25.651: INFO: Node appserv10 is running more than one daemon pod
Sep  5 21:55:26.657: INFO: Number of nodes with available pods: 0
Sep  5 21:55:26.657: INFO: Node appserv10 is running more than one daemon pod
Sep  5 21:55:27.659: INFO: Number of nodes with available pods: 2
Sep  5 21:55:27.659: INFO: Node appserv10 is running more than one daemon pod
Sep  5 21:55:28.659: INFO: Number of nodes with available pods: 3
Sep  5 21:55:28.659: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  5 21:55:28.681: INFO: Number of nodes with available pods: 2
Sep  5 21:55:28.681: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:29.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:29.689: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:30.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:30.689: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:31.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:31.689: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:32.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:32.689: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:33.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:33.689: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:34.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:34.689: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:35.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:35.689: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:36.688: INFO: Number of nodes with available pods: 2
Sep  5 21:55:36.688: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:37.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:37.689: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:38.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:38.689: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:39.689: INFO: Number of nodes with available pods: 2
Sep  5 21:55:39.690: INFO: Node appserv11 is running more than one daemon pod
Sep  5 21:55:40.689: INFO: Number of nodes with available pods: 3
Sep  5 21:55:40.689: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2717, will wait for the garbage collector to delete the pods
Sep  5 21:55:40.752: INFO: Deleting DaemonSet.extensions daemon-set took: 7.138458ms
Sep  5 21:55:41.252: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.263549ms
Sep  5 21:55:52.456: INFO: Number of nodes with available pods: 0
Sep  5 21:55:52.456: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 21:55:52.460: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2717/daemonsets","resourceVersion":"21797"},"items":null}

Sep  5 21:55:52.463: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2717/pods","resourceVersion":"21797"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:55:52.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2717" for this suite.
Sep  5 21:55:58.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:55:58.591: INFO: namespace daemonsets-2717 deletion completed in 6.107764691s

• [SLOW TEST:33.101 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:55:58.591: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  5 21:55:58.747: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5531,SelfLink:/api/v1/namespaces/watch-5531/configmaps/e2e-watch-test-label-changed,UID:f19b8afa-d027-11e9-9b5e-2c600c82ec72,ResourceVersion:21842,Generation:0,CreationTimestamp:2019-09-05 21:55:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 21:55:58.747: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5531,SelfLink:/api/v1/namespaces/watch-5531/configmaps/e2e-watch-test-label-changed,UID:f19b8afa-d027-11e9-9b5e-2c600c82ec72,ResourceVersion:21843,Generation:0,CreationTimestamp:2019-09-05 21:55:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  5 21:55:58.747: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5531,SelfLink:/api/v1/namespaces/watch-5531/configmaps/e2e-watch-test-label-changed,UID:f19b8afa-d027-11e9-9b5e-2c600c82ec72,ResourceVersion:21844,Generation:0,CreationTimestamp:2019-09-05 21:55:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  5 21:56:08.774: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5531,SelfLink:/api/v1/namespaces/watch-5531/configmaps/e2e-watch-test-label-changed,UID:f19b8afa-d027-11e9-9b5e-2c600c82ec72,ResourceVersion:21864,Generation:0,CreationTimestamp:2019-09-05 21:55:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 21:56:08.774: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5531,SelfLink:/api/v1/namespaces/watch-5531/configmaps/e2e-watch-test-label-changed,UID:f19b8afa-d027-11e9-9b5e-2c600c82ec72,ResourceVersion:21865,Generation:0,CreationTimestamp:2019-09-05 21:55:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep  5 21:56:08.774: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5531,SelfLink:/api/v1/namespaces/watch-5531/configmaps/e2e-watch-test-label-changed,UID:f19b8afa-d027-11e9-9b5e-2c600c82ec72,ResourceVersion:21866,Generation:0,CreationTimestamp:2019-09-05 21:55:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:56:08.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5531" for this suite.
Sep  5 21:56:14.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:56:14.888: INFO: namespace watch-5531 deletion completed in 6.109120444s

• [SLOW TEST:16.297 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:56:14.888: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5517
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  5 21:56:18.055: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:56:19.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5517" for this suite.
Sep  5 21:56:41.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:56:41.188: INFO: namespace replicaset-5517 deletion completed in 22.113984613s

• [SLOW TEST:26.300 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:56:41.188: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 21:56:41.329: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:56:43.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4778" for this suite.
Sep  5 21:57:33.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:57:33.572: INFO: namespace pods-4778 deletion completed in 50.116063397s

• [SLOW TEST:52.384 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:57:33.573: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:57:33.721: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a388b36-d028-11e9-a0ad-8693e9898db7" in namespace "projected-7920" to be "success or failure"
Sep  5 21:57:33.725: INFO: Pod "downwardapi-volume-2a388b36-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.577ms
Sep  5 21:57:35.729: INFO: Pod "downwardapi-volume-2a388b36-d028-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007534516s
STEP: Saw pod success
Sep  5 21:57:35.729: INFO: Pod "downwardapi-volume-2a388b36-d028-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:57:35.731: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-2a388b36-d028-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:57:35.747: INFO: Waiting for pod downwardapi-volume-2a388b36-d028-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:57:35.750: INFO: Pod downwardapi-volume-2a388b36-d028-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:57:35.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7920" for this suite.
Sep  5 21:57:41.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:57:41.862: INFO: namespace projected-7920 deletion completed in 6.10831615s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:57:41.862: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9499
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  5 21:57:42.009: INFO: Waiting up to 5m0s for pod "downward-api-2f294098-d028-11e9-a0ad-8693e9898db7" in namespace "downward-api-9499" to be "success or failure"
Sep  5 21:57:42.012: INFO: Pod "downward-api-2f294098-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.217723ms
Sep  5 21:57:44.017: INFO: Pod "downward-api-2f294098-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00764111s
Sep  5 21:57:46.021: INFO: Pod "downward-api-2f294098-d028-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011809283s
STEP: Saw pod success
Sep  5 21:57:46.021: INFO: Pod "downward-api-2f294098-d028-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:57:46.023: INFO: Trying to get logs from node appserv10 pod downward-api-2f294098-d028-11e9-a0ad-8693e9898db7 container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:57:46.041: INFO: Waiting for pod downward-api-2f294098-d028-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:57:46.043: INFO: Pod downward-api-2f294098-d028-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:57:46.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9499" for this suite.
Sep  5 21:57:52.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:57:52.160: INFO: namespace downward-api-9499 deletion completed in 6.112601293s

• [SLOW TEST:10.298 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:57:52.160: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep  5 21:57:52.299: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  5 21:57:52.307: INFO: Waiting for terminating namespaces to be deleted...
Sep  5 21:57:52.309: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Sep  5 21:57:52.316: INFO: collectd-v0.8-dflpk from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (4 container statuses recorded)
Sep  5 21:57:52.316: INFO: 	Container cadvisor ready: true, restart count 0
Sep  5 21:57:52.316: INFO: 	Container collectd-es ready: true, restart count 0
Sep  5 21:57:52.316: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  5 21:57:52.316: INFO: 	Container node-exporter ready: true, restart count 0
Sep  5 21:57:52.316: INFO: helm-chart-547cc86f54-kdx6m from kube-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:57:52.316: INFO: 	Container helm-chart ready: true, restart count 0
Sep  5 21:57:52.316: INFO: snapshot-controller-7cbc6c6455-2jwnr from diamanti-system started at 2019-09-05 20:39:12 +0000 UTC (2 container statuses recorded)
Sep  5 21:57:52.316: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep  5 21:57:52.316: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Sep  5 21:57:52.316: INFO: prometheus-v1-0 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:57:52.316: INFO: 	Container prometheus ready: true, restart count 0
Sep  5 21:57:52.316: INFO: sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-5x8tc from heptio-sonobuoy started at 2019-09-05 20:39:30 +0000 UTC (2 container statuses recorded)
Sep  5 21:57:52.316: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  5 21:57:52.316: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  5 21:57:52.316: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Sep  5 21:57:52.323: INFO: collectd-v0.8-8b4p5 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (4 container statuses recorded)
Sep  5 21:57:52.323: INFO: 	Container cadvisor ready: true, restart count 0
Sep  5 21:57:52.323: INFO: 	Container collectd-es ready: true, restart count 0
Sep  5 21:57:52.323: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  5 21:57:52.323: INFO: 	Container node-exporter ready: true, restart count 0
Sep  5 21:57:52.323: INFO: prometheus-v1-2 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:57:52.323: INFO: 	Container prometheus ready: true, restart count 0
Sep  5 21:57:52.323: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-05 20:39:30 +0000 UTC (1 container statuses recorded)
Sep  5 21:57:52.323: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  5 21:57:52.323: INFO: sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-8248f from heptio-sonobuoy started at 2019-09-05 20:39:31 +0000 UTC (2 container statuses recorded)
Sep  5 21:57:52.323: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  5 21:57:52.323: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  5 21:57:52.323: INFO: provisioner-677d65965-9ld29 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:57:52.323: INFO: 	Container provisioner ready: true, restart count 0
Sep  5 21:57:52.323: INFO: sonobuoy-e2e-job-f5a25811b59246f1 from heptio-sonobuoy started at 2019-09-05 20:39:31 +0000 UTC (2 container statuses recorded)
Sep  5 21:57:52.323: INFO: 	Container e2e ready: true, restart count 0
Sep  5 21:57:52.323: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  5 21:57:52.323: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Sep  5 21:57:52.330: INFO: alertmanager-0 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:57:52.330: INFO: 	Container alertmanager ready: true, restart count 0
Sep  5 21:57:52.330: INFO: sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-75kqn from heptio-sonobuoy started at 2019-09-05 20:39:31 +0000 UTC (2 container statuses recorded)
Sep  5 21:57:52.330: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  5 21:57:52.330: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  5 21:57:52.330: INFO: tiller-deploy-7ddbd797c7-kxxhl from kube-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:57:52.330: INFO: 	Container tiller ready: true, restart count 0
Sep  5 21:57:52.330: INFO: collectd-v0.8-7bjvl from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (4 container statuses recorded)
Sep  5 21:57:52.330: INFO: 	Container cadvisor ready: true, restart count 0
Sep  5 21:57:52.330: INFO: 	Container collectd-es ready: true, restart count 0
Sep  5 21:57:52.330: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  5 21:57:52.330: INFO: 	Container node-exporter ready: true, restart count 0
Sep  5 21:57:52.330: INFO: prometheus-v1-1 from diamanti-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:57:52.330: INFO: 	Container prometheus ready: true, restart count 0
Sep  5 21:57:52.330: INFO: metrics-server-v1-f8c95fb9c-jv47r from kube-system started at 2019-09-05 20:39:13 +0000 UTC (1 container statuses recorded)
Sep  5 21:57:52.330: INFO: 	Container metrics-server ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node appserv10
STEP: verifying the node has the label node appserv11
STEP: verifying the node has the label node appserv9
Sep  5 21:57:52.376: INFO: Pod alertmanager-0 requesting resource cpu=0m on Node appserv9
Sep  5 21:57:52.376: INFO: Pod collectd-v0.8-7bjvl requesting resource cpu=0m on Node appserv9
Sep  5 21:57:52.376: INFO: Pod collectd-v0.8-8b4p5 requesting resource cpu=0m on Node appserv11
Sep  5 21:57:52.376: INFO: Pod collectd-v0.8-dflpk requesting resource cpu=0m on Node appserv10
Sep  5 21:57:52.376: INFO: Pod prometheus-v1-0 requesting resource cpu=0m on Node appserv10
Sep  5 21:57:52.376: INFO: Pod prometheus-v1-1 requesting resource cpu=0m on Node appserv9
Sep  5 21:57:52.376: INFO: Pod prometheus-v1-2 requesting resource cpu=0m on Node appserv11
Sep  5 21:57:52.376: INFO: Pod provisioner-677d65965-9ld29 requesting resource cpu=0m on Node appserv11
Sep  5 21:57:52.376: INFO: Pod snapshot-controller-7cbc6c6455-2jwnr requesting resource cpu=0m on Node appserv10
Sep  5 21:57:52.376: INFO: Pod sonobuoy requesting resource cpu=0m on Node appserv11
Sep  5 21:57:52.376: INFO: Pod sonobuoy-e2e-job-f5a25811b59246f1 requesting resource cpu=0m on Node appserv11
Sep  5 21:57:52.376: INFO: Pod sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-5x8tc requesting resource cpu=0m on Node appserv10
Sep  5 21:57:52.376: INFO: Pod sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-75kqn requesting resource cpu=0m on Node appserv9
Sep  5 21:57:52.376: INFO: Pod sonobuoy-systemd-logs-daemon-set-a4d9dc11a4d64386-8248f requesting resource cpu=0m on Node appserv11
Sep  5 21:57:52.376: INFO: Pod helm-chart-547cc86f54-kdx6m requesting resource cpu=0m on Node appserv10
Sep  5 21:57:52.376: INFO: Pod metrics-server-v1-f8c95fb9c-jv47r requesting resource cpu=0m on Node appserv9
Sep  5 21:57:52.376: INFO: Pod tiller-deploy-7ddbd797c7-kxxhl requesting resource cpu=250m on Node appserv9
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35586b3d-d028-11e9-a0ad-8693e9898db7.15c1a9a53b77cd1e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2846/filler-pod-35586b3d-d028-11e9-a0ad-8693e9898db7 to appserv10]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35586b3d-d028-11e9-a0ad-8693e9898db7.15c1a9a579cd43d1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35586b3d-d028-11e9-a0ad-8693e9898db7.15c1a9a57c52c640], Reason = [Created], Message = [Created container filler-pod-35586b3d-d028-11e9-a0ad-8693e9898db7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35586b3d-d028-11e9-a0ad-8693e9898db7.15c1a9a5846c0cb1], Reason = [Started], Message = [Started container filler-pod-35586b3d-d028-11e9-a0ad-8693e9898db7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35599a31-d028-11e9-a0ad-8693e9898db7.15c1a9a53e58ac56], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2846/filler-pod-35599a31-d028-11e9-a0ad-8693e9898db7 to appserv11]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35599a31-d028-11e9-a0ad-8693e9898db7.15c1a9a57a1cda81], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35599a31-d028-11e9-a0ad-8693e9898db7.15c1a9a57cdbbfab], Reason = [Created], Message = [Created container filler-pod-35599a31-d028-11e9-a0ad-8693e9898db7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35599a31-d028-11e9-a0ad-8693e9898db7.15c1a9a584be191e], Reason = [Started], Message = [Started container filler-pod-35599a31-d028-11e9-a0ad-8693e9898db7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-355a500c-d028-11e9-a0ad-8693e9898db7.15c1a9a54108e6b0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2846/filler-pod-355a500c-d028-11e9-a0ad-8693e9898db7 to appserv9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-355a500c-d028-11e9-a0ad-8693e9898db7.15c1a9a57a4a597f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-355a500c-d028-11e9-a0ad-8693e9898db7.15c1a9a57c4cc4e0], Reason = [Created], Message = [Created container filler-pod-355a500c-d028-11e9-a0ad-8693e9898db7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-355a500c-d028-11e9-a0ad-8693e9898db7.15c1a9a58547445e], Reason = [Started], Message = [Started container filler-pod-355a500c-d028-11e9-a0ad-8693e9898db7]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c1a9a628e52bb4], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node appserv11
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node appserv9
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node appserv10
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:57:57.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2846" for this suite.
Sep  5 21:58:03.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:58:03.579: INFO: namespace sched-pred-2846 deletion completed in 6.114968962s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.419 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:58:03.580: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0905 21:58:04.759776      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 21:58:04.759: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:58:04.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4137" for this suite.
Sep  5 21:58:10.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:58:10.874: INFO: namespace gc-4137 deletion completed in 6.110548497s

• [SLOW TEST:7.294 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:58:10.874: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  5 21:58:11.020: INFO: Waiting up to 5m0s for pod "downward-api-4073e478-d028-11e9-a0ad-8693e9898db7" in namespace "downward-api-2825" to be "success or failure"
Sep  5 21:58:11.023: INFO: Pod "downward-api-4073e478-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.053291ms
Sep  5 21:58:13.028: INFO: Pod "downward-api-4073e478-d028-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007688285s
Sep  5 21:58:15.033: INFO: Pod "downward-api-4073e478-d028-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012485493s
STEP: Saw pod success
Sep  5 21:58:15.033: INFO: Pod "downward-api-4073e478-d028-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:58:15.036: INFO: Trying to get logs from node appserv9 pod downward-api-4073e478-d028-11e9-a0ad-8693e9898db7 container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:58:15.056: INFO: Waiting for pod downward-api-4073e478-d028-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:58:15.066: INFO: Pod downward-api-4073e478-d028-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:58:15.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2825" for this suite.
Sep  5 21:58:21.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:58:21.180: INFO: namespace downward-api-2825 deletion completed in 6.110459132s

• [SLOW TEST:10.306 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:58:21.181: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-4698f567-d028-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 21:58:21.332: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-469990e5-d028-11e9-a0ad-8693e9898db7" in namespace "projected-6531" to be "success or failure"
Sep  5 21:58:21.335: INFO: Pod "pod-projected-secrets-469990e5-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.842896ms
Sep  5 21:58:23.340: INFO: Pod "pod-projected-secrets-469990e5-d028-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007288987s
STEP: Saw pod success
Sep  5 21:58:23.340: INFO: Pod "pod-projected-secrets-469990e5-d028-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:58:23.343: INFO: Trying to get logs from node appserv9 pod pod-projected-secrets-469990e5-d028-11e9-a0ad-8693e9898db7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:58:23.363: INFO: Waiting for pod pod-projected-secrets-469990e5-d028-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:58:23.366: INFO: Pod pod-projected-secrets-469990e5-d028-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:58:23.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6531" for this suite.
Sep  5 21:58:29.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:58:29.475: INFO: namespace projected-6531 deletion completed in 6.105614473s

• [SLOW TEST:8.295 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:58:29.476: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-4b8a44a6-d028-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 21:58:29.624: INFO: Waiting up to 5m0s for pod "pod-secrets-4b8abbaf-d028-11e9-a0ad-8693e9898db7" in namespace "secrets-4296" to be "success or failure"
Sep  5 21:58:29.627: INFO: Pod "pod-secrets-4b8abbaf-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.709432ms
Sep  5 21:58:31.630: INFO: Pod "pod-secrets-4b8abbaf-d028-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006546753s
STEP: Saw pod success
Sep  5 21:58:31.630: INFO: Pod "pod-secrets-4b8abbaf-d028-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:58:31.634: INFO: Trying to get logs from node appserv10 pod pod-secrets-4b8abbaf-d028-11e9-a0ad-8693e9898db7 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:58:31.654: INFO: Waiting for pod pod-secrets-4b8abbaf-d028-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:58:31.656: INFO: Pod pod-secrets-4b8abbaf-d028-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:58:31.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4296" for this suite.
Sep  5 21:58:37.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:58:37.767: INFO: namespace secrets-4296 deletion completed in 6.105801665s

• [SLOW TEST:8.291 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:58:37.767: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-93
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep  5 21:58:37.905: INFO: namespace kubectl-93
Sep  5 21:58:37.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-93'
Sep  5 21:58:38.125: INFO: stderr: ""
Sep  5 21:58:38.125: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  5 21:58:39.129: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 21:58:39.129: INFO: Found 0 / 1
Sep  5 21:58:40.129: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 21:58:40.129: INFO: Found 1 / 1
Sep  5 21:58:40.129: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  5 21:58:40.132: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 21:58:40.132: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 21:58:40.132: INFO: wait on redis-master startup in kubectl-93 
Sep  5 21:58:40.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 logs redis-master-q5vmh redis-master --namespace=kubectl-93'
Sep  5 21:58:40.269: INFO: stderr: ""
Sep  5 21:58:40.270: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Sep 21:58:39.361 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Sep 21:58:39.361 # Server started, Redis version 3.2.12\n1:M 05 Sep 21:58:39.361 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Sep 21:58:39.361 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep  5 21:58:40.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-93'
Sep  5 21:58:40.409: INFO: stderr: ""
Sep  5 21:58:40.409: INFO: stdout: "service/rm2 exposed\n"
Sep  5 21:58:40.412: INFO: Service rm2 in namespace kubectl-93 found.
STEP: exposing service
Sep  5 21:58:42.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-93'
Sep  5 21:58:42.545: INFO: stderr: ""
Sep  5 21:58:42.545: INFO: stdout: "service/rm3 exposed\n"
Sep  5 21:58:42.548: INFO: Service rm3 in namespace kubectl-93 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:58:44.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-93" for this suite.
Sep  5 21:59:06.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:59:06.660: INFO: namespace kubectl-93 deletion completed in 22.100663163s

• [SLOW TEST:28.893 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:59:06.660: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  5 21:59:06.816: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5974,SelfLink:/api/v1/namespaces/watch-5974/configmaps/e2e-watch-test-resource-version,UID:61b38bd2-d028-11e9-9b5e-2c600c82ec72,ResourceVersion:22671,Generation:0,CreationTimestamp:2019-09-05 21:59:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 21:59:06.816: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5974,SelfLink:/api/v1/namespaces/watch-5974/configmaps/e2e-watch-test-resource-version,UID:61b38bd2-d028-11e9-9b5e-2c600c82ec72,ResourceVersion:22672,Generation:0,CreationTimestamp:2019-09-05 21:59:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:59:06.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5974" for this suite.
Sep  5 21:59:12.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:59:12.931: INFO: namespace watch-5974 deletion completed in 6.111813308s

• [SLOW TEST:6.271 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:59:12.932: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 21:59:13.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8131'
Sep  5 21:59:13.209: INFO: stderr: ""
Sep  5 21:59:13.209: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Sep  5 21:59:13.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete pods e2e-test-nginx-pod --namespace=kubectl-8131'
Sep  5 21:59:22.382: INFO: stderr: ""
Sep  5 21:59:22.382: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:59:22.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8131" for this suite.
Sep  5 21:59:28.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:59:28.498: INFO: namespace kubectl-8131 deletion completed in 6.110829953s

• [SLOW TEST:15.566 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:59:28.499: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7973
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Sep  5 21:59:28.645: INFO: Waiting up to 5m0s for pod "var-expansion-6eb89dbd-d028-11e9-a0ad-8693e9898db7" in namespace "var-expansion-7973" to be "success or failure"
Sep  5 21:59:28.648: INFO: Pod "var-expansion-6eb89dbd-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.280559ms
Sep  5 21:59:30.652: INFO: Pod "var-expansion-6eb89dbd-d028-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007111788s
STEP: Saw pod success
Sep  5 21:59:30.652: INFO: Pod "var-expansion-6eb89dbd-d028-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:59:30.655: INFO: Trying to get logs from node appserv10 pod var-expansion-6eb89dbd-d028-11e9-a0ad-8693e9898db7 container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:59:30.675: INFO: Waiting for pod var-expansion-6eb89dbd-d028-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:59:30.678: INFO: Pod var-expansion-6eb89dbd-d028-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:59:30.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7973" for this suite.
Sep  5 21:59:36.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:59:36.786: INFO: namespace var-expansion-7973 deletion completed in 6.103281756s

• [SLOW TEST:8.287 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:59:36.786: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  5 21:59:36.929: INFO: Waiting up to 5m0s for pod "pod-73a8cc78-d028-11e9-a0ad-8693e9898db7" in namespace "emptydir-9183" to be "success or failure"
Sep  5 21:59:36.932: INFO: Pod "pod-73a8cc78-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.845455ms
Sep  5 21:59:38.936: INFO: Pod "pod-73a8cc78-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006531294s
Sep  5 21:59:40.940: INFO: Pod "pod-73a8cc78-d028-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010645554s
STEP: Saw pod success
Sep  5 21:59:40.940: INFO: Pod "pod-73a8cc78-d028-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 21:59:40.943: INFO: Trying to get logs from node appserv10 pod pod-73a8cc78-d028-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 21:59:40.963: INFO: Waiting for pod pod-73a8cc78-d028-11e9-a0ad-8693e9898db7 to disappear
Sep  5 21:59:40.965: INFO: Pod pod-73a8cc78-d028-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:59:40.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9183" for this suite.
Sep  5 21:59:46.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:59:47.091: INFO: namespace emptydir-9183 deletion completed in 6.119722556s

• [SLOW TEST:10.305 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:59:47.091: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 21:59:47.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1178" for this suite.
Sep  5 21:59:53.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 21:59:53.355: INFO: namespace kubelet-test-1178 deletion completed in 6.109374509s

• [SLOW TEST:6.264 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 21:59:53.357: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7755
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7755.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7755.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7755.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7755.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7755.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 21:59:57.536: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-7755/dns-test-7d89a0f2-d028-11e9-a0ad-8693e9898db7: the server could not find the requested resource (get pods dns-test-7d89a0f2-d028-11e9-a0ad-8693e9898db7)
Sep  5 21:59:57.540: INFO: Unable to read jessie_udp@PodARecord from pod dns-7755/dns-test-7d89a0f2-d028-11e9-a0ad-8693e9898db7: the server could not find the requested resource (get pods dns-test-7d89a0f2-d028-11e9-a0ad-8693e9898db7)
Sep  5 21:59:57.543: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7755/dns-test-7d89a0f2-d028-11e9-a0ad-8693e9898db7: the server could not find the requested resource (get pods dns-test-7d89a0f2-d028-11e9-a0ad-8693e9898db7)
Sep  5 21:59:57.543: INFO: Lookups using dns-7755/dns-test-7d89a0f2-d028-11e9-a0ad-8693e9898db7 failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  5 22:00:02.574: INFO: DNS probes using dns-7755/dns-test-7d89a0f2-d028-11e9-a0ad-8693e9898db7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:00:02.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7755" for this suite.
Sep  5 22:00:08.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:00:08.700: INFO: namespace dns-7755 deletion completed in 6.110652131s

• [SLOW TEST:15.343 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:00:08.700: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7235
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-86af9b62-d028-11e9-a0ad-8693e9898db7
STEP: Creating secret with name s-test-opt-upd-86af9baf-d028-11e9-a0ad-8693e9898db7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-86af9b62-d028-11e9-a0ad-8693e9898db7
STEP: Updating secret s-test-opt-upd-86af9baf-d028-11e9-a0ad-8693e9898db7
STEP: Creating secret with name s-test-opt-create-86af9bd5-d028-11e9-a0ad-8693e9898db7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:01:45.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7235" for this suite.
Sep  5 22:02:07.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:02:07.546: INFO: namespace secrets-7235 deletion completed in 22.10841701s

• [SLOW TEST:118.845 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:02:07.546: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:02:09.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5598" for this suite.
Sep  5 22:02:49.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:02:49.817: INFO: namespace kubelet-test-5598 deletion completed in 40.103389289s

• [SLOW TEST:42.271 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:02:49.817: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Sep  5 22:02:49.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 --namespace=kubectl-104 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep  5 22:02:51.923: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep  5 22:02:51.923: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:02:53.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-104" for this suite.
Sep  5 22:02:59.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:03:00.046: INFO: namespace kubectl-104 deletion completed in 6.111762583s

• [SLOW TEST:10.228 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:03:00.046: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-ecd0326b-d028-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 22:03:00.196: INFO: Waiting up to 5m0s for pod "pod-secrets-ecd0c7fd-d028-11e9-a0ad-8693e9898db7" in namespace "secrets-9104" to be "success or failure"
Sep  5 22:03:00.201: INFO: Pod "pod-secrets-ecd0c7fd-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.702706ms
Sep  5 22:03:02.205: INFO: Pod "pod-secrets-ecd0c7fd-d028-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009023979s
Sep  5 22:03:04.209: INFO: Pod "pod-secrets-ecd0c7fd-d028-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013483114s
STEP: Saw pod success
Sep  5 22:03:04.210: INFO: Pod "pod-secrets-ecd0c7fd-d028-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:03:04.213: INFO: Trying to get logs from node appserv9 pod pod-secrets-ecd0c7fd-d028-11e9-a0ad-8693e9898db7 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 22:03:04.233: INFO: Waiting for pod pod-secrets-ecd0c7fd-d028-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:03:04.236: INFO: Pod pod-secrets-ecd0c7fd-d028-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:03:04.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9104" for this suite.
Sep  5 22:03:10.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:03:10.366: INFO: namespace secrets-9104 deletion completed in 6.126087435s

• [SLOW TEST:10.320 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:03:10.367: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-998
Sep  5 22:03:12.518: INFO: Started pod liveness-exec in namespace container-probe-998
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 22:03:12.522: INFO: Initial restart count of pod liveness-exec is 0
Sep  5 22:04:02.629: INFO: Restart count of pod container-probe-998/liveness-exec is now 1 (50.10776565s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:04:02.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-998" for this suite.
Sep  5 22:04:08.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:04:08.756: INFO: namespace container-probe-998 deletion completed in 6.112418586s

• [SLOW TEST:58.390 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:04:08.756: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  5 22:04:08.904: INFO: Waiting up to 5m0s for pod "pod-15c4b170-d029-11e9-a0ad-8693e9898db7" in namespace "emptydir-8605" to be "success or failure"
Sep  5 22:04:08.906: INFO: Pod "pod-15c4b170-d029-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.741155ms
Sep  5 22:04:10.911: INFO: Pod "pod-15c4b170-d029-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007155439s
Sep  5 22:04:12.915: INFO: Pod "pod-15c4b170-d029-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011663988s
STEP: Saw pod success
Sep  5 22:04:12.915: INFO: Pod "pod-15c4b170-d029-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:04:12.919: INFO: Trying to get logs from node appserv9 pod pod-15c4b170-d029-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 22:04:12.938: INFO: Waiting for pod pod-15c4b170-d029-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:04:12.941: INFO: Pod pod-15c4b170-d029-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:04:12.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8605" for this suite.
Sep  5 22:04:18.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:04:19.068: INFO: namespace emptydir-8605 deletion completed in 6.122739257s

• [SLOW TEST:10.312 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:04:19.069: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:04:19.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1bea444b-d029-11e9-a0ad-8693e9898db7" in namespace "downward-api-1914" to be "success or failure"
Sep  5 22:04:19.220: INFO: Pod "downwardapi-volume-1bea444b-d029-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.357602ms
Sep  5 22:04:21.225: INFO: Pod "downwardapi-volume-1bea444b-d029-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007808604s
STEP: Saw pod success
Sep  5 22:04:21.225: INFO: Pod "downwardapi-volume-1bea444b-d029-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:04:21.228: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-1bea444b-d029-11e9-a0ad-8693e9898db7 container client-container: <nil>
STEP: delete the pod
Sep  5 22:04:21.248: INFO: Waiting for pod downwardapi-volume-1bea444b-d029-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:04:21.251: INFO: Pod downwardapi-volume-1bea444b-d029-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:04:21.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1914" for this suite.
Sep  5 22:04:27.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:04:27.358: INFO: namespace downward-api-1914 deletion completed in 6.102972712s

• [SLOW TEST:8.289 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:04:27.358: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-6437
Sep  5 22:04:29.508: INFO: Started pod liveness-exec in namespace container-probe-6437
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 22:04:29.512: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:08:30.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6437" for this suite.
Sep  5 22:08:36.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:08:36.110: INFO: namespace container-probe-6437 deletion completed in 6.096646612s

• [SLOW TEST:248.752 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:08:36.110: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:08:36.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2682" for this suite.
Sep  5 22:08:42.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:08:42.402: INFO: namespace services-2682 deletion completed in 6.151368968s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.292 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:08:42.403: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-b8dfd1e4-d029-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 22:08:42.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8e0640a-d029-11e9-a0ad-8693e9898db7" in namespace "configmap-6834" to be "success or failure"
Sep  5 22:08:42.557: INFO: Pod "pod-configmaps-b8e0640a-d029-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017969ms
Sep  5 22:08:44.561: INFO: Pod "pod-configmaps-b8e0640a-d029-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007129394s
Sep  5 22:08:46.565: INFO: Pod "pod-configmaps-b8e0640a-d029-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010938152s
STEP: Saw pod success
Sep  5 22:08:46.565: INFO: Pod "pod-configmaps-b8e0640a-d029-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:08:46.568: INFO: Trying to get logs from node appserv10 pod pod-configmaps-b8e0640a-d029-11e9-a0ad-8693e9898db7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 22:08:46.585: INFO: Waiting for pod pod-configmaps-b8e0640a-d029-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:08:46.588: INFO: Pod pod-configmaps-b8e0640a-d029-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:08:46.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6834" for this suite.
Sep  5 22:08:52.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:08:52.701: INFO: namespace configmap-6834 deletion completed in 6.109535679s

• [SLOW TEST:10.299 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:08:52.701: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3169
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-bf03dfb1-d029-11e9-a0ad-8693e9898db7
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-bf03dfb1-d029-11e9-a0ad-8693e9898db7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:08:56.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3169" for this suite.
Sep  5 22:09:18.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:09:19.015: INFO: namespace configmap-3169 deletion completed in 22.115282696s

• [SLOW TEST:26.314 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:09:19.015: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9469
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  5 22:09:19.162: INFO: Waiting up to 5m0s for pod "pod-ceb26c1d-d029-11e9-a0ad-8693e9898db7" in namespace "emptydir-9469" to be "success or failure"
Sep  5 22:09:19.165: INFO: Pod "pod-ceb26c1d-d029-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039374ms
Sep  5 22:09:21.169: INFO: Pod "pod-ceb26c1d-d029-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007147679s
Sep  5 22:09:23.174: INFO: Pod "pod-ceb26c1d-d029-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011297512s
STEP: Saw pod success
Sep  5 22:09:23.174: INFO: Pod "pod-ceb26c1d-d029-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:09:23.177: INFO: Trying to get logs from node appserv10 pod pod-ceb26c1d-d029-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 22:09:23.197: INFO: Waiting for pod pod-ceb26c1d-d029-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:09:23.200: INFO: Pod pod-ceb26c1d-d029-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:09:23.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9469" for this suite.
Sep  5 22:09:29.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:09:29.319: INFO: namespace emptydir-9469 deletion completed in 6.115280421s

• [SLOW TEST:10.304 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:09:29.320: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-d4d6ce20-d029-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 22:09:29.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4d76987-d029-11e9-a0ad-8693e9898db7" in namespace "configmap-7601" to be "success or failure"
Sep  5 22:09:29.474: INFO: Pod "pod-configmaps-d4d76987-d029-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.24163ms
Sep  5 22:09:31.479: INFO: Pod "pod-configmaps-d4d76987-d029-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007602001s
Sep  5 22:09:33.483: INFO: Pod "pod-configmaps-d4d76987-d029-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011625594s
STEP: Saw pod success
Sep  5 22:09:33.483: INFO: Pod "pod-configmaps-d4d76987-d029-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:09:33.486: INFO: Trying to get logs from node appserv9 pod pod-configmaps-d4d76987-d029-11e9-a0ad-8693e9898db7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 22:09:33.506: INFO: Waiting for pod pod-configmaps-d4d76987-d029-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:09:33.509: INFO: Pod pod-configmaps-d4d76987-d029-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:09:33.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7601" for this suite.
Sep  5 22:09:39.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:09:39.625: INFO: namespace configmap-7601 deletion completed in 6.111444797s

• [SLOW TEST:10.305 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:09:39.625: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1328
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-dafba1a8-d029-11e9-a0ad-8693e9898db7
STEP: Creating secret with name s-test-opt-upd-dafba210-d029-11e9-a0ad-8693e9898db7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-dafba1a8-d029-11e9-a0ad-8693e9898db7
STEP: Updating secret s-test-opt-upd-dafba210-d029-11e9-a0ad-8693e9898db7
STEP: Creating secret with name s-test-opt-create-dafba265-d029-11e9-a0ad-8693e9898db7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:11:08.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1328" for this suite.
Sep  5 22:11:30.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:11:30.436: INFO: namespace projected-1328 deletion completed in 22.11021403s

• [SLOW TEST:110.810 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:11:30.436: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  5 22:11:30.573: INFO: Creating deployment "test-recreate-deployment"
Sep  5 22:11:30.577: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  5 22:11:30.582: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  5 22:11:32.590: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  5 22:11:32.594: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  5 22:11:32.601: INFO: Updating deployment test-recreate-deployment
Sep  5 22:11:32.601: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  5 22:11:32.644: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-433,SelfLink:/apis/apps/v1/namespaces/deployment-433/deployments/test-recreate-deployment,UID:1d0759c0-d02a-11e9-9b5e-2c600c82ec72,ResourceVersion:24770,Generation:2,CreationTimestamp:2019-09-05 22:11:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-05 22:11:32 +0000 UTC 2019-09-05 22:11:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-05 22:11:32 +0000 UTC 2019-09-05 22:11:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep  5 22:11:32.648: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-433,SelfLink:/apis/apps/v1/namespaces/deployment-433/replicasets/test-recreate-deployment-c9cbd8684,UID:1e400fcf-d02a-11e9-9b5e-2c600c82ec72,ResourceVersion:24769,Generation:1,CreationTimestamp:2019-09-05 22:11:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1d0759c0-d02a-11e9-9b5e-2c600c82ec72 0xc001568860 0xc001568861}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 22:11:32.648: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  5 22:11:32.648: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-433,SelfLink:/apis/apps/v1/namespaces/deployment-433/replicasets/test-recreate-deployment-7d57d5ff7c,UID:1d07fb03-d02a-11e9-9b5e-2c600c82ec72,ResourceVersion:24760,Generation:2,CreationTimestamp:2019-09-05 22:11:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1d0759c0-d02a-11e9-9b5e-2c600c82ec72 0xc001568737 0xc001568738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 22:11:32.651: INFO: Pod "test-recreate-deployment-c9cbd8684-mk6mv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-mk6mv,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-433,SelfLink:/api/v1/namespaces/deployment-433/pods/test-recreate-deployment-c9cbd8684-mk6mv,UID:1e40ae41-d02a-11e9-9b5e-2c600c82ec72,ResourceVersion:24765,Generation:0,CreationTimestamp:2019-09-05 22:11:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 1e400fcf-d02a-11e9-9b5e-2c600c82ec72 0xc0015692a0 0xc0015692a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sz5n5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sz5n5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sz5n5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001569300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001569320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:11:32.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-433" for this suite.
Sep  5 22:11:38.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:11:38.765: INFO: namespace deployment-433 deletion completed in 6.110152084s

• [SLOW TEST:8.329 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:11:38.766: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Sep  5 22:11:38.912: INFO: Waiting up to 5m0s for pod "client-containers-21fea520-d02a-11e9-a0ad-8693e9898db7" in namespace "containers-5980" to be "success or failure"
Sep  5 22:11:38.915: INFO: Pod "client-containers-21fea520-d02a-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.635672ms
Sep  5 22:11:40.919: INFO: Pod "client-containers-21fea520-d02a-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006588595s
Sep  5 22:11:42.924: INFO: Pod "client-containers-21fea520-d02a-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011026437s
STEP: Saw pod success
Sep  5 22:11:42.924: INFO: Pod "client-containers-21fea520-d02a-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:11:42.927: INFO: Trying to get logs from node appserv9 pod client-containers-21fea520-d02a-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 22:11:42.947: INFO: Waiting for pod client-containers-21fea520-d02a-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:11:42.950: INFO: Pod client-containers-21fea520-d02a-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:11:42.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5980" for this suite.
Sep  5 22:11:48.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:11:49.069: INFO: namespace containers-5980 deletion completed in 6.115141322s

• [SLOW TEST:10.304 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:11:49.070: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Sep  5 22:11:49.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 create -f - --namespace=kubectl-6522'
Sep  5 22:11:49.495: INFO: stderr: ""
Sep  5 22:11:49.495: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Sep  5 22:11:50.499: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 22:11:50.500: INFO: Found 0 / 1
Sep  5 22:11:51.499: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 22:11:51.499: INFO: Found 0 / 1
Sep  5 22:11:52.499: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 22:11:52.499: INFO: Found 1 / 1
Sep  5 22:11:52.499: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  5 22:11:52.503: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 22:11:52.503: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep  5 22:11:52.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 logs redis-master-kk4z4 redis-master --namespace=kubectl-6522'
Sep  5 22:11:52.634: INFO: stderr: ""
Sep  5 22:11:52.634: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Sep 22:11:50.866 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Sep 22:11:50.866 # Server started, Redis version 3.2.12\n1:M 05 Sep 22:11:50.866 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Sep 22:11:50.866 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep  5 22:11:52.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 log redis-master-kk4z4 redis-master --namespace=kubectl-6522 --tail=1'
Sep  5 22:11:52.761: INFO: stderr: ""
Sep  5 22:11:52.761: INFO: stdout: "1:M 05 Sep 22:11:50.866 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep  5 22:11:52.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 log redis-master-kk4z4 redis-master --namespace=kubectl-6522 --limit-bytes=1'
Sep  5 22:11:52.896: INFO: stderr: ""
Sep  5 22:11:52.896: INFO: stdout: " "
STEP: exposing timestamps
Sep  5 22:11:52.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 log redis-master-kk4z4 redis-master --namespace=kubectl-6522 --tail=1 --timestamps'
Sep  5 22:11:53.026: INFO: stderr: ""
Sep  5 22:11:53.026: INFO: stdout: "2019-09-05T22:11:50.867483124Z 1:M 05 Sep 22:11:50.866 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep  5 22:11:55.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 log redis-master-kk4z4 redis-master --namespace=kubectl-6522 --since=1s'
Sep  5 22:11:55.643: INFO: stderr: ""
Sep  5 22:11:55.643: INFO: stdout: ""
Sep  5 22:11:55.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 log redis-master-kk4z4 redis-master --namespace=kubectl-6522 --since=24h'
Sep  5 22:11:55.737: INFO: stderr: ""
Sep  5 22:11:55.737: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Sep 22:11:50.866 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Sep 22:11:50.866 # Server started, Redis version 3.2.12\n1:M 05 Sep 22:11:50.866 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Sep 22:11:50.866 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Sep  5 22:11:55.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 delete --grace-period=0 --force -f - --namespace=kubectl-6522'
Sep  5 22:11:55.830: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 22:11:55.831: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep  5 22:11:55.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6522'
Sep  5 22:11:55.925: INFO: stderr: "No resources found.\n"
Sep  5 22:11:55.925: INFO: stdout: ""
Sep  5 22:11:55.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-156594675 get pods -l name=nginx --namespace=kubectl-6522 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 22:11:56.005: INFO: stderr: ""
Sep  5 22:11:56.006: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:11:56.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6522" for this suite.
Sep  5 22:12:02.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:12:02.120: INFO: namespace kubectl-6522 deletion completed in 6.111138356s

• [SLOW TEST:13.050 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:12:02.121: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6755
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-2fea6360-d02a-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 22:12:02.272: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2feafd5a-d02a-11e9-a0ad-8693e9898db7" in namespace "projected-6755" to be "success or failure"
Sep  5 22:12:02.276: INFO: Pod "pod-projected-secrets-2feafd5a-d02a-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117783ms
Sep  5 22:12:04.280: INFO: Pod "pod-projected-secrets-2feafd5a-d02a-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007645759s
STEP: Saw pod success
Sep  5 22:12:04.280: INFO: Pod "pod-projected-secrets-2feafd5a-d02a-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:12:04.284: INFO: Trying to get logs from node appserv9 pod pod-projected-secrets-2feafd5a-d02a-11e9-a0ad-8693e9898db7 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 22:12:04.303: INFO: Waiting for pod pod-projected-secrets-2feafd5a-d02a-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:12:04.306: INFO: Pod pod-projected-secrets-2feafd5a-d02a-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:12:04.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6755" for this suite.
Sep  5 22:12:10.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:12:10.419: INFO: namespace projected-6755 deletion completed in 6.109313556s

• [SLOW TEST:8.298 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:12:10.419: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-3503/secret-test-34dc6b81-d02a-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume secrets
Sep  5 22:12:10.569: INFO: Waiting up to 5m0s for pod "pod-configmaps-34dcf8e2-d02a-11e9-a0ad-8693e9898db7" in namespace "secrets-3503" to be "success or failure"
Sep  5 22:12:10.572: INFO: Pod "pod-configmaps-34dcf8e2-d02a-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.884715ms
Sep  5 22:12:12.576: INFO: Pod "pod-configmaps-34dcf8e2-d02a-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006945064s
STEP: Saw pod success
Sep  5 22:12:12.576: INFO: Pod "pod-configmaps-34dcf8e2-d02a-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:12:12.579: INFO: Trying to get logs from node appserv10 pod pod-configmaps-34dcf8e2-d02a-11e9-a0ad-8693e9898db7 container env-test: <nil>
STEP: delete the pod
Sep  5 22:12:12.598: INFO: Waiting for pod pod-configmaps-34dcf8e2-d02a-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:12:12.601: INFO: Pod pod-configmaps-34dcf8e2-d02a-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:12:12.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3503" for this suite.
Sep  5 22:12:18.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:12:18.714: INFO: namespace secrets-3503 deletion completed in 6.108183509s

• [SLOW TEST:8.295 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:12:18.714: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1431
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  5 22:12:18.861: INFO: Waiting up to 5m0s for pod "pod-39ce436c-d02a-11e9-a0ad-8693e9898db7" in namespace "emptydir-1431" to be "success or failure"
Sep  5 22:12:18.864: INFO: Pod "pod-39ce436c-d02a-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297043ms
Sep  5 22:12:20.869: INFO: Pod "pod-39ce436c-d02a-11e9-a0ad-8693e9898db7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007761473s
Sep  5 22:12:22.873: INFO: Pod "pod-39ce436c-d02a-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012240369s
STEP: Saw pod success
Sep  5 22:12:22.873: INFO: Pod "pod-39ce436c-d02a-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:12:22.877: INFO: Trying to get logs from node appserv9 pod pod-39ce436c-d02a-11e9-a0ad-8693e9898db7 container test-container: <nil>
STEP: delete the pod
Sep  5 22:12:22.897: INFO: Waiting for pod pod-39ce436c-d02a-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:12:22.900: INFO: Pod pod-39ce436c-d02a-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:12:22.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1431" for this suite.
Sep  5 22:12:28.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:12:29.015: INFO: namespace emptydir-1431 deletion completed in 6.111151741s

• [SLOW TEST:10.301 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:12:29.016: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  5 22:12:29.159: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:12:36.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8507" for this suite.
Sep  5 22:12:42.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:12:42.975: INFO: namespace pods-8507 deletion completed in 6.118982779s

• [SLOW TEST:13.959 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:12:42.975: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  5 22:12:43.113: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:12:46.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1702" for this suite.
Sep  5 22:13:05.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:13:05.107: INFO: namespace init-container-1702 deletion completed in 18.103653977s

• [SLOW TEST:22.133 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:13:05.108: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-5575170b-d02a-11e9-a0ad-8693e9898db7
STEP: Creating secret with name secret-projected-all-test-volume-557516ec-d02a-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  5 22:13:05.259: INFO: Waiting up to 5m0s for pod "projected-volume-557516a2-d02a-11e9-a0ad-8693e9898db7" in namespace "projected-3541" to be "success or failure"
Sep  5 22:13:05.263: INFO: Pod "projected-volume-557516a2-d02a-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.864295ms
Sep  5 22:13:07.266: INFO: Pod "projected-volume-557516a2-d02a-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007544832s
STEP: Saw pod success
Sep  5 22:13:07.266: INFO: Pod "projected-volume-557516a2-d02a-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:13:07.269: INFO: Trying to get logs from node appserv10 pod projected-volume-557516a2-d02a-11e9-a0ad-8693e9898db7 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  5 22:13:07.287: INFO: Waiting for pod projected-volume-557516a2-d02a-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:13:07.290: INFO: Pod projected-volume-557516a2-d02a-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:13:07.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3541" for this suite.
Sep  5 22:13:13.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:13:13.408: INFO: namespace projected-3541 deletion completed in 6.114461779s

• [SLOW TEST:8.301 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:13:13.409: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-5a680faa-d02a-11e9-a0ad-8693e9898db7
STEP: Creating a pod to test consume configMaps
Sep  5 22:13:13.560: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5a68a802-d02a-11e9-a0ad-8693e9898db7" in namespace "projected-6036" to be "success or failure"
Sep  5 22:13:13.564: INFO: Pod "pod-projected-configmaps-5a68a802-d02a-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.537105ms
Sep  5 22:13:15.568: INFO: Pod "pod-projected-configmaps-5a68a802-d02a-11e9-a0ad-8693e9898db7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008289352s
Sep  5 22:13:17.572: INFO: Pod "pod-projected-configmaps-5a68a802-d02a-11e9-a0ad-8693e9898db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011918004s
STEP: Saw pod success
Sep  5 22:13:17.572: INFO: Pod "pod-projected-configmaps-5a68a802-d02a-11e9-a0ad-8693e9898db7" satisfied condition "success or failure"
Sep  5 22:13:17.575: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-5a68a802-d02a-11e9-a0ad-8693e9898db7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 22:13:17.592: INFO: Waiting for pod pod-projected-configmaps-5a68a802-d02a-11e9-a0ad-8693e9898db7 to disappear
Sep  5 22:13:17.595: INFO: Pod pod-projected-configmaps-5a68a802-d02a-11e9-a0ad-8693e9898db7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:13:17.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6036" for this suite.
Sep  5 22:13:23.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:13:23.713: INFO: namespace projected-6036 deletion completed in 6.114086637s

• [SLOW TEST:10.305 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:13:23.714: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  5 22:13:26.378: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3186 pod-service-account-60da3541-d02a-11e9-a0ad-8693e9898db7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  5 22:13:26.582: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3186 pod-service-account-60da3541-d02a-11e9-a0ad-8693e9898db7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  5 22:13:26.791: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3186 pod-service-account-60da3541-d02a-11e9-a0ad-8693e9898db7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:13:27.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3186" for this suite.
Sep  5 22:13:33.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:13:33.132: INFO: namespace svcaccounts-3186 deletion completed in 6.125363912s

• [SLOW TEST:9.418 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  5 22:13:33.132: INFO: >>> kubeConfig: /tmp/kubeconfig-156594675
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  5 22:14:33.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7281" for this suite.
Sep  5 22:14:55.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 22:14:55.399: INFO: namespace container-probe-7281 deletion completed in 22.111640477s

• [SLOW TEST:82.268 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSep  5 22:14:55.400: INFO: Running AfterSuite actions on all nodes
Sep  5 22:14:55.400: INFO: Running AfterSuite actions on node 1
Sep  5 22:14:55.400: INFO: Skipping dumping logs from cluster

Ran 204 of 3585 Specs in 5719.368 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3381 Skipped PASS

Ginkgo ran 1 suite in 1h35m20.984556117s
Test Suite Passed
