I0906 14:42:53.133619      14 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-262345838
I0906 14:42:53.133779      14 e2e.go:240] Starting e2e run "9ad7541d-d0b4-11e9-9ba3-f679ea11f394" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1567780972 - Will randomize all specs
Will run 204 of 3586 specs

Sep  6 14:42:53.281: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 14:42:53.283: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  6 14:42:53.295: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  6 14:42:53.324: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  6 14:42:53.324: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep  6 14:42:53.324: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  6 14:42:53.331: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep  6 14:42:53.331: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  6 14:42:53.331: INFO: e2e test version: v1.14.6
Sep  6 14:42:53.332: INFO: kube-apiserver version: v1.14.6
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:42:53.332: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
Sep  6 14:42:53.356: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 14:42:53.361: INFO: Waiting up to 5m0s for pod "downward-api-9b87ecb3-d0b4-11e9-9ba3-f679ea11f394" in namespace "downward-api-6022" to be "success or failure"
Sep  6 14:42:53.363: INFO: Pod "downward-api-9b87ecb3-d0b4-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.449162ms
Sep  6 14:42:55.757: INFO: Pod "downward-api-9b87ecb3-d0b4-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395318622s
Sep  6 14:42:57.760: INFO: Pod "downward-api-9b87ecb3-d0b4-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.398439308s
STEP: Saw pod success
Sep  6 14:42:57.760: INFO: Pod "downward-api-9b87ecb3-d0b4-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:42:57.762: INFO: Trying to get logs from node metalk8s-23 pod downward-api-9b87ecb3-d0b4-11e9-9ba3-f679ea11f394 container dapi-container: <nil>
STEP: delete the pod
Sep  6 14:42:57.782: INFO: Waiting for pod downward-api-9b87ecb3-d0b4-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:42:57.784: INFO: Pod downward-api-9b87ecb3-d0b4-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:42:57.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6022" for this suite.
Sep  6 14:43:03.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:43:03.859: INFO: namespace downward-api-6022 deletion completed in 6.072970393s

• [SLOW TEST:10.527 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:43:03.860: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 14:43:03.888: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1cdeb67-d0b4-11e9-9ba3-f679ea11f394" in namespace "downward-api-7273" to be "success or failure"
Sep  6 14:43:03.891: INFO: Pod "downwardapi-volume-a1cdeb67-d0b4-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.633203ms
Sep  6 14:43:05.894: INFO: Pod "downwardapi-volume-a1cdeb67-d0b4-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005559274s
Sep  6 14:43:07.897: INFO: Pod "downwardapi-volume-a1cdeb67-d0b4-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008725481s
STEP: Saw pod success
Sep  6 14:43:07.897: INFO: Pod "downwardapi-volume-a1cdeb67-d0b4-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:43:07.899: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-a1cdeb67-d0b4-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 14:43:07.911: INFO: Waiting for pod downwardapi-volume-a1cdeb67-d0b4-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:43:07.912: INFO: Pod downwardapi-volume-a1cdeb67-d0b4-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:43:07.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7273" for this suite.
Sep  6 14:43:13.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:43:14.010: INFO: namespace downward-api-7273 deletion completed in 6.09611727s

• [SLOW TEST:10.151 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:43:14.011: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 14:43:14.052: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  6 14:43:14.056: INFO: Number of nodes with available pods: 0
Sep  6 14:43:14.056: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  6 14:43:14.066: INFO: Number of nodes with available pods: 0
Sep  6 14:43:14.067: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:15.070: INFO: Number of nodes with available pods: 0
Sep  6 14:43:15.070: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:16.069: INFO: Number of nodes with available pods: 0
Sep  6 14:43:16.069: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:18.062: INFO: Number of nodes with available pods: 0
Sep  6 14:43:18.062: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:19.537: INFO: Number of nodes with available pods: 1
Sep  6 14:43:19.537: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  6 14:43:19.558: INFO: Number of nodes with available pods: 1
Sep  6 14:43:19.558: INFO: Number of running nodes: 0, number of available pods: 1
Sep  6 14:43:20.561: INFO: Number of nodes with available pods: 0
Sep  6 14:43:20.561: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  6 14:43:20.569: INFO: Number of nodes with available pods: 0
Sep  6 14:43:20.570: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:21.572: INFO: Number of nodes with available pods: 0
Sep  6 14:43:21.572: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:22.572: INFO: Number of nodes with available pods: 0
Sep  6 14:43:22.573: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:23.573: INFO: Number of nodes with available pods: 0
Sep  6 14:43:23.573: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:25.320: INFO: Number of nodes with available pods: 0
Sep  6 14:43:25.320: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:25.572: INFO: Number of nodes with available pods: 0
Sep  6 14:43:25.572: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:26.572: INFO: Number of nodes with available pods: 0
Sep  6 14:43:26.573: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:27.572: INFO: Number of nodes with available pods: 0
Sep  6 14:43:27.572: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:28.572: INFO: Number of nodes with available pods: 0
Sep  6 14:43:28.572: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:29.573: INFO: Number of nodes with available pods: 0
Sep  6 14:43:29.573: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:43:30.588: INFO: Number of nodes with available pods: 1
Sep  6 14:43:30.588: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1555, will wait for the garbage collector to delete the pods
Sep  6 14:43:30.647: INFO: Deleting DaemonSet.extensions daemon-set took: 4.3926ms
Sep  6 14:43:31.048: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.205788ms
Sep  6 14:43:38.650: INFO: Number of nodes with available pods: 0
Sep  6 14:43:38.650: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 14:43:38.653: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1555/daemonsets","resourceVersion":"1565"},"items":null}

Sep  6 14:43:38.655: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1555/pods","resourceVersion":"1565"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:43:38.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1555" for this suite.
Sep  6 14:43:44.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:43:44.731: INFO: namespace daemonsets-1555 deletion completed in 6.065348934s

• [SLOW TEST:30.720 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:43:44.731: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-ba2acc9c-d0b4-11e9-9ba3-f679ea11f394
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-ba2acc9c-d0b4-11e9-9ba3-f679ea11f394
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:43:48.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-813" for this suite.
Sep  6 14:44:10.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:44:10.929: INFO: namespace configmap-813 deletion completed in 22.106177036s

• [SLOW TEST:26.198 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:44:10.929: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  6 14:44:13.559: INFO: Successfully updated pod "labelsupdatec9d006c5-d0b4-11e9-9ba3-f679ea11f394"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:44:17.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-131" for this suite.
Sep  6 14:44:35.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:44:35.647: INFO: namespace projected-131 deletion completed in 18.069779781s

• [SLOW TEST:24.718 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:44:35.647: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 14:44:35.669: INFO: Creating deployment "nginx-deployment"
Sep  6 14:44:35.673: INFO: Waiting for observed generation 1
Sep  6 14:44:37.678: INFO: Waiting for all required pods to come up
Sep  6 14:44:37.681: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  6 14:44:43.688: INFO: Waiting for deployment "nginx-deployment" to complete
Sep  6 14:44:43.692: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep  6 14:44:43.697: INFO: Updating deployment nginx-deployment
Sep  6 14:44:43.697: INFO: Waiting for observed generation 2
Sep  6 14:44:45.702: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  6 14:44:45.704: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  6 14:44:45.706: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  6 14:44:45.712: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  6 14:44:45.712: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  6 14:44:45.714: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  6 14:44:45.717: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep  6 14:44:45.717: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep  6 14:44:45.722: INFO: Updating deployment nginx-deployment
Sep  6 14:44:45.722: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep  6 14:44:45.733: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  6 14:44:47.745: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 14:44:47.756: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-2812,SelfLink:/apis/apps/v1/namespaces/deployment-2812/deployments/nginx-deployment,UID:d883a432-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2106,Generation:3,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-09-06 14:44:45 +0000 UTC 2019-09-06 14:44:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-06 14:44:45 +0000 UTC 2019-09-06 14:44:35 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-b79c9d74d" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep  6 14:44:47.760: INFO: New ReplicaSet "nginx-deployment-b79c9d74d" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d,GenerateName:,Namespace:deployment-2812,SelfLink:/apis/apps/v1/namespaces/deployment-2812/replicasets/nginx-deployment-b79c9d74d,UID:dd4c8d50-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2098,Generation:3,CreationTimestamp:2019-09-06 14:44:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment d883a432-d0b4-11e9-b447-fa163e0a078f 0xc002dd7247 0xc002dd7248}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 14:44:47.760: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep  6 14:44:47.761: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5,GenerateName:,Namespace:deployment-2812,SelfLink:/apis/apps/v1/namespaces/deployment-2812/replicasets/nginx-deployment-85db8c99c5,UID:d8843405-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2097,Generation:3,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment d883a432-d0b4-11e9-b447-fa163e0a078f 0xc002dd7177 0xc002dd7178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep  6 14:44:47.780: INFO: Pod "nginx-deployment-85db8c99c5-2sj2d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-2sj2d,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-2sj2d,UID:de867364-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2286,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.238/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcf2a0 0xc002dcf2a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcf310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcf330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.780: INFO: Pod "nginx-deployment-85db8c99c5-4l8kp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-4l8kp,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-4l8kp,UID:d8873972-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:1897,Generation:0,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.221/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcf3f0 0xc002dcf3f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcf460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcf480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.221,StartTime:2019-09-06 14:44:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 14:44:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://7a7d3a54a944ef18222ff72c06b3469f94f0d720535ae5f8e4d85bb5e22c3018}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.780: INFO: Pod "nginx-deployment-85db8c99c5-4v6vd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-4v6vd,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-4v6vd,UID:d886322f-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:1921,Generation:0,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.222/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcf550 0xc002dcf551}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcf5c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcf5e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.222,StartTime:2019-09-06 14:44:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 14:44:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://7b4ac04147762d12f1ed52061e9d7503efca147ec88bdb6dff34b2568a5fe864}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.782: INFO: Pod "nginx-deployment-85db8c99c5-5cksx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-5cksx,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-5cksx,UID:de82f7aa-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2146,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.231/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcf6b0 0xc002dcf6b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcf720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcf740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.783: INFO: Pod "nginx-deployment-85db8c99c5-7wrq4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-7wrq4,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-7wrq4,UID:de867c92-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2197,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.235/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcf800 0xc002dcf801}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcf870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcf890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.797: INFO: Pod "nginx-deployment-85db8c99c5-b4pnl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-b4pnl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-b4pnl,UID:d8872677-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:1925,Generation:0,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.224/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcf910 0xc002dcf911}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcf980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcf9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.224,StartTime:2019-09-06 14:44:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 14:44:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://0c1eb2a056b8799d5c1bb2e06a1f4761e3d91caa497c98fcfa6b91b5782f147e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.799: INFO: Pod "nginx-deployment-85db8c99c5-c5smt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-c5smt,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-c5smt,UID:de8684e1-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2245,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.242/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcfa70 0xc002dcfa71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcfae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcfb00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.799: INFO: Pod "nginx-deployment-85db8c99c5-gkg8k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-gkg8k,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-gkg8k,UID:de82dc40-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2155,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.232/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcfb80 0xc002dcfb81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcfbf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcfc10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.799: INFO: Pod "nginx-deployment-85db8c99c5-lf8l4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-lf8l4,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-lf8l4,UID:de89c117-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2192,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.236/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcfcd0 0xc002dcfcd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcfd40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcfd60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.800: INFO: Pod "nginx-deployment-85db8c99c5-n4nkt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-n4nkt,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-n4nkt,UID:d886375e-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:1887,Generation:0,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.216/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcfde0 0xc002dcfde1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcfe50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcfe70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.216,StartTime:2019-09-06 14:44:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 14:44:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://d2667ede586f2a0639c7561b260536f546cda04df7aedc637542dd387b61497e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.800: INFO: Pod "nginx-deployment-85db8c99c5-n5nzg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-n5nzg,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-n5nzg,UID:d8856ec5-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:1908,Generation:0,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.217/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc002dcff40 0xc002dcff41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dcffb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dcffd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.217,StartTime:2019-09-06 14:44:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 14:44:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://6a67bf79a3273f0815441e4e1046fb3adcb4e7c63d12a3b59a60585f82002a6d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.800: INFO: Pod "nginx-deployment-85db8c99c5-nh9k5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-nh9k5,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-nh9k5,UID:d884ea3e-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:1881,Generation:0,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.218/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc0030640a0 0xc0030640a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003064110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.218,StartTime:2019-09-06 14:44:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 14:44:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://27b0f719a64141145a375ce3366cc9321ed5cedc401a2ca1c552b4d735ed4290}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.801: INFO: Pod "nginx-deployment-85db8c99c5-pv826" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-pv826,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-pv826,UID:d8864141-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:1903,Generation:0,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.219/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc003064200 0xc003064201}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003064270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.219,StartTime:2019-09-06 14:44:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 14:44:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://2a34efcd0ea5d32c2942dad5a4742c15c65760b92fb95733b530c6ec2df1db25}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.801: INFO: Pod "nginx-deployment-85db8c99c5-q2ffg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-q2ffg,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-q2ffg,UID:de89be24-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2274,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.245/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc003064360 0xc003064361}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030643d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030643f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.801: INFO: Pod "nginx-deployment-85db8c99c5-rtf6q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-rtf6q,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-rtf6q,UID:de820f32-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2189,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.233/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc003064470 0xc003064471}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030644e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.802: INFO: Pod "nginx-deployment-85db8c99c5-scmbw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-scmbw,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-scmbw,UID:de86798a-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2181,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.234/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc0030645c0 0xc0030645c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003064630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.802: INFO: Pod "nginx-deployment-85db8c99c5-tt5fn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-tt5fn,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-tt5fn,UID:d885794b-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:1915,Generation:0,CreationTimestamp:2019-09-06 14:44:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.215/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc0030646d0 0xc0030646d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003064740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:35 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.215,StartTime:2019-09-06 14:44:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 14:44:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://edbb311b31988cdc03dfab305593ece13ff1c3156efa1d5e4d9b75560c7b2356}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.803: INFO: Pod "nginx-deployment-85db8c99c5-tvx5j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-tvx5j,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-tvx5j,UID:de895564-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2237,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.241/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc003064830 0xc003064831}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030648a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030648c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.803: INFO: Pod "nginx-deployment-85db8c99c5-wbw7d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-wbw7d,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-wbw7d,UID:de89b060-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2283,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.248/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc003064940 0xc003064941}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030649b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030649d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.804: INFO: Pod "nginx-deployment-85db8c99c5-x7lwb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-x7lwb,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-85db8c99c5-x7lwb,UID:de89b6d7-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2281,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.247/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 d8843405-d0b4-11e9-b447-fa163e0a078f 0xc003064a50 0xc003064a51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003064ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064ae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.804: INFO: Pod "nginx-deployment-b79c9d74d-2db7r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-2db7r,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-2db7r,UID:de8c5fe0-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2285,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.249/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc003064b60 0xc003064b61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003064be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.805: INFO: Pod "nginx-deployment-b79c9d74d-8ncg9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-8ncg9,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-8ncg9,UID:de89a56a-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2269,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.243/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc003064c80 0xc003064c81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003064d00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064d20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.805: INFO: Pod "nginx-deployment-b79c9d74d-dkp46" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-dkp46,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-dkp46,UID:dd4d68d2-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2034,Generation:0,CreationTimestamp:2019-09-06 14:44:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.225/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc003064da0 0xc003064da1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003064e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.225,StartTime:2019-09-06 14:44:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.805: INFO: Pod "nginx-deployment-b79c9d74d-f4fzh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-f4fzh,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-f4fzh,UID:dd53e72b-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2025,Generation:0,CreationTimestamp:2019-09-06 14:44:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.229/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc003064f30 0xc003064f31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003064fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003064fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.806: INFO: Pod "nginx-deployment-b79c9d74d-j4s2g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-j4s2g,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-j4s2g,UID:de89bb38-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2267,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.244/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc0030650a0 0xc0030650a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003065120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003065140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.806: INFO: Pod "nginx-deployment-b79c9d74d-ph4jq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-ph4jq,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-ph4jq,UID:de8311fb-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2141,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.230/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc0030651c0 0xc0030651c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003065240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003065260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.806: INFO: Pod "nginx-deployment-b79c9d74d-rc9gg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-rc9gg,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-rc9gg,UID:de89818b-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2219,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.240/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc003065330 0xc003065331}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030653b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030653d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.806: INFO: Pod "nginx-deployment-b79c9d74d-svj79" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-svj79,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-svj79,UID:de85233a-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2204,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.237/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc003065450 0xc003065451}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030654d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030654f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.806: INFO: Pod "nginx-deployment-b79c9d74d-tf24w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-tf24w,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-tf24w,UID:de852c41-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2228,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.239/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc0030655c0 0xc0030655c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003065640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003065660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.807: INFO: Pod "nginx-deployment-b79c9d74d-tnf52" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-tnf52,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-tnf52,UID:dd52b8b3-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2022,Generation:0,CreationTimestamp:2019-09-06 14:44:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.228/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc003065730 0xc003065731}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030657b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030657d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.808: INFO: Pod "nginx-deployment-b79c9d74d-trbml" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-trbml,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-trbml,UID:dd4ce89e-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2015,Generation:0,CreationTimestamp:2019-09-06 14:44:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.226/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc0030658a0 0xc0030658a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003065920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003065940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.809: INFO: Pod "nginx-deployment-b79c9d74d-w84xm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-w84xm,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-w84xm,UID:de89c5b8-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2279,Generation:0,CreationTimestamp:2019-09-06 14:44:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.246/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc003065a10 0xc003065a11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003065a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003065ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:45 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 14:44:47.809: INFO: Pod "nginx-deployment-b79c9d74d-xbpgz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-xbpgz,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-2812,SelfLink:/api/v1/namespaces/deployment-2812/pods/nginx-deployment-b79c9d74d-xbpgz,UID:dd4d5b0e-d0b4-11e9-b447-fa163e0a078f,ResourceVersion:2019,Generation:0,CreationTimestamp:2019-09-06 14:44:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.227/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d dd4c8d50-d0b4-11e9-b447-fa163e0a078f 0xc003065b30 0xc003065b31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ck89l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ck89l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ck89l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003065bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003065bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:44:43 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 14:44:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:44:47.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2812" for this suite.
Sep  6 14:44:55.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:44:55.955: INFO: namespace deployment-2812 deletion completed in 8.132081837s

• [SLOW TEST:20.308 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:44:55.955: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-e4a2ad17-d0b4-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 14:44:56.015: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394" in namespace "projected-7738" to be "success or failure"
Sep  6 14:44:56.029: INFO: Pod "pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 13.38807ms
Sep  6 14:44:58.032: INFO: Pod "pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01626174s
Sep  6 14:45:00.042: INFO: Pod "pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394": Phase="Running", Reason="", readiness=true. Elapsed: 4.026228297s
Sep  6 14:45:02.050: INFO: Pod "pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394": Phase="Running", Reason="", readiness=true. Elapsed: 6.034805182s
Sep  6 14:45:04.053: INFO: Pod "pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394": Phase="Running", Reason="", readiness=true. Elapsed: 8.037826076s
Sep  6 14:45:06.056: INFO: Pod "pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394": Phase="Running", Reason="", readiness=true. Elapsed: 10.040620254s
Sep  6 14:45:08.059: INFO: Pod "pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.043449187s
STEP: Saw pod success
Sep  6 14:45:08.059: INFO: Pod "pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:45:08.061: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 14:45:08.070: INFO: Waiting for pod pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:45:08.072: INFO: Pod pod-projected-secrets-e4a323a0-d0b4-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:45:08.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7738" for this suite.
Sep  6 14:45:14.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:45:14.178: INFO: namespace projected-7738 deletion completed in 6.10368395s

• [SLOW TEST:18.223 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:45:14.179: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0906 14:45:44.239381      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 14:45:44.239: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:45:44.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8807" for this suite.
Sep  6 14:45:50.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:45:50.322: INFO: namespace gc-8807 deletion completed in 6.079654112s

• [SLOW TEST:36.143 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:45:50.323: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-fwd8
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 14:45:50.376: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fwd8" in namespace "subpath-3621" to be "success or failure"
Sep  6 14:45:50.382: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.196634ms
Sep  6 14:45:52.385: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008097754s
Sep  6 14:45:54.387: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 4.010803164s
Sep  6 14:45:56.390: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 6.013741611s
Sep  6 14:45:58.393: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 8.016576455s
Sep  6 14:46:00.397: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 10.02091916s
Sep  6 14:46:02.400: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 12.02382844s
Sep  6 14:46:04.403: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 14.026409514s
Sep  6 14:46:06.407: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 16.030847792s
Sep  6 14:46:08.410: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 18.033670704s
Sep  6 14:46:10.413: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Running", Reason="", readiness=true. Elapsed: 20.036312644s
Sep  6 14:46:12.415: INFO: Pod "pod-subpath-test-configmap-fwd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.038977516s
STEP: Saw pod success
Sep  6 14:46:12.415: INFO: Pod "pod-subpath-test-configmap-fwd8" satisfied condition "success or failure"
Sep  6 14:46:12.418: INFO: Trying to get logs from node metalk8s-23 pod pod-subpath-test-configmap-fwd8 container test-container-subpath-configmap-fwd8: <nil>
STEP: delete the pod
Sep  6 14:46:12.428: INFO: Waiting for pod pod-subpath-test-configmap-fwd8 to disappear
Sep  6 14:46:12.431: INFO: Pod pod-subpath-test-configmap-fwd8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fwd8
Sep  6 14:46:12.432: INFO: Deleting pod "pod-subpath-test-configmap-fwd8" in namespace "subpath-3621"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:46:12.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3621" for this suite.
Sep  6 14:46:18.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:46:18.512: INFO: namespace subpath-3621 deletion completed in 6.073285525s

• [SLOW TEST:28.189 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:46:18.513: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 14:46:18.543: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15d3772f-d0b5-11e9-9ba3-f679ea11f394" in namespace "projected-2180" to be "success or failure"
Sep  6 14:46:18.556: INFO: Pod "downwardapi-volume-15d3772f-d0b5-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 13.174815ms
Sep  6 14:46:20.559: INFO: Pod "downwardapi-volume-15d3772f-d0b5-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016302992s
STEP: Saw pod success
Sep  6 14:46:20.559: INFO: Pod "downwardapi-volume-15d3772f-d0b5-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:46:20.562: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-15d3772f-d0b5-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 14:46:20.573: INFO: Waiting for pod downwardapi-volume-15d3772f-d0b5-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:46:20.575: INFO: Pod downwardapi-volume-15d3772f-d0b5-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:46:20.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2180" for this suite.
Sep  6 14:46:26.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:46:26.660: INFO: namespace projected-2180 deletion completed in 6.082577063s

• [SLOW TEST:8.147 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:46:26.661: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 14:46:26.704: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ab092c3-d0b5-11e9-9ba3-f679ea11f394" in namespace "projected-4876" to be "success or failure"
Sep  6 14:46:26.708: INFO: Pod "downwardapi-volume-1ab092c3-d0b5-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074301ms
Sep  6 14:46:28.711: INFO: Pod "downwardapi-volume-1ab092c3-d0b5-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007045899s
STEP: Saw pod success
Sep  6 14:46:28.711: INFO: Pod "downwardapi-volume-1ab092c3-d0b5-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:46:28.713: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-1ab092c3-d0b5-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 14:46:28.729: INFO: Waiting for pod downwardapi-volume-1ab092c3-d0b5-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:46:28.731: INFO: Pod downwardapi-volume-1ab092c3-d0b5-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:46:28.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4876" for this suite.
Sep  6 14:46:34.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:46:34.808: INFO: namespace projected-4876 deletion completed in 6.074789683s

• [SLOW TEST:8.147 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:46:34.808: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 14:46:34.830: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  6 14:46:34.838: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  6 14:46:39.841: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 14:46:39.841: INFO: Creating deployment "test-rolling-update-deployment"
Sep  6 14:46:39.844: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  6 14:46:39.849: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  6 14:46:41.859: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  6 14:46:41.863: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703377999, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703377999, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703377999, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703377999, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 14:46:43.866: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 14:46:43.872: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9679,SelfLink:/apis/apps/v1/namespaces/deployment-9679/deployments/test-rolling-update-deployment,UID:2286b425-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3037,Generation:1,CreationTimestamp:2019-09-06 14:46:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-06 14:46:39 +0000 UTC 2019-09-06 14:46:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-06 14:46:43 +0000 UTC 2019-09-06 14:46:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-57b6b5bb54" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 14:46:43.874: INFO: New ReplicaSet "test-rolling-update-deployment-57b6b5bb54" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54,GenerateName:,Namespace:deployment-9679,SelfLink:/apis/apps/v1/namespaces/deployment-9679/replicasets/test-rolling-update-deployment-57b6b5bb54,UID:22884da3-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3026,Generation:1,CreationTimestamp:2019-09-06 14:46:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2286b425-d0b5-11e9-b447-fa163e0a078f 0xc002622ab7 0xc002622ab8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 14:46:43.874: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  6 14:46:43.874: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9679,SelfLink:/apis/apps/v1/namespaces/deployment-9679/replicasets/test-rolling-update-controller,UID:1f8a26d0-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3036,Generation:2,CreationTimestamp:2019-09-06 14:46:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2286b425-d0b5-11e9-b447-fa163e0a078f 0xc0026229df 0xc0026229f0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 14:46:43.877: INFO: Pod "test-rolling-update-deployment-57b6b5bb54-flzlp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54-flzlp,GenerateName:test-rolling-update-deployment-57b6b5bb54-,Namespace:deployment-9679,SelfLink:/api/v1/namespaces/deployment-9679/pods/test-rolling-update-deployment-57b6b5bb54-flzlp,UID:2288b92c-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3025,Generation:0,CreationTimestamp:2019-09-06 14:46:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.210/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-57b6b5bb54 22884da3-d0b5-11e9-b447-fa163e0a078f 0xc00264b8a7 0xc00264b8a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-v5g2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v5g2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-v5g2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00264b920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00264b940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:46:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:46:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:46:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 14:46:39 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.210,StartTime:2019-09-06 14:46:39 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-06 14:46:42 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://cd94d24f3d1e2857084446fbee708a7df8d12b9e3d237eccf3820d23f526d99f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:46:43.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9679" for this suite.
Sep  6 14:46:49.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:46:49.949: INFO: namespace deployment-9679 deletion completed in 6.069745029s

• [SLOW TEST:15.141 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:46:49.949: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 14:46:58.008: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:46:58.014: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 14:47:00.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:47:00.017: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 14:47:02.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:47:02.017: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 14:47:04.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:47:04.017: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 14:47:06.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:47:06.017: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 14:47:08.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:47:08.017: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 14:47:10.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:47:10.017: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 14:47:12.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:47:12.017: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 14:47:14.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:47:14.017: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 14:47:16.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 14:47:16.017: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:47:16.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9476" for this suite.
Sep  6 14:47:38.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:47:38.105: INFO: namespace container-lifecycle-hook-9476 deletion completed in 22.081364541s

• [SLOW TEST:48.155 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:47:38.105: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Sep  6 14:47:38.139: INFO: Waiting up to 5m0s for pod "client-containers-45451cc3-d0b5-11e9-9ba3-f679ea11f394" in namespace "containers-7978" to be "success or failure"
Sep  6 14:47:38.145: INFO: Pod "client-containers-45451cc3-d0b5-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 5.927148ms
Sep  6 14:47:40.149: INFO: Pod "client-containers-45451cc3-d0b5-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009837908s
Sep  6 14:47:42.154: INFO: Pod "client-containers-45451cc3-d0b5-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014730683s
STEP: Saw pod success
Sep  6 14:47:42.154: INFO: Pod "client-containers-45451cc3-d0b5-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:47:42.157: INFO: Trying to get logs from node metalk8s-23 pod client-containers-45451cc3-d0b5-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 14:47:42.179: INFO: Waiting for pod client-containers-45451cc3-d0b5-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:47:42.188: INFO: Pod client-containers-45451cc3-d0b5-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:47:42.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7978" for this suite.
Sep  6 14:47:48.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:47:48.265: INFO: namespace containers-7978 deletion completed in 6.07447174s

• [SLOW TEST:10.161 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:47:48.266: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  6 14:47:48.299: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-a,UID:4b53ef76-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3257,Generation:0,CreationTimestamp:2019-09-06 14:47:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 14:47:48.300: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-a,UID:4b53ef76-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3257,Generation:0,CreationTimestamp:2019-09-06 14:47:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  6 14:47:58.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-a,UID:4b53ef76-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3271,Generation:0,CreationTimestamp:2019-09-06 14:47:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  6 14:47:58.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-a,UID:4b53ef76-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3271,Generation:0,CreationTimestamp:2019-09-06 14:47:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  6 14:48:08.313: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-a,UID:4b53ef76-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3286,Generation:0,CreationTimestamp:2019-09-06 14:47:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 14:48:08.313: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-a,UID:4b53ef76-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3286,Generation:0,CreationTimestamp:2019-09-06 14:47:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  6 14:48:18.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-a,UID:4b53ef76-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3301,Generation:0,CreationTimestamp:2019-09-06 14:47:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 14:48:18.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-a,UID:4b53ef76-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3301,Generation:0,CreationTimestamp:2019-09-06 14:47:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  6 14:48:28.322: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-b,UID:632ee9ac-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3315,Generation:0,CreationTimestamp:2019-09-06 14:48:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 14:48:28.322: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-b,UID:632ee9ac-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3315,Generation:0,CreationTimestamp:2019-09-06 14:48:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  6 14:48:38.327: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-b,UID:632ee9ac-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3329,Generation:0,CreationTimestamp:2019-09-06 14:48:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 14:48:38.327: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9960,SelfLink:/api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-configmap-b,UID:632ee9ac-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3329,Generation:0,CreationTimestamp:2019-09-06 14:48:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:48:48.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9960" for this suite.
Sep  6 14:48:54.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:48:54.405: INFO: namespace watch-9960 deletion completed in 6.074615188s

• [SLOW TEST:66.139 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:48:54.405: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 14:48:54.437: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72bf0dfe-d0b5-11e9-9ba3-f679ea11f394" in namespace "downward-api-325" to be "success or failure"
Sep  6 14:48:54.439: INFO: Pod "downwardapi-volume-72bf0dfe-d0b5-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.933194ms
Sep  6 14:48:56.442: INFO: Pod "downwardapi-volume-72bf0dfe-d0b5-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00520874s
STEP: Saw pod success
Sep  6 14:48:56.442: INFO: Pod "downwardapi-volume-72bf0dfe-d0b5-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:48:56.444: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-72bf0dfe-d0b5-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 14:48:56.457: INFO: Waiting for pod downwardapi-volume-72bf0dfe-d0b5-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:48:56.459: INFO: Pod downwardapi-volume-72bf0dfe-d0b5-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:48:56.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-325" for this suite.
Sep  6 14:49:02.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:49:02.535: INFO: namespace downward-api-325 deletion completed in 6.074048822s

• [SLOW TEST:8.130 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:49:02.535: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep  6 14:49:02.555: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 14:49:02.561: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 14:49:02.563: INFO: 
Logging pods the kubelet thinks is on node metalk8s-23 before test
Sep  6 14:49:02.574: INFO: kube-proxy-smw98 from kube-system started at 2019-09-06 14:39:34 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 14:49:02.574: INFO: kube-state-metrics-685fc7775f-nbg55 from metalk8s-monitoring started at 2019-09-06 14:40:00 +0000 UTC (4 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  6 14:49:02.574: INFO: sonobuoy-e2e-job-b2dd9a0f0ff64346 from heptio-sonobuoy started at 2019-09-06 14:42:24 +0000 UTC (2 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container e2e ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 14:49:02.574: INFO: alertmanager-main-1 from metalk8s-monitoring started at 2019-09-06 14:40:17 +0000 UTC (2 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 14:49:02.574: INFO: salt-master-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:49:02.574: INFO: kube-controller-manager-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:49:02.574: INFO: prometheus-adapter-7cfccdc649-nh8nw from metalk8s-monitoring started at 2019-09-06 14:39:54 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep  6 14:49:02.574: INFO: alertmanager-main-2 from metalk8s-monitoring started at 2019-09-06 14:40:25 +0000 UTC (2 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 14:49:02.574: INFO: node-exporter-lwh4f from metalk8s-monitoring started at 2019-09-06 14:40:01 +0000 UTC (2 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 14:49:02.574: INFO: alertmanager-main-0 from metalk8s-monitoring started at 2019-09-06 14:40:07 +0000 UTC (2 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 14:49:02.574: INFO: sonobuoy-systemd-logs-daemon-set-e00271a3bb5e42b1-t5zmh from heptio-sonobuoy started at 2019-09-06 14:42:24 +0000 UTC (2 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 14:49:02.574: INFO: calico-node-xlrzb from kube-system started at 2019-09-06 14:39:41 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 14:49:02.574: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 14:42:19 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 14:49:02.574: INFO: kube-apiserver-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:49:02.574: INFO: grafana-5468d6ff9b-dkqdl from metalk8s-monitoring started at 2019-09-06 14:40:08 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container grafana ready: true, restart count 0
Sep  6 14:49:02.574: INFO: nginx-ingress-controller-97jbb from metalk8s-ingress started at 2019-09-06 14:40:08 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  6 14:49:02.574: INFO: prometheus-k8s-1 from metalk8s-monitoring started at 2019-09-06 14:40:13 +0000 UTC (3 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 14:49:02.574: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 14:49:02.574: INFO: kube-scheduler-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:49:02.574: INFO: prometheus-operator-6d89bf894c-tzxp9 from metalk8s-monitoring started at 2019-09-06 14:39:47 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  6 14:49:02.574: INFO: repositories-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:49:02.574: INFO: coredns-55d57d558b-9z5x6 from kube-system started at 2019-09-06 14:39:43 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container coredns ready: true, restart count 0
Sep  6 14:49:02.574: INFO: nginx-ingress-default-backend-7d98759d5f-z8zsq from metalk8s-ingress started at 2019-09-06 14:40:08 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep  6 14:49:02.574: INFO: calico-kube-controllers-75579b5cb4-kzvqd from kube-system started at 2019-09-06 14:39:41 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 14:49:02.574: INFO: prometheus-k8s-0 from metalk8s-monitoring started at 2019-09-06 14:40:13 +0000 UTC (3 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 14:49:02.574: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 14:49:02.574: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 14:49:02.574: INFO: coredns-55d57d558b-fr555 from kube-system started at 2019-09-06 14:39:43 +0000 UTC (1 container statuses recorded)
Sep  6 14:49:02.574: INFO: 	Container coredns ready: true, restart count 0
Sep  6 14:49:02.574: INFO: etcd-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-78cea669-d0b5-11e9-9ba3-f679ea11f394 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-78cea669-d0b5-11e9-9ba3-f679ea11f394 off the node metalk8s-23
STEP: verifying the node doesn't have the label kubernetes.io/e2e-78cea669-d0b5-11e9-9ba3-f679ea11f394
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:49:06.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4492" for this suite.
Sep  6 14:49:24.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:49:24.719: INFO: namespace sched-pred-4492 deletion completed in 18.074547226s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:22.184 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:49:24.720: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 14:49:24.755: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84d175e8-d0b5-11e9-9ba3-f679ea11f394" in namespace "downward-api-9091" to be "success or failure"
Sep  6 14:49:24.757: INFO: Pod "downwardapi-volume-84d175e8-d0b5-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023865ms
Sep  6 14:49:26.761: INFO: Pod "downwardapi-volume-84d175e8-d0b5-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00611667s
STEP: Saw pod success
Sep  6 14:49:26.761: INFO: Pod "downwardapi-volume-84d175e8-d0b5-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:49:26.764: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-84d175e8-d0b5-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 14:49:26.782: INFO: Waiting for pod downwardapi-volume-84d175e8-d0b5-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:49:26.784: INFO: Pod downwardapi-volume-84d175e8-d0b5-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:49:26.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9091" for this suite.
Sep  6 14:49:32.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:49:32.858: INFO: namespace downward-api-9091 deletion completed in 6.072243062s

• [SLOW TEST:8.139 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:49:32.858: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  6 14:49:32.900: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6073,SelfLink:/api/v1/namespaces/watch-6073/configmaps/e2e-watch-test-resource-version,UID:89aae103-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3530,Generation:0,CreationTimestamp:2019-09-06 14:49:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 14:49:32.901: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6073,SelfLink:/api/v1/namespaces/watch-6073/configmaps/e2e-watch-test-resource-version,UID:89aae103-d0b5-11e9-b447-fa163e0a078f,ResourceVersion:3531,Generation:0,CreationTimestamp:2019-09-06 14:49:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:49:32.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6073" for this suite.
Sep  6 14:49:38.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:49:38.966: INFO: namespace watch-6073 deletion completed in 6.063710716s

• [SLOW TEST:6.108 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:49:38.966: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Sep  6 14:49:38.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 api-versions'
Sep  6 14:49:39.103: INFO: stderr: ""
Sep  6 14:49:39.103: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:49:39.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1641" for this suite.
Sep  6 14:49:45.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:49:45.199: INFO: namespace kubectl-1641 deletion completed in 6.093529763s

• [SLOW TEST:6.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:49:45.199: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:49:45.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6342" for this suite.
Sep  6 14:50:07.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:50:07.302: INFO: namespace pods-6342 deletion completed in 22.070267061s

• [SLOW TEST:22.103 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:50:07.302: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  6 14:50:07.324: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:50:09.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8960" for this suite.
Sep  6 14:50:15.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:50:16.026: INFO: namespace init-container-8960 deletion completed in 6.080263593s

• [SLOW TEST:8.724 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:50:16.027: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6761
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 14:50:16.058: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 14:50:36.101: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.221.249 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6761 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 14:50:36.101: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 14:50:37.229: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:50:37.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6761" for this suite.
Sep  6 14:50:59.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:50:59.298: INFO: namespace pod-network-test-6761 deletion completed in 22.066917669s

• [SLOW TEST:43.271 seconds]
[sig-network] Networking
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:50:59.298: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 14:50:59.351: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 14:50:59.375: INFO: Number of nodes with available pods: 0
Sep  6 14:50:59.376: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:51:00.382: INFO: Number of nodes with available pods: 1
Sep  6 14:51:00.383: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  6 14:51:00.403: INFO: Wrong image for pod: daemon-set-6q5fd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 14:51:01.408: INFO: Wrong image for pod: daemon-set-6q5fd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 14:51:02.408: INFO: Wrong image for pod: daemon-set-6q5fd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 14:51:03.408: INFO: Wrong image for pod: daemon-set-6q5fd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 14:51:03.408: INFO: Pod daemon-set-6q5fd is not available
Sep  6 14:51:04.407: INFO: Wrong image for pod: daemon-set-6q5fd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 14:51:04.407: INFO: Pod daemon-set-6q5fd is not available
Sep  6 14:51:05.408: INFO: Wrong image for pod: daemon-set-6q5fd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 14:51:05.408: INFO: Pod daemon-set-6q5fd is not available
Sep  6 14:51:06.407: INFO: Wrong image for pod: daemon-set-6q5fd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 14:51:06.407: INFO: Pod daemon-set-6q5fd is not available
Sep  6 14:51:07.407: INFO: Wrong image for pod: daemon-set-6q5fd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 14:51:07.407: INFO: Pod daemon-set-6q5fd is not available
Sep  6 14:51:08.407: INFO: Wrong image for pod: daemon-set-6q5fd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 14:51:08.407: INFO: Pod daemon-set-6q5fd is not available
Sep  6 14:51:09.408: INFO: Pod daemon-set-mfclq is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  6 14:51:09.418: INFO: Number of nodes with available pods: 0
Sep  6 14:51:09.418: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:51:10.422: INFO: Number of nodes with available pods: 1
Sep  6 14:51:10.422: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2141, will wait for the garbage collector to delete the pods
Sep  6 14:51:10.489: INFO: Deleting DaemonSet.extensions daemon-set took: 4.011125ms
Sep  6 14:51:10.890: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.300252ms
Sep  6 14:51:18.693: INFO: Number of nodes with available pods: 0
Sep  6 14:51:18.693: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 14:51:18.695: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2141/daemonsets","resourceVersion":"3876"},"items":null}

Sep  6 14:51:18.696: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2141/pods","resourceVersion":"3876"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:51:18.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2141" for this suite.
Sep  6 14:51:24.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:51:24.772: INFO: namespace daemonsets-2141 deletion completed in 6.069838332s

• [SLOW TEST:25.474 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:51:24.773: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7499
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 14:51:24.794: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 14:51:40.833: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.221.240:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7499 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 14:51:40.833: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 14:51:40.957: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:51:40.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7499" for this suite.
Sep  6 14:52:02.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:52:03.033: INFO: namespace pod-network-test-7499 deletion completed in 22.071637835s

• [SLOW TEST:38.260 seconds]
[sig-network] Networking
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:52:03.033: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-5gkg
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 14:52:03.065: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5gkg" in namespace "subpath-5749" to be "success or failure"
Sep  6 14:52:03.068: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592065ms
Sep  6 14:52:05.073: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 2.007128826s
Sep  6 14:52:07.076: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 4.010269756s
Sep  6 14:52:09.079: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 6.01311842s
Sep  6 14:52:11.083: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 8.017018269s
Sep  6 14:52:13.086: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 10.019982134s
Sep  6 14:52:15.088: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 12.022952267s
Sep  6 14:52:17.091: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 14.025637702s
Sep  6 14:52:19.094: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 16.028531188s
Sep  6 14:52:21.097: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 18.031825163s
Sep  6 14:52:23.100: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Running", Reason="", readiness=true. Elapsed: 20.034631314s
Sep  6 14:52:25.103: INFO: Pod "pod-subpath-test-configmap-5gkg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.037309483s
STEP: Saw pod success
Sep  6 14:52:25.103: INFO: Pod "pod-subpath-test-configmap-5gkg" satisfied condition "success or failure"
Sep  6 14:52:25.104: INFO: Trying to get logs from node metalk8s-23 pod pod-subpath-test-configmap-5gkg container test-container-subpath-configmap-5gkg: <nil>
STEP: delete the pod
Sep  6 14:52:25.116: INFO: Waiting for pod pod-subpath-test-configmap-5gkg to disappear
Sep  6 14:52:25.118: INFO: Pod pod-subpath-test-configmap-5gkg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5gkg
Sep  6 14:52:25.119: INFO: Deleting pod "pod-subpath-test-configmap-5gkg" in namespace "subpath-5749"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:52:25.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5749" for this suite.
Sep  6 14:52:31.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:52:31.203: INFO: namespace subpath-5749 deletion completed in 6.080321543s

• [SLOW TEST:28.169 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:52:31.203: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 14:52:57.238: INFO: Container started at 2019-09-06 14:52:33 +0000 UTC, pod became ready at 2019-09-06 14:52:55 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:52:57.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3679" for this suite.
Sep  6 14:53:19.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:53:19.315: INFO: namespace container-probe-3679 deletion completed in 22.073479974s

• [SLOW TEST:48.112 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:53:19.318: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-10a65b8a-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:53:21.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2333" for this suite.
Sep  6 14:53:43.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:53:43.458: INFO: namespace configmap-2333 deletion completed in 22.078833153s

• [SLOW TEST:24.140 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:53:43.459: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Sep  6 14:53:43.490: INFO: Waiting up to 5m0s for pod "client-containers-1f09876b-d0b6-11e9-9ba3-f679ea11f394" in namespace "containers-5038" to be "success or failure"
Sep  6 14:53:43.494: INFO: Pod "client-containers-1f09876b-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029284ms
Sep  6 14:53:45.497: INFO: Pod "client-containers-1f09876b-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006876383s
STEP: Saw pod success
Sep  6 14:53:45.497: INFO: Pod "client-containers-1f09876b-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:53:45.499: INFO: Trying to get logs from node metalk8s-23 pod client-containers-1f09876b-d0b6-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 14:53:45.510: INFO: Waiting for pod client-containers-1f09876b-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:53:45.512: INFO: Pod client-containers-1f09876b-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:53:45.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5038" for this suite.
Sep  6 14:53:51.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:53:51.589: INFO: namespace containers-5038 deletion completed in 6.074952583s

• [SLOW TEST:8.130 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:53:51.591: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 14:53:51.624: INFO: Waiting up to 5m0s for pod "pod-23e2ad9a-d0b6-11e9-9ba3-f679ea11f394" in namespace "emptydir-8980" to be "success or failure"
Sep  6 14:53:51.626: INFO: Pod "pod-23e2ad9a-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324488ms
Sep  6 14:53:53.629: INFO: Pod "pod-23e2ad9a-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005502758s
STEP: Saw pod success
Sep  6 14:53:53.629: INFO: Pod "pod-23e2ad9a-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:53:53.632: INFO: Trying to get logs from node metalk8s-23 pod pod-23e2ad9a-d0b6-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 14:53:53.644: INFO: Waiting for pod pod-23e2ad9a-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:53:53.647: INFO: Pod pod-23e2ad9a-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:53:53.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8980" for this suite.
Sep  6 14:53:59.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:53:59.719: INFO: namespace emptydir-8980 deletion completed in 6.070073728s

• [SLOW TEST:8.127 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:53:59.719: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1174
STEP: creating the pod
Sep  6 14:53:59.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-6761'
Sep  6 14:54:00.090: INFO: stderr: ""
Sep  6 14:54:00.090: INFO: stdout: "pod/pause created\n"
Sep  6 14:54:00.090: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  6 14:54:00.090: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6761" to be "running and ready"
Sep  6 14:54:00.094: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657428ms
Sep  6 14:54:02.098: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007574508s
Sep  6 14:54:02.098: INFO: Pod "pause" satisfied condition "running and ready"
Sep  6 14:54:02.098: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  6 14:54:02.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 label pods pause testing-label=testing-label-value --namespace=kubectl-6761'
Sep  6 14:54:02.185: INFO: stderr: ""
Sep  6 14:54:02.185: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  6 14:54:02.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pod pause -L testing-label --namespace=kubectl-6761'
Sep  6 14:54:02.264: INFO: stderr: ""
Sep  6 14:54:02.264: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  6 14:54:02.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 label pods pause testing-label- --namespace=kubectl-6761'
Sep  6 14:54:02.371: INFO: stderr: ""
Sep  6 14:54:02.371: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  6 14:54:02.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pod pause -L testing-label --namespace=kubectl-6761'
Sep  6 14:54:02.457: INFO: stderr: ""
Sep  6 14:54:02.457: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1181
STEP: using delete to clean up resources
Sep  6 14:54:02.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-6761'
Sep  6 14:54:02.529: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 14:54:02.529: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  6 14:54:02.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get rc,svc -l name=pause --no-headers --namespace=kubectl-6761'
Sep  6 14:54:02.611: INFO: stderr: "No resources found.\n"
Sep  6 14:54:02.611: INFO: stdout: ""
Sep  6 14:54:02.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -l name=pause --namespace=kubectl-6761 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 14:54:02.683: INFO: stderr: ""
Sep  6 14:54:02.683: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:54:02.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6761" for this suite.
Sep  6 14:54:08.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:54:08.753: INFO: namespace kubectl-6761 deletion completed in 6.067594602s

• [SLOW TEST:9.034 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:54:08.754: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2136.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2136.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2136.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2136.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2136.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2136.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2136.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2136.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2136.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2136.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2136.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2136.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2136.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 135.187.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.187.135_udp@PTR;check="$$(dig +tcp +noall +answer +search 135.187.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.187.135_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2136.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2136.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2136.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2136.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2136.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2136.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2136.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2136.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2136.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2136.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2136.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2136.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2136.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 135.187.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.187.135_udp@PTR;check="$$(dig +tcp +noall +answer +search 135.187.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.187.135_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 14:55:00.823: INFO: Unable to read wheezy_udp@dns-test-service.dns-2136.svc.cluster.local from pod dns-2136/dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394: the server could not find the requested resource (get pods dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394)
Sep  6 14:55:00.826: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2136.svc.cluster.local from pod dns-2136/dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394: the server could not find the requested resource (get pods dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394)
Sep  6 14:55:00.828: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2136.svc.cluster.local from pod dns-2136/dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394: the server could not find the requested resource (get pods dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394)
Sep  6 14:55:00.845: INFO: Unable to read jessie_udp@dns-test-service.dns-2136.svc.cluster.local from pod dns-2136/dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394: the server could not find the requested resource (get pods dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394)
Sep  6 14:55:00.847: INFO: Unable to read jessie_tcp@dns-test-service.dns-2136.svc.cluster.local from pod dns-2136/dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394: the server could not find the requested resource (get pods dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394)
Sep  6 14:55:00.850: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2136.svc.cluster.local from pod dns-2136/dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394: the server could not find the requested resource (get pods dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394)
Sep  6 14:55:00.865: INFO: Lookups using dns-2136/dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394 failed for: [wheezy_udp@dns-test-service.dns-2136.svc.cluster.local wheezy_tcp@dns-test-service.dns-2136.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2136.svc.cluster.local jessie_udp@dns-test-service.dns-2136.svc.cluster.local jessie_tcp@dns-test-service.dns-2136.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2136.svc.cluster.local]

Sep  6 14:55:05.905: INFO: DNS probes using dns-2136/dns-test-2e208652-d0b6-11e9-9ba3-f679ea11f394 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:55:05.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2136" for this suite.
Sep  6 14:55:11.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:55:12.049: INFO: namespace dns-2136 deletion completed in 6.081100573s

• [SLOW TEST:63.296 seconds]
[sig-network] DNS
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:55:12.050: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 14:55:12.098: INFO: Number of nodes with available pods: 0
Sep  6 14:55:12.098: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:55:13.102: INFO: Number of nodes with available pods: 1
Sep  6 14:55:13.102: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  6 14:55:13.119: INFO: Number of nodes with available pods: 0
Sep  6 14:55:13.119: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 14:55:14.124: INFO: Number of nodes with available pods: 1
Sep  6 14:55:14.124: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4800, will wait for the garbage collector to delete the pods
Sep  6 14:55:14.188: INFO: Deleting DaemonSet.extensions daemon-set took: 6.327238ms
Sep  6 14:55:14.588: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.225413ms
Sep  6 14:55:28.691: INFO: Number of nodes with available pods: 0
Sep  6 14:55:28.691: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 14:55:28.693: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4800/daemonsets","resourceVersion":"4622"},"items":null}

Sep  6 14:55:28.695: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4800/pods","resourceVersion":"4622"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:55:28.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4800" for this suite.
Sep  6 14:55:34.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:55:34.769: INFO: namespace daemonsets-4800 deletion completed in 6.06793323s

• [SLOW TEST:22.720 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:55:34.770: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-61632a17-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 14:55:34.821: INFO: Waiting up to 5m0s for pod "pod-configmaps-616486a8-d0b6-11e9-9ba3-f679ea11f394" in namespace "configmap-693" to be "success or failure"
Sep  6 14:55:34.827: INFO: Pod "pod-configmaps-616486a8-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 6.419438ms
Sep  6 14:55:36.830: INFO: Pod "pod-configmaps-616486a8-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009591022s
STEP: Saw pod success
Sep  6 14:55:36.830: INFO: Pod "pod-configmaps-616486a8-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:55:36.832: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-616486a8-d0b6-11e9-9ba3-f679ea11f394 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 14:55:36.848: INFO: Waiting for pod pod-configmaps-616486a8-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:55:36.856: INFO: Pod pod-configmaps-616486a8-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:55:36.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-693" for this suite.
Sep  6 14:55:42.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:55:42.943: INFO: namespace configmap-693 deletion completed in 6.069613017s

• [SLOW TEST:8.173 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:55:42.943: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep  6 14:55:42.974: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 14:55:42.978: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 14:55:42.982: INFO: 
Logging pods the kubelet thinks is on node metalk8s-23 before test
Sep  6 14:55:42.992: INFO: alertmanager-main-0 from metalk8s-monitoring started at 2019-09-06 14:40:07 +0000 UTC (2 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 14:55:42.992: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 14:55:42.992: INFO: sonobuoy-systemd-logs-daemon-set-e00271a3bb5e42b1-t5zmh from heptio-sonobuoy started at 2019-09-06 14:42:24 +0000 UTC (2 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 14:55:42.992: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 14:55:42.992: INFO: calico-node-xlrzb from kube-system started at 2019-09-06 14:39:41 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 14:55:42.992: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 14:42:19 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 14:55:42.992: INFO: kube-apiserver-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:55:42.992: INFO: grafana-5468d6ff9b-dkqdl from metalk8s-monitoring started at 2019-09-06 14:40:08 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container grafana ready: true, restart count 0
Sep  6 14:55:42.992: INFO: nginx-ingress-controller-97jbb from metalk8s-ingress started at 2019-09-06 14:40:08 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  6 14:55:42.992: INFO: prometheus-k8s-1 from metalk8s-monitoring started at 2019-09-06 14:40:13 +0000 UTC (3 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 14:55:42.992: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 14:55:42.992: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 14:55:42.992: INFO: kube-scheduler-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:55:42.992: INFO: prometheus-operator-6d89bf894c-tzxp9 from metalk8s-monitoring started at 2019-09-06 14:39:47 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  6 14:55:42.992: INFO: repositories-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:55:42.992: INFO: coredns-55d57d558b-9z5x6 from kube-system started at 2019-09-06 14:39:43 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container coredns ready: true, restart count 0
Sep  6 14:55:42.992: INFO: nginx-ingress-default-backend-7d98759d5f-z8zsq from metalk8s-ingress started at 2019-09-06 14:40:08 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep  6 14:55:42.992: INFO: calico-kube-controllers-75579b5cb4-kzvqd from kube-system started at 2019-09-06 14:39:41 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 14:55:42.992: INFO: prometheus-k8s-0 from metalk8s-monitoring started at 2019-09-06 14:40:13 +0000 UTC (3 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 14:55:42.992: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 14:55:42.992: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 14:55:42.992: INFO: coredns-55d57d558b-fr555 from kube-system started at 2019-09-06 14:39:43 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container coredns ready: true, restart count 0
Sep  6 14:55:42.992: INFO: etcd-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:55:42.992: INFO: kube-proxy-smw98 from kube-system started at 2019-09-06 14:39:34 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 14:55:42.992: INFO: kube-state-metrics-685fc7775f-nbg55 from metalk8s-monitoring started at 2019-09-06 14:40:00 +0000 UTC (4 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  6 14:55:42.992: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  6 14:55:42.992: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  6 14:55:42.992: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  6 14:55:42.992: INFO: sonobuoy-e2e-job-b2dd9a0f0ff64346 from heptio-sonobuoy started at 2019-09-06 14:42:24 +0000 UTC (2 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container e2e ready: true, restart count 0
Sep  6 14:55:42.992: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 14:55:42.992: INFO: alertmanager-main-1 from metalk8s-monitoring started at 2019-09-06 14:40:17 +0000 UTC (2 container statuses recorded)
Sep  6 14:55:42.992: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 14:55:42.992: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 14:55:42.993: INFO: salt-master-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:55:42.993: INFO: kube-controller-manager-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 14:55:42.993: INFO: prometheus-adapter-7cfccdc649-nh8nw from metalk8s-monitoring started at 2019-09-06 14:39:54 +0000 UTC (1 container statuses recorded)
Sep  6 14:55:42.993: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep  6 14:55:42.993: INFO: alertmanager-main-2 from metalk8s-monitoring started at 2019-09-06 14:40:25 +0000 UTC (2 container statuses recorded)
Sep  6 14:55:42.993: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 14:55:42.993: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 14:55:42.993: INFO: node-exporter-lwh4f from metalk8s-monitoring started at 2019-09-06 14:40:01 +0000 UTC (2 container statuses recorded)
Sep  6 14:55:42.993: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 14:55:42.993: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c1e130556d7235], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:55:44.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7220" for this suite.
Sep  6 14:55:50.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:55:50.091: INFO: namespace sched-pred-7220 deletion completed in 6.075053902s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.147 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:55:50.091: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:55:52.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4799" for this suite.
Sep  6 14:56:30.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:56:30.213: INFO: namespace kubelet-test-4799 deletion completed in 38.075414361s

• [SLOW TEST:40.122 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:56:30.213: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1384
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 14:56:30.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8679'
Sep  6 14:56:30.322: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 14:56:30.322: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Sep  6 14:56:30.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8679'
Sep  6 14:56:30.416: INFO: stderr: ""
Sep  6 14:56:30.416: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:56:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8679" for this suite.
Sep  6 14:56:36.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:56:36.495: INFO: namespace kubectl-8679 deletion completed in 6.076631049s

• [SLOW TEST:6.282 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:56:36.495: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 14:56:36.526: INFO: Waiting up to 5m0s for pod "downward-api-862c369f-d0b6-11e9-9ba3-f679ea11f394" in namespace "downward-api-8366" to be "success or failure"
Sep  6 14:56:36.530: INFO: Pod "downward-api-862c369f-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.574628ms
Sep  6 14:56:38.536: INFO: Pod "downward-api-862c369f-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010735982s
STEP: Saw pod success
Sep  6 14:56:38.536: INFO: Pod "downward-api-862c369f-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:56:38.539: INFO: Trying to get logs from node metalk8s-23 pod downward-api-862c369f-d0b6-11e9-9ba3-f679ea11f394 container dapi-container: <nil>
STEP: delete the pod
Sep  6 14:56:38.551: INFO: Waiting for pod downward-api-862c369f-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:56:38.553: INFO: Pod downward-api-862c369f-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:56:38.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8366" for this suite.
Sep  6 14:56:44.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:56:44.648: INFO: namespace downward-api-8366 deletion completed in 6.092718355s

• [SLOW TEST:8.152 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:56:44.648: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Sep  6 14:56:45.198: INFO: created pod pod-service-account-defaultsa
Sep  6 14:56:45.198: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  6 14:56:45.206: INFO: created pod pod-service-account-mountsa
Sep  6 14:56:45.206: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  6 14:56:45.211: INFO: created pod pod-service-account-nomountsa
Sep  6 14:56:45.211: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  6 14:56:45.221: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  6 14:56:45.221: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  6 14:56:45.228: INFO: created pod pod-service-account-mountsa-mountspec
Sep  6 14:56:45.228: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  6 14:56:45.246: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  6 14:56:45.246: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  6 14:56:45.270: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  6 14:56:45.270: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  6 14:56:45.274: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  6 14:56:45.274: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  6 14:56:45.279: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  6 14:56:45.279: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:56:45.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8875" for this suite.
Sep  6 14:57:09.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:57:09.372: INFO: namespace svcaccounts-8875 deletion completed in 24.087308964s

• [SLOW TEST:24.724 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:57:09.373: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-99c60f2d-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 14:57:09.409: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-99c66286-d0b6-11e9-9ba3-f679ea11f394" in namespace "projected-9580" to be "success or failure"
Sep  6 14:57:09.411: INFO: Pod "pod-projected-configmaps-99c66286-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274791ms
Sep  6 14:57:11.415: INFO: Pod "pod-projected-configmaps-99c66286-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006006945s
STEP: Saw pod success
Sep  6 14:57:11.415: INFO: Pod "pod-projected-configmaps-99c66286-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:57:11.418: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-configmaps-99c66286-d0b6-11e9-9ba3-f679ea11f394 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 14:57:11.430: INFO: Waiting for pod pod-projected-configmaps-99c66286-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:57:11.431: INFO: Pod pod-projected-configmaps-99c66286-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:57:11.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9580" for this suite.
Sep  6 14:57:17.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:57:17.503: INFO: namespace projected-9580 deletion completed in 6.070047713s

• [SLOW TEST:8.131 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:57:17.503: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:57:19.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-293" for this suite.
Sep  6 14:57:25.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:57:25.635: INFO: namespace emptydir-wrapper-293 deletion completed in 6.070941125s

• [SLOW TEST:8.132 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:57:25.635: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-a3769be7-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 14:57:25.690: INFO: Waiting up to 5m0s for pod "pod-secrets-a37aa014-d0b6-11e9-9ba3-f679ea11f394" in namespace "secrets-3162" to be "success or failure"
Sep  6 14:57:25.693: INFO: Pod "pod-secrets-a37aa014-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533232ms
Sep  6 14:57:27.696: INFO: Pod "pod-secrets-a37aa014-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005282821s
STEP: Saw pod success
Sep  6 14:57:27.696: INFO: Pod "pod-secrets-a37aa014-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:57:27.698: INFO: Trying to get logs from node metalk8s-23 pod pod-secrets-a37aa014-d0b6-11e9-9ba3-f679ea11f394 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 14:57:27.713: INFO: Waiting for pod pod-secrets-a37aa014-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:57:27.724: INFO: Pod pod-secrets-a37aa014-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:57:27.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3162" for this suite.
Sep  6 14:57:33.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:57:33.800: INFO: namespace secrets-3162 deletion completed in 6.074222857s
STEP: Destroying namespace "secret-namespace-3646" for this suite.
Sep  6 14:57:39.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:57:39.862: INFO: namespace secret-namespace-3646 deletion completed in 6.061368984s

• [SLOW TEST:14.227 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:57:39.862: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 14:57:39.891: INFO: Waiting up to 5m0s for pod "pod-abf0ef76-d0b6-11e9-9ba3-f679ea11f394" in namespace "emptydir-1930" to be "success or failure"
Sep  6 14:57:39.893: INFO: Pod "pod-abf0ef76-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.841628ms
Sep  6 14:57:41.897: INFO: Pod "pod-abf0ef76-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005737431s
STEP: Saw pod success
Sep  6 14:57:41.897: INFO: Pod "pod-abf0ef76-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:57:41.900: INFO: Trying to get logs from node metalk8s-23 pod pod-abf0ef76-d0b6-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 14:57:41.944: INFO: Waiting for pod pod-abf0ef76-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:57:41.952: INFO: Pod pod-abf0ef76-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:57:41.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1930" for this suite.
Sep  6 14:57:47.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:57:48.042: INFO: namespace emptydir-1930 deletion completed in 6.085707291s

• [SLOW TEST:8.180 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:57:48.042: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6864/configmap-test-b0d16a9b-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 14:57:48.080: INFO: Waiting up to 5m0s for pod "pod-configmaps-b0d22cd5-d0b6-11e9-9ba3-f679ea11f394" in namespace "configmap-6864" to be "success or failure"
Sep  6 14:57:48.084: INFO: Pod "pod-configmaps-b0d22cd5-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958568ms
Sep  6 14:57:50.087: INFO: Pod "pod-configmaps-b0d22cd5-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006724305s
STEP: Saw pod success
Sep  6 14:57:50.087: INFO: Pod "pod-configmaps-b0d22cd5-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:57:50.089: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-b0d22cd5-d0b6-11e9-9ba3-f679ea11f394 container env-test: <nil>
STEP: delete the pod
Sep  6 14:57:50.106: INFO: Waiting for pod pod-configmaps-b0d22cd5-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:57:50.109: INFO: Pod pod-configmaps-b0d22cd5-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:57:50.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6864" for this suite.
Sep  6 14:57:56.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:57:56.193: INFO: namespace configmap-6864 deletion completed in 6.081346533s

• [SLOW TEST:8.151 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:57:56.194: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 14:57:56.239: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b5af8425-d0b6-11e9-9ba3-f679ea11f394" in namespace "downward-api-6430" to be "success or failure"
Sep  6 14:57:56.245: INFO: Pod "downwardapi-volume-b5af8425-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 5.41363ms
Sep  6 14:57:58.248: INFO: Pod "downwardapi-volume-b5af8425-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008345949s
STEP: Saw pod success
Sep  6 14:57:58.248: INFO: Pod "downwardapi-volume-b5af8425-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:57:58.250: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-b5af8425-d0b6-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 14:57:58.260: INFO: Waiting for pod downwardapi-volume-b5af8425-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:57:58.262: INFO: Pod downwardapi-volume-b5af8425-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:57:58.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6430" for this suite.
Sep  6 14:58:04.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:58:04.336: INFO: namespace downward-api-6430 deletion completed in 6.072478844s

• [SLOW TEST:8.142 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:58:04.336: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ba87ddb5-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 14:58:04.369: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba884633-d0b6-11e9-9ba3-f679ea11f394" in namespace "configmap-7252" to be "success or failure"
Sep  6 14:58:04.376: INFO: Pod "pod-configmaps-ba884633-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 6.830796ms
Sep  6 14:58:06.378: INFO: Pod "pod-configmaps-ba884633-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009663355s
STEP: Saw pod success
Sep  6 14:58:06.378: INFO: Pod "pod-configmaps-ba884633-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:58:06.381: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-ba884633-d0b6-11e9-9ba3-f679ea11f394 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 14:58:06.393: INFO: Waiting for pod pod-configmaps-ba884633-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:58:06.397: INFO: Pod pod-configmaps-ba884633-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:58:06.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7252" for this suite.
Sep  6 14:58:12.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:58:12.486: INFO: namespace configmap-7252 deletion completed in 6.087034255s

• [SLOW TEST:8.150 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:58:12.486: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 14:58:12.516: INFO: Waiting up to 5m0s for pod "pod-bf63a888-d0b6-11e9-9ba3-f679ea11f394" in namespace "emptydir-7010" to be "success or failure"
Sep  6 14:58:12.519: INFO: Pod "pod-bf63a888-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.756581ms
Sep  6 14:58:14.529: INFO: Pod "pod-bf63a888-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012028078s
Sep  6 14:58:16.531: INFO: Pod "pod-bf63a888-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014551048s
STEP: Saw pod success
Sep  6 14:58:16.531: INFO: Pod "pod-bf63a888-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:58:16.533: INFO: Trying to get logs from node metalk8s-23 pod pod-bf63a888-d0b6-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 14:58:16.543: INFO: Waiting for pod pod-bf63a888-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:58:16.545: INFO: Pod pod-bf63a888-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:58:16.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7010" for this suite.
Sep  6 14:58:22.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:58:22.618: INFO: namespace emptydir-7010 deletion completed in 6.070870949s

• [SLOW TEST:10.132 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:58:22.619: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-c56d1b40-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 14:58:22.646: INFO: Waiting up to 5m0s for pod "pod-secrets-c56d7e3a-d0b6-11e9-9ba3-f679ea11f394" in namespace "secrets-2334" to be "success or failure"
Sep  6 14:58:22.654: INFO: Pod "pod-secrets-c56d7e3a-d0b6-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039068ms
Sep  6 14:58:24.657: INFO: Pod "pod-secrets-c56d7e3a-d0b6-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010629137s
STEP: Saw pod success
Sep  6 14:58:24.657: INFO: Pod "pod-secrets-c56d7e3a-d0b6-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 14:58:24.659: INFO: Trying to get logs from node metalk8s-23 pod pod-secrets-c56d7e3a-d0b6-11e9-9ba3-f679ea11f394 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 14:58:24.670: INFO: Waiting for pod pod-secrets-c56d7e3a-d0b6-11e9-9ba3-f679ea11f394 to disappear
Sep  6 14:58:24.673: INFO: Pod pod-secrets-c56d7e3a-d0b6-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 14:58:24.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2334" for this suite.
Sep  6 14:58:30.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 14:58:30.760: INFO: namespace secrets-2334 deletion completed in 6.085098094s

• [SLOW TEST:8.141 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 14:58:30.760: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-ca488743-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating secret with name s-test-opt-upd-ca4888c8-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ca488743-d0b6-11e9-9ba3-f679ea11f394
STEP: Updating secret s-test-opt-upd-ca4888c8-d0b6-11e9-9ba3-f679ea11f394
STEP: Creating secret with name s-test-opt-create-ca4888e9-d0b6-11e9-9ba3-f679ea11f394
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:00:03.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1325" for this suite.
Sep  6 15:00:25.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:00:25.212: INFO: namespace secrets-1325 deletion completed in 22.080581948s

• [SLOW TEST:114.452 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:00:25.213: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 15:00:25.240: INFO: Waiting up to 5m0s for pod "pod-0e7fb6b9-d0b7-11e9-9ba3-f679ea11f394" in namespace "emptydir-2762" to be "success or failure"
Sep  6 15:00:25.250: INFO: Pod "pod-0e7fb6b9-d0b7-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 10.260619ms
Sep  6 15:00:27.253: INFO: Pod "pod-0e7fb6b9-d0b7-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01266975s
STEP: Saw pod success
Sep  6 15:00:27.253: INFO: Pod "pod-0e7fb6b9-d0b7-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:00:27.255: INFO: Trying to get logs from node metalk8s-23 pod pod-0e7fb6b9-d0b7-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:00:27.268: INFO: Waiting for pod pod-0e7fb6b9-d0b7-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:00:27.270: INFO: Pod pod-0e7fb6b9-d0b7-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:00:27.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2762" for this suite.
Sep  6 15:00:33.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:00:33.354: INFO: namespace emptydir-2762 deletion completed in 6.081810055s

• [SLOW TEST:8.141 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:00:33.355: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  6 15:00:33.391: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3795,SelfLink:/api/v1/namespaces/watch-3795/configmaps/e2e-watch-test-label-changed,UID:135a0fc3-d0b7-11e9-b447-fa163e0a078f,ResourceVersion:5737,Generation:0,CreationTimestamp:2019-09-06 15:00:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 15:00:33.391: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3795,SelfLink:/api/v1/namespaces/watch-3795/configmaps/e2e-watch-test-label-changed,UID:135a0fc3-d0b7-11e9-b447-fa163e0a078f,ResourceVersion:5738,Generation:0,CreationTimestamp:2019-09-06 15:00:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  6 15:00:33.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3795,SelfLink:/api/v1/namespaces/watch-3795/configmaps/e2e-watch-test-label-changed,UID:135a0fc3-d0b7-11e9-b447-fa163e0a078f,ResourceVersion:5739,Generation:0,CreationTimestamp:2019-09-06 15:00:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  6 15:00:43.413: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3795,SelfLink:/api/v1/namespaces/watch-3795/configmaps/e2e-watch-test-label-changed,UID:135a0fc3-d0b7-11e9-b447-fa163e0a078f,ResourceVersion:5755,Generation:0,CreationTimestamp:2019-09-06 15:00:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 15:00:43.413: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3795,SelfLink:/api/v1/namespaces/watch-3795/configmaps/e2e-watch-test-label-changed,UID:135a0fc3-d0b7-11e9-b447-fa163e0a078f,ResourceVersion:5756,Generation:0,CreationTimestamp:2019-09-06 15:00:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep  6 15:00:43.413: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3795,SelfLink:/api/v1/namespaces/watch-3795/configmaps/e2e-watch-test-label-changed,UID:135a0fc3-d0b7-11e9-b447-fa163e0a078f,ResourceVersion:5757,Generation:0,CreationTimestamp:2019-09-06 15:00:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:00:43.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3795" for this suite.
Sep  6 15:00:49.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:00:49.489: INFO: namespace watch-3795 deletion completed in 6.07303342s

• [SLOW TEST:16.135 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:00:49.490: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  6 15:00:52.033: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5066 pod-service-account-1d45fe75-d0b7-11e9-9ba3-f679ea11f394 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  6 15:00:52.240: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5066 pod-service-account-1d45fe75-d0b7-11e9-9ba3-f679ea11f394 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  6 15:00:52.433: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5066 pod-service-account-1d45fe75-d0b7-11e9-9ba3-f679ea11f394 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:00:52.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5066" for this suite.
Sep  6 15:00:58.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:00:58.719: INFO: namespace svcaccounts-5066 deletion completed in 6.079135658s

• [SLOW TEST:9.229 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:00:58.719: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4826
Sep  6 15:01:00.766: INFO: Started pod liveness-http in namespace container-probe-4826
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 15:01:00.771: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:05:01.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4826" for this suite.
Sep  6 15:05:07.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:05:07.199: INFO: namespace container-probe-4826 deletion completed in 6.077100918s

• [SLOW TEST:248.481 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:05:07.200: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6086/configmap-test-b6933a9a-d0b7-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:05:07.232: INFO: Waiting up to 5m0s for pod "pod-configmaps-b693953d-d0b7-11e9-9ba3-f679ea11f394" in namespace "configmap-6086" to be "success or failure"
Sep  6 15:05:07.239: INFO: Pod "pod-configmaps-b693953d-d0b7-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 6.841688ms
Sep  6 15:05:09.242: INFO: Pod "pod-configmaps-b693953d-d0b7-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009750318s
STEP: Saw pod success
Sep  6 15:05:09.242: INFO: Pod "pod-configmaps-b693953d-d0b7-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:05:09.244: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-b693953d-d0b7-11e9-9ba3-f679ea11f394 container env-test: <nil>
STEP: delete the pod
Sep  6 15:05:09.254: INFO: Waiting for pod pod-configmaps-b693953d-d0b7-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:05:09.256: INFO: Pod pod-configmaps-b693953d-d0b7-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:05:09.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6086" for this suite.
Sep  6 15:05:15.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:05:15.358: INFO: namespace configmap-6086 deletion completed in 6.095760885s

• [SLOW TEST:8.159 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:05:15.359: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-bb731191-d0b7-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:05:15.415: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bb73d7eb-d0b7-11e9-9ba3-f679ea11f394" in namespace "projected-3632" to be "success or failure"
Sep  6 15:05:15.422: INFO: Pod "pod-projected-secrets-bb73d7eb-d0b7-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 6.416253ms
Sep  6 15:05:18.061: INFO: Pod "pod-projected-secrets-bb73d7eb-d0b7-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.645413334s
STEP: Saw pod success
Sep  6 15:05:18.061: INFO: Pod "pod-projected-secrets-bb73d7eb-d0b7-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:05:18.063: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-secrets-bb73d7eb-d0b7-11e9-9ba3-f679ea11f394 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:05:18.095: INFO: Waiting for pod pod-projected-secrets-bb73d7eb-d0b7-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:05:18.100: INFO: Pod pod-projected-secrets-bb73d7eb-d0b7-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:05:18.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3632" for this suite.
Sep  6 15:05:24.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:05:24.191: INFO: namespace projected-3632 deletion completed in 6.088804604s

• [SLOW TEST:8.832 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:05:24.191: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:05:48.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3152" for this suite.
Sep  6 15:05:54.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:05:54.359: INFO: namespace namespaces-3152 deletion completed in 6.072065816s
STEP: Destroying namespace "nsdeletetest-1919" for this suite.
Sep  6 15:05:54.361: INFO: Namespace nsdeletetest-1919 was already deleted
STEP: Destroying namespace "nsdeletetest-4660" for this suite.
Sep  6 15:06:00.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:06:00.432: INFO: namespace nsdeletetest-4660 deletion completed in 6.070627763s

• [SLOW TEST:36.240 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:06:00.432: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 15:06:00.465: INFO: Waiting up to 5m0s for pod "pod-d64e9bf4-d0b7-11e9-9ba3-f679ea11f394" in namespace "emptydir-4396" to be "success or failure"
Sep  6 15:06:00.468: INFO: Pod "pod-d64e9bf4-d0b7-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362742ms
Sep  6 15:06:02.471: INFO: Pod "pod-d64e9bf4-d0b7-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005278697s
STEP: Saw pod success
Sep  6 15:06:02.471: INFO: Pod "pod-d64e9bf4-d0b7-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:06:02.473: INFO: Trying to get logs from node metalk8s-23 pod pod-d64e9bf4-d0b7-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:06:02.485: INFO: Waiting for pod pod-d64e9bf4-d0b7-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:06:02.487: INFO: Pod pod-d64e9bf4-d0b7-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:06:02.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4396" for this suite.
Sep  6 15:06:08.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:06:08.557: INFO: namespace emptydir-4396 deletion completed in 6.066831572s

• [SLOW TEST:8.125 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:06:08.558: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:06:14.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1188" for this suite.
Sep  6 15:06:20.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:06:20.816: INFO: namespace namespaces-1188 deletion completed in 6.078769358s
STEP: Destroying namespace "nsdeletetest-7325" for this suite.
Sep  6 15:06:20.818: INFO: Namespace nsdeletetest-7325 was already deleted
STEP: Destroying namespace "nsdeletetest-281" for this suite.
Sep  6 15:06:26.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:06:26.884: INFO: namespace nsdeletetest-281 deletion completed in 6.0659761s

• [SLOW TEST:18.326 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:06:26.884: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  6 15:06:26.919: INFO: Waiting up to 5m0s for pod "pod-e61319cc-d0b7-11e9-9ba3-f679ea11f394" in namespace "emptydir-5471" to be "success or failure"
Sep  6 15:06:26.926: INFO: Pod "pod-e61319cc-d0b7-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 6.952398ms
Sep  6 15:06:28.929: INFO: Pod "pod-e61319cc-d0b7-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009521891s
STEP: Saw pod success
Sep  6 15:06:28.929: INFO: Pod "pod-e61319cc-d0b7-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:06:28.931: INFO: Trying to get logs from node metalk8s-23 pod pod-e61319cc-d0b7-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:06:28.943: INFO: Waiting for pod pod-e61319cc-d0b7-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:06:28.946: INFO: Pod pod-e61319cc-d0b7-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:06:28.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5471" for this suite.
Sep  6 15:06:34.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:06:35.025: INFO: namespace emptydir-5471 deletion completed in 6.076456229s

• [SLOW TEST:8.141 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:06:35.025: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0906 15:06:45.150138      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 15:06:45.150: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:06:45.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2349" for this suite.
Sep  6 15:06:51.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:06:51.246: INFO: namespace gc-2349 deletion completed in 6.091342936s

• [SLOW TEST:16.220 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:06:51.246: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:06:51.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9609" for this suite.
Sep  6 15:06:57.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:06:57.392: INFO: namespace kubelet-test-9609 deletion completed in 6.064111661s

• [SLOW TEST:6.146 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:06:57.394: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 15:07:01.448: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 15:07:01.450: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 15:07:03.450: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 15:07:03.453: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 15:07:05.450: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 15:07:05.453: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:07:05.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8717" for this suite.
Sep  6 15:07:27.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:07:27.536: INFO: namespace container-lifecycle-hook-8717 deletion completed in 22.075475674s

• [SLOW TEST:30.142 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:07:27.536: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:07:27.566: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a39458b-d0b8-11e9-9ba3-f679ea11f394" in namespace "projected-4684" to be "success or failure"
Sep  6 15:07:27.572: INFO: Pod "downwardapi-volume-0a39458b-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 5.937362ms
Sep  6 15:07:29.575: INFO: Pod "downwardapi-volume-0a39458b-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009004122s
STEP: Saw pod success
Sep  6 15:07:29.575: INFO: Pod "downwardapi-volume-0a39458b-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:07:29.577: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-0a39458b-d0b8-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:07:29.591: INFO: Waiting for pod downwardapi-volume-0a39458b-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:07:29.593: INFO: Pod downwardapi-volume-0a39458b-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:07:29.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4684" for this suite.
Sep  6 15:07:35.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:07:35.662: INFO: namespace projected-4684 deletion completed in 6.06700642s

• [SLOW TEST:8.126 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:07:35.662: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 15:07:35.686: INFO: Waiting up to 5m0s for pod "pod-0f10c40e-d0b8-11e9-9ba3-f679ea11f394" in namespace "emptydir-9255" to be "success or failure"
Sep  6 15:07:35.691: INFO: Pod "pod-0f10c40e-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 5.128434ms
Sep  6 15:07:37.694: INFO: Pod "pod-0f10c40e-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007574777s
STEP: Saw pod success
Sep  6 15:07:37.694: INFO: Pod "pod-0f10c40e-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:07:37.696: INFO: Trying to get logs from node metalk8s-23 pod pod-0f10c40e-d0b8-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:07:37.712: INFO: Waiting for pod pod-0f10c40e-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:07:37.716: INFO: Pod pod-0f10c40e-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:07:37.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9255" for this suite.
Sep  6 15:07:43.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:07:43.788: INFO: namespace emptydir-9255 deletion completed in 6.06950197s

• [SLOW TEST:8.126 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:07:43.789: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Sep  6 15:07:43.817: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5657" to be "success or failure"
Sep  6 15:07:43.821: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22439ms
Sep  6 15:07:45.824: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006998679s
STEP: Saw pod success
Sep  6 15:07:45.824: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep  6 15:07:45.826: INFO: Trying to get logs from node metalk8s-23 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep  6 15:07:45.839: INFO: Waiting for pod pod-host-path-test to disappear
Sep  6 15:07:45.840: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:07:45.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5657" for this suite.
Sep  6 15:07:51.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:07:51.920: INFO: namespace hostpath-5657 deletion completed in 6.078098655s

• [SLOW TEST:8.131 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:07:51.921: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-18c2382d-d0b8-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:07:51.951: INFO: Waiting up to 5m0s for pod "pod-secrets-18c29ccf-d0b8-11e9-9ba3-f679ea11f394" in namespace "secrets-3841" to be "success or failure"
Sep  6 15:07:51.953: INFO: Pod "pod-secrets-18c29ccf-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19615ms
Sep  6 15:07:53.956: INFO: Pod "pod-secrets-18c29ccf-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004877427s
STEP: Saw pod success
Sep  6 15:07:53.956: INFO: Pod "pod-secrets-18c29ccf-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:07:53.958: INFO: Trying to get logs from node metalk8s-23 pod pod-secrets-18c29ccf-d0b8-11e9-9ba3-f679ea11f394 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:07:53.971: INFO: Waiting for pod pod-secrets-18c29ccf-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:07:53.974: INFO: Pod pod-secrets-18c29ccf-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:07:53.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3841" for this suite.
Sep  6 15:07:59.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:08:00.055: INFO: namespace secrets-3841 deletion completed in 6.078596814s

• [SLOW TEST:8.135 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:08:00.056: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-1d9b788e-d0b8-11e9-9ba3-f679ea11f394
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:08:00.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7591" for this suite.
Sep  6 15:08:06.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:08:06.158: INFO: namespace configmap-7591 deletion completed in 6.071559114s

• [SLOW TEST:6.103 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:08:06.159: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Sep  6 15:08:06.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 cluster-info'
Sep  6 15:08:06.334: INFO: stderr: ""
Sep  6 15:08:06.334: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:08:06.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7131" for this suite.
Sep  6 15:08:12.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:08:12.418: INFO: namespace kubectl-7131 deletion completed in 6.081442778s

• [SLOW TEST:6.259 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:08:12.418: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Sep  6 15:08:12.441: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-262345838 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:08:12.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-270" for this suite.
Sep  6 15:08:18.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:08:18.580: INFO: namespace kubectl-270 deletion completed in 6.072710283s

• [SLOW TEST:6.162 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:08:18.580: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  6 15:08:18.605: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:08:22.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2637" for this suite.
Sep  6 15:08:44.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:08:44.586: INFO: namespace init-container-2637 deletion completed in 22.178227377s

• [SLOW TEST:26.006 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:08:44.587: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-3829d55b-d0b8-11e9-9ba3-f679ea11f394
STEP: Creating secret with name secret-projected-all-test-volume-3829d541-d0b8-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  6 15:08:44.698: INFO: Waiting up to 5m0s for pod "projected-volume-3829d4f3-d0b8-11e9-9ba3-f679ea11f394" in namespace "projected-6355" to be "success or failure"
Sep  6 15:08:44.716: INFO: Pod "projected-volume-3829d4f3-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 18.087884ms
Sep  6 15:08:46.725: INFO: Pod "projected-volume-3829d4f3-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026793011s
STEP: Saw pod success
Sep  6 15:08:46.725: INFO: Pod "projected-volume-3829d4f3-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:08:46.727: INFO: Trying to get logs from node metalk8s-23 pod projected-volume-3829d4f3-d0b8-11e9-9ba3-f679ea11f394 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  6 15:08:46.745: INFO: Waiting for pod projected-volume-3829d4f3-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:08:46.748: INFO: Pod projected-volume-3829d4f3-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:08:46.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6355" for this suite.
Sep  6 15:08:52.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:08:52.832: INFO: namespace projected-6355 deletion completed in 6.081998877s

• [SLOW TEST:8.246 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:08:52.832: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-3d102e2e-d0b8-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:08:52.861: INFO: Waiting up to 5m0s for pod "pod-configmaps-3d108a73-d0b8-11e9-9ba3-f679ea11f394" in namespace "configmap-3956" to be "success or failure"
Sep  6 15:08:52.863: INFO: Pod "pod-configmaps-3d108a73-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.743351ms
Sep  6 15:08:54.866: INFO: Pod "pod-configmaps-3d108a73-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004839497s
STEP: Saw pod success
Sep  6 15:08:54.866: INFO: Pod "pod-configmaps-3d108a73-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:08:54.868: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-3d108a73-d0b8-11e9-9ba3-f679ea11f394 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:08:54.883: INFO: Waiting for pod pod-configmaps-3d108a73-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:08:54.885: INFO: Pod pod-configmaps-3d108a73-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:08:54.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3956" for this suite.
Sep  6 15:09:00.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:09:00.964: INFO: namespace configmap-3956 deletion completed in 6.076693754s

• [SLOW TEST:8.132 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:09:00.965: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:09:00.994: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  6 15:09:05.997: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 15:09:05.997: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 15:09:06.011: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-9869,SelfLink:/apis/apps/v1/namespaces/deployment-9869/deployments/test-cleanup-deployment,UID:44e659cf-d0b8-11e9-b447-fa163e0a078f,ResourceVersion:7390,Generation:1,CreationTimestamp:2019-09-06 15:09:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 15:09:06.015: INFO: New ReplicaSet "test-cleanup-deployment-6865c98b76" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76,GenerateName:,Namespace:deployment-9869,SelfLink:/apis/apps/v1/namespaces/deployment-9869/replicasets/test-cleanup-deployment-6865c98b76,UID:44e7e61d-d0b8-11e9-b447-fa163e0a078f,ResourceVersion:7392,Generation:1,CreationTimestamp:2019-09-06 15:09:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 44e659cf-d0b8-11e9-b447-fa163e0a078f 0xc0027e5157 0xc0027e5158}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 15:09:06.015: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  6 15:09:06.015: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-9869,SelfLink:/apis/apps/v1/namespaces/deployment-9869/replicasets/test-cleanup-controller,UID:41e9b454-d0b8-11e9-b447-fa163e0a078f,ResourceVersion:7391,Generation:1,CreationTimestamp:2019-09-06 15:09:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 44e659cf-d0b8-11e9-b447-fa163e0a078f 0xc0027e507f 0xc0027e5090}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 15:09:06.020: INFO: Pod "test-cleanup-controller-2q65v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-2q65v,GenerateName:test-cleanup-controller-,Namespace:deployment-9869,SelfLink:/api/v1/namespaces/deployment-9869/pods/test-cleanup-controller-2q65v,UID:41ea74d8-d0b8-11e9-b447-fa163e0a078f,ResourceVersion:7381,Generation:0,CreationTimestamp:2019-09-06 15:09:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.233/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 41e9b454-d0b8-11e9-b447-fa163e0a078f 0xc0026f01df 0xc0026f01f0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bzs9k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bzs9k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bzs9k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026f0260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026f0280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:09:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:09:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:09:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:09:01 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.233,StartTime:2019-09-06 15:09:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 15:09:01 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://b843d4be37342c80d92b91cde975e03c144cd247e28d04b60d21e3364a6c87c5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 15:09:06.020: INFO: Pod "test-cleanup-deployment-6865c98b76-6mwpm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76-6mwpm,GenerateName:test-cleanup-deployment-6865c98b76-,Namespace:deployment-9869,SelfLink:/api/v1/namespaces/deployment-9869/pods/test-cleanup-deployment-6865c98b76-6mwpm,UID:44e846b3-d0b8-11e9-b447-fa163e0a078f,ResourceVersion:7393,Generation:0,CreationTimestamp:2019-09-06 15:09:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-6865c98b76 44e7e61d-d0b8-11e9-b447-fa163e0a078f 0xc0026f0357 0xc0026f0358}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bzs9k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bzs9k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bzs9k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026f03c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026f03e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:09:06.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9869" for this suite.
Sep  6 15:09:12.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:09:12.111: INFO: namespace deployment-9869 deletion completed in 6.0850198s

• [SLOW TEST:11.146 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:09:12.111: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:09:12.154: INFO: Waiting up to 5m0s for pod "downwardapi-volume-488fec0a-d0b8-11e9-9ba3-f679ea11f394" in namespace "projected-4833" to be "success or failure"
Sep  6 15:09:12.171: INFO: Pod "downwardapi-volume-488fec0a-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 16.924619ms
Sep  6 15:09:14.175: INFO: Pod "downwardapi-volume-488fec0a-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020142309s
STEP: Saw pod success
Sep  6 15:09:14.175: INFO: Pod "downwardapi-volume-488fec0a-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:09:14.177: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-488fec0a-d0b8-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:09:14.188: INFO: Waiting for pod downwardapi-volume-488fec0a-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:09:14.192: INFO: Pod downwardapi-volume-488fec0a-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:09:14.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4833" for this suite.
Sep  6 15:09:20.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:09:20.271: INFO: namespace projected-4833 deletion completed in 6.07745372s

• [SLOW TEST:8.161 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:09:20.272: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  6 15:09:22.826: INFO: Successfully updated pod "labelsupdate4d6b8c89-d0b8-11e9-9ba3-f679ea11f394"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:09:24.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2046" for this suite.
Sep  6 15:09:46.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:09:46.914: INFO: namespace downward-api-2046 deletion completed in 22.073598065s

• [SLOW TEST:26.643 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:09:46.915: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:09:50.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3245" for this suite.
Sep  6 15:09:56.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:09:57.021: INFO: namespace kubelet-test-3245 deletion completed in 6.071551963s

• [SLOW TEST:10.107 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:09:57.021: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Sep  6 15:09:57.044: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep  6 15:09:57.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-7134'
Sep  6 15:09:57.226: INFO: stderr: ""
Sep  6 15:09:57.226: INFO: stdout: "service/redis-slave created\n"
Sep  6 15:09:57.226: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep  6 15:09:57.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-7134'
Sep  6 15:09:57.367: INFO: stderr: ""
Sep  6 15:09:57.367: INFO: stdout: "service/redis-master created\n"
Sep  6 15:09:57.367: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  6 15:09:57.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-7134'
Sep  6 15:09:57.554: INFO: stderr: ""
Sep  6 15:09:57.554: INFO: stdout: "service/frontend created\n"
Sep  6 15:09:57.554: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep  6 15:09:57.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-7134'
Sep  6 15:09:57.688: INFO: stderr: ""
Sep  6 15:09:57.688: INFO: stdout: "deployment.apps/frontend created\n"
Sep  6 15:09:57.688: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  6 15:09:57.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-7134'
Sep  6 15:09:57.859: INFO: stderr: ""
Sep  6 15:09:57.859: INFO: stdout: "deployment.apps/redis-master created\n"
Sep  6 15:09:57.860: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep  6 15:09:57.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-7134'
Sep  6 15:09:58.014: INFO: stderr: ""
Sep  6 15:09:58.014: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep  6 15:09:58.014: INFO: Waiting for all frontend pods to be Running.
Sep  6 15:10:13.068: INFO: Waiting for frontend to serve content.
Sep  6 15:10:16.088: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'No route to host [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('No route to hos...', 113)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\StreamCo in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Sep  6 15:10:21.103: INFO: Trying to add a new entry to the guestbook.
Sep  6 15:10:21.116: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep  6 15:10:21.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-7134'
Sep  6 15:10:21.259: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 15:10:21.259: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 15:10:21.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-7134'
Sep  6 15:10:21.378: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 15:10:21.379: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 15:10:21.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-7134'
Sep  6 15:10:21.480: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 15:10:21.480: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 15:10:21.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-7134'
Sep  6 15:10:21.564: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 15:10:21.564: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 15:10:21.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-7134'
Sep  6 15:10:21.645: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 15:10:21.645: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 15:10:21.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-7134'
Sep  6 15:10:21.710: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 15:10:21.710: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:10:21.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7134" for this suite.
Sep  6 15:10:59.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:10:59.778: INFO: namespace kubectl-7134 deletion completed in 38.066098824s

• [SLOW TEST:62.757 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:10:59.779: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 15:10:59.855: INFO: Waiting up to 5m0s for pod "pod-88c25483-d0b8-11e9-9ba3-f679ea11f394" in namespace "emptydir-9508" to be "success or failure"
Sep  6 15:10:59.859: INFO: Pod "pod-88c25483-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189664ms
Sep  6 15:11:01.862: INFO: Pod "pod-88c25483-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007286087s
STEP: Saw pod success
Sep  6 15:11:01.862: INFO: Pod "pod-88c25483-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:11:01.866: INFO: Trying to get logs from node metalk8s-23 pod pod-88c25483-d0b8-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:11:01.934: INFO: Waiting for pod pod-88c25483-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:11:01.944: INFO: Pod pod-88c25483-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:11:01.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9508" for this suite.
Sep  6 15:11:07.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:11:08.056: INFO: namespace emptydir-9508 deletion completed in 6.104985306s

• [SLOW TEST:8.277 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:11:08.056: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1649
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 15:11:08.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2554'
Sep  6 15:11:08.188: INFO: stderr: ""
Sep  6 15:11:08.188: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1654
Sep  6 15:11:08.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete pods e2e-test-nginx-pod --namespace=kubectl-2554'
Sep  6 15:11:18.605: INFO: stderr: ""
Sep  6 15:11:18.605: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:11:18.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2554" for this suite.
Sep  6 15:11:24.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:11:24.677: INFO: namespace kubectl-2554 deletion completed in 6.066592279s

• [SLOW TEST:16.621 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:11:24.677: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:11:24.703: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9791d344-d0b8-11e9-9ba3-f679ea11f394" in namespace "downward-api-49" to be "success or failure"
Sep  6 15:11:24.707: INFO: Pod "downwardapi-volume-9791d344-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 3.962734ms
Sep  6 15:11:26.712: INFO: Pod "downwardapi-volume-9791d344-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009442183s
STEP: Saw pod success
Sep  6 15:11:26.712: INFO: Pod "downwardapi-volume-9791d344-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:11:26.714: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-9791d344-d0b8-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:11:26.726: INFO: Waiting for pod downwardapi-volume-9791d344-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:11:26.727: INFO: Pod downwardapi-volume-9791d344-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:11:26.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-49" for this suite.
Sep  6 15:11:32.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:11:32.814: INFO: namespace downward-api-49 deletion completed in 6.08389975s

• [SLOW TEST:8.137 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:11:32.814: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-9c6b47b3-d0b8-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:11:32.843: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c6bb124-d0b8-11e9-9ba3-f679ea11f394" in namespace "projected-206" to be "success or failure"
Sep  6 15:11:32.845: INFO: Pod "pod-projected-secrets-9c6bb124-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.877366ms
Sep  6 15:11:34.848: INFO: Pod "pod-projected-secrets-9c6bb124-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004948859s
STEP: Saw pod success
Sep  6 15:11:34.848: INFO: Pod "pod-projected-secrets-9c6bb124-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:11:34.850: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-secrets-9c6bb124-d0b8-11e9-9ba3-f679ea11f394 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:11:34.868: INFO: Waiting for pod pod-projected-secrets-9c6bb124-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:11:34.870: INFO: Pod pod-projected-secrets-9c6bb124-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:11:34.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-206" for this suite.
Sep  6 15:11:40.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:11:40.945: INFO: namespace projected-206 deletion completed in 6.069855541s

• [SLOW TEST:8.130 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:11:40.945: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-a144213c-d0b8-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:11:40.974: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a1448985-d0b8-11e9-9ba3-f679ea11f394" in namespace "projected-2235" to be "success or failure"
Sep  6 15:11:40.979: INFO: Pod "pod-projected-secrets-a1448985-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.956709ms
Sep  6 15:11:42.982: INFO: Pod "pod-projected-secrets-a1448985-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007670487s
STEP: Saw pod success
Sep  6 15:11:42.982: INFO: Pod "pod-projected-secrets-a1448985-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:11:42.984: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-secrets-a1448985-d0b8-11e9-9ba3-f679ea11f394 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:11:42.997: INFO: Waiting for pod pod-projected-secrets-a1448985-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:11:43.000: INFO: Pod pod-projected-secrets-a1448985-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:11:43.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2235" for this suite.
Sep  6 15:11:49.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:11:49.068: INFO: namespace projected-2235 deletion completed in 6.066187664s

• [SLOW TEST:8.123 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:11:49.069: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4272.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4272.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 15:11:51.128: INFO: DNS probes using dns-4272/dns-test-a61c6153-d0b8-11e9-9ba3-f679ea11f394 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:11:51.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4272" for this suite.
Sep  6 15:11:57.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:11:57.220: INFO: namespace dns-4272 deletion completed in 6.080300273s

• [SLOW TEST:8.152 seconds]
[sig-network] DNS
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:11:57.221: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-7820
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7820 to expose endpoints map[]
Sep  6 15:11:57.253: INFO: successfully validated that service multi-endpoint-test in namespace services-7820 exposes endpoints map[] (2.917926ms elapsed)
STEP: Creating pod pod1 in namespace services-7820
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7820 to expose endpoints map[pod1:[100]]
Sep  6 15:11:59.281: INFO: successfully validated that service multi-endpoint-test in namespace services-7820 exposes endpoints map[pod1:[100]] (2.01968644s elapsed)
STEP: Creating pod pod2 in namespace services-7820
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7820 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  6 15:12:01.301: INFO: successfully validated that service multi-endpoint-test in namespace services-7820 exposes endpoints map[pod1:[100] pod2:[101]] (2.017321765s elapsed)
STEP: Deleting pod pod1 in namespace services-7820
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7820 to expose endpoints map[pod2:[101]]
Sep  6 15:12:01.313: INFO: successfully validated that service multi-endpoint-test in namespace services-7820 exposes endpoints map[pod2:[101]] (7.194243ms elapsed)
STEP: Deleting pod pod2 in namespace services-7820
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7820 to expose endpoints map[]
Sep  6 15:12:02.322: INFO: successfully validated that service multi-endpoint-test in namespace services-7820 exposes endpoints map[] (1.005137866s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:12:02.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7820" for this suite.
Sep  6 15:12:24.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:12:24.401: INFO: namespace services-7820 deletion completed in 22.065880725s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:27.180 seconds]
[sig-network] Services
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:12:24.402: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-1093
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1093
STEP: Deleting pre-stop pod
Sep  6 15:12:35.460: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:12:35.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1093" for this suite.
Sep  6 15:13:13.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:13:13.536: INFO: namespace prestop-1093 deletion completed in 38.070194521s

• [SLOW TEST:49.135 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:13:13.537: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  6 15:13:17.572: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-d8745d01-d0b8-11e9-9ba3-f679ea11f394,GenerateName:,Namespace:events-7904,SelfLink:/api/v1/namespaces/events-7904/pods/send-events-d8745d01-d0b8-11e9-9ba3-f679ea11f394,UID:d874b032-d0b8-11e9-b447-fa163e0a078f,ResourceVersion:8445,Generation:0,CreationTimestamp:2019-09-06 15:13:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 557534089,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.211/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-szhrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-szhrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-szhrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b88c80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b88ca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:13:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:13:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:13:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:13:13 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.211,StartTime:2019-09-06 15:13:13 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-06 15:13:15 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://ac1e085c6e01a947abf2934d59295afee441610776a5c81cef132257c8c47f8f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep  6 15:13:19.576: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  6 15:13:21.578: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:13:21.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7904" for this suite.
Sep  6 15:13:59.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:13:59.676: INFO: namespace events-7904 deletion completed in 38.084671147s

• [SLOW TEST:46.139 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:13:59.676: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Sep  6 15:13:59.702: INFO: Waiting up to 5m0s for pod "var-expansion-f3f4f47d-d0b8-11e9-9ba3-f679ea11f394" in namespace "var-expansion-424" to be "success or failure"
Sep  6 15:13:59.704: INFO: Pod "var-expansion-f3f4f47d-d0b8-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.769566ms
Sep  6 15:14:01.707: INFO: Pod "var-expansion-f3f4f47d-d0b8-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004702314s
STEP: Saw pod success
Sep  6 15:14:01.707: INFO: Pod "var-expansion-f3f4f47d-d0b8-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:14:01.709: INFO: Trying to get logs from node metalk8s-23 pod var-expansion-f3f4f47d-d0b8-11e9-9ba3-f679ea11f394 container dapi-container: <nil>
STEP: delete the pod
Sep  6 15:14:01.729: INFO: Waiting for pod var-expansion-f3f4f47d-d0b8-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:14:01.732: INFO: Pod var-expansion-f3f4f47d-d0b8-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:14:01.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-424" for this suite.
Sep  6 15:14:07.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:14:07.804: INFO: namespace var-expansion-424 deletion completed in 6.068414283s

• [SLOW TEST:8.128 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:14:07.804: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-876
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-876 to expose endpoints map[]
Sep  6 15:14:07.841: INFO: Get endpoints failed (1.865878ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep  6 15:14:08.843: INFO: successfully validated that service endpoint-test2 in namespace services-876 exposes endpoints map[] (1.004279305s elapsed)
STEP: Creating pod pod1 in namespace services-876
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-876 to expose endpoints map[pod1:[80]]
Sep  6 15:14:09.856: INFO: successfully validated that service endpoint-test2 in namespace services-876 exposes endpoints map[pod1:[80]] (1.00813309s elapsed)
STEP: Creating pod pod2 in namespace services-876
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-876 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  6 15:14:10.872: INFO: successfully validated that service endpoint-test2 in namespace services-876 exposes endpoints map[pod1:[80] pod2:[80]] (1.014134415s elapsed)
STEP: Deleting pod pod1 in namespace services-876
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-876 to expose endpoints map[pod2:[80]]
Sep  6 15:14:11.887: INFO: successfully validated that service endpoint-test2 in namespace services-876 exposes endpoints map[pod2:[80]] (1.008926306s elapsed)
STEP: Deleting pod pod2 in namespace services-876
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-876 to expose endpoints map[]
Sep  6 15:14:11.897: INFO: successfully validated that service endpoint-test2 in namespace services-876 exposes endpoints map[] (1.917323ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:14:11.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-876" for this suite.
Sep  6 15:14:33.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:14:34.007: INFO: namespace services-876 deletion completed in 22.078769659s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:26.203 seconds]
[sig-network] Services
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:14:34.007: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 15:14:36.560: INFO: Successfully updated pod "pod-update-activedeadlineseconds-086c4224-d0b9-11e9-9ba3-f679ea11f394"
Sep  6 15:14:36.560: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-086c4224-d0b9-11e9-9ba3-f679ea11f394" in namespace "pods-4933" to be "terminated due to deadline exceeded"
Sep  6 15:14:36.562: INFO: Pod "pod-update-activedeadlineseconds-086c4224-d0b9-11e9-9ba3-f679ea11f394": Phase="Running", Reason="", readiness=true. Elapsed: 2.004459ms
Sep  6 15:14:38.565: INFO: Pod "pod-update-activedeadlineseconds-086c4224-d0b9-11e9-9ba3-f679ea11f394": Phase="Running", Reason="", readiness=true. Elapsed: 2.004960291s
Sep  6 15:14:40.568: INFO: Pod "pod-update-activedeadlineseconds-086c4224-d0b9-11e9-9ba3-f679ea11f394": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007791314s
Sep  6 15:14:40.568: INFO: Pod "pod-update-activedeadlineseconds-086c4224-d0b9-11e9-9ba3-f679ea11f394" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:14:40.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4933" for this suite.
Sep  6 15:14:46.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:14:46.651: INFO: namespace pods-4933 deletion completed in 6.080490387s

• [SLOW TEST:12.644 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:14:46.652: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4698.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4698.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4698.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4698.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4698.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4698.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 15:14:48.721: INFO: DNS probes using dns-4698/dns-test-0ff562ba-d0b9-11e9-9ba3-f679ea11f394 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:14:48.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4698" for this suite.
Sep  6 15:14:54.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:14:54.811: INFO: namespace dns-4698 deletion completed in 6.077371604s

• [SLOW TEST:8.159 seconds]
[sig-network] DNS
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:14:54.811: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-14d21029-d0b9-11e9-9ba3-f679ea11f394
STEP: Creating secret with name s-test-opt-upd-14d21082-d0b9-11e9-9ba3-f679ea11f394
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-14d21029-d0b9-11e9-9ba3-f679ea11f394
STEP: Updating secret s-test-opt-upd-14d21082-d0b9-11e9-9ba3-f679ea11f394
STEP: Creating secret with name s-test-opt-create-14d21096-d0b9-11e9-9ba3-f679ea11f394
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:14:58.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-462" for this suite.
Sep  6 15:15:20.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:15:20.961: INFO: namespace projected-462 deletion completed in 22.073631842s

• [SLOW TEST:26.150 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:15:20.961: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 15:15:20.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7330'
Sep  6 15:15:21.090: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 15:15:21.090: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Sep  6 15:15:21.100: INFO: scanned /root for discovery docs: <nil>
Sep  6 15:15:21.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7330'
Sep  6 15:15:36.849: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  6 15:15:36.849: INFO: stdout: "Created e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412\nScaling up e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep  6 15:15:36.849: INFO: stdout: "Created e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412\nScaling up e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep  6 15:15:36.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7330'
Sep  6 15:15:36.918: INFO: stderr: ""
Sep  6 15:15:36.918: INFO: stdout: "e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412-v7l92 e2e-test-nginx-rc-jkp52 "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Sep  6 15:15:41.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7330'
Sep  6 15:15:41.985: INFO: stderr: ""
Sep  6 15:15:41.985: INFO: stdout: "e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412-v7l92 "
Sep  6 15:15:41.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412-v7l92 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7330'
Sep  6 15:15:42.055: INFO: stderr: ""
Sep  6 15:15:42.055: INFO: stdout: "true"
Sep  6 15:15:42.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412-v7l92 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7330'
Sep  6 15:15:42.138: INFO: stderr: ""
Sep  6 15:15:42.138: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep  6 15:15:42.138: INFO: e2e-test-nginx-rc-66ce39c20b9b45001ea67608f5427412-v7l92 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1486
Sep  6 15:15:42.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete rc e2e-test-nginx-rc --namespace=kubectl-7330'
Sep  6 15:15:42.229: INFO: stderr: ""
Sep  6 15:15:42.229: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:15:42.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7330" for this suite.
Sep  6 15:16:04.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:16:04.300: INFO: namespace kubectl-7330 deletion completed in 22.067831611s

• [SLOW TEST:43.339 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:16:04.301: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:16:04.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 version'
Sep  6 15:16:04.386: INFO: stderr: ""
Sep  6 15:16:04.386: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.6\", GitCommit:\"96fac5cd13a5dc064f7d9f4f23030a6aeface6cc\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:49Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.6\", GitCommit:\"96fac5cd13a5dc064f7d9f4f23030a6aeface6cc\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:16Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:16:04.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1599" for this suite.
Sep  6 15:16:10.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:16:10.460: INFO: namespace kubectl-1599 deletion completed in 6.07157591s

• [SLOW TEST:6.159 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:16:10.460: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0906 15:16:11.038298      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 15:16:11.038: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:16:11.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-376" for this suite.
Sep  6 15:16:17.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:16:17.098: INFO: namespace gc-376 deletion completed in 6.058397066s

• [SLOW TEST:6.638 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:16:17.098: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep  6 15:16:17.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-2043'
Sep  6 15:16:17.301: INFO: stderr: ""
Sep  6 15:16:17.301: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 15:16:17.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2043'
Sep  6 15:16:17.398: INFO: stderr: ""
Sep  6 15:16:17.398: INFO: stdout: "update-demo-nautilus-5d927 update-demo-nautilus-bskkm "
Sep  6 15:16:17.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5d927 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:17.465: INFO: stderr: ""
Sep  6 15:16:17.465: INFO: stdout: ""
Sep  6 15:16:17.465: INFO: update-demo-nautilus-5d927 is created but not running
Sep  6 15:16:22.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2043'
Sep  6 15:16:22.545: INFO: stderr: ""
Sep  6 15:16:22.545: INFO: stdout: "update-demo-nautilus-5d927 update-demo-nautilus-bskkm "
Sep  6 15:16:22.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5d927 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:22.629: INFO: stderr: ""
Sep  6 15:16:22.629: INFO: stdout: "true"
Sep  6 15:16:22.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5d927 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:22.693: INFO: stderr: ""
Sep  6 15:16:22.693: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 15:16:22.693: INFO: validating pod update-demo-nautilus-5d927
Sep  6 15:16:22.696: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 15:16:22.696: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 15:16:22.696: INFO: update-demo-nautilus-5d927 is verified up and running
Sep  6 15:16:22.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-bskkm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:22.762: INFO: stderr: ""
Sep  6 15:16:22.762: INFO: stdout: "true"
Sep  6 15:16:22.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-bskkm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:22.834: INFO: stderr: ""
Sep  6 15:16:22.834: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 15:16:22.834: INFO: validating pod update-demo-nautilus-bskkm
Sep  6 15:16:22.837: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 15:16:22.837: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 15:16:22.837: INFO: update-demo-nautilus-bskkm is verified up and running
STEP: scaling down the replication controller
Sep  6 15:16:22.839: INFO: scanned /root for discovery docs: <nil>
Sep  6 15:16:22.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2043'
Sep  6 15:16:23.924: INFO: stderr: ""
Sep  6 15:16:23.924: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 15:16:23.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2043'
Sep  6 15:16:24.002: INFO: stderr: ""
Sep  6 15:16:24.002: INFO: stdout: "update-demo-nautilus-5d927 update-demo-nautilus-bskkm "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  6 15:16:29.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2043'
Sep  6 15:16:29.089: INFO: stderr: ""
Sep  6 15:16:29.089: INFO: stdout: "update-demo-nautilus-5d927 "
Sep  6 15:16:29.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5d927 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:29.160: INFO: stderr: ""
Sep  6 15:16:29.160: INFO: stdout: "true"
Sep  6 15:16:29.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5d927 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:29.231: INFO: stderr: ""
Sep  6 15:16:29.231: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 15:16:29.231: INFO: validating pod update-demo-nautilus-5d927
Sep  6 15:16:29.233: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 15:16:29.234: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 15:16:29.234: INFO: update-demo-nautilus-5d927 is verified up and running
STEP: scaling up the replication controller
Sep  6 15:16:29.240: INFO: scanned /root for discovery docs: <nil>
Sep  6 15:16:29.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2043'
Sep  6 15:16:30.333: INFO: stderr: ""
Sep  6 15:16:30.333: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 15:16:30.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2043'
Sep  6 15:16:30.402: INFO: stderr: ""
Sep  6 15:16:30.402: INFO: stdout: "update-demo-nautilus-5d927 update-demo-nautilus-kn7fb "
Sep  6 15:16:30.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5d927 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:30.467: INFO: stderr: ""
Sep  6 15:16:30.467: INFO: stdout: "true"
Sep  6 15:16:30.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5d927 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:30.536: INFO: stderr: ""
Sep  6 15:16:30.536: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 15:16:30.536: INFO: validating pod update-demo-nautilus-5d927
Sep  6 15:16:30.539: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 15:16:30.539: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 15:16:30.539: INFO: update-demo-nautilus-5d927 is verified up and running
Sep  6 15:16:30.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-kn7fb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:30.613: INFO: stderr: ""
Sep  6 15:16:30.613: INFO: stdout: "true"
Sep  6 15:16:30.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-kn7fb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2043'
Sep  6 15:16:30.681: INFO: stderr: ""
Sep  6 15:16:30.681: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 15:16:30.681: INFO: validating pod update-demo-nautilus-kn7fb
Sep  6 15:16:30.684: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 15:16:30.684: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 15:16:30.684: INFO: update-demo-nautilus-kn7fb is verified up and running
STEP: using delete to clean up resources
Sep  6 15:16:30.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-2043'
Sep  6 15:16:30.753: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 15:16:30.753: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 15:16:30.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2043'
Sep  6 15:16:30.830: INFO: stderr: "No resources found.\n"
Sep  6 15:16:30.830: INFO: stdout: ""
Sep  6 15:16:30.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -l name=update-demo --namespace=kubectl-2043 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 15:16:30.903: INFO: stderr: ""
Sep  6 15:16:30.903: INFO: stdout: "update-demo-nautilus-5d927\nupdate-demo-nautilus-kn7fb\n"
Sep  6 15:16:31.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2043'
Sep  6 15:16:31.478: INFO: stderr: "No resources found.\n"
Sep  6 15:16:31.478: INFO: stdout: ""
Sep  6 15:16:31.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -l name=update-demo --namespace=kubectl-2043 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 15:16:31.570: INFO: stderr: ""
Sep  6 15:16:31.570: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:16:31.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2043" for this suite.
Sep  6 15:16:53.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:16:53.640: INFO: namespace kubectl-2043 deletion completed in 22.066735238s

• [SLOW TEST:36.542 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:16:53.640: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:16:53.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ba57e91-d0b9-11e9-9ba3-f679ea11f394" in namespace "projected-4291" to be "success or failure"
Sep  6 15:16:53.666: INFO: Pod "downwardapi-volume-5ba57e91-d0b9-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.519741ms
Sep  6 15:16:55.669: INFO: Pod "downwardapi-volume-5ba57e91-d0b9-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004026298s
STEP: Saw pod success
Sep  6 15:16:55.669: INFO: Pod "downwardapi-volume-5ba57e91-d0b9-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:16:55.670: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-5ba57e91-d0b9-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:16:55.684: INFO: Waiting for pod downwardapi-volume-5ba57e91-d0b9-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:16:55.686: INFO: Pod downwardapi-volume-5ba57e91-d0b9-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:16:55.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4291" for this suite.
Sep  6 15:17:01.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:17:01.796: INFO: namespace projected-4291 deletion completed in 6.10776432s

• [SLOW TEST:8.156 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:17:01.796: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:17:01.829: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:17:03.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7480" for this suite.
Sep  6 15:17:41.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:17:42.042: INFO: namespace pods-7480 deletion completed in 38.074806327s

• [SLOW TEST:40.246 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:17:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-787fb15c-d0b9-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:17:42.075: INFO: Waiting up to 5m0s for pod "pod-secrets-788024bc-d0b9-11e9-9ba3-f679ea11f394" in namespace "secrets-165" to be "success or failure"
Sep  6 15:17:42.077: INFO: Pod "pod-secrets-788024bc-d0b9-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.732649ms
Sep  6 15:17:44.079: INFO: Pod "pod-secrets-788024bc-d0b9-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004388105s
STEP: Saw pod success
Sep  6 15:17:44.079: INFO: Pod "pod-secrets-788024bc-d0b9-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:17:44.082: INFO: Trying to get logs from node metalk8s-23 pod pod-secrets-788024bc-d0b9-11e9-9ba3-f679ea11f394 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:17:44.099: INFO: Waiting for pod pod-secrets-788024bc-d0b9-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:17:44.101: INFO: Pod pod-secrets-788024bc-d0b9-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:17:44.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-165" for this suite.
Sep  6 15:17:50.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:17:50.174: INFO: namespace secrets-165 deletion completed in 6.068879376s

• [SLOW TEST:8.131 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:17:50.174: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:17:50.203: INFO: Conformance test suite needs a cluster with at least 2 nodes.
Sep  6 15:17:50.203: INFO: Create a RollingUpdate DaemonSet
Sep  6 15:17:50.205: INFO: Check that daemon pods launch on every node of the cluster
Sep  6 15:17:50.210: INFO: Number of nodes with available pods: 0
Sep  6 15:17:50.210: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 15:17:51.215: INFO: Number of nodes with available pods: 0
Sep  6 15:17:51.215: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 15:17:52.215: INFO: Number of nodes with available pods: 1
Sep  6 15:17:52.215: INFO: Number of running nodes: 1, number of available pods: 1
Sep  6 15:17:52.215: INFO: Update the DaemonSet to trigger a rollout
Sep  6 15:17:52.221: INFO: Updating DaemonSet daemon-set
Sep  6 15:17:56.228: INFO: Roll back the DaemonSet before rollout is complete
Sep  6 15:17:56.232: INFO: Updating DaemonSet daemon-set
Sep  6 15:17:56.232: INFO: Make sure DaemonSet rollback is complete
Sep  6 15:17:56.234: INFO: Wrong image for pod: daemon-set-9mt65. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 15:17:56.234: INFO: Pod daemon-set-9mt65 is not available
Sep  6 15:17:57.238: INFO: Wrong image for pod: daemon-set-9mt65. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 15:17:57.239: INFO: Pod daemon-set-9mt65 is not available
Sep  6 15:17:58.239: INFO: Pod daemon-set-nwqcb is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7866, will wait for the garbage collector to delete the pods
Sep  6 15:17:58.300: INFO: Deleting DaemonSet.extensions daemon-set took: 5.008282ms
Sep  6 15:17:58.700: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.232609ms
Sep  6 15:18:08.702: INFO: Number of nodes with available pods: 0
Sep  6 15:18:08.703: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 15:18:08.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7866/daemonsets","resourceVersion":"9518"},"items":null}

Sep  6 15:18:08.706: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7866/pods","resourceVersion":"9518"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:18:08.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7866" for this suite.
Sep  6 15:18:14.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:18:14.783: INFO: namespace daemonsets-7866 deletion completed in 6.071750971s

• [SLOW TEST:24.609 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:18:14.783: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  6 15:18:17.830: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:18:18.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1778" for this suite.
Sep  6 15:18:40.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:18:40.911: INFO: namespace replicaset-1778 deletion completed in 22.069993587s

• [SLOW TEST:26.128 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:18:40.912: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  6 15:18:43.463: INFO: Successfully updated pod "annotationupdate9b95f7da-d0b9-11e9-9ba3-f679ea11f394"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:18:47.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1371" for this suite.
Sep  6 15:19:09.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:19:09.541: INFO: namespace downward-api-1371 deletion completed in 22.063018818s

• [SLOW TEST:28.629 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:19:09.541: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  6 15:19:09.562: INFO: PodSpec: initContainers in spec.initContainers
Sep  6 15:19:49.876: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-aca67214-d0b9-11e9-9ba3-f679ea11f394", GenerateName:"", Namespace:"init-container-785", SelfLink:"/api/v1/namespaces/init-container-785/pods/pod-init-aca67214-d0b9-11e9-9ba3-f679ea11f394", UID:"aca6da7b-d0b9-11e9-b447-fa163e0a078f", ResourceVersion:"9814", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63703379949, loc:(*time.Location)(0x8825120)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"562733569"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.233.221.242/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8x5w5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0031a1e80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8x5w5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8x5w5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8x5w5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000c3d598), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"metalk8s-23", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0026263c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c3d6e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c3d700)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000c3d708), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000c3d70c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703379949, loc:(*time.Location)(0x8825120)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703379949, loc:(*time.Location)(0x8825120)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703379949, loc:(*time.Location)(0x8825120)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703379949, loc:(*time.Location)(0x8825120)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.0.14", PodIP:"10.233.221.242", StartTime:(*v1.Time)(0xc0030bfac0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0030bfb00), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0024092d0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://b402894028e5db15b5863aa0891891ef5e75e612857c98d9778ad558250d1fa5"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030bfb20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030bfae0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:19:49.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-785" for this suite.
Sep  6 15:20:11.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:20:11.960: INFO: namespace init-container-785 deletion completed in 22.079605152s

• [SLOW TEST:62.419 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:20:11.960: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 15:20:11.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-8246'
Sep  6 15:20:12.167: INFO: stderr: ""
Sep  6 15:20:12.167: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep  6 15:20:17.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pod e2e-test-nginx-pod --namespace=kubectl-8246 -o json'
Sep  6 15:20:17.316: INFO: stderr: ""
Sep  6 15:20:17.316: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.233.221.236/32\"\n        },\n        \"creationTimestamp\": \"2019-09-06T15:20:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-8246\",\n        \"resourceVersion\": \"9883\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8246/pods/e2e-test-nginx-pod\",\n        \"uid\": \"d1f3c701-d0b9-11e9-b447-fa163e0a078f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-gqjjt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"metalk8s-23\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-gqjjt\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-gqjjt\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T15:20:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T15:20:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T15:20:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T15:20:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://b28337471719e0ea6c0b11fdf650146440ba3d78e37445f10881caabed5ad1e0\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-06T15:20:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.0.14\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.221.236\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-06T15:20:12Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  6 15:20:17.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 replace -f - --namespace=kubectl-8246'
Sep  6 15:20:17.510: INFO: stderr: ""
Sep  6 15:20:17.510: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Sep  6 15:20:17.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete pods e2e-test-nginx-pod --namespace=kubectl-8246'
Sep  6 15:20:18.983: INFO: stderr: ""
Sep  6 15:20:18.983: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:20:18.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8246" for this suite.
Sep  6 15:20:24.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:20:25.075: INFO: namespace kubectl-8246 deletion completed in 6.089473022s

• [SLOW TEST:13.115 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:20:25.076: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-d9ad5cb2-d0b9-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:20:25.114: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9adcbd2-d0b9-11e9-9ba3-f679ea11f394" in namespace "configmap-4752" to be "success or failure"
Sep  6 15:20:25.118: INFO: Pod "pod-configmaps-d9adcbd2-d0b9-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.119613ms
Sep  6 15:20:27.122: INFO: Pod "pod-configmaps-d9adcbd2-d0b9-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00765357s
STEP: Saw pod success
Sep  6 15:20:27.122: INFO: Pod "pod-configmaps-d9adcbd2-d0b9-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:20:27.125: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-d9adcbd2-d0b9-11e9-9ba3-f679ea11f394 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:20:27.134: INFO: Waiting for pod pod-configmaps-d9adcbd2-d0b9-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:20:27.136: INFO: Pod pod-configmaps-d9adcbd2-d0b9-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:20:27.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4752" for this suite.
Sep  6 15:20:33.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:20:33.205: INFO: namespace configmap-4752 deletion completed in 6.066090199s

• [SLOW TEST:8.128 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:20:33.205: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:20:33.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8810" for this suite.
Sep  6 15:20:39.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:20:39.312: INFO: namespace services-8810 deletion completed in 6.080686182s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.107 seconds]
[sig-network] Services
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:20:39.312: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:20:39.342: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:20:39.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9454" for this suite.
Sep  6 15:20:45.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:20:45.957: INFO: namespace custom-resource-definition-9454 deletion completed in 6.073297832s

• [SLOW TEST:6.645 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:32
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:20:45.957: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep  6 15:20:45.978: INFO: namespace kubectl-4621
Sep  6 15:20:45.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-4621'
Sep  6 15:20:46.113: INFO: stderr: ""
Sep  6 15:20:46.113: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 15:20:47.116: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:20:47.116: INFO: Found 1 / 1
Sep  6 15:20:47.116: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 15:20:47.119: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:20:47.119: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 15:20:47.119: INFO: wait on redis-master startup in kubectl-4621 
Sep  6 15:20:47.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 logs redis-master-wxvpc redis-master --namespace=kubectl-4621'
Sep  6 15:20:47.189: INFO: stderr: ""
Sep  6 15:20:47.189: INFO: stdout: "1:M 06 Sep 15:20:46.850 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 06 Sep 15:20:46.850 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 06 Sep 15:20:46.850 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 15:20:46.851 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 15:20:46.851 # Server started, Redis version 3.2.12\n1:M 06 Sep 15:20:46.851 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 15:20:46.851 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep  6 15:20:47.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4621'
Sep  6 15:20:47.268: INFO: stderr: ""
Sep  6 15:20:47.268: INFO: stdout: "service/rm2 exposed\n"
Sep  6 15:20:47.270: INFO: Service rm2 in namespace kubectl-4621 found.
STEP: exposing service
Sep  6 15:20:49.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4621'
Sep  6 15:20:49.348: INFO: stderr: ""
Sep  6 15:20:49.348: INFO: stdout: "service/rm3 exposed\n"
Sep  6 15:20:49.350: INFO: Service rm3 in namespace kubectl-4621 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:20:51.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4621" for this suite.
Sep  6 15:21:13.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:21:13.426: INFO: namespace kubectl-4621 deletion completed in 22.069442489s

• [SLOW TEST:27.468 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:21:13.426: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-2tk8
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 15:21:13.463: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2tk8" in namespace "subpath-6887" to be "success or failure"
Sep  6 15:21:13.465: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.667818ms
Sep  6 15:21:15.467: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 2.004276455s
Sep  6 15:21:17.470: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 4.007255469s
Sep  6 15:21:19.473: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 6.009790203s
Sep  6 15:21:21.475: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 8.012219987s
Sep  6 15:21:23.478: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 10.01480041s
Sep  6 15:21:25.480: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 12.017358994s
Sep  6 15:21:27.483: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 14.020269429s
Sep  6 15:21:29.486: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 16.023065718s
Sep  6 15:21:31.489: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 18.025860458s
Sep  6 15:21:33.491: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Running", Reason="", readiness=true. Elapsed: 20.028408645s
Sep  6 15:21:35.494: INFO: Pod "pod-subpath-test-downwardapi-2tk8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.031000662s
STEP: Saw pod success
Sep  6 15:21:35.494: INFO: Pod "pod-subpath-test-downwardapi-2tk8" satisfied condition "success or failure"
Sep  6 15:21:35.496: INFO: Trying to get logs from node metalk8s-23 pod pod-subpath-test-downwardapi-2tk8 container test-container-subpath-downwardapi-2tk8: <nil>
STEP: delete the pod
Sep  6 15:21:35.509: INFO: Waiting for pod pod-subpath-test-downwardapi-2tk8 to disappear
Sep  6 15:21:35.513: INFO: Pod pod-subpath-test-downwardapi-2tk8 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2tk8
Sep  6 15:21:35.513: INFO: Deleting pod "pod-subpath-test-downwardapi-2tk8" in namespace "subpath-6887"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:21:35.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6887" for this suite.
Sep  6 15:21:41.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:21:41.590: INFO: namespace subpath-6887 deletion completed in 6.071676362s

• [SLOW TEST:28.163 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:21:41.590: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:21:41.628: INFO: (0) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.865474ms)
Sep  6 15:21:41.631: INFO: (1) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.902065ms)
Sep  6 15:21:41.634: INFO: (2) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.614426ms)
Sep  6 15:21:41.636: INFO: (3) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.557124ms)
Sep  6 15:21:41.639: INFO: (4) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.402776ms)
Sep  6 15:21:41.641: INFO: (5) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.516435ms)
Sep  6 15:21:41.643: INFO: (6) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.202219ms)
Sep  6 15:21:41.646: INFO: (7) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.030531ms)
Sep  6 15:21:41.648: INFO: (8) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.112602ms)
Sep  6 15:21:41.650: INFO: (9) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.176901ms)
Sep  6 15:21:41.652: INFO: (10) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.196449ms)
Sep  6 15:21:41.654: INFO: (11) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.170222ms)
Sep  6 15:21:41.657: INFO: (12) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.254604ms)
Sep  6 15:21:41.659: INFO: (13) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.325579ms)
Sep  6 15:21:41.662: INFO: (14) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.716149ms)
Sep  6 15:21:41.665: INFO: (15) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.028959ms)
Sep  6 15:21:41.667: INFO: (16) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.417049ms)
Sep  6 15:21:41.670: INFO: (17) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.618443ms)
Sep  6 15:21:41.672: INFO: (18) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.426335ms)
Sep  6 15:21:41.675: INFO: (19) /api/v1/nodes/metalk8s-23/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.458491ms)
[AfterEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:21:41.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-449" for this suite.
Sep  6 15:21:47.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:21:47.779: INFO: namespace proxy-449 deletion completed in 6.102537361s

• [SLOW TEST:6.189 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:21:47.780: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6075
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 15:21:47.802: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 15:22:03.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.221.245:8080/dial?request=hostName&protocol=http&host=10.233.221.233&port=8080&tries=1'] Namespace:pod-network-test-6075 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:22:03.845: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:22:03.965: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:22:03.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6075" for this suite.
Sep  6 15:22:25.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:22:26.046: INFO: namespace pod-network-test-6075 deletion completed in 22.078496813s

• [SLOW TEST:38.266 seconds]
[sig-network] Networking
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:22:26.047: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 15:22:28.596: INFO: Successfully updated pod "pod-update-21c7420c-d0ba-11e9-9ba3-f679ea11f394"
STEP: verifying the updated pod is in kubernetes
Sep  6 15:22:28.600: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:22:28.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1673" for this suite.
Sep  6 15:22:50.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:22:50.662: INFO: namespace pods-1673 deletion completed in 22.059841034s

• [SLOW TEST:24.616 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:22:50.662: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:22:52.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-634" for this suite.
Sep  6 15:23:42.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:23:42.768: INFO: namespace kubelet-test-634 deletion completed in 50.066540693s

• [SLOW TEST:52.106 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:23:42.769: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  6 15:23:42.801: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6182,SelfLink:/api/v1/namespaces/watch-6182/configmaps/e2e-watch-test-watch-closed,UID:4f823cfd-d0ba-11e9-b447-fa163e0a078f,ResourceVersion:10474,Generation:0,CreationTimestamp:2019-09-06 15:23:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 15:23:42.801: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6182,SelfLink:/api/v1/namespaces/watch-6182/configmaps/e2e-watch-test-watch-closed,UID:4f823cfd-d0ba-11e9-b447-fa163e0a078f,ResourceVersion:10475,Generation:0,CreationTimestamp:2019-09-06 15:23:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  6 15:23:42.809: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6182,SelfLink:/api/v1/namespaces/watch-6182/configmaps/e2e-watch-test-watch-closed,UID:4f823cfd-d0ba-11e9-b447-fa163e0a078f,ResourceVersion:10476,Generation:0,CreationTimestamp:2019-09-06 15:23:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 15:23:42.809: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6182,SelfLink:/api/v1/namespaces/watch-6182/configmaps/e2e-watch-test-watch-closed,UID:4f823cfd-d0ba-11e9-b447-fa163e0a078f,ResourceVersion:10477,Generation:0,CreationTimestamp:2019-09-06 15:23:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:23:42.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6182" for this suite.
Sep  6 15:23:48.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:23:48.873: INFO: namespace watch-6182 deletion completed in 6.061567948s

• [SLOW TEST:6.105 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:23:48.874: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7712
Sep  6 15:23:52.903: INFO: Started pod liveness-http in namespace container-probe-7712
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 15:23:52.904: INFO: Initial restart count of pod liveness-http is 0
Sep  6 15:24:08.929: INFO: Restart count of pod container-probe-7712/liveness-http is now 1 (16.025108752s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:24:08.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7712" for this suite.
Sep  6 15:24:14.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:24:15.029: INFO: namespace container-probe-7712 deletion completed in 6.087033517s

• [SLOW TEST:26.156 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:24:15.029: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-4g7j
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 15:24:15.069: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-4g7j" in namespace "subpath-5613" to be "success or failure"
Sep  6 15:24:15.072: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.600769ms
Sep  6 15:24:17.075: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 2.00585271s
Sep  6 15:24:19.078: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 4.008256025s
Sep  6 15:24:21.080: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 6.010816068s
Sep  6 15:24:23.083: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 8.013473224s
Sep  6 15:24:25.086: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 10.016251084s
Sep  6 15:24:27.088: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 12.018774475s
Sep  6 15:24:29.092: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 14.022424177s
Sep  6 15:24:31.094: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 16.02511222s
Sep  6 15:24:33.098: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 18.02817452s
Sep  6 15:24:35.100: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Running", Reason="", readiness=true. Elapsed: 20.030883416s
Sep  6 15:24:37.103: INFO: Pod "pod-subpath-test-projected-4g7j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.033641822s
STEP: Saw pod success
Sep  6 15:24:37.103: INFO: Pod "pod-subpath-test-projected-4g7j" satisfied condition "success or failure"
Sep  6 15:24:37.105: INFO: Trying to get logs from node metalk8s-23 pod pod-subpath-test-projected-4g7j container test-container-subpath-projected-4g7j: <nil>
STEP: delete the pod
Sep  6 15:24:37.118: INFO: Waiting for pod pod-subpath-test-projected-4g7j to disappear
Sep  6 15:24:37.120: INFO: Pod pod-subpath-test-projected-4g7j no longer exists
STEP: Deleting pod pod-subpath-test-projected-4g7j
Sep  6 15:24:37.120: INFO: Deleting pod "pod-subpath-test-projected-4g7j" in namespace "subpath-5613"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:24:37.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5613" for this suite.
Sep  6 15:24:43.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:24:43.186: INFO: namespace subpath-5613 deletion completed in 6.061750985s

• [SLOW TEST:28.156 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:24:43.186: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 15:24:47.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 15:24:47.236: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 15:24:49.236: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 15:24:49.239: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 15:24:51.237: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 15:24:51.253: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:24:51.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1331" for this suite.
Sep  6 15:25:13.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:25:13.322: INFO: namespace container-lifecycle-hook-1331 deletion completed in 22.065262976s

• [SLOW TEST:30.136 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:25:13.322: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 15:25:13.346: INFO: Waiting up to 5m0s for pod "pod-857ac731-d0ba-11e9-9ba3-f679ea11f394" in namespace "emptydir-4222" to be "success or failure"
Sep  6 15:25:13.348: INFO: Pod "pod-857ac731-d0ba-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229855ms
Sep  6 15:25:15.351: INFO: Pod "pod-857ac731-d0ba-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005239597s
STEP: Saw pod success
Sep  6 15:25:15.351: INFO: Pod "pod-857ac731-d0ba-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:25:15.354: INFO: Trying to get logs from node metalk8s-23 pod pod-857ac731-d0ba-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:25:15.369: INFO: Waiting for pod pod-857ac731-d0ba-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:25:15.375: INFO: Pod pod-857ac731-d0ba-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:25:15.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4222" for this suite.
Sep  6 15:25:21.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:25:21.446: INFO: namespace emptydir-4222 deletion completed in 6.067709462s

• [SLOW TEST:8.124 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:25:21.446: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:25:23.504: INFO: Waiting up to 5m0s for pod "client-envvars-8b88d078-d0ba-11e9-9ba3-f679ea11f394" in namespace "pods-5759" to be "success or failure"
Sep  6 15:25:23.507: INFO: Pod "client-envvars-8b88d078-d0ba-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226975ms
Sep  6 15:25:25.509: INFO: Pod "client-envvars-8b88d078-d0ba-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005629761s
STEP: Saw pod success
Sep  6 15:25:25.509: INFO: Pod "client-envvars-8b88d078-d0ba-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:25:25.511: INFO: Trying to get logs from node metalk8s-23 pod client-envvars-8b88d078-d0ba-11e9-9ba3-f679ea11f394 container env3cont: <nil>
STEP: delete the pod
Sep  6 15:25:25.521: INFO: Waiting for pod client-envvars-8b88d078-d0ba-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:25:25.523: INFO: Pod client-envvars-8b88d078-d0ba-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:25:25.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5759" for this suite.
Sep  6 15:26:09.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:26:09.594: INFO: namespace pods-5759 deletion completed in 44.067932479s

• [SLOW TEST:48.148 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:26:09.594: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-a70525f7-d0ba-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:26:09.621: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a705a35b-d0ba-11e9-9ba3-f679ea11f394" in namespace "projected-7204" to be "success or failure"
Sep  6 15:26:09.623: INFO: Pod "pod-projected-configmaps-a705a35b-d0ba-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912698ms
Sep  6 15:26:11.625: INFO: Pod "pod-projected-configmaps-a705a35b-d0ba-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004631388s
STEP: Saw pod success
Sep  6 15:26:11.625: INFO: Pod "pod-projected-configmaps-a705a35b-d0ba-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:26:11.627: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-configmaps-a705a35b-d0ba-11e9-9ba3-f679ea11f394 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:26:11.640: INFO: Waiting for pod pod-projected-configmaps-a705a35b-d0ba-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:26:11.642: INFO: Pod pod-projected-configmaps-a705a35b-d0ba-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:26:11.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7204" for this suite.
Sep  6 15:26:17.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:26:17.706: INFO: namespace projected-7204 deletion completed in 6.061450812s

• [SLOW TEST:8.112 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:26:17.706: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:26:17.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abdb307f-d0ba-11e9-9ba3-f679ea11f394" in namespace "downward-api-1895" to be "success or failure"
Sep  6 15:26:17.736: INFO: Pod "downwardapi-volume-abdb307f-d0ba-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.673198ms
Sep  6 15:26:19.739: INFO: Pod "downwardapi-volume-abdb307f-d0ba-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007581244s
STEP: Saw pod success
Sep  6 15:26:19.739: INFO: Pod "downwardapi-volume-abdb307f-d0ba-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:26:19.741: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-abdb307f-d0ba-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:26:19.751: INFO: Waiting for pod downwardapi-volume-abdb307f-d0ba-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:26:19.753: INFO: Pod downwardapi-volume-abdb307f-d0ba-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:26:19.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1895" for this suite.
Sep  6 15:26:25.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:26:25.824: INFO: namespace downward-api-1895 deletion completed in 6.068737861s

• [SLOW TEST:8.118 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:26:25.824: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Sep  6 15:26:27.863: INFO: Pod pod-hostip-b0b24b8f-d0ba-11e9-9ba3-f679ea11f394 has hostIP: 10.10.0.14
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:26:27.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1655" for this suite.
Sep  6 15:26:49.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:26:49.934: INFO: namespace pods-1655 deletion completed in 22.0692627s

• [SLOW TEST:24.110 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:26:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4735
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 15:26:49.960: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 15:27:04.005: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.221.253:8080/dial?request=hostName&protocol=udp&host=10.233.221.252&port=8081&tries=1'] Namespace:pod-network-test-4735 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:27:04.005: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:27:04.136: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:27:04.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4735" for this suite.
Sep  6 15:27:26.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:27:26.218: INFO: namespace pod-network-test-4735 deletion completed in 22.078316391s

• [SLOW TEST:36.283 seconds]
[sig-network] Networking
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:27:26.218: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Sep  6 15:27:26.251: INFO: Waiting up to 5m0s for pod "client-containers-d4b2035d-d0ba-11e9-9ba3-f679ea11f394" in namespace "containers-9930" to be "success or failure"
Sep  6 15:27:26.253: INFO: Pod "client-containers-d4b2035d-d0ba-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.962026ms
Sep  6 15:27:28.256: INFO: Pod "client-containers-d4b2035d-d0ba-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004858585s
STEP: Saw pod success
Sep  6 15:27:28.256: INFO: Pod "client-containers-d4b2035d-d0ba-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:27:28.258: INFO: Trying to get logs from node metalk8s-23 pod client-containers-d4b2035d-d0ba-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:27:28.274: INFO: Waiting for pod client-containers-d4b2035d-d0ba-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:27:28.278: INFO: Pod client-containers-d4b2035d-d0ba-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:27:28.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9930" for this suite.
Sep  6 15:27:34.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:27:34.359: INFO: namespace containers-9930 deletion completed in 6.07840469s

• [SLOW TEST:8.141 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:27:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 15:27:34.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8888'
Sep  6 15:27:34.468: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 15:27:34.468: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep  6 15:27:34.478: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-kd2p8]
Sep  6 15:27:34.478: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-kd2p8" in namespace "kubectl-8888" to be "running and ready"
Sep  6 15:27:34.481: INFO: Pod "e2e-test-nginx-rc-kd2p8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.756958ms
Sep  6 15:27:36.485: INFO: Pod "e2e-test-nginx-rc-kd2p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.007340121s
Sep  6 15:27:36.485: INFO: Pod "e2e-test-nginx-rc-kd2p8" satisfied condition "running and ready"
Sep  6 15:27:36.485: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-kd2p8]
Sep  6 15:27:36.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 logs rc/e2e-test-nginx-rc --namespace=kubectl-8888'
Sep  6 15:27:36.582: INFO: stderr: ""
Sep  6 15:27:36.582: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1425
Sep  6 15:27:36.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete rc e2e-test-nginx-rc --namespace=kubectl-8888'
Sep  6 15:27:36.656: INFO: stderr: ""
Sep  6 15:27:36.656: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:27:36.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8888" for this suite.
Sep  6 15:27:58.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:27:58.740: INFO: namespace kubectl-8888 deletion completed in 22.07981679s

• [SLOW TEST:24.380 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:27:58.740: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6065
I0906 15:27:58.767580      14 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6065, replica count: 1
I0906 15:27:59.818022      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 15:27:59.925: INFO: Created: latency-svc-czvr6
Sep  6 15:27:59.928: INFO: Got endpoints: latency-svc-czvr6 [10.024156ms]
Sep  6 15:27:59.937: INFO: Created: latency-svc-thqxq
Sep  6 15:27:59.941: INFO: Got endpoints: latency-svc-thqxq [12.149312ms]
Sep  6 15:27:59.945: INFO: Created: latency-svc-2pwb2
Sep  6 15:27:59.949: INFO: Got endpoints: latency-svc-2pwb2 [19.046199ms]
Sep  6 15:27:59.949: INFO: Created: latency-svc-gxjvs
Sep  6 15:27:59.952: INFO: Got endpoints: latency-svc-gxjvs [22.126477ms]
Sep  6 15:27:59.955: INFO: Created: latency-svc-xqhl7
Sep  6 15:27:59.961: INFO: Got endpoints: latency-svc-xqhl7 [31.302761ms]
Sep  6 15:27:59.961: INFO: Created: latency-svc-hn4pb
Sep  6 15:27:59.965: INFO: Created: latency-svc-xs9p7
Sep  6 15:27:59.965: INFO: Got endpoints: latency-svc-hn4pb [36.014694ms]
Sep  6 15:27:59.972: INFO: Got endpoints: latency-svc-xs9p7 [42.041783ms]
Sep  6 15:27:59.978: INFO: Created: latency-svc-xwxt5
Sep  6 15:27:59.981: INFO: Got endpoints: latency-svc-xwxt5 [51.556979ms]
Sep  6 15:27:59.983: INFO: Created: latency-svc-6wkrx
Sep  6 15:27:59.987: INFO: Got endpoints: latency-svc-6wkrx [57.565092ms]
Sep  6 15:27:59.993: INFO: Created: latency-svc-db9nw
Sep  6 15:27:59.996: INFO: Got endpoints: latency-svc-db9nw [65.911559ms]
Sep  6 15:27:59.996: INFO: Created: latency-svc-5xgvt
Sep  6 15:28:00.000: INFO: Got endpoints: latency-svc-5xgvt [70.33239ms]
Sep  6 15:28:00.000: INFO: Created: latency-svc-fcpkm
Sep  6 15:28:00.009: INFO: Created: latency-svc-9w9rm
Sep  6 15:28:00.014: INFO: Created: latency-svc-wzrxt
Sep  6 15:28:00.014: INFO: Got endpoints: latency-svc-9w9rm [84.031835ms]
Sep  6 15:28:00.014: INFO: Got endpoints: latency-svc-fcpkm [84.43059ms]
Sep  6 15:28:00.016: INFO: Got endpoints: latency-svc-wzrxt [86.618302ms]
Sep  6 15:28:00.018: INFO: Created: latency-svc-pmgzm
Sep  6 15:28:00.022: INFO: Got endpoints: latency-svc-pmgzm [92.519044ms]
Sep  6 15:28:00.029: INFO: Created: latency-svc-bzrj8
Sep  6 15:28:00.031: INFO: Got endpoints: latency-svc-bzrj8 [101.609804ms]
Sep  6 15:28:00.035: INFO: Created: latency-svc-zgwn5
Sep  6 15:28:00.037: INFO: Got endpoints: latency-svc-zgwn5 [96.049248ms]
Sep  6 15:28:00.044: INFO: Created: latency-svc-fghtx
Sep  6 15:28:00.048: INFO: Got endpoints: latency-svc-fghtx [17.127704ms]
Sep  6 15:28:00.049: INFO: Created: latency-svc-zfqlx
Sep  6 15:28:00.052: INFO: Got endpoints: latency-svc-zfqlx [103.022116ms]
Sep  6 15:28:00.056: INFO: Created: latency-svc-kzwch
Sep  6 15:28:00.062: INFO: Got endpoints: latency-svc-kzwch [110.332944ms]
Sep  6 15:28:00.063: INFO: Created: latency-svc-ccbbk
Sep  6 15:28:00.064: INFO: Got endpoints: latency-svc-ccbbk [103.512089ms]
Sep  6 15:28:00.073: INFO: Created: latency-svc-zrzkz
Sep  6 15:28:00.092: INFO: Got endpoints: latency-svc-zrzkz [126.811097ms]
Sep  6 15:28:00.094: INFO: Created: latency-svc-mzknw
Sep  6 15:28:00.107: INFO: Got endpoints: latency-svc-mzknw [135.144512ms]
Sep  6 15:28:00.107: INFO: Created: latency-svc-z2xzv
Sep  6 15:28:00.111: INFO: Created: latency-svc-7qj94
Sep  6 15:28:00.111: INFO: Got endpoints: latency-svc-z2xzv [129.950653ms]
Sep  6 15:28:00.116: INFO: Got endpoints: latency-svc-7qj94 [128.989518ms]
Sep  6 15:28:00.118: INFO: Created: latency-svc-nwxwr
Sep  6 15:28:00.123: INFO: Created: latency-svc-wg284
Sep  6 15:28:00.123: INFO: Got endpoints: latency-svc-nwxwr [126.936585ms]
Sep  6 15:28:00.125: INFO: Got endpoints: latency-svc-wg284 [125.206383ms]
Sep  6 15:28:00.133: INFO: Created: latency-svc-qvgkr
Sep  6 15:28:00.136: INFO: Got endpoints: latency-svc-qvgkr [122.257757ms]
Sep  6 15:28:00.140: INFO: Created: latency-svc-c27f4
Sep  6 15:28:00.144: INFO: Got endpoints: latency-svc-c27f4 [130.25907ms]
Sep  6 15:28:00.147: INFO: Created: latency-svc-486wj
Sep  6 15:28:00.149: INFO: Got endpoints: latency-svc-486wj [132.758343ms]
Sep  6 15:28:00.154: INFO: Created: latency-svc-zmptk
Sep  6 15:28:00.157: INFO: Got endpoints: latency-svc-zmptk [134.112402ms]
Sep  6 15:28:00.158: INFO: Created: latency-svc-6hvrc
Sep  6 15:28:00.160: INFO: Got endpoints: latency-svc-6hvrc [122.703505ms]
Sep  6 15:28:00.166: INFO: Created: latency-svc-f78hv
Sep  6 15:28:00.170: INFO: Got endpoints: latency-svc-f78hv [121.364682ms]
Sep  6 15:28:00.170: INFO: Created: latency-svc-zz5vt
Sep  6 15:28:00.172: INFO: Got endpoints: latency-svc-zz5vt [120.878307ms]
Sep  6 15:28:00.179: INFO: Created: latency-svc-ggbz7
Sep  6 15:28:00.182: INFO: Got endpoints: latency-svc-ggbz7 [120.155276ms]
Sep  6 15:28:00.183: INFO: Created: latency-svc-v2pdh
Sep  6 15:28:00.189: INFO: Created: latency-svc-c774n
Sep  6 15:28:00.201: INFO: Created: latency-svc-2zg9m
Sep  6 15:28:00.209: INFO: Created: latency-svc-hwzkw
Sep  6 15:28:00.222: INFO: Created: latency-svc-t2nct
Sep  6 15:28:00.226: INFO: Created: latency-svc-nvkjz
Sep  6 15:28:00.229: INFO: Got endpoints: latency-svc-v2pdh [164.308944ms]
Sep  6 15:28:00.233: INFO: Created: latency-svc-6ldk9
Sep  6 15:28:00.237: INFO: Created: latency-svc-tfg66
Sep  6 15:28:00.260: INFO: Created: latency-svc-4g2rl
Sep  6 15:28:00.271: INFO: Created: latency-svc-l7qkq
Sep  6 15:28:00.278: INFO: Created: latency-svc-hhwhm
Sep  6 15:28:00.281: INFO: Got endpoints: latency-svc-c774n [188.685867ms]
Sep  6 15:28:00.285: INFO: Created: latency-svc-99s7l
Sep  6 15:28:00.298: INFO: Created: latency-svc-s7rlg
Sep  6 15:28:00.311: INFO: Created: latency-svc-mqz44
Sep  6 15:28:00.316: INFO: Created: latency-svc-9rt7x
Sep  6 15:28:00.322: INFO: Created: latency-svc-kfwjk
Sep  6 15:28:00.330: INFO: Created: latency-svc-7s59h
Sep  6 15:28:00.330: INFO: Got endpoints: latency-svc-2zg9m [222.95769ms]
Sep  6 15:28:00.340: INFO: Created: latency-svc-l92gr
Sep  6 15:28:00.378: INFO: Got endpoints: latency-svc-hwzkw [267.212734ms]
Sep  6 15:28:00.386: INFO: Created: latency-svc-znlv6
Sep  6 15:28:00.428: INFO: Got endpoints: latency-svc-t2nct [312.175017ms]
Sep  6 15:28:00.437: INFO: Created: latency-svc-27hbg
Sep  6 15:28:00.479: INFO: Got endpoints: latency-svc-nvkjz [355.929765ms]
Sep  6 15:28:00.488: INFO: Created: latency-svc-qzcdw
Sep  6 15:28:00.529: INFO: Got endpoints: latency-svc-6ldk9 [403.362441ms]
Sep  6 15:28:00.539: INFO: Created: latency-svc-9j9lv
Sep  6 15:28:00.578: INFO: Got endpoints: latency-svc-tfg66 [442.170335ms]
Sep  6 15:28:00.586: INFO: Created: latency-svc-ww4t5
Sep  6 15:28:00.628: INFO: Got endpoints: latency-svc-4g2rl [483.831936ms]
Sep  6 15:28:00.637: INFO: Created: latency-svc-cvp82
Sep  6 15:28:00.679: INFO: Got endpoints: latency-svc-l7qkq [529.544983ms]
Sep  6 15:28:00.686: INFO: Created: latency-svc-v7686
Sep  6 15:28:00.730: INFO: Got endpoints: latency-svc-hhwhm [573.018776ms]
Sep  6 15:28:00.755: INFO: Created: latency-svc-qqbsr
Sep  6 15:28:00.785: INFO: Got endpoints: latency-svc-99s7l [624.34624ms]
Sep  6 15:28:00.794: INFO: Created: latency-svc-jn582
Sep  6 15:28:00.833: INFO: Got endpoints: latency-svc-s7rlg [663.256003ms]
Sep  6 15:28:00.840: INFO: Created: latency-svc-wg72d
Sep  6 15:28:00.878: INFO: Got endpoints: latency-svc-mqz44 [705.77692ms]
Sep  6 15:28:00.885: INFO: Created: latency-svc-j4wl6
Sep  6 15:28:00.939: INFO: Got endpoints: latency-svc-9rt7x [756.970904ms]
Sep  6 15:28:00.948: INFO: Created: latency-svc-jknpc
Sep  6 15:28:00.979: INFO: Got endpoints: latency-svc-kfwjk [750.031214ms]
Sep  6 15:28:00.987: INFO: Created: latency-svc-t79z2
Sep  6 15:28:01.030: INFO: Got endpoints: latency-svc-7s59h [748.922013ms]
Sep  6 15:28:01.042: INFO: Created: latency-svc-fxbc5
Sep  6 15:28:01.078: INFO: Got endpoints: latency-svc-l92gr [748.232661ms]
Sep  6 15:28:01.088: INFO: Created: latency-svc-dsn8b
Sep  6 15:28:01.129: INFO: Got endpoints: latency-svc-znlv6 [750.287264ms]
Sep  6 15:28:01.136: INFO: Created: latency-svc-qtffh
Sep  6 15:28:01.178: INFO: Got endpoints: latency-svc-27hbg [749.927248ms]
Sep  6 15:28:01.186: INFO: Created: latency-svc-zh4gr
Sep  6 15:28:01.229: INFO: Got endpoints: latency-svc-qzcdw [749.909844ms]
Sep  6 15:28:01.237: INFO: Created: latency-svc-9v27p
Sep  6 15:28:01.279: INFO: Got endpoints: latency-svc-9j9lv [749.882028ms]
Sep  6 15:28:01.286: INFO: Created: latency-svc-pndrg
Sep  6 15:28:01.328: INFO: Got endpoints: latency-svc-ww4t5 [749.761346ms]
Sep  6 15:28:01.337: INFO: Created: latency-svc-9lm94
Sep  6 15:28:01.378: INFO: Got endpoints: latency-svc-cvp82 [749.772681ms]
Sep  6 15:28:01.395: INFO: Created: latency-svc-d6ds7
Sep  6 15:28:01.429: INFO: Got endpoints: latency-svc-v7686 [750.156317ms]
Sep  6 15:28:01.442: INFO: Created: latency-svc-2sn4z
Sep  6 15:28:01.478: INFO: Got endpoints: latency-svc-qqbsr [748.759556ms]
Sep  6 15:28:01.486: INFO: Created: latency-svc-jsdtc
Sep  6 15:28:01.528: INFO: Got endpoints: latency-svc-jn582 [743.832331ms]
Sep  6 15:28:01.536: INFO: Created: latency-svc-znmb7
Sep  6 15:28:01.578: INFO: Got endpoints: latency-svc-wg72d [745.325427ms]
Sep  6 15:28:01.586: INFO: Created: latency-svc-w2sq4
Sep  6 15:28:01.629: INFO: Got endpoints: latency-svc-j4wl6 [750.177943ms]
Sep  6 15:28:01.643: INFO: Created: latency-svc-d9ppr
Sep  6 15:28:01.680: INFO: Got endpoints: latency-svc-jknpc [740.582591ms]
Sep  6 15:28:01.690: INFO: Created: latency-svc-7rw7s
Sep  6 15:28:01.728: INFO: Got endpoints: latency-svc-t79z2 [749.231469ms]
Sep  6 15:28:01.737: INFO: Created: latency-svc-2v4pr
Sep  6 15:28:01.779: INFO: Got endpoints: latency-svc-fxbc5 [749.880777ms]
Sep  6 15:28:01.790: INFO: Created: latency-svc-4xbdn
Sep  6 15:28:01.830: INFO: Got endpoints: latency-svc-dsn8b [751.61324ms]
Sep  6 15:28:01.839: INFO: Created: latency-svc-vtjkp
Sep  6 15:28:01.884: INFO: Got endpoints: latency-svc-qtffh [755.171745ms]
Sep  6 15:28:01.896: INFO: Created: latency-svc-fvvx8
Sep  6 15:28:01.928: INFO: Got endpoints: latency-svc-zh4gr [749.992417ms]
Sep  6 15:28:01.941: INFO: Created: latency-svc-hnwbl
Sep  6 15:28:01.978: INFO: Got endpoints: latency-svc-9v27p [749.948775ms]
Sep  6 15:28:01.997: INFO: Created: latency-svc-btjmm
Sep  6 15:28:02.029: INFO: Got endpoints: latency-svc-pndrg [750.256539ms]
Sep  6 15:28:02.038: INFO: Created: latency-svc-hmc8s
Sep  6 15:28:02.078: INFO: Got endpoints: latency-svc-9lm94 [750.224549ms]
Sep  6 15:28:02.085: INFO: Created: latency-svc-sln8w
Sep  6 15:28:02.128: INFO: Got endpoints: latency-svc-d6ds7 [749.730626ms]
Sep  6 15:28:02.137: INFO: Created: latency-svc-cmg2h
Sep  6 15:28:02.178: INFO: Got endpoints: latency-svc-2sn4z [749.602592ms]
Sep  6 15:28:02.187: INFO: Created: latency-svc-xht2m
Sep  6 15:28:02.228: INFO: Got endpoints: latency-svc-jsdtc [749.526969ms]
Sep  6 15:28:02.241: INFO: Created: latency-svc-htfrb
Sep  6 15:28:02.278: INFO: Got endpoints: latency-svc-znmb7 [750.021357ms]
Sep  6 15:28:02.286: INFO: Created: latency-svc-bc7s2
Sep  6 15:28:02.328: INFO: Got endpoints: latency-svc-w2sq4 [750.211086ms]
Sep  6 15:28:02.337: INFO: Created: latency-svc-snhtb
Sep  6 15:28:02.379: INFO: Got endpoints: latency-svc-d9ppr [749.993009ms]
Sep  6 15:28:02.388: INFO: Created: latency-svc-nt9jl
Sep  6 15:28:02.429: INFO: Got endpoints: latency-svc-7rw7s [748.574059ms]
Sep  6 15:28:02.437: INFO: Created: latency-svc-v6jln
Sep  6 15:28:02.478: INFO: Got endpoints: latency-svc-2v4pr [749.918825ms]
Sep  6 15:28:02.485: INFO: Created: latency-svc-bgrws
Sep  6 15:28:02.528: INFO: Got endpoints: latency-svc-4xbdn [748.470806ms]
Sep  6 15:28:02.536: INFO: Created: latency-svc-hdxdm
Sep  6 15:28:02.579: INFO: Got endpoints: latency-svc-vtjkp [748.968867ms]
Sep  6 15:28:02.589: INFO: Created: latency-svc-2sxnl
Sep  6 15:28:02.628: INFO: Got endpoints: latency-svc-fvvx8 [744.198297ms]
Sep  6 15:28:02.636: INFO: Created: latency-svc-rw7wb
Sep  6 15:28:02.679: INFO: Got endpoints: latency-svc-hnwbl [750.626769ms]
Sep  6 15:28:02.687: INFO: Created: latency-svc-rtxfr
Sep  6 15:28:02.728: INFO: Got endpoints: latency-svc-btjmm [749.760233ms]
Sep  6 15:28:02.736: INFO: Created: latency-svc-l2sqk
Sep  6 15:28:02.780: INFO: Got endpoints: latency-svc-hmc8s [751.049007ms]
Sep  6 15:28:02.787: INFO: Created: latency-svc-6pxbd
Sep  6 15:28:02.828: INFO: Got endpoints: latency-svc-sln8w [749.536973ms]
Sep  6 15:28:02.835: INFO: Created: latency-svc-dw4j9
Sep  6 15:28:02.878: INFO: Got endpoints: latency-svc-cmg2h [749.910871ms]
Sep  6 15:28:02.885: INFO: Created: latency-svc-l8mpn
Sep  6 15:28:02.929: INFO: Got endpoints: latency-svc-xht2m [750.123285ms]
Sep  6 15:28:02.937: INFO: Created: latency-svc-96rlg
Sep  6 15:28:02.978: INFO: Got endpoints: latency-svc-htfrb [749.85356ms]
Sep  6 15:28:02.985: INFO: Created: latency-svc-27rdk
Sep  6 15:28:03.033: INFO: Got endpoints: latency-svc-bc7s2 [754.585566ms]
Sep  6 15:28:03.041: INFO: Created: latency-svc-5tt8b
Sep  6 15:28:03.078: INFO: Got endpoints: latency-svc-snhtb [749.670218ms]
Sep  6 15:28:03.086: INFO: Created: latency-svc-gqv6c
Sep  6 15:28:03.128: INFO: Got endpoints: latency-svc-nt9jl [749.784188ms]
Sep  6 15:28:03.139: INFO: Created: latency-svc-mn7bt
Sep  6 15:28:03.178: INFO: Got endpoints: latency-svc-v6jln [749.763795ms]
Sep  6 15:28:03.193: INFO: Created: latency-svc-mfrs5
Sep  6 15:28:03.228: INFO: Got endpoints: latency-svc-bgrws [750.187424ms]
Sep  6 15:28:03.235: INFO: Created: latency-svc-bcg2r
Sep  6 15:28:03.278: INFO: Got endpoints: latency-svc-hdxdm [750.223206ms]
Sep  6 15:28:03.287: INFO: Created: latency-svc-976rk
Sep  6 15:28:03.329: INFO: Got endpoints: latency-svc-2sxnl [749.569597ms]
Sep  6 15:28:03.335: INFO: Created: latency-svc-gd6j9
Sep  6 15:28:03.378: INFO: Got endpoints: latency-svc-rw7wb [750.179087ms]
Sep  6 15:28:03.389: INFO: Created: latency-svc-ngd7n
Sep  6 15:28:03.429: INFO: Got endpoints: latency-svc-rtxfr [749.88477ms]
Sep  6 15:28:03.437: INFO: Created: latency-svc-kr9nm
Sep  6 15:28:03.478: INFO: Got endpoints: latency-svc-l2sqk [749.957534ms]
Sep  6 15:28:03.488: INFO: Created: latency-svc-wcc5g
Sep  6 15:28:03.528: INFO: Got endpoints: latency-svc-6pxbd [748.383345ms]
Sep  6 15:28:03.535: INFO: Created: latency-svc-xdn6m
Sep  6 15:28:03.578: INFO: Got endpoints: latency-svc-dw4j9 [750.131472ms]
Sep  6 15:28:03.586: INFO: Created: latency-svc-dfkv7
Sep  6 15:28:03.628: INFO: Got endpoints: latency-svc-l8mpn [750.534075ms]
Sep  6 15:28:03.635: INFO: Created: latency-svc-mksrm
Sep  6 15:28:03.678: INFO: Got endpoints: latency-svc-96rlg [749.500857ms]
Sep  6 15:28:03.686: INFO: Created: latency-svc-cxfgg
Sep  6 15:28:03.728: INFO: Got endpoints: latency-svc-27rdk [750.556799ms]
Sep  6 15:28:03.737: INFO: Created: latency-svc-sgg7j
Sep  6 15:28:03.778: INFO: Got endpoints: latency-svc-5tt8b [745.124232ms]
Sep  6 15:28:03.789: INFO: Created: latency-svc-nbdzg
Sep  6 15:28:03.828: INFO: Got endpoints: latency-svc-gqv6c [749.537972ms]
Sep  6 15:28:03.836: INFO: Created: latency-svc-x6jvv
Sep  6 15:28:03.879: INFO: Got endpoints: latency-svc-mn7bt [750.578625ms]
Sep  6 15:28:03.889: INFO: Created: latency-svc-ntdrv
Sep  6 15:28:03.929: INFO: Got endpoints: latency-svc-mfrs5 [750.938005ms]
Sep  6 15:28:03.942: INFO: Created: latency-svc-c67dk
Sep  6 15:28:03.978: INFO: Got endpoints: latency-svc-bcg2r [750.262804ms]
Sep  6 15:28:03.989: INFO: Created: latency-svc-9kdgf
Sep  6 15:28:04.029: INFO: Got endpoints: latency-svc-976rk [750.692003ms]
Sep  6 15:28:04.038: INFO: Created: latency-svc-lg6sq
Sep  6 15:28:04.082: INFO: Got endpoints: latency-svc-gd6j9 [753.675794ms]
Sep  6 15:28:04.094: INFO: Created: latency-svc-2pfnt
Sep  6 15:28:04.130: INFO: Got endpoints: latency-svc-ngd7n [751.279506ms]
Sep  6 15:28:04.150: INFO: Created: latency-svc-jqwz2
Sep  6 15:28:04.207: INFO: Got endpoints: latency-svc-kr9nm [778.084259ms]
Sep  6 15:28:04.223: INFO: Created: latency-svc-kwbwh
Sep  6 15:28:04.228: INFO: Got endpoints: latency-svc-wcc5g [749.630104ms]
Sep  6 15:28:04.240: INFO: Created: latency-svc-j7zkq
Sep  6 15:28:04.281: INFO: Got endpoints: latency-svc-xdn6m [752.175143ms]
Sep  6 15:28:04.295: INFO: Created: latency-svc-ghrpb
Sep  6 15:28:04.329: INFO: Got endpoints: latency-svc-dfkv7 [750.348408ms]
Sep  6 15:28:04.339: INFO: Created: latency-svc-2k4wd
Sep  6 15:28:04.379: INFO: Got endpoints: latency-svc-mksrm [750.195217ms]
Sep  6 15:28:04.422: INFO: Created: latency-svc-9wflb
Sep  6 15:28:04.431: INFO: Got endpoints: latency-svc-cxfgg [752.438636ms]
Sep  6 15:28:04.442: INFO: Created: latency-svc-q4cjw
Sep  6 15:28:04.478: INFO: Got endpoints: latency-svc-sgg7j [749.591717ms]
Sep  6 15:28:04.486: INFO: Created: latency-svc-vb75v
Sep  6 15:28:04.529: INFO: Got endpoints: latency-svc-nbdzg [750.741621ms]
Sep  6 15:28:04.537: INFO: Created: latency-svc-xpht4
Sep  6 15:28:04.578: INFO: Got endpoints: latency-svc-x6jvv [750.182868ms]
Sep  6 15:28:04.585: INFO: Created: latency-svc-kq9xm
Sep  6 15:28:04.628: INFO: Got endpoints: latency-svc-ntdrv [749.017776ms]
Sep  6 15:28:04.637: INFO: Created: latency-svc-ddp58
Sep  6 15:28:04.679: INFO: Got endpoints: latency-svc-c67dk [749.242429ms]
Sep  6 15:28:04.693: INFO: Created: latency-svc-4dgcz
Sep  6 15:28:04.729: INFO: Got endpoints: latency-svc-9kdgf [750.566498ms]
Sep  6 15:28:04.738: INFO: Created: latency-svc-l8j76
Sep  6 15:28:04.779: INFO: Got endpoints: latency-svc-lg6sq [749.723481ms]
Sep  6 15:28:04.787: INFO: Created: latency-svc-2z5hz
Sep  6 15:28:04.828: INFO: Got endpoints: latency-svc-2pfnt [746.190837ms]
Sep  6 15:28:04.840: INFO: Created: latency-svc-plsn6
Sep  6 15:28:04.878: INFO: Got endpoints: latency-svc-jqwz2 [748.579483ms]
Sep  6 15:28:04.891: INFO: Created: latency-svc-c9krb
Sep  6 15:28:04.930: INFO: Got endpoints: latency-svc-kwbwh [722.88553ms]
Sep  6 15:28:04.947: INFO: Created: latency-svc-7f8vz
Sep  6 15:28:04.978: INFO: Got endpoints: latency-svc-j7zkq [749.869556ms]
Sep  6 15:28:04.990: INFO: Created: latency-svc-vq4tk
Sep  6 15:28:05.034: INFO: Got endpoints: latency-svc-ghrpb [752.978994ms]
Sep  6 15:28:05.046: INFO: Created: latency-svc-kxwfv
Sep  6 15:28:05.079: INFO: Got endpoints: latency-svc-2k4wd [750.193824ms]
Sep  6 15:28:05.088: INFO: Created: latency-svc-4tlds
Sep  6 15:28:05.131: INFO: Got endpoints: latency-svc-9wflb [752.332277ms]
Sep  6 15:28:05.142: INFO: Created: latency-svc-d4kx8
Sep  6 15:28:05.178: INFO: Got endpoints: latency-svc-q4cjw [747.656934ms]
Sep  6 15:28:05.189: INFO: Created: latency-svc-kt87n
Sep  6 15:28:05.228: INFO: Got endpoints: latency-svc-vb75v [750.100971ms]
Sep  6 15:28:05.239: INFO: Created: latency-svc-mmdjc
Sep  6 15:28:05.280: INFO: Got endpoints: latency-svc-xpht4 [750.71965ms]
Sep  6 15:28:05.291: INFO: Created: latency-svc-rfqpz
Sep  6 15:28:05.330: INFO: Got endpoints: latency-svc-kq9xm [752.023386ms]
Sep  6 15:28:05.339: INFO: Created: latency-svc-z4xmb
Sep  6 15:28:05.378: INFO: Got endpoints: latency-svc-ddp58 [750.244641ms]
Sep  6 15:28:05.392: INFO: Created: latency-svc-6tlpn
Sep  6 15:28:05.429: INFO: Got endpoints: latency-svc-4dgcz [750.194908ms]
Sep  6 15:28:05.440: INFO: Created: latency-svc-wplcv
Sep  6 15:28:05.479: INFO: Got endpoints: latency-svc-l8j76 [749.75159ms]
Sep  6 15:28:05.488: INFO: Created: latency-svc-vh5n6
Sep  6 15:28:05.529: INFO: Got endpoints: latency-svc-2z5hz [749.633061ms]
Sep  6 15:28:05.538: INFO: Created: latency-svc-v6d8f
Sep  6 15:28:05.578: INFO: Got endpoints: latency-svc-plsn6 [749.747185ms]
Sep  6 15:28:05.587: INFO: Created: latency-svc-s9z8k
Sep  6 15:28:05.630: INFO: Got endpoints: latency-svc-c9krb [751.239093ms]
Sep  6 15:28:05.638: INFO: Created: latency-svc-88wbw
Sep  6 15:28:05.678: INFO: Got endpoints: latency-svc-7f8vz [747.602152ms]
Sep  6 15:28:05.686: INFO: Created: latency-svc-sp7zl
Sep  6 15:28:05.729: INFO: Got endpoints: latency-svc-vq4tk [750.553296ms]
Sep  6 15:28:05.736: INFO: Created: latency-svc-lxtwd
Sep  6 15:28:05.779: INFO: Got endpoints: latency-svc-kxwfv [745.473545ms]
Sep  6 15:28:05.788: INFO: Created: latency-svc-p44lx
Sep  6 15:28:05.828: INFO: Got endpoints: latency-svc-4tlds [749.305667ms]
Sep  6 15:28:05.835: INFO: Created: latency-svc-vk9t6
Sep  6 15:28:05.878: INFO: Got endpoints: latency-svc-d4kx8 [747.151529ms]
Sep  6 15:28:05.891: INFO: Created: latency-svc-qbgqk
Sep  6 15:28:05.929: INFO: Got endpoints: latency-svc-kt87n [750.210143ms]
Sep  6 15:28:05.938: INFO: Created: latency-svc-8b5h5
Sep  6 15:28:05.979: INFO: Got endpoints: latency-svc-mmdjc [750.710047ms]
Sep  6 15:28:05.989: INFO: Created: latency-svc-jqm8p
Sep  6 15:28:06.028: INFO: Got endpoints: latency-svc-rfqpz [748.506948ms]
Sep  6 15:28:06.036: INFO: Created: latency-svc-xdw92
Sep  6 15:28:06.079: INFO: Got endpoints: latency-svc-z4xmb [749.183645ms]
Sep  6 15:28:06.092: INFO: Created: latency-svc-8m57c
Sep  6 15:28:06.129: INFO: Got endpoints: latency-svc-6tlpn [750.19876ms]
Sep  6 15:28:06.137: INFO: Created: latency-svc-6ldgz
Sep  6 15:28:06.178: INFO: Got endpoints: latency-svc-wplcv [749.169258ms]
Sep  6 15:28:06.186: INFO: Created: latency-svc-th7gx
Sep  6 15:28:06.228: INFO: Got endpoints: latency-svc-vh5n6 [749.197163ms]
Sep  6 15:28:06.236: INFO: Created: latency-svc-5qrs2
Sep  6 15:28:06.278: INFO: Got endpoints: latency-svc-v6d8f [749.747762ms]
Sep  6 15:28:06.287: INFO: Created: latency-svc-t2g7v
Sep  6 15:28:06.329: INFO: Got endpoints: latency-svc-s9z8k [750.573873ms]
Sep  6 15:28:06.336: INFO: Created: latency-svc-bbl75
Sep  6 15:28:06.379: INFO: Got endpoints: latency-svc-88wbw [748.988176ms]
Sep  6 15:28:06.392: INFO: Created: latency-svc-r5zs9
Sep  6 15:28:06.428: INFO: Got endpoints: latency-svc-sp7zl [749.95977ms]
Sep  6 15:28:06.439: INFO: Created: latency-svc-rskgj
Sep  6 15:28:06.478: INFO: Got endpoints: latency-svc-lxtwd [749.172453ms]
Sep  6 15:28:06.485: INFO: Created: latency-svc-5qlxg
Sep  6 15:28:06.528: INFO: Got endpoints: latency-svc-p44lx [748.595733ms]
Sep  6 15:28:06.535: INFO: Created: latency-svc-8ds52
Sep  6 15:28:06.578: INFO: Got endpoints: latency-svc-vk9t6 [750.178375ms]
Sep  6 15:28:06.585: INFO: Created: latency-svc-ms995
Sep  6 15:28:06.628: INFO: Got endpoints: latency-svc-qbgqk [750.129521ms]
Sep  6 15:28:06.637: INFO: Created: latency-svc-pc5gr
Sep  6 15:28:06.680: INFO: Got endpoints: latency-svc-8b5h5 [751.010874ms]
Sep  6 15:28:06.693: INFO: Created: latency-svc-8kd4f
Sep  6 15:28:06.733: INFO: Got endpoints: latency-svc-jqm8p [753.654724ms]
Sep  6 15:28:06.742: INFO: Created: latency-svc-rqjq2
Sep  6 15:28:06.779: INFO: Got endpoints: latency-svc-xdw92 [750.08399ms]
Sep  6 15:28:06.785: INFO: Created: latency-svc-rp4hh
Sep  6 15:28:06.828: INFO: Got endpoints: latency-svc-8m57c [748.935362ms]
Sep  6 15:28:06.836: INFO: Created: latency-svc-27k85
Sep  6 15:28:06.878: INFO: Got endpoints: latency-svc-6ldgz [749.19214ms]
Sep  6 15:28:06.885: INFO: Created: latency-svc-qvbzr
Sep  6 15:28:06.928: INFO: Got endpoints: latency-svc-th7gx [749.865167ms]
Sep  6 15:28:06.936: INFO: Created: latency-svc-vn5zj
Sep  6 15:28:06.978: INFO: Got endpoints: latency-svc-5qrs2 [750.162205ms]
Sep  6 15:28:06.986: INFO: Created: latency-svc-6zbgj
Sep  6 15:28:07.028: INFO: Got endpoints: latency-svc-t2g7v [749.85168ms]
Sep  6 15:28:07.035: INFO: Created: latency-svc-2ct8b
Sep  6 15:28:07.079: INFO: Got endpoints: latency-svc-bbl75 [749.704706ms]
Sep  6 15:28:07.087: INFO: Created: latency-svc-cph5m
Sep  6 15:28:07.132: INFO: Got endpoints: latency-svc-r5zs9 [752.750149ms]
Sep  6 15:28:07.140: INFO: Created: latency-svc-swlxd
Sep  6 15:28:07.178: INFO: Got endpoints: latency-svc-rskgj [749.467787ms]
Sep  6 15:28:07.185: INFO: Created: latency-svc-wb2pp
Sep  6 15:28:07.228: INFO: Got endpoints: latency-svc-5qlxg [749.730719ms]
Sep  6 15:28:07.241: INFO: Created: latency-svc-7ndhk
Sep  6 15:28:07.278: INFO: Got endpoints: latency-svc-8ds52 [750.583173ms]
Sep  6 15:28:07.286: INFO: Created: latency-svc-jd7pg
Sep  6 15:28:07.329: INFO: Got endpoints: latency-svc-ms995 [750.148237ms]
Sep  6 15:28:07.341: INFO: Created: latency-svc-m94rj
Sep  6 15:28:07.379: INFO: Got endpoints: latency-svc-pc5gr [750.186649ms]
Sep  6 15:28:07.388: INFO: Created: latency-svc-mcfnz
Sep  6 15:28:07.429: INFO: Got endpoints: latency-svc-8kd4f [749.366052ms]
Sep  6 15:28:07.437: INFO: Created: latency-svc-b8v5x
Sep  6 15:28:07.478: INFO: Got endpoints: latency-svc-rqjq2 [745.169509ms]
Sep  6 15:28:07.486: INFO: Created: latency-svc-l4p5d
Sep  6 15:28:07.530: INFO: Got endpoints: latency-svc-rp4hh [750.936159ms]
Sep  6 15:28:07.539: INFO: Created: latency-svc-x7nfx
Sep  6 15:28:07.579: INFO: Got endpoints: latency-svc-27k85 [750.559588ms]
Sep  6 15:28:07.599: INFO: Created: latency-svc-2szsw
Sep  6 15:28:07.629: INFO: Got endpoints: latency-svc-qvbzr [750.910339ms]
Sep  6 15:28:07.642: INFO: Created: latency-svc-b7724
Sep  6 15:28:07.678: INFO: Got endpoints: latency-svc-vn5zj [750.330668ms]
Sep  6 15:28:07.686: INFO: Created: latency-svc-zd5l6
Sep  6 15:28:07.729: INFO: Got endpoints: latency-svc-6zbgj [750.55573ms]
Sep  6 15:28:07.737: INFO: Created: latency-svc-d4hxh
Sep  6 15:28:07.778: INFO: Got endpoints: latency-svc-2ct8b [749.77236ms]
Sep  6 15:28:07.828: INFO: Got endpoints: latency-svc-cph5m [749.122173ms]
Sep  6 15:28:07.878: INFO: Got endpoints: latency-svc-swlxd [746.507324ms]
Sep  6 15:28:07.928: INFO: Got endpoints: latency-svc-wb2pp [750.008531ms]
Sep  6 15:28:07.979: INFO: Got endpoints: latency-svc-7ndhk [750.864947ms]
Sep  6 15:28:08.029: INFO: Got endpoints: latency-svc-jd7pg [750.643812ms]
Sep  6 15:28:08.078: INFO: Got endpoints: latency-svc-m94rj [749.599084ms]
Sep  6 15:28:08.129: INFO: Got endpoints: latency-svc-mcfnz [749.938559ms]
Sep  6 15:28:08.179: INFO: Got endpoints: latency-svc-b8v5x [749.518031ms]
Sep  6 15:28:08.229: INFO: Got endpoints: latency-svc-l4p5d [751.130323ms]
Sep  6 15:28:08.278: INFO: Got endpoints: latency-svc-x7nfx [748.176769ms]
Sep  6 15:28:08.329: INFO: Got endpoints: latency-svc-2szsw [749.852546ms]
Sep  6 15:28:08.379: INFO: Got endpoints: latency-svc-b7724 [749.704153ms]
Sep  6 15:28:08.431: INFO: Got endpoints: latency-svc-zd5l6 [752.500006ms]
Sep  6 15:28:08.479: INFO: Got endpoints: latency-svc-d4hxh [750.292192ms]
Sep  6 15:28:08.479: INFO: Latencies: [12.149312ms 17.127704ms 19.046199ms 22.126477ms 31.302761ms 36.014694ms 42.041783ms 51.556979ms 57.565092ms 65.911559ms 70.33239ms 84.031835ms 84.43059ms 86.618302ms 92.519044ms 96.049248ms 101.609804ms 103.022116ms 103.512089ms 110.332944ms 120.155276ms 120.878307ms 121.364682ms 122.257757ms 122.703505ms 125.206383ms 126.811097ms 126.936585ms 128.989518ms 129.950653ms 130.25907ms 132.758343ms 134.112402ms 135.144512ms 164.308944ms 188.685867ms 222.95769ms 267.212734ms 312.175017ms 355.929765ms 403.362441ms 442.170335ms 483.831936ms 529.544983ms 573.018776ms 624.34624ms 663.256003ms 705.77692ms 722.88553ms 740.582591ms 743.832331ms 744.198297ms 745.124232ms 745.169509ms 745.325427ms 745.473545ms 746.190837ms 746.507324ms 747.151529ms 747.602152ms 747.656934ms 748.176769ms 748.232661ms 748.383345ms 748.470806ms 748.506948ms 748.574059ms 748.579483ms 748.595733ms 748.759556ms 748.922013ms 748.935362ms 748.968867ms 748.988176ms 749.017776ms 749.122173ms 749.169258ms 749.172453ms 749.183645ms 749.19214ms 749.197163ms 749.231469ms 749.242429ms 749.305667ms 749.366052ms 749.467787ms 749.500857ms 749.518031ms 749.526969ms 749.536973ms 749.537972ms 749.569597ms 749.591717ms 749.599084ms 749.602592ms 749.630104ms 749.633061ms 749.670218ms 749.704153ms 749.704706ms 749.723481ms 749.730626ms 749.730719ms 749.747185ms 749.747762ms 749.75159ms 749.760233ms 749.761346ms 749.763795ms 749.77236ms 749.772681ms 749.784188ms 749.85168ms 749.852546ms 749.85356ms 749.865167ms 749.869556ms 749.880777ms 749.882028ms 749.88477ms 749.909844ms 749.910871ms 749.918825ms 749.927248ms 749.938559ms 749.948775ms 749.957534ms 749.95977ms 749.992417ms 749.993009ms 750.008531ms 750.021357ms 750.031214ms 750.08399ms 750.100971ms 750.123285ms 750.129521ms 750.131472ms 750.148237ms 750.156317ms 750.162205ms 750.177943ms 750.178375ms 750.179087ms 750.182868ms 750.186649ms 750.187424ms 750.193824ms 750.194908ms 750.195217ms 750.19876ms 750.210143ms 750.211086ms 750.223206ms 750.224549ms 750.244641ms 750.256539ms 750.262804ms 750.287264ms 750.292192ms 750.330668ms 750.348408ms 750.534075ms 750.553296ms 750.55573ms 750.556799ms 750.559588ms 750.566498ms 750.573873ms 750.578625ms 750.583173ms 750.626769ms 750.643812ms 750.692003ms 750.710047ms 750.71965ms 750.741621ms 750.864947ms 750.910339ms 750.936159ms 750.938005ms 751.010874ms 751.049007ms 751.130323ms 751.239093ms 751.279506ms 751.61324ms 752.023386ms 752.175143ms 752.332277ms 752.438636ms 752.500006ms 752.750149ms 752.978994ms 753.654724ms 753.675794ms 754.585566ms 755.171745ms 756.970904ms 778.084259ms]
Sep  6 15:28:08.479: INFO: 50 %ile: 749.723481ms
Sep  6 15:28:08.479: INFO: 90 %ile: 750.938005ms
Sep  6 15:28:08.479: INFO: 99 %ile: 756.970904ms
Sep  6 15:28:08.479: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:28:08.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6065" for this suite.
Sep  6 15:28:20.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:28:20.552: INFO: namespace svc-latency-6065 deletion completed in 12.070015216s

• [SLOW TEST:21.812 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:28:20.553: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-f514c691-d0ba-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:28:20.585: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5151995-d0ba-11e9-9ba3-f679ea11f394" in namespace "projected-1613" to be "success or failure"
Sep  6 15:28:20.587: INFO: Pod "pod-projected-configmaps-f5151995-d0ba-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10772ms
Sep  6 15:28:22.590: INFO: Pod "pod-projected-configmaps-f5151995-d0ba-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004943686s
STEP: Saw pod success
Sep  6 15:28:22.590: INFO: Pod "pod-projected-configmaps-f5151995-d0ba-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:28:22.591: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-configmaps-f5151995-d0ba-11e9-9ba3-f679ea11f394 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:28:22.605: INFO: Waiting for pod pod-projected-configmaps-f5151995-d0ba-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:28:22.607: INFO: Pod pod-projected-configmaps-f5151995-d0ba-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:28:22.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1613" for this suite.
Sep  6 15:28:28.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:28:28.672: INFO: namespace projected-1613 deletion completed in 6.063099218s

• [SLOW TEST:8.119 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:28:28.672: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-dbxh
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 15:28:28.722: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-dbxh" in namespace "subpath-8350" to be "success or failure"
Sep  6 15:28:28.728: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.337ms
Sep  6 15:28:30.731: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009429737s
Sep  6 15:28:32.734: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 4.012032485s
Sep  6 15:28:34.737: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 6.015010755s
Sep  6 15:28:36.739: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 8.017559098s
Sep  6 15:28:38.742: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 10.020081632s
Sep  6 15:28:40.745: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 12.023003759s
Sep  6 15:28:42.748: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 14.026030167s
Sep  6 15:28:44.757: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 16.035407231s
Sep  6 15:28:46.760: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 18.038128125s
Sep  6 15:28:48.763: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Running", Reason="", readiness=true. Elapsed: 20.040631895s
Sep  6 15:28:50.765: INFO: Pod "pod-subpath-test-secret-dbxh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.043142357s
STEP: Saw pod success
Sep  6 15:28:50.765: INFO: Pod "pod-subpath-test-secret-dbxh" satisfied condition "success or failure"
Sep  6 15:28:50.767: INFO: Trying to get logs from node metalk8s-23 pod pod-subpath-test-secret-dbxh container test-container-subpath-secret-dbxh: <nil>
STEP: delete the pod
Sep  6 15:28:50.776: INFO: Waiting for pod pod-subpath-test-secret-dbxh to disappear
Sep  6 15:28:50.778: INFO: Pod pod-subpath-test-secret-dbxh no longer exists
STEP: Deleting pod pod-subpath-test-secret-dbxh
Sep  6 15:28:50.778: INFO: Deleting pod "pod-subpath-test-secret-dbxh" in namespace "subpath-8350"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:28:50.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8350" for this suite.
Sep  6 15:28:56.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:28:56.856: INFO: namespace subpath-8350 deletion completed in 6.07471261s

• [SLOW TEST:28.184 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:28:56.856: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Sep  6 15:28:56.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-8261'
Sep  6 15:28:57.050: INFO: stderr: ""
Sep  6 15:28:57.050: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 15:28:57.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8261'
Sep  6 15:28:57.133: INFO: stderr: ""
Sep  6 15:28:57.133: INFO: stdout: "update-demo-nautilus-5lv9d update-demo-nautilus-njxrj "
Sep  6 15:28:57.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5lv9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8261'
Sep  6 15:28:57.196: INFO: stderr: ""
Sep  6 15:28:57.196: INFO: stdout: ""
Sep  6 15:28:57.196: INFO: update-demo-nautilus-5lv9d is created but not running
Sep  6 15:29:02.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8261'
Sep  6 15:29:02.275: INFO: stderr: ""
Sep  6 15:29:02.275: INFO: stdout: "update-demo-nautilus-5lv9d update-demo-nautilus-njxrj "
Sep  6 15:29:02.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5lv9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8261'
Sep  6 15:29:02.339: INFO: stderr: ""
Sep  6 15:29:02.339: INFO: stdout: "true"
Sep  6 15:29:02.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-5lv9d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8261'
Sep  6 15:29:02.407: INFO: stderr: ""
Sep  6 15:29:02.407: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 15:29:02.407: INFO: validating pod update-demo-nautilus-5lv9d
Sep  6 15:29:02.410: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 15:29:02.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 15:29:02.411: INFO: update-demo-nautilus-5lv9d is verified up and running
Sep  6 15:29:02.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-njxrj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8261'
Sep  6 15:29:02.492: INFO: stderr: ""
Sep  6 15:29:02.492: INFO: stdout: "true"
Sep  6 15:29:02.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-njxrj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8261'
Sep  6 15:29:02.560: INFO: stderr: ""
Sep  6 15:29:02.560: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 15:29:02.560: INFO: validating pod update-demo-nautilus-njxrj
Sep  6 15:29:02.564: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 15:29:02.564: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 15:29:02.564: INFO: update-demo-nautilus-njxrj is verified up and running
STEP: rolling-update to new replication controller
Sep  6 15:29:02.565: INFO: scanned /root for discovery docs: <nil>
Sep  6 15:29:02.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8261'
Sep  6 15:29:22.864: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  6 15:29:22.864: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 15:29:22.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8261'
Sep  6 15:29:22.947: INFO: stderr: ""
Sep  6 15:29:22.947: INFO: stdout: "update-demo-kitten-cq2f9 update-demo-kitten-jfrjs "
Sep  6 15:29:22.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-kitten-cq2f9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8261'
Sep  6 15:29:23.021: INFO: stderr: ""
Sep  6 15:29:23.021: INFO: stdout: "true"
Sep  6 15:29:23.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-kitten-cq2f9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8261'
Sep  6 15:29:23.099: INFO: stderr: ""
Sep  6 15:29:23.099: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  6 15:29:23.099: INFO: validating pod update-demo-kitten-cq2f9
Sep  6 15:29:23.102: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  6 15:29:23.102: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  6 15:29:23.102: INFO: update-demo-kitten-cq2f9 is verified up and running
Sep  6 15:29:23.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-kitten-jfrjs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8261'
Sep  6 15:29:23.168: INFO: stderr: ""
Sep  6 15:29:23.168: INFO: stdout: "true"
Sep  6 15:29:23.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-kitten-jfrjs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8261'
Sep  6 15:29:23.233: INFO: stderr: ""
Sep  6 15:29:23.233: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  6 15:29:23.233: INFO: validating pod update-demo-kitten-jfrjs
Sep  6 15:29:23.236: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  6 15:29:23.236: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  6 15:29:23.236: INFO: update-demo-kitten-jfrjs is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:29:23.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8261" for this suite.
Sep  6 15:29:51.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:29:51.327: INFO: namespace kubectl-8261 deletion completed in 28.088330017s

• [SLOW TEST:54.471 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:29:51.327: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  6 15:29:51.354: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:29:55.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-660" for this suite.
Sep  6 15:30:01.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:30:01.195: INFO: namespace init-container-660 deletion completed in 6.067010012s

• [SLOW TEST:9.868 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:30:01.195: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Sep  6 15:30:01.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 --namespace=kubectl-6799 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep  6 15:30:02.285: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep  6 15:30:02.286: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:30:04.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6799" for this suite.
Sep  6 15:30:10.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:30:10.392: INFO: namespace kubectl-6799 deletion completed in 6.087703933s

• [SLOW TEST:9.197 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:30:10.393: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  6 15:30:10.654: INFO: Pod name wrapped-volume-race-36a22c4b-d0bb-11e9-9ba3-f679ea11f394: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-36a22c4b-d0bb-11e9-9ba3-f679ea11f394 in namespace emptydir-wrapper-2005, will wait for the garbage collector to delete the pods
Sep  6 15:30:24.770: INFO: Deleting ReplicationController wrapped-volume-race-36a22c4b-d0bb-11e9-9ba3-f679ea11f394 took: 4.785809ms
Sep  6 15:30:25.173: INFO: Terminating ReplicationController wrapped-volume-race-36a22c4b-d0bb-11e9-9ba3-f679ea11f394 pods took: 402.772366ms
STEP: Creating RC which spawns configmap-volume pods
Sep  6 15:31:09.684: INFO: Pod name wrapped-volume-race-59dea3e3-d0bb-11e9-9ba3-f679ea11f394: Found 0 pods out of 5
Sep  6 15:31:14.689: INFO: Pod name wrapped-volume-race-59dea3e3-d0bb-11e9-9ba3-f679ea11f394: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-59dea3e3-d0bb-11e9-9ba3-f679ea11f394 in namespace emptydir-wrapper-2005, will wait for the garbage collector to delete the pods
Sep  6 15:31:26.773: INFO: Deleting ReplicationController wrapped-volume-race-59dea3e3-d0bb-11e9-9ba3-f679ea11f394 took: 5.479093ms
Sep  6 15:31:27.274: INFO: Terminating ReplicationController wrapped-volume-race-59dea3e3-d0bb-11e9-9ba3-f679ea11f394 pods took: 500.240336ms
STEP: Creating RC which spawns configmap-volume pods
Sep  6 15:32:09.087: INFO: Pod name wrapped-volume-race-7d467e44-d0bb-11e9-9ba3-f679ea11f394: Found 0 pods out of 5
Sep  6 15:32:14.092: INFO: Pod name wrapped-volume-race-7d467e44-d0bb-11e9-9ba3-f679ea11f394: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7d467e44-d0bb-11e9-9ba3-f679ea11f394 in namespace emptydir-wrapper-2005, will wait for the garbage collector to delete the pods
Sep  6 15:32:26.167: INFO: Deleting ReplicationController wrapped-volume-race-7d467e44-d0bb-11e9-9ba3-f679ea11f394 took: 5.095166ms
Sep  6 15:32:26.669: INFO: Terminating ReplicationController wrapped-volume-race-7d467e44-d0bb-11e9-9ba3-f679ea11f394 pods took: 501.851948ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:33:09.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2005" for this suite.
Sep  6 15:33:15.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:33:15.939: INFO: namespace emptydir-wrapper-2005 deletion completed in 6.07322084s

• [SLOW TEST:185.547 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:33:15.939: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 15:33:15.983: INFO: Number of nodes with available pods: 0
Sep  6 15:33:15.983: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 15:33:16.988: INFO: Number of nodes with available pods: 0
Sep  6 15:33:16.988: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 15:33:17.988: INFO: Number of nodes with available pods: 1
Sep  6 15:33:17.988: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  6 15:33:18.002: INFO: Number of nodes with available pods: 0
Sep  6 15:33:18.002: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 15:33:19.009: INFO: Number of nodes with available pods: 0
Sep  6 15:33:19.009: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 15:33:20.008: INFO: Number of nodes with available pods: 0
Sep  6 15:33:20.008: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 15:33:21.008: INFO: Number of nodes with available pods: 0
Sep  6 15:33:21.008: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 15:33:22.008: INFO: Number of nodes with available pods: 0
Sep  6 15:33:22.008: INFO: Node metalk8s-23 is running more than one daemon pod
Sep  6 15:33:23.008: INFO: Number of nodes with available pods: 1
Sep  6 15:33:23.008: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-427, will wait for the garbage collector to delete the pods
Sep  6 15:33:23.069: INFO: Deleting DaemonSet.extensions daemon-set took: 5.134518ms
Sep  6 15:33:23.469: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.249364ms
Sep  6 15:33:28.674: INFO: Number of nodes with available pods: 0
Sep  6 15:33:28.674: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 15:33:28.676: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-427/daemonsets","resourceVersion":"14274"},"items":null}

Sep  6 15:33:28.677: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-427/pods","resourceVersion":"14274"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:33:28.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-427" for this suite.
Sep  6 15:33:34.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:33:34.752: INFO: namespace daemonsets-427 deletion completed in 6.069208265s

• [SLOW TEST:18.813 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:33:34.753: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-1557
Sep  6 15:33:36.786: INFO: Started pod liveness-http in namespace container-probe-1557
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 15:33:36.788: INFO: Initial restart count of pod liveness-http is 0
Sep  6 15:33:54.821: INFO: Restart count of pod container-probe-1557/liveness-http is now 1 (18.032815905s elapsed)
Sep  6 15:34:14.854: INFO: Restart count of pod container-probe-1557/liveness-http is now 2 (38.066487386s elapsed)
Sep  6 15:34:34.881: INFO: Restart count of pod container-probe-1557/liveness-http is now 3 (58.093270591s elapsed)
Sep  6 15:34:54.914: INFO: Restart count of pod container-probe-1557/liveness-http is now 4 (1m18.125907298s elapsed)
Sep  6 15:35:56.070: INFO: Restart count of pod container-probe-1557/liveness-http is now 5 (2m19.282595708s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:35:56.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1557" for this suite.
Sep  6 15:36:02.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:36:02.161: INFO: namespace container-probe-1557 deletion completed in 6.068825958s

• [SLOW TEST:147.409 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:36:02.162: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:36:02.195: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  6 15:36:07.197: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 15:36:07.197: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  6 15:36:09.200: INFO: Creating deployment "test-rollover-deployment"
Sep  6 15:36:09.205: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  6 15:36:11.210: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  6 15:36:11.214: INFO: Ensure that both replica sets have 1 created replica
Sep  6 15:36:11.218: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  6 15:36:11.222: INFO: Updating deployment test-rollover-deployment
Sep  6 15:36:11.222: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  6 15:36:13.226: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  6 15:36:13.229: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  6 15:36:13.232: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 15:36:13.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380972, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 15:36:15.237: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 15:36:15.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380972, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 15:36:17.238: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 15:36:17.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380972, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 15:36:19.237: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 15:36:19.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380972, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 15:36:21.259: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 15:36:21.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380972, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703380969, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 15:36:23.237: INFO: 
Sep  6 15:36:23.237: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 15:36:23.243: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5780,SelfLink:/apis/apps/v1/namespaces/deployment-5780/deployments/test-rollover-deployment,UID:0c66f8f7-d0bc-11e9-b447-fa163e0a078f,ResourceVersion:14676,Generation:2,CreationTimestamp:2019-09-06 15:36:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-06 15:36:09 +0000 UTC 2019-09-06 15:36:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-06 15:36:22 +0000 UTC 2019-09-06 15:36:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-659c699649" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 15:36:23.246: INFO: New ReplicaSet "test-rollover-deployment-659c699649" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649,GenerateName:,Namespace:deployment-5780,SelfLink:/apis/apps/v1/namespaces/deployment-5780/replicasets/test-rollover-deployment-659c699649,UID:0d9b7546-d0bc-11e9-b447-fa163e0a078f,ResourceVersion:14665,Generation:2,CreationTimestamp:2019-09-06 15:36:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0c66f8f7-d0bc-11e9-b447-fa163e0a078f 0xc002da06b7 0xc002da06b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 15:36:23.246: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  6 15:36:23.247: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5780,SelfLink:/apis/apps/v1/namespaces/deployment-5780/replicasets/test-rollover-controller,UID:0839466f-d0bc-11e9-b447-fa163e0a078f,ResourceVersion:14675,Generation:2,CreationTimestamp:2019-09-06 15:36:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0c66f8f7-d0bc-11e9-b447-fa163e0a078f 0xc002da05a7 0xc002da05a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 15:36:23.247: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-7b45b6464,GenerateName:,Namespace:deployment-5780,SelfLink:/apis/apps/v1/namespaces/deployment-5780/replicasets/test-rollover-deployment-7b45b6464,UID:0c68333f-d0bc-11e9-b447-fa163e0a078f,ResourceVersion:14628,Generation:2,CreationTimestamp:2019-09-06 15:36:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0c66f8f7-d0bc-11e9-b447-fa163e0a078f 0xc002da0790 0xc002da0791}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 15:36:23.249: INFO: Pod "test-rollover-deployment-659c699649-8bc9w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649-8bc9w,GenerateName:test-rollover-deployment-659c699649-,Namespace:deployment-5780,SelfLink:/api/v1/namespaces/deployment-5780/pods/test-rollover-deployment-659c699649-8bc9w,UID:0d9e03ac-d0bc-11e9-b447-fa163e0a078f,ResourceVersion:14648,Generation:0,CreationTimestamp:2019-09-06 15:36:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.221.233/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-659c699649 0d9b7546-d0bc-11e9-b447-fa163e0a078f 0xc0014a3f57 0xc0014a3f58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-79f77 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-79f77,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-79f77 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0014a3fd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0014a3ff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:36:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:36:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:36:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:36:11 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:10.233.221.233,StartTime:2019-09-06 15:36:11 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-06 15:36:11 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://f25e0d38bb875af29e0d633f8a128ddacd9117285eb1783e287a7c40c6a91783}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:36:23.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5780" for this suite.
Sep  6 15:36:29.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:36:29.318: INFO: namespace deployment-5780 deletion completed in 6.067431253s

• [SLOW TEST:27.157 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:36:29.319: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0906 15:37:09.366942      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 15:37:09.367: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:37:09.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9755" for this suite.
Sep  6 15:37:15.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:37:15.499: INFO: namespace gc-9755 deletion completed in 6.130509096s

• [SLOW TEST:46.180 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:37:15.499: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:37:15.555: INFO: (0) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.626516ms)
Sep  6 15:37:15.560: INFO: (1) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.634851ms)
Sep  6 15:37:15.564: INFO: (2) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.082457ms)
Sep  6 15:37:15.570: INFO: (3) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.015939ms)
Sep  6 15:37:15.573: INFO: (4) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.835153ms)
Sep  6 15:37:15.576: INFO: (5) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.738562ms)
Sep  6 15:37:15.580: INFO: (6) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.567557ms)
Sep  6 15:37:15.584: INFO: (7) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.89624ms)
Sep  6 15:37:15.587: INFO: (8) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.944484ms)
Sep  6 15:37:15.593: INFO: (9) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.481353ms)
Sep  6 15:37:15.598: INFO: (10) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.793896ms)
Sep  6 15:37:15.602: INFO: (11) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.12354ms)
Sep  6 15:37:15.610: INFO: (12) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.825984ms)
Sep  6 15:37:15.615: INFO: (13) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.123462ms)
Sep  6 15:37:15.623: INFO: (14) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.452323ms)
Sep  6 15:37:15.627: INFO: (15) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.11794ms)
Sep  6 15:37:15.630: INFO: (16) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.193777ms)
Sep  6 15:37:15.633: INFO: (17) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.763907ms)
Sep  6 15:37:15.639: INFO: (18) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.169013ms)
Sep  6 15:37:15.642: INFO: (19) /api/v1/nodes/metalk8s-23:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.558711ms)
[AfterEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:37:15.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5071" for this suite.
Sep  6 15:37:21.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:37:21.710: INFO: namespace proxy-5071 deletion completed in 6.066329767s

• [SLOW TEST:6.211 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:37:21.711: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-37a21931-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:37:21.738: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37a27d2e-d0bc-11e9-9ba3-f679ea11f394" in namespace "projected-8446" to be "success or failure"
Sep  6 15:37:21.742: INFO: Pod "pod-projected-configmaps-37a27d2e-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 3.625065ms
Sep  6 15:37:23.745: INFO: Pod "pod-projected-configmaps-37a27d2e-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006598279s
STEP: Saw pod success
Sep  6 15:37:23.745: INFO: Pod "pod-projected-configmaps-37a27d2e-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:37:23.747: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-configmaps-37a27d2e-d0bc-11e9-9ba3-f679ea11f394 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:37:23.760: INFO: Waiting for pod pod-projected-configmaps-37a27d2e-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:37:23.762: INFO: Pod pod-projected-configmaps-37a27d2e-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:37:23.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8446" for this suite.
Sep  6 15:37:29.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:37:29.836: INFO: namespace projected-8446 deletion completed in 6.07102615s

• [SLOW TEST:8.126 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:37:29.837: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-3c7abd11-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:37:29.870: INFO: Waiting up to 5m0s for pod "pod-secrets-3c7b2038-d0bc-11e9-9ba3-f679ea11f394" in namespace "secrets-7751" to be "success or failure"
Sep  6 15:37:29.874: INFO: Pod "pod-secrets-3c7b2038-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351397ms
Sep  6 15:37:31.877: INFO: Pod "pod-secrets-3c7b2038-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00755794s
STEP: Saw pod success
Sep  6 15:37:31.877: INFO: Pod "pod-secrets-3c7b2038-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:37:31.879: INFO: Trying to get logs from node metalk8s-23 pod pod-secrets-3c7b2038-d0bc-11e9-9ba3-f679ea11f394 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:37:31.890: INFO: Waiting for pod pod-secrets-3c7b2038-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:37:31.892: INFO: Pod pod-secrets-3c7b2038-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:37:31.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7751" for this suite.
Sep  6 15:37:37.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:37:37.972: INFO: namespace secrets-7751 deletion completed in 6.078098826s

• [SLOW TEST:8.135 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:37:37.972: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-41534726-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:37:38.000: INFO: Waiting up to 5m0s for pod "pod-secrets-4153df79-d0bc-11e9-9ba3-f679ea11f394" in namespace "secrets-2198" to be "success or failure"
Sep  6 15:37:38.016: INFO: Pod "pod-secrets-4153df79-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 15.761046ms
Sep  6 15:37:40.019: INFO: Pod "pod-secrets-4153df79-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018726523s
STEP: Saw pod success
Sep  6 15:37:40.019: INFO: Pod "pod-secrets-4153df79-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:37:40.021: INFO: Trying to get logs from node metalk8s-23 pod pod-secrets-4153df79-d0bc-11e9-9ba3-f679ea11f394 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:37:40.030: INFO: Waiting for pod pod-secrets-4153df79-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:37:40.032: INFO: Pod pod-secrets-4153df79-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:37:40.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2198" for this suite.
Sep  6 15:37:46.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:37:46.103: INFO: namespace secrets-2198 deletion completed in 6.069106743s

• [SLOW TEST:8.131 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:37:46.103: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-b6gpn in namespace proxy-231
I0906 15:37:46.144518      14 runners.go:184] Created replication controller with name: proxy-service-b6gpn, namespace: proxy-231, replica count: 1
I0906 15:37:47.194927      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 15:37:48.195289      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 15:37:49.195514      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 15:37:50.195728      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 15:37:51.195886      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 15:37:52.196100      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 15:37:53.196355      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 15:37:54.196537      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 15:37:55.196737      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 15:37:56.196952      14 runners.go:184] proxy-service-b6gpn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 15:37:56.199: INFO: setup took 10.071200619s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  6 15:37:56.203: INFO: (0) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 3.405599ms)
Sep  6 15:37:56.207: INFO: (0) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 7.456026ms)
Sep  6 15:37:56.207: INFO: (0) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 7.362777ms)
Sep  6 15:37:56.207: INFO: (0) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 7.376846ms)
Sep  6 15:37:56.218: INFO: (0) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 18.03795ms)
Sep  6 15:37:56.218: INFO: (0) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 18.163114ms)
Sep  6 15:37:56.218: INFO: (0) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 18.188114ms)
Sep  6 15:37:56.218: INFO: (0) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 18.149682ms)
Sep  6 15:37:56.218: INFO: (0) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 18.203633ms)
Sep  6 15:37:56.219: INFO: (0) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 19.55787ms)
Sep  6 15:37:56.219: INFO: (0) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 19.525172ms)
Sep  6 15:37:56.222: INFO: (0) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 22.815817ms)
Sep  6 15:37:56.222: INFO: (0) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 23.051906ms)
Sep  6 15:37:56.222: INFO: (0) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 23.034839ms)
Sep  6 15:37:56.223: INFO: (0) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 23.256579ms)
Sep  6 15:37:56.223: INFO: (0) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 23.161101ms)
Sep  6 15:37:56.227: INFO: (1) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 3.840872ms)
Sep  6 15:37:56.227: INFO: (1) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 3.990324ms)
Sep  6 15:37:56.227: INFO: (1) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 3.979364ms)
Sep  6 15:37:56.229: INFO: (1) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 6.281713ms)
Sep  6 15:37:56.229: INFO: (1) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 6.29122ms)
Sep  6 15:37:56.229: INFO: (1) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 6.600737ms)
Sep  6 15:37:56.230: INFO: (1) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 6.648554ms)
Sep  6 15:37:56.230: INFO: (1) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 6.576902ms)
Sep  6 15:37:56.230: INFO: (1) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 6.470474ms)
Sep  6 15:37:56.230: INFO: (1) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 6.779113ms)
Sep  6 15:37:56.230: INFO: (1) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 7.207957ms)
Sep  6 15:37:56.230: INFO: (1) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 7.350993ms)
Sep  6 15:37:56.230: INFO: (1) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 7.196225ms)
Sep  6 15:37:56.230: INFO: (1) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 7.521039ms)
Sep  6 15:37:56.230: INFO: (1) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 7.313174ms)
Sep  6 15:37:56.231: INFO: (1) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 7.652422ms)
Sep  6 15:37:56.235: INFO: (2) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 3.572842ms)
Sep  6 15:37:56.235: INFO: (2) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 3.403145ms)
Sep  6 15:37:56.238: INFO: (2) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 6.905019ms)
Sep  6 15:37:56.238: INFO: (2) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 6.875528ms)
Sep  6 15:37:56.238: INFO: (2) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 6.486652ms)
Sep  6 15:37:56.238: INFO: (2) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 6.579947ms)
Sep  6 15:37:56.238: INFO: (2) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 7.037968ms)
Sep  6 15:37:56.238: INFO: (2) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 6.650481ms)
Sep  6 15:37:56.238: INFO: (2) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 6.428766ms)
Sep  6 15:37:56.238: INFO: (2) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 7.501014ms)
Sep  6 15:37:56.238: INFO: (2) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 7.386829ms)
Sep  6 15:37:56.239: INFO: (2) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 7.41028ms)
Sep  6 15:37:56.239: INFO: (2) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 6.881874ms)
Sep  6 15:37:56.239: INFO: (2) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 7.323899ms)
Sep  6 15:37:56.239: INFO: (2) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 7.703469ms)
Sep  6 15:37:56.241: INFO: (2) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 10.033334ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 5.924211ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 5.857642ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 5.754041ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 5.776787ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 5.875662ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 5.636069ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 5.72832ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 5.782849ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 5.732301ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 5.766458ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 5.944783ms)
Sep  6 15:37:56.247: INFO: (3) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 5.731988ms)
Sep  6 15:37:56.248: INFO: (3) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 6.328336ms)
Sep  6 15:37:56.248: INFO: (3) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 6.850599ms)
Sep  6 15:37:56.248: INFO: (3) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 6.779164ms)
Sep  6 15:37:56.248: INFO: (3) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 7.005516ms)
Sep  6 15:37:56.252: INFO: (4) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 2.655423ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 4.054889ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 4.857695ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 4.080772ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 4.98042ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 5.136574ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 5.587686ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 4.691674ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 5.486596ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 4.526421ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 5.754361ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 4.885517ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 5.7124ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 4.823969ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 5.992783ms)
Sep  6 15:37:56.254: INFO: (4) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 6.089082ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 5.000223ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 5.59162ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 5.493288ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 5.419579ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 5.375055ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 5.499389ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 5.202842ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 5.380327ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 5.479766ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 5.427224ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 5.162999ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 5.707529ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 5.251202ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 5.42693ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 5.336855ms)
Sep  6 15:37:56.260: INFO: (5) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 5.335864ms)
Sep  6 15:37:56.262: INFO: (6) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 2.192248ms)
Sep  6 15:37:56.263: INFO: (6) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 2.23817ms)
Sep  6 15:37:56.264: INFO: (6) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 3.480118ms)
Sep  6 15:37:56.264: INFO: (6) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 3.632537ms)
Sep  6 15:37:56.264: INFO: (6) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 3.763957ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 3.579364ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 3.943361ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 4.241789ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 3.502374ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 3.814802ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 4.626463ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 4.259421ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 3.950184ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 4.515567ms)
Sep  6 15:37:56.265: INFO: (6) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 4.346091ms)
Sep  6 15:37:56.266: INFO: (6) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 4.154017ms)
Sep  6 15:37:56.267: INFO: (7) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 1.746333ms)
Sep  6 15:37:56.268: INFO: (7) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 2.4956ms)
Sep  6 15:37:56.269: INFO: (7) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 3.010647ms)
Sep  6 15:37:56.269: INFO: (7) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 3.415657ms)
Sep  6 15:37:56.270: INFO: (7) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 4.192129ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 4.318289ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 4.896471ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 4.373488ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 5.003718ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 4.514167ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 4.825683ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 4.827796ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 4.868932ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 5.531984ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 5.138102ms)
Sep  6 15:37:56.271: INFO: (7) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 4.681721ms)
Sep  6 15:37:56.275: INFO: (8) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 2.84581ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 3.203159ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 4.078374ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 3.660689ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 3.908826ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 3.239649ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 4.253797ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 3.675098ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 3.337057ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 4.552553ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 4.021536ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 3.973863ms)
Sep  6 15:37:56.276: INFO: (8) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 4.089668ms)
Sep  6 15:37:56.277: INFO: (8) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 5.273116ms)
Sep  6 15:37:56.278: INFO: (8) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 6.386362ms)
Sep  6 15:37:56.278: INFO: (8) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 6.494151ms)
Sep  6 15:37:56.281: INFO: (9) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 2.573425ms)
Sep  6 15:37:56.281: INFO: (9) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 2.489987ms)
Sep  6 15:37:56.281: INFO: (9) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 2.767743ms)
Sep  6 15:37:56.285: INFO: (9) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 7.034803ms)
Sep  6 15:37:56.285: INFO: (9) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 6.858671ms)
Sep  6 15:37:56.285: INFO: (9) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 6.956825ms)
Sep  6 15:37:56.285: INFO: (9) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 6.890782ms)
Sep  6 15:37:56.285: INFO: (9) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 7.060516ms)
Sep  6 15:37:56.285: INFO: (9) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 7.059307ms)
Sep  6 15:37:56.285: INFO: (9) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 7.037194ms)
Sep  6 15:37:56.285: INFO: (9) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 6.986516ms)
Sep  6 15:37:56.285: INFO: (9) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 7.258279ms)
Sep  6 15:37:56.286: INFO: (9) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 7.271802ms)
Sep  6 15:37:56.286: INFO: (9) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 7.873423ms)
Sep  6 15:37:56.286: INFO: (9) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 7.800387ms)
Sep  6 15:37:56.286: INFO: (9) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 7.928588ms)
Sep  6 15:37:56.291: INFO: (10) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 4.400823ms)
Sep  6 15:37:56.291: INFO: (10) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 5.159267ms)
Sep  6 15:37:56.291: INFO: (10) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 4.842804ms)
Sep  6 15:37:56.291: INFO: (10) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 4.379194ms)
Sep  6 15:37:56.292: INFO: (10) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 4.790462ms)
Sep  6 15:37:56.292: INFO: (10) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 5.090878ms)
Sep  6 15:37:56.292: INFO: (10) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 5.071343ms)
Sep  6 15:37:56.292: INFO: (10) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 5.019594ms)
Sep  6 15:37:56.292: INFO: (10) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 4.916079ms)
Sep  6 15:37:56.294: INFO: (10) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 6.003473ms)
Sep  6 15:37:56.294: INFO: (10) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 6.024382ms)
Sep  6 15:37:56.294: INFO: (10) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 6.105602ms)
Sep  6 15:37:56.294: INFO: (10) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 6.673098ms)
Sep  6 15:37:56.294: INFO: (10) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 7.00125ms)
Sep  6 15:37:56.294: INFO: (10) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 6.87773ms)
Sep  6 15:37:56.295: INFO: (10) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 7.362478ms)
Sep  6 15:37:56.298: INFO: (11) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 3.042691ms)
Sep  6 15:37:56.298: INFO: (11) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 3.212544ms)
Sep  6 15:37:56.298: INFO: (11) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 3.38308ms)
Sep  6 15:37:56.303: INFO: (11) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 7.895291ms)
Sep  6 15:37:56.303: INFO: (11) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 7.430719ms)
Sep  6 15:37:56.303: INFO: (11) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 7.516712ms)
Sep  6 15:37:56.303: INFO: (11) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 7.719104ms)
Sep  6 15:37:56.303: INFO: (11) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 7.914853ms)
Sep  6 15:37:56.303: INFO: (11) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 7.887768ms)
Sep  6 15:37:56.303: INFO: (11) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 8.252419ms)
Sep  6 15:37:56.304: INFO: (11) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 8.088937ms)
Sep  6 15:37:56.304: INFO: (11) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 8.241244ms)
Sep  6 15:37:56.304: INFO: (11) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 8.072664ms)
Sep  6 15:37:56.304: INFO: (11) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 7.629053ms)
Sep  6 15:37:56.304: INFO: (11) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 7.603948ms)
Sep  6 15:37:56.304: INFO: (11) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 8.018484ms)
Sep  6 15:37:56.311: INFO: (12) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 7.375976ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 6.350552ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 7.561575ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 7.911857ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 6.945581ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 7.129626ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 7.106738ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 7.252649ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 7.884848ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 8.616585ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 7.434912ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 7.448406ms)
Sep  6 15:37:56.312: INFO: (12) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 8.176216ms)
Sep  6 15:37:56.313: INFO: (12) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 7.964373ms)
Sep  6 15:37:56.314: INFO: (12) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 9.271142ms)
Sep  6 15:37:56.315: INFO: (12) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 10.262619ms)
Sep  6 15:37:56.317: INFO: (13) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 1.902864ms)
Sep  6 15:37:56.319: INFO: (13) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 4.023388ms)
Sep  6 15:37:56.319: INFO: (13) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 3.817678ms)
Sep  6 15:37:56.319: INFO: (13) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 3.731562ms)
Sep  6 15:37:56.319: INFO: (13) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 3.925381ms)
Sep  6 15:37:56.320: INFO: (13) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 3.455144ms)
Sep  6 15:37:56.320: INFO: (13) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 3.655409ms)
Sep  6 15:37:56.320: INFO: (13) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 4.525668ms)
Sep  6 15:37:56.320: INFO: (13) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 5.442011ms)
Sep  6 15:37:56.321: INFO: (13) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 5.339577ms)
Sep  6 15:37:56.321: INFO: (13) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 5.614937ms)
Sep  6 15:37:56.321: INFO: (13) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 5.366118ms)
Sep  6 15:37:56.321: INFO: (13) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 5.369222ms)
Sep  6 15:37:56.321: INFO: (13) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 5.630715ms)
Sep  6 15:37:56.321: INFO: (13) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 4.968654ms)
Sep  6 15:37:56.321: INFO: (13) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 5.685451ms)
Sep  6 15:37:56.323: INFO: (14) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 2.251544ms)
Sep  6 15:37:56.324: INFO: (14) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 2.266749ms)
Sep  6 15:37:56.326: INFO: (14) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 4.709598ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 6.076823ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 6.211776ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 6.093014ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 6.313538ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 6.203801ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 6.347848ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 6.348425ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 6.48268ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 6.325013ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 6.485498ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 6.671732ms)
Sep  6 15:37:56.328: INFO: (14) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 6.394042ms)
Sep  6 15:37:56.329: INFO: (14) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 7.632624ms)
Sep  6 15:37:56.333: INFO: (15) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 3.691324ms)
Sep  6 15:37:56.333: INFO: (15) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 3.896054ms)
Sep  6 15:37:56.333: INFO: (15) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 3.877749ms)
Sep  6 15:37:56.333: INFO: (15) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 3.906925ms)
Sep  6 15:37:56.334: INFO: (15) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 5.584312ms)
Sep  6 15:37:56.335: INFO: (15) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 5.527568ms)
Sep  6 15:37:56.335: INFO: (15) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 5.616287ms)
Sep  6 15:37:56.338: INFO: (15) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 9.084647ms)
Sep  6 15:37:56.338: INFO: (15) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 9.180533ms)
Sep  6 15:37:56.338: INFO: (15) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 9.295119ms)
Sep  6 15:37:56.338: INFO: (15) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 9.263523ms)
Sep  6 15:37:56.338: INFO: (15) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 9.274665ms)
Sep  6 15:37:56.338: INFO: (15) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 9.347919ms)
Sep  6 15:37:56.338: INFO: (15) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 9.47247ms)
Sep  6 15:37:56.338: INFO: (15) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 9.313221ms)
Sep  6 15:37:56.338: INFO: (15) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 9.389682ms)
Sep  6 15:37:56.344: INFO: (16) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 5.477305ms)
Sep  6 15:37:56.345: INFO: (16) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 5.300173ms)
Sep  6 15:37:56.346: INFO: (16) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 7.796014ms)
Sep  6 15:37:56.346: INFO: (16) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 7.033253ms)
Sep  6 15:37:56.346: INFO: (16) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 7.262106ms)
Sep  6 15:37:56.346: INFO: (16) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 7.537223ms)
Sep  6 15:37:56.346: INFO: (16) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 7.38805ms)
Sep  6 15:37:56.346: INFO: (16) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 7.312213ms)
Sep  6 15:37:56.346: INFO: (16) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 7.380304ms)
Sep  6 15:37:56.347: INFO: (16) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 8.103379ms)
Sep  6 15:37:56.347: INFO: (16) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 8.301471ms)
Sep  6 15:37:56.348: INFO: (16) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 8.963327ms)
Sep  6 15:37:56.348: INFO: (16) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 8.903481ms)
Sep  6 15:37:56.348: INFO: (16) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 8.29805ms)
Sep  6 15:37:56.348: INFO: (16) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 8.820685ms)
Sep  6 15:37:56.349: INFO: (16) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 10.166212ms)
Sep  6 15:37:56.352: INFO: (17) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 2.716734ms)
Sep  6 15:37:56.353: INFO: (17) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 2.944281ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 5.681871ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 5.503257ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 5.484731ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 5.81537ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 5.878909ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 6.722615ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 6.526171ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 5.8047ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 6.499951ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 6.215245ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 5.740062ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 6.107784ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 6.193408ms)
Sep  6 15:37:56.356: INFO: (17) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 6.257191ms)
Sep  6 15:37:56.359: INFO: (18) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 2.748191ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 5.314841ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 5.251902ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 4.982123ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 5.02407ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 5.15283ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 4.971265ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 4.959765ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 4.950613ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 5.137343ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 5.622707ms)
Sep  6 15:37:56.362: INFO: (18) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 5.056808ms)
Sep  6 15:37:56.363: INFO: (18) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 6.386733ms)
Sep  6 15:37:56.363: INFO: (18) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 6.403543ms)
Sep  6 15:37:56.363: INFO: (18) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 6.402866ms)
Sep  6 15:37:56.363: INFO: (18) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 6.676474ms)
Sep  6 15:37:56.367: INFO: (19) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 3.494626ms)
Sep  6 15:37:56.368: INFO: (19) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:160/proxy/: foo (200; 4.511917ms)
Sep  6 15:37:56.369: INFO: (19) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 5.180683ms)
Sep  6 15:37:56.370: INFO: (19) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">test</... (200; 5.480853ms)
Sep  6 15:37:56.370: INFO: (19) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:162/proxy/: bar (200; 5.72914ms)
Sep  6 15:37:56.370: INFO: (19) /api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/http:proxy-service-b6gpn-jmtv2:1080/proxy/rewriteme">t... (200; 5.671983ms)
Sep  6 15:37:56.370: INFO: (19) /api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/proxy-service-b6gpn-jmtv2/proxy/rewriteme">test</a> (200; 5.568941ms)
Sep  6 15:37:56.370: INFO: (19) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:460/proxy/: tls baz (200; 5.705233ms)
Sep  6 15:37:56.370: INFO: (19) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:462/proxy/: tls qux (200; 5.649047ms)
Sep  6 15:37:56.370: INFO: (19) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname1/proxy/: foo (200; 5.686074ms)
Sep  6 15:37:56.370: INFO: (19) /api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/: <a href="/api/v1/namespaces/proxy-231/pods/https:proxy-service-b6gpn-jmtv2:443/proxy/tlsrewriteme... (200; 5.833911ms)
Sep  6 15:37:56.371: INFO: (19) /api/v1/namespaces/proxy-231/services/proxy-service-b6gpn:portname2/proxy/: bar (200; 6.816888ms)
Sep  6 15:37:56.371: INFO: (19) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname1/proxy/: tls baz (200; 6.801576ms)
Sep  6 15:37:56.371: INFO: (19) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname2/proxy/: bar (200; 6.886284ms)
Sep  6 15:37:56.371: INFO: (19) /api/v1/namespaces/proxy-231/services/https:proxy-service-b6gpn:tlsportname2/proxy/: tls qux (200; 7.311518ms)
Sep  6 15:37:56.371: INFO: (19) /api/v1/namespaces/proxy-231/services/http:proxy-service-b6gpn:portname1/proxy/: foo (200; 7.498052ms)
STEP: deleting ReplicationController proxy-service-b6gpn in namespace proxy-231, will wait for the garbage collector to delete the pods
Sep  6 15:37:56.428: INFO: Deleting ReplicationController proxy-service-b6gpn took: 4.280771ms
Sep  6 15:37:56.828: INFO: Terminating ReplicationController proxy-service-b6gpn pods took: 400.283358ms
[AfterEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:38:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-231" for this suite.
Sep  6 15:38:14.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:38:14.706: INFO: namespace proxy-231 deletion completed in 6.075181711s

• [SLOW TEST:28.603 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:38:14.706: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0906 15:38:20.756603      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 15:38:20.756: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:38:20.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2529" for this suite.
Sep  6 15:38:26.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:38:26.846: INFO: namespace gc-2529 deletion completed in 6.088075466s

• [SLOW TEST:12.140 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:38:26.847: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep  6 15:38:26.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-2605'
Sep  6 15:38:27.136: INFO: stderr: ""
Sep  6 15:38:27.136: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 15:38:27.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2605'
Sep  6 15:38:27.224: INFO: stderr: ""
Sep  6 15:38:27.224: INFO: stdout: "update-demo-nautilus-j4xlq update-demo-nautilus-qm6zs "
Sep  6 15:38:27.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-j4xlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2605'
Sep  6 15:38:27.292: INFO: stderr: ""
Sep  6 15:38:27.292: INFO: stdout: ""
Sep  6 15:38:27.292: INFO: update-demo-nautilus-j4xlq is created but not running
Sep  6 15:38:32.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2605'
Sep  6 15:38:32.362: INFO: stderr: ""
Sep  6 15:38:32.362: INFO: stdout: "update-demo-nautilus-j4xlq update-demo-nautilus-qm6zs "
Sep  6 15:38:32.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-j4xlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2605'
Sep  6 15:38:32.427: INFO: stderr: ""
Sep  6 15:38:32.427: INFO: stdout: "true"
Sep  6 15:38:32.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-j4xlq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2605'
Sep  6 15:38:32.491: INFO: stderr: ""
Sep  6 15:38:32.491: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 15:38:32.491: INFO: validating pod update-demo-nautilus-j4xlq
Sep  6 15:38:32.494: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 15:38:32.494: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 15:38:32.494: INFO: update-demo-nautilus-j4xlq is verified up and running
Sep  6 15:38:32.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-qm6zs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2605'
Sep  6 15:38:32.557: INFO: stderr: ""
Sep  6 15:38:32.557: INFO: stdout: "true"
Sep  6 15:38:32.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods update-demo-nautilus-qm6zs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2605'
Sep  6 15:38:32.626: INFO: stderr: ""
Sep  6 15:38:32.626: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 15:38:32.626: INFO: validating pod update-demo-nautilus-qm6zs
Sep  6 15:38:32.630: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 15:38:32.630: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 15:38:32.630: INFO: update-demo-nautilus-qm6zs is verified up and running
STEP: using delete to clean up resources
Sep  6 15:38:32.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-2605'
Sep  6 15:38:32.704: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 15:38:32.704: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 15:38:32.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2605'
Sep  6 15:38:32.796: INFO: stderr: "No resources found.\n"
Sep  6 15:38:32.796: INFO: stdout: ""
Sep  6 15:38:32.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -l name=update-demo --namespace=kubectl-2605 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 15:38:32.874: INFO: stderr: ""
Sep  6 15:38:32.874: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:38:32.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2605" for this suite.
Sep  6 15:38:54.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:38:54.951: INFO: namespace kubectl-2605 deletion completed in 22.072902575s

• [SLOW TEST:28.104 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:38:54.951: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-6f38da2e-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:38:55.003: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6f395273-d0bc-11e9-9ba3-f679ea11f394" in namespace "projected-1182" to be "success or failure"
Sep  6 15:38:55.018: INFO: Pod "pod-projected-secrets-6f395273-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 15.150137ms
Sep  6 15:38:57.021: INFO: Pod "pod-projected-secrets-6f395273-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017746723s
STEP: Saw pod success
Sep  6 15:38:57.021: INFO: Pod "pod-projected-secrets-6f395273-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:38:57.023: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-secrets-6f395273-d0bc-11e9-9ba3-f679ea11f394 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:38:57.035: INFO: Waiting for pod pod-projected-secrets-6f395273-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:38:57.038: INFO: Pod pod-projected-secrets-6f395273-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:38:57.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1182" for this suite.
Sep  6 15:39:03.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:39:03.109: INFO: namespace projected-1182 deletion completed in 6.069673111s

• [SLOW TEST:8.158 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:39:03.109: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:39:03.136: INFO: Waiting up to 5m0s for pod "downwardapi-volume-741245ae-d0bc-11e9-9ba3-f679ea11f394" in namespace "projected-4108" to be "success or failure"
Sep  6 15:39:03.138: INFO: Pod "downwardapi-volume-741245ae-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283403ms
Sep  6 15:39:05.140: INFO: Pod "downwardapi-volume-741245ae-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004661409s
STEP: Saw pod success
Sep  6 15:39:05.140: INFO: Pod "downwardapi-volume-741245ae-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:39:05.142: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-741245ae-d0bc-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:39:05.153: INFO: Waiting for pod downwardapi-volume-741245ae-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:39:05.156: INFO: Pod downwardapi-volume-741245ae-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:39:05.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4108" for this suite.
Sep  6 15:39:11.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:39:11.228: INFO: namespace projected-4108 deletion completed in 6.069418172s

• [SLOW TEST:8.119 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:39:11.228: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:39:11.255: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78e9a8b1-d0bc-11e9-9ba3-f679ea11f394" in namespace "projected-6234" to be "success or failure"
Sep  6 15:39:11.257: INFO: Pod "downwardapi-volume-78e9a8b1-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123933ms
Sep  6 15:39:13.260: INFO: Pod "downwardapi-volume-78e9a8b1-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004655831s
STEP: Saw pod success
Sep  6 15:39:13.260: INFO: Pod "downwardapi-volume-78e9a8b1-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:39:13.262: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-78e9a8b1-d0bc-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:39:13.271: INFO: Waiting for pod downwardapi-volume-78e9a8b1-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:39:13.273: INFO: Pod downwardapi-volume-78e9a8b1-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:39:13.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6234" for this suite.
Sep  6 15:39:19.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:39:19.354: INFO: namespace projected-6234 deletion completed in 6.079633235s

• [SLOW TEST:8.126 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:39:19.355: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 15:39:19.395: INFO: Waiting up to 5m0s for pod "downward-api-7dc35dd7-d0bc-11e9-9ba3-f679ea11f394" in namespace "downward-api-5655" to be "success or failure"
Sep  6 15:39:19.405: INFO: Pod "downward-api-7dc35dd7-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 10.075148ms
Sep  6 15:39:21.408: INFO: Pod "downward-api-7dc35dd7-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012835282s
STEP: Saw pod success
Sep  6 15:39:21.408: INFO: Pod "downward-api-7dc35dd7-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:39:21.410: INFO: Trying to get logs from node metalk8s-23 pod downward-api-7dc35dd7-d0bc-11e9-9ba3-f679ea11f394 container dapi-container: <nil>
STEP: delete the pod
Sep  6 15:39:21.428: INFO: Waiting for pod downward-api-7dc35dd7-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:39:21.433: INFO: Pod downward-api-7dc35dd7-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:39:21.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5655" for this suite.
Sep  6 15:39:27.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:39:27.497: INFO: namespace downward-api-5655 deletion completed in 6.061405376s

• [SLOW TEST:8.142 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:39:27.498: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-829ba474-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:39:27.529: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-829beb3d-d0bc-11e9-9ba3-f679ea11f394" in namespace "projected-9259" to be "success or failure"
Sep  6 15:39:27.535: INFO: Pod "pod-projected-configmaps-829beb3d-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009986ms
Sep  6 15:39:29.538: INFO: Pod "pod-projected-configmaps-829beb3d-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009743535s
STEP: Saw pod success
Sep  6 15:39:29.538: INFO: Pod "pod-projected-configmaps-829beb3d-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:39:29.541: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-configmaps-829beb3d-d0bc-11e9-9ba3-f679ea11f394 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:39:29.551: INFO: Waiting for pod pod-projected-configmaps-829beb3d-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:39:29.553: INFO: Pod pod-projected-configmaps-829beb3d-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:39:29.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9259" for this suite.
Sep  6 15:39:35.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:39:35.618: INFO: namespace projected-9259 deletion completed in 6.062779632s

• [SLOW TEST:8.121 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:39:35.618: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-8773c0a3-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:39:35.659: INFO: Waiting up to 5m0s for pod "pod-secrets-8774af77-d0bc-11e9-9ba3-f679ea11f394" in namespace "secrets-3471" to be "success or failure"
Sep  6 15:39:35.663: INFO: Pod "pod-secrets-8774af77-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 3.725827ms
Sep  6 15:39:37.665: INFO: Pod "pod-secrets-8774af77-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006582598s
STEP: Saw pod success
Sep  6 15:39:37.665: INFO: Pod "pod-secrets-8774af77-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:39:37.668: INFO: Trying to get logs from node metalk8s-23 pod pod-secrets-8774af77-d0bc-11e9-9ba3-f679ea11f394 container secret-env-test: <nil>
STEP: delete the pod
Sep  6 15:39:37.679: INFO: Waiting for pod pod-secrets-8774af77-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:39:37.681: INFO: Pod pod-secrets-8774af77-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:39:37.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3471" for this suite.
Sep  6 15:39:43.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:39:43.754: INFO: namespace secrets-3471 deletion completed in 6.070089402s

• [SLOW TEST:8.136 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:39:43.754: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-8c4cc438-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:39:43.787: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8c4d1595-d0bc-11e9-9ba3-f679ea11f394" in namespace "projected-3823" to be "success or failure"
Sep  6 15:39:43.790: INFO: Pod "pod-projected-configmaps-8c4d1595-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.704983ms
Sep  6 15:39:45.793: INFO: Pod "pod-projected-configmaps-8c4d1595-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005817986s
STEP: Saw pod success
Sep  6 15:39:45.793: INFO: Pod "pod-projected-configmaps-8c4d1595-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:39:45.795: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-configmaps-8c4d1595-d0bc-11e9-9ba3-f679ea11f394 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:39:45.809: INFO: Waiting for pod pod-projected-configmaps-8c4d1595-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:39:45.811: INFO: Pod pod-projected-configmaps-8c4d1595-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:39:45.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3823" for this suite.
Sep  6 15:39:51.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:39:51.900: INFO: namespace projected-3823 deletion completed in 6.086447865s

• [SLOW TEST:8.146 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:39:51.900: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-9127ba52-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating configMap with name cm-test-opt-upd-9127ba8c-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9127ba52-d0bc-11e9-9ba3-f679ea11f394
STEP: Updating configmap cm-test-opt-upd-9127ba8c-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating configMap with name cm-test-opt-create-9127ba9e-d0bc-11e9-9ba3-f679ea11f394
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:39:57.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9435" for this suite.
Sep  6 15:40:19.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:40:20.064: INFO: namespace projected-9435 deletion completed in 22.073275818s

• [SLOW TEST:28.164 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:40:20.064: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:40:20.093: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1f15b13-d0bc-11e9-9ba3-f679ea11f394" in namespace "downward-api-8281" to be "success or failure"
Sep  6 15:40:20.096: INFO: Pod "downwardapi-volume-a1f15b13-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 3.229996ms
Sep  6 15:40:22.101: INFO: Pod "downwardapi-volume-a1f15b13-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007790353s
STEP: Saw pod success
Sep  6 15:40:22.101: INFO: Pod "downwardapi-volume-a1f15b13-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:40:22.102: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-a1f15b13-d0bc-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:40:22.114: INFO: Waiting for pod downwardapi-volume-a1f15b13-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:40:22.116: INFO: Pod downwardapi-volume-a1f15b13-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:40:22.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8281" for this suite.
Sep  6 15:40:28.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:40:28.187: INFO: namespace downward-api-8281 deletion completed in 6.068805877s

• [SLOW TEST:8.123 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:40:28.188: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:40:28.240: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a6cbede9-d0bc-11e9-b447-fa163e0a078f", Controller:(*bool)(0xc002f630f6), BlockOwnerDeletion:(*bool)(0xc002f630f7)}}
Sep  6 15:40:28.246: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a6c96cd5-d0bc-11e9-b447-fa163e0a078f", Controller:(*bool)(0xc0027e5496), BlockOwnerDeletion:(*bool)(0xc0027e5497)}}
Sep  6 15:40:28.252: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a6cb6f0d-d0bc-11e9-b447-fa163e0a078f", Controller:(*bool)(0xc0027e5656), BlockOwnerDeletion:(*bool)(0xc0027e5657)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:40:33.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5883" for this suite.
Sep  6 15:40:39.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:40:39.344: INFO: namespace gc-5883 deletion completed in 6.080401582s

• [SLOW TEST:11.157 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:40:39.345: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:41:00.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2195" for this suite.
Sep  6 15:41:06.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:41:06.589: INFO: namespace container-runtime-2195 deletion completed in 6.067207595s

• [SLOW TEST:27.244 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:41:06.590: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  6 15:41:09.140: INFO: Successfully updated pod "annotationupdatebdac8d50-d0bc-11e9-9ba3-f679ea11f394"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:41:11.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8443" for this suite.
Sep  6 15:41:33.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:41:33.233: INFO: namespace projected-8443 deletion completed in 22.081180748s

• [SLOW TEST:26.643 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:41:33.233: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-cd8e0231-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:41:33.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd8f1a82-d0bc-11e9-9ba3-f679ea11f394" in namespace "configmap-4025" to be "success or failure"
Sep  6 15:41:33.272: INFO: Pod "pod-configmaps-cd8f1a82-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.696607ms
Sep  6 15:41:35.274: INFO: Pod "pod-configmaps-cd8f1a82-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004321709s
STEP: Saw pod success
Sep  6 15:41:35.274: INFO: Pod "pod-configmaps-cd8f1a82-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:41:35.276: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-cd8f1a82-d0bc-11e9-9ba3-f679ea11f394 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:41:35.290: INFO: Waiting for pod pod-configmaps-cd8f1a82-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:41:35.292: INFO: Pod pod-configmaps-cd8f1a82-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:41:35.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4025" for this suite.
Sep  6 15:41:41.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:41:41.369: INFO: namespace configmap-4025 deletion completed in 6.073692537s

• [SLOW TEST:8.136 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:41:41.369: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-d26776a0-d0bc-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:41:41.402: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d267dbdf-d0bc-11e9-9ba3-f679ea11f394" in namespace "projected-2709" to be "success or failure"
Sep  6 15:41:41.404: INFO: Pod "pod-projected-secrets-d267dbdf-d0bc-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.154306ms
Sep  6 15:41:43.407: INFO: Pod "pod-projected-secrets-d267dbdf-d0bc-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005035423s
STEP: Saw pod success
Sep  6 15:41:43.407: INFO: Pod "pod-projected-secrets-d267dbdf-d0bc-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:41:43.409: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-secrets-d267dbdf-d0bc-11e9-9ba3-f679ea11f394 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:41:43.419: INFO: Waiting for pod pod-projected-secrets-d267dbdf-d0bc-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:41:43.421: INFO: Pod pod-projected-secrets-d267dbdf-d0bc-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:41:43.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2709" for this suite.
Sep  6 15:41:49.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:41:49.491: INFO: namespace projected-2709 deletion completed in 6.068251777s

• [SLOW TEST:8.122 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:41:49.492: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:41:49.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-4657'
Sep  6 15:41:49.669: INFO: stderr: ""
Sep  6 15:41:49.669: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep  6 15:41:49.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-4657'
Sep  6 15:41:49.812: INFO: stderr: ""
Sep  6 15:41:49.812: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 15:41:50.816: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:41:50.816: INFO: Found 0 / 1
Sep  6 15:41:51.815: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:41:51.815: INFO: Found 1 / 1
Sep  6 15:41:51.815: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 15:41:51.817: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:41:51.817: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 15:41:51.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 describe pod redis-master-xvxxg --namespace=kubectl-4657'
Sep  6 15:41:51.899: INFO: stderr: ""
Sep  6 15:41:51.899: INFO: stdout: "Name:               redis-master-xvxxg\nNamespace:          kubectl-4657\nPriority:           0\nPriorityClassName:  <none>\nNode:               metalk8s-23/10.10.0.14\nStart Time:         Fri, 06 Sep 2019 15:41:49 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 10.233.221.217/32\nStatus:             Running\nIP:                 10.233.221.217\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://3dbd2f6645a4739a8bdd4bcb8cf72f9f8f52b7424c7838cec5ef663aab559913\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 06 Sep 2019 15:41:50 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8qdtf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-8qdtf:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-8qdtf\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                  Message\n  ----    ------     ----  ----                  -------\n  Normal  Scheduled  2s    default-scheduler     Successfully assigned kubectl-4657/redis-master-xvxxg to metalk8s-23\n  Normal  Pulled     1s    kubelet, metalk8s-23  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, metalk8s-23  Created container redis-master\n  Normal  Started    1s    kubelet, metalk8s-23  Started container redis-master\n"
Sep  6 15:41:51.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 describe rc redis-master --namespace=kubectl-4657'
Sep  6 15:41:51.988: INFO: stderr: ""
Sep  6 15:41:51.988: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4657\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-xvxxg\n"
Sep  6 15:41:51.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 describe service redis-master --namespace=kubectl-4657'
Sep  6 15:41:52.062: INFO: stderr: ""
Sep  6 15:41:52.062: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4657\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.96.197.36\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.221.217:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  6 15:41:52.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 describe node metalk8s-23'
Sep  6 15:41:52.160: INFO: stderr: ""
Sep  6 15:41:52.160: INFO: stdout: "Name:               metalk8s-23\nRoles:              bootstrap,etcd,infra,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=metalk8s-23\n                    kubernetes.io/os=linux\n                    metalk8s.scality.com/version=2.3.0-dev\n                    node-role.kubernetes.io/bootstrap=\n                    node-role.kubernetes.io/etcd=\n                    node-role.kubernetes.io/infra=\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.20.0.12/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.233.221.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 06 Sep 2019 14:39:09 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 06 Sep 2019 14:39:48 +0000   Fri, 06 Sep 2019 14:39:48 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 06 Sep 2019 15:41:02 +0000   Fri, 06 Sep 2019 14:39:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 06 Sep 2019 15:41:02 +0000   Fri, 06 Sep 2019 14:39:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 06 Sep 2019 15:41:02 +0000   Fri, 06 Sep 2019 14:39:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 06 Sep 2019 15:41:02 +0000   Fri, 06 Sep 2019 14:39:07 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.0.14\n  Hostname:    metalk8s-23\nCapacity:\n cpu:                8\n ephemeral-storage:  41931756Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16266524Ki\n pods:               110\nAllocatable:\n cpu:                8\n ephemeral-storage:  38644306266\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16164124Ki\n pods:               110\nSystem Info:\n Machine ID:                 b30d0f2110ac3807b210c19ede3ce88f\n System UUID:                B6BA9699-BA95-49A8-AAED-63DD8DF9EF5C\n Boot ID:                    b5d2f1cf-8529-4c70-8fb1-0e1fe34c1a49\n Kernel Version:             3.10.0-862.3.2.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.4\n Kubelet Version:            v1.14.6\n Kube-Proxy Version:         v1.14.6\nPodCIDR:                     10.233.0.0/24\nNon-terminated Pods:         (27 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\n  heptio-sonobuoy            sonobuoy-e2e-job-b2dd9a0f0ff64346                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-e00271a3bb5e42b1-t5zmh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                calico-kube-controllers-75579b5cb4-kzvqd                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         62m\n  kube-system                calico-node-xlrzb                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         62m\n  kube-system                coredns-55d57d558b-9z5x6                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     62m\n  kube-system                coredns-55d57d558b-fr555                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     62m\n  kube-system                etcd-metalk8s-23                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\n  kube-system                kube-apiserver-metalk8s-23                                 250m (3%)     0 (0%)      0 (0%)           0 (0%)         61m\n  kube-system                kube-controller-manager-metalk8s-23                        200m (2%)     0 (0%)      0 (0%)           0 (0%)         62m\n  kube-system                kube-proxy-smw98                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         62m\n  kube-system                kube-scheduler-metalk8s-23                                 100m (1%)     0 (0%)      0 (0%)           0 (0%)         62m\n  kube-system                repositories-metalk8s-23                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         62m\n  kube-system                salt-master-metalk8s-23                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         62m\n  kubectl-4657               redis-master-xvxxg                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  metalk8s-ingress           nginx-ingress-controller-97jbb                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\n  metalk8s-ingress           nginx-ingress-default-backend-7d98759d5f-z8zsq             0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\n  metalk8s-monitoring        alertmanager-main-0                                        50m (0%)      50m (0%)    210Mi (1%)       10Mi (0%)      61m\n  metalk8s-monitoring        alertmanager-main-1                                        50m (0%)      50m (0%)    210Mi (1%)       10Mi (0%)      61m\n  metalk8s-monitoring        alertmanager-main-2                                        50m (0%)      50m (0%)    210Mi (1%)       10Mi (0%)      61m\n  metalk8s-monitoring        grafana-5468d6ff9b-dkqdl                                   100m (1%)     200m (2%)   100Mi (0%)       200Mi (1%)     61m\n  metalk8s-monitoring        kube-state-metrics-685fc7775f-nbg55                        130m (1%)     190m (2%)   220Mi (1%)       260Mi (1%)     61m\n  metalk8s-monitoring        node-exporter-lwh4f                                        112m (1%)     270m (3%)   200Mi (1%)       220Mi (1%)     61m\n  metalk8s-monitoring        prometheus-adapter-7cfccdc649-nh8nw                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\n  metalk8s-monitoring        prometheus-k8s-0                                           75m (0%)      75m (0%)    460Mi (2%)       60Mi (0%)      61m\n  metalk8s-monitoring        prometheus-k8s-1                                           75m (0%)      75m (0%)    460Mi (2%)       60Mi (0%)      61m\n  metalk8s-monitoring        prometheus-operator-6d89bf894c-tzxp9                       100m (1%)     200m (2%)   100Mi (0%)       200Mi (1%)     62m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1742m (21%)   1160m (14%)\n  memory             2310Mi (14%)  1370Mi (8%)\n  ephemeral-storage  0 (0%)        0 (0%)\nEvents:              <none>\n"
Sep  6 15:41:52.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 describe namespace kubectl-4657'
Sep  6 15:41:52.233: INFO: stderr: ""
Sep  6 15:41:52.233: INFO: stdout: "Name:         kubectl-4657\nLabels:       e2e-framework=kubectl\n              e2e-run=9ad7541d-d0b4-11e9-9ba3-f679ea11f394\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:41:52.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4657" for this suite.
Sep  6 15:42:14.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:42:14.302: INFO: namespace kubectl-4657 deletion completed in 22.066144711s

• [SLOW TEST:24.811 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:42:14.302: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:43:14.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1292" for this suite.
Sep  6 15:43:36.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:43:36.410: INFO: namespace container-probe-1292 deletion completed in 22.069722653s

• [SLOW TEST:82.108 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:43:36.411: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:43:36.432: INFO: Creating deployment "test-recreate-deployment"
Sep  6 15:43:36.435: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  6 15:43:36.442: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  6 15:43:38.449: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  6 15:43:38.451: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  6 15:43:38.459: INFO: Updating deployment test-recreate-deployment
Sep  6 15:43:38.460: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 15:43:38.511: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-8040,SelfLink:/apis/apps/v1/namespaces/deployment-8040/deployments/test-recreate-deployment,UID:16f92341-d0bd-11e9-b447-fa163e0a078f,ResourceVersion:16778,Generation:2,CreationTimestamp:2019-09-06 15:43:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-06 15:43:38 +0000 UTC 2019-09-06 15:43:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-06 15:43:38 +0000 UTC 2019-09-06 15:43:36 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-745fb9c84c" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 15:43:38.514: INFO: New ReplicaSet "test-recreate-deployment-745fb9c84c" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c,GenerateName:,Namespace:deployment-8040,SelfLink:/apis/apps/v1/namespaces/deployment-8040/replicasets/test-recreate-deployment-745fb9c84c,UID:18322584-d0bd-11e9-b447-fa163e0a078f,ResourceVersion:16776,Generation:1,CreationTimestamp:2019-09-06 15:43:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 16f92341-d0bd-11e9-b447-fa163e0a078f 0xc00245a687 0xc00245a688}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 15:43:38.514: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  6 15:43:38.514: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6566d46b4b,GenerateName:,Namespace:deployment-8040,SelfLink:/apis/apps/v1/namespaces/deployment-8040/replicasets/test-recreate-deployment-6566d46b4b,UID:16f99fc9-d0bd-11e9-b447-fa163e0a078f,ResourceVersion:16768,Generation:2,CreationTimestamp:2019-09-06 15:43:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 16f92341-d0bd-11e9-b447-fa163e0a078f 0xc00245a5a7 0xc00245a5a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 15:43:38.516: INFO: Pod "test-recreate-deployment-745fb9c84c-6z77g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c-6z77g,GenerateName:test-recreate-deployment-745fb9c84c-,Namespace:deployment-8040,SelfLink:/api/v1/namespaces/deployment-8040/pods/test-recreate-deployment-745fb9c84c-6z77g,UID:18329225-d0bd-11e9-b447-fa163e0a078f,ResourceVersion:16777,Generation:0,CreationTimestamp:2019-09-06 15:43:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-745fb9c84c 18322584-d0bd-11e9-b447-fa163e0a078f 0xc00245b127 0xc00245b128}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lzdf5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lzdf5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lzdf5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-23,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00245b1a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00245b1c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:43:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:43:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:43:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:43:38 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.14,PodIP:,StartTime:2019-09-06 15:43:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:43:38.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8040" for this suite.
Sep  6 15:43:44.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:43:44.686: INFO: namespace deployment-8040 deletion completed in 6.167833945s

• [SLOW TEST:8.276 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:43:44.687: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 15:43:44.729: INFO: Waiting up to 5m0s for pod "pod-1bea1efb-d0bd-11e9-9ba3-f679ea11f394" in namespace "emptydir-6024" to be "success or failure"
Sep  6 15:43:44.732: INFO: Pod "pod-1bea1efb-d0bd-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514491ms
Sep  6 15:43:46.735: INFO: Pod "pod-1bea1efb-d0bd-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005252225s
STEP: Saw pod success
Sep  6 15:43:46.735: INFO: Pod "pod-1bea1efb-d0bd-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:43:46.740: INFO: Trying to get logs from node metalk8s-23 pod pod-1bea1efb-d0bd-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:43:46.758: INFO: Waiting for pod pod-1bea1efb-d0bd-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:43:46.762: INFO: Pod pod-1bea1efb-d0bd-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:43:46.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6024" for this suite.
Sep  6 15:43:52.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:43:52.832: INFO: namespace emptydir-6024 deletion completed in 6.068030398s

• [SLOW TEST:8.146 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:43:52.833: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  6 15:43:52.859: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  6 15:43:57.862: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:43:58.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8116" for this suite.
Sep  6 15:44:04.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:44:04.966: INFO: namespace replication-controller-8116 deletion completed in 6.091926164s

• [SLOW TEST:12.133 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:44:04.966: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:44:05.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27ffc21e-d0bd-11e9-9ba3-f679ea11f394" in namespace "downward-api-6674" to be "success or failure"
Sep  6 15:44:05.024: INFO: Pod "downwardapi-volume-27ffc21e-d0bd-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 8.216779ms
Sep  6 15:44:07.027: INFO: Pod "downwardapi-volume-27ffc21e-d0bd-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010950363s
STEP: Saw pod success
Sep  6 15:44:07.027: INFO: Pod "downwardapi-volume-27ffc21e-d0bd-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:44:07.029: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-27ffc21e-d0bd-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:44:07.043: INFO: Waiting for pod downwardapi-volume-27ffc21e-d0bd-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:44:07.046: INFO: Pod downwardapi-volume-27ffc21e-d0bd-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:44:07.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6674" for this suite.
Sep  6 15:44:13.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:44:13.131: INFO: namespace downward-api-6674 deletion completed in 6.081341497s

• [SLOW TEST:8.165 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:44:13.131: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-813
Sep  6 15:44:15.171: INFO: Started pod liveness-exec in namespace container-probe-813
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 15:44:15.172: INFO: Initial restart count of pod liveness-exec is 0
Sep  6 15:45:09.250: INFO: Restart count of pod container-probe-813/liveness-exec is now 1 (54.077707857s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:45:09.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-813" for this suite.
Sep  6 15:45:15.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:45:15.330: INFO: namespace container-probe-813 deletion completed in 6.069233095s

• [SLOW TEST:62.199 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:45:15.330: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-51efbe71-d0bd-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:45:15.373: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-51f11e5d-d0bd-11e9-9ba3-f679ea11f394" in namespace "projected-903" to be "success or failure"
Sep  6 15:45:15.375: INFO: Pod "pod-projected-configmaps-51f11e5d-d0bd-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533019ms
Sep  6 15:45:18.483: INFO: Pod "pod-projected-configmaps-51f11e5d-d0bd-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 3.109596752s
STEP: Saw pod success
Sep  6 15:45:18.483: INFO: Pod "pod-projected-configmaps-51f11e5d-d0bd-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:45:18.487: INFO: Trying to get logs from node metalk8s-23 pod pod-projected-configmaps-51f11e5d-d0bd-11e9-9ba3-f679ea11f394 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:45:18.506: INFO: Waiting for pod pod-projected-configmaps-51f11e5d-d0bd-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:45:18.509: INFO: Pod pod-projected-configmaps-51f11e5d-d0bd-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:45:18.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-903" for this suite.
Sep  6 15:45:24.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:45:24.583: INFO: namespace projected-903 deletion completed in 6.071944632s

• [SLOW TEST:9.253 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:45:24.583: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep  6 15:45:28.630: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:28.630: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:28.764: INFO: Exec stderr: ""
Sep  6 15:45:28.764: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:28.764: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:28.909: INFO: Exec stderr: ""
Sep  6 15:45:28.909: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:28.909: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:29.029: INFO: Exec stderr: ""
Sep  6 15:45:29.029: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:29.029: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:29.157: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep  6 15:45:29.157: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:29.157: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:29.275: INFO: Exec stderr: ""
Sep  6 15:45:29.275: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:29.275: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:29.390: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep  6 15:45:29.390: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:29.390: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:29.499: INFO: Exec stderr: ""
Sep  6 15:45:29.499: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:29.499: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:29.631: INFO: Exec stderr: ""
Sep  6 15:45:29.631: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:29.631: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:29.759: INFO: Exec stderr: ""
Sep  6 15:45:29.759: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2637 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 15:45:29.759: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
Sep  6 15:45:29.869: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:45:29.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2637" for this suite.
Sep  6 15:46:07.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:46:07.941: INFO: namespace e2e-kubelet-etc-hosts-2637 deletion completed in 38.070137881s

• [SLOW TEST:43.358 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:46:07.942: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-714a9286-d0bd-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:46:07.968: INFO: Waiting up to 5m0s for pod "pod-configmaps-714aeb11-d0bd-11e9-9ba3-f679ea11f394" in namespace "configmap-2129" to be "success or failure"
Sep  6 15:46:07.970: INFO: Pod "pod-configmaps-714aeb11-d0bd-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.453283ms
Sep  6 15:46:09.973: INFO: Pod "pod-configmaps-714aeb11-d0bd-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005464307s
STEP: Saw pod success
Sep  6 15:46:09.973: INFO: Pod "pod-configmaps-714aeb11-d0bd-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:46:09.976: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-714aeb11-d0bd-11e9-9ba3-f679ea11f394 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:46:09.988: INFO: Waiting for pod pod-configmaps-714aeb11-d0bd-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:46:09.990: INFO: Pod pod-configmaps-714aeb11-d0bd-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:46:09.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2129" for this suite.
Sep  6 15:46:16.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:46:16.065: INFO: namespace configmap-2129 deletion completed in 6.072941268s

• [SLOW TEST:8.124 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:46:16.065: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-7622ee18-d0bd-11e9-9ba3-f679ea11f394
STEP: Creating configMap with name cm-test-opt-upd-7622ee53-d0bd-11e9-9ba3-f679ea11f394
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7622ee18-d0bd-11e9-9ba3-f679ea11f394
STEP: Updating configmap cm-test-opt-upd-7622ee53-d0bd-11e9-9ba3-f679ea11f394
STEP: Creating configMap with name cm-test-opt-create-7622ee65-d0bd-11e9-9ba3-f679ea11f394
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:46:20.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3642" for this suite.
Sep  6 15:46:42.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:46:42.221: INFO: namespace configmap-3642 deletion completed in 22.065730099s

• [SLOW TEST:26.156 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:46:42.221: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1256
STEP: creating an rc
Sep  6 15:46:42.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-2009'
Sep  6 15:46:42.381: INFO: stderr: ""
Sep  6 15:46:42.381: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Sep  6 15:46:43.383: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:46:43.383: INFO: Found 1 / 1
Sep  6 15:46:43.383: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 15:46:43.385: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:46:43.385: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep  6 15:46:43.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 logs redis-master-7767j redis-master --namespace=kubectl-2009'
Sep  6 15:46:43.483: INFO: stderr: ""
Sep  6 15:46:43.483: INFO: stdout: "1:M 06 Sep 15:46:43.156 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 06 Sep 15:46:43.156 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 06 Sep 15:46:43.156 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 15:46:43.158 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 15:46:43.158 # Server started, Redis version 3.2.12\n1:M 06 Sep 15:46:43.158 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 15:46:43.158 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep  6 15:46:43.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 log redis-master-7767j redis-master --namespace=kubectl-2009 --tail=1'
Sep  6 15:46:43.558: INFO: stderr: ""
Sep  6 15:46:43.558: INFO: stdout: "1:M 06 Sep 15:46:43.158 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep  6 15:46:43.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 log redis-master-7767j redis-master --namespace=kubectl-2009 --limit-bytes=1'
Sep  6 15:46:43.635: INFO: stderr: ""
Sep  6 15:46:43.635: INFO: stdout: "1"
STEP: exposing timestamps
Sep  6 15:46:43.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 log redis-master-7767j redis-master --namespace=kubectl-2009 --tail=1 --timestamps'
Sep  6 15:46:43.710: INFO: stderr: ""
Sep  6 15:46:43.710: INFO: stdout: "2019-09-06T15:46:43.158506509Z 1:M 06 Sep 15:46:43.158 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep  6 15:46:46.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 log redis-master-7767j redis-master --namespace=kubectl-2009 --since=1s'
Sep  6 15:46:46.286: INFO: stderr: ""
Sep  6 15:46:46.286: INFO: stdout: ""
Sep  6 15:46:46.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 log redis-master-7767j redis-master --namespace=kubectl-2009 --since=24h'
Sep  6 15:46:46.362: INFO: stderr: ""
Sep  6 15:46:46.362: INFO: stdout: "1:M 06 Sep 15:46:43.156 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 06 Sep 15:46:43.156 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 06 Sep 15:46:43.156 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 15:46:43.158 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 15:46:43.158 # Server started, Redis version 3.2.12\n1:M 06 Sep 15:46:43.158 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 15:46:43.158 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
STEP: using delete to clean up resources
Sep  6 15:46:46.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete --grace-period=0 --force -f - --namespace=kubectl-2009'
Sep  6 15:46:46.436: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 15:46:46.436: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep  6 15:46:46.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get rc,svc -l name=nginx --no-headers --namespace=kubectl-2009'
Sep  6 15:46:46.510: INFO: stderr: "No resources found.\n"
Sep  6 15:46:46.510: INFO: stdout: ""
Sep  6 15:46:46.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 get pods -l name=nginx --namespace=kubectl-2009 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 15:46:46.571: INFO: stderr: ""
Sep  6 15:46:46.571: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:46:46.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2009" for this suite.
Sep  6 15:47:08.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:47:08.641: INFO: namespace kubectl-2009 deletion completed in 22.06743121s

• [SLOW TEST:26.420 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:47:08.641: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 15:47:08.670: INFO: Waiting up to 5m0s for pod "pod-957937f2-d0bd-11e9-9ba3-f679ea11f394" in namespace "emptydir-9109" to be "success or failure"
Sep  6 15:47:08.673: INFO: Pod "pod-957937f2-d0bd-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421049ms
Sep  6 15:47:10.675: INFO: Pod "pod-957937f2-d0bd-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005145989s
STEP: Saw pod success
Sep  6 15:47:10.675: INFO: Pod "pod-957937f2-d0bd-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:47:10.677: INFO: Trying to get logs from node metalk8s-23 pod pod-957937f2-d0bd-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:47:10.693: INFO: Waiting for pod pod-957937f2-d0bd-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:47:10.696: INFO: Pod pod-957937f2-d0bd-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:47:10.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9109" for this suite.
Sep  6 15:47:16.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:47:16.783: INFO: namespace emptydir-9109 deletion completed in 6.083618709s

• [SLOW TEST:8.142 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:47:16.783: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep  6 15:47:16.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 create -f - --namespace=kubectl-9866'
Sep  6 15:47:17.021: INFO: stderr: ""
Sep  6 15:47:17.021: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 15:47:18.024: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:47:18.024: INFO: Found 0 / 1
Sep  6 15:47:19.024: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:47:19.024: INFO: Found 1 / 1
Sep  6 15:47:19.024: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  6 15:47:19.027: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:47:19.027: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 15:47:19.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 patch pod redis-master-f9v55 --namespace=kubectl-9866 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  6 15:47:19.110: INFO: stderr: ""
Sep  6 15:47:19.110: INFO: stdout: "pod/redis-master-f9v55 patched\n"
STEP: checking annotations
Sep  6 15:47:19.118: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 15:47:19.118: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:47:19.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9866" for this suite.
Sep  6 15:47:41.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:47:41.201: INFO: namespace kubectl-9866 deletion completed in 22.080383003s

• [SLOW TEST:24.418 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:47:41.201: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 15:47:41.231: INFO: Waiting up to 5m0s for pod "downward-api-a8e15e11-d0bd-11e9-9ba3-f679ea11f394" in namespace "downward-api-6977" to be "success or failure"
Sep  6 15:47:41.233: INFO: Pod "downward-api-a8e15e11-d0bd-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.861627ms
Sep  6 15:47:43.236: INFO: Pod "downward-api-a8e15e11-d0bd-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005494267s
STEP: Saw pod success
Sep  6 15:47:43.236: INFO: Pod "downward-api-a8e15e11-d0bd-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:47:43.238: INFO: Trying to get logs from node metalk8s-23 pod downward-api-a8e15e11-d0bd-11e9-9ba3-f679ea11f394 container dapi-container: <nil>
STEP: delete the pod
Sep  6 15:47:43.251: INFO: Waiting for pod downward-api-a8e15e11-d0bd-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:47:43.253: INFO: Pod downward-api-a8e15e11-d0bd-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:47:43.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6977" for this suite.
Sep  6 15:47:49.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:47:49.332: INFO: namespace downward-api-6977 deletion completed in 6.076185715s

• [SLOW TEST:8.131 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:47:49.332: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-adba7c7a-d0bd-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 15:47:49.372: INFO: Waiting up to 5m0s for pod "pod-secrets-adbaf561-d0bd-11e9-9ba3-f679ea11f394" in namespace "secrets-9524" to be "success or failure"
Sep  6 15:47:49.376: INFO: Pod "pod-secrets-adbaf561-d0bd-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 3.840481ms
Sep  6 15:47:51.380: INFO: Pod "pod-secrets-adbaf561-d0bd-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007569122s
STEP: Saw pod success
Sep  6 15:47:51.380: INFO: Pod "pod-secrets-adbaf561-d0bd-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:47:51.383: INFO: Trying to get logs from node metalk8s-23 pod pod-secrets-adbaf561-d0bd-11e9-9ba3-f679ea11f394 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 15:47:51.398: INFO: Waiting for pod pod-secrets-adbaf561-d0bd-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:47:51.402: INFO: Pod pod-secrets-adbaf561-d0bd-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:47:51.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9524" for this suite.
Sep  6 15:47:57.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:47:57.521: INFO: namespace secrets-9524 deletion completed in 6.115628415s

• [SLOW TEST:8.189 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:47:57.521: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7595
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7595
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7595
Sep  6 15:47:57.570: INFO: Found 0 stateful pods, waiting for 1
Sep  6 15:48:07.573: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  6 15:48:07.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:48:07.757: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:48:07.757: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:48:07.757: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 15:48:07.760: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 15:48:17.763: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 15:48:17.763: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 15:48:17.778: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999461s
Sep  6 15:48:18.781: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99682389s
Sep  6 15:48:19.784: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993588403s
Sep  6 15:48:20.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990815767s
Sep  6 15:48:21.790: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.987494286s
Sep  6 15:48:22.793: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.984671641s
Sep  6 15:48:23.796: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.981928577s
Sep  6 15:48:24.799: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.979103533s
Sep  6 15:48:25.802: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.975987796s
Sep  6 15:48:26.805: INFO: Verifying statefulset ss doesn't scale past 1 for another 973.116564ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7595
Sep  6 15:48:27.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 15:48:28.040: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 15:48:28.040: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 15:48:28.040: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 15:48:28.043: INFO: Found 1 stateful pods, waiting for 3
Sep  6 15:48:38.047: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:48:38.047: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:48:38.047: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  6 15:48:38.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:48:38.239: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:48:38.239: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:48:38.239: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 15:48:38.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-7595 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:48:38.424: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:48:38.424: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:48:38.424: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 15:48:38.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-7595 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:48:38.611: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:48:38.611: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:48:38.611: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 15:48:38.611: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 15:48:38.613: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  6 15:48:48.617: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 15:48:48.618: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 15:48:48.618: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 15:48:48.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999762s
Sep  6 15:48:49.628: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99766898s
Sep  6 15:48:50.631: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99429027s
Sep  6 15:48:51.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991314917s
Sep  6 15:48:52.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988717761s
Sep  6 15:48:53.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985855958s
Sep  6 15:48:54.644: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983027294s
Sep  6 15:48:55.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977709717s
Sep  6 15:48:56.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955401496s
Sep  6 15:48:57.672: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.522464ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7595
Sep  6 15:48:58.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 15:48:58.884: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 15:48:58.884: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 15:48:58.884: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 15:48:58.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-7595 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 15:48:59.073: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 15:48:59.073: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 15:48:59.073: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 15:48:59.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-7595 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 15:48:59.269: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 15:48:59.269: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 15:48:59.269: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 15:48:59.269: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 15:49:19.281: INFO: Deleting all statefulset in ns statefulset-7595
Sep  6 15:49:19.284: INFO: Scaling statefulset ss to 0
Sep  6 15:49:19.289: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 15:49:19.290: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:49:19.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7595" for this suite.
Sep  6 15:49:25.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:49:25.384: INFO: namespace statefulset-7595 deletion completed in 6.084070525s

• [SLOW TEST:87.863 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:49:25.385: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8720
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-8720
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8720
Sep  6 15:49:25.432: INFO: Found 0 stateful pods, waiting for 1
Sep  6 15:49:35.435: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  6 15:49:35.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8720 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:49:35.616: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:49:35.616: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:49:35.616: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 15:49:35.619: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 15:49:45.622: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 15:49:45.622: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 15:49:45.630: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep  6 15:49:45.630: INFO: ss-0  metalk8s-23  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:25 +0000 UTC  }]
Sep  6 15:49:45.630: INFO: 
Sep  6 15:49:45.630: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  6 15:49:46.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997891933s
Sep  6 15:49:47.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994587544s
Sep  6 15:49:48.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991209637s
Sep  6 15:49:49.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988385744s
Sep  6 15:49:50.646: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985065173s
Sep  6 15:49:51.649: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981946896s
Sep  6 15:49:52.653: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978524162s
Sep  6 15:49:53.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975213498s
Sep  6 15:49:54.664: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.767006ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8720
Sep  6 15:49:55.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8720 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 15:49:55.877: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 15:49:55.877: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 15:49:55.877: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 15:49:55.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8720 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 15:49:56.115: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 15:49:56.115: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 15:49:56.115: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 15:49:56.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8720 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 15:49:56.371: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 15:49:56.371: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 15:49:56.371: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 15:49:56.374: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:49:56.374: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:49:56.374: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  6 15:49:56.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8720 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:49:56.580: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:49:56.580: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:49:56.580: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 15:49:56.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8720 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:49:56.855: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:49:56.856: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:49:56.856: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 15:49:56.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8720 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:49:57.065: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:49:57.065: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:49:57.065: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 15:49:57.065: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 15:49:57.068: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep  6 15:50:07.073: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 15:50:07.073: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 15:50:07.073: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 15:50:07.079: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep  6 15:50:07.079: INFO: ss-0  metalk8s-23  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:25 +0000 UTC  }]
Sep  6 15:50:07.079: INFO: ss-1  metalk8s-23  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:45 +0000 UTC  }]
Sep  6 15:50:07.079: INFO: ss-2  metalk8s-23  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:45 +0000 UTC  }]
Sep  6 15:50:07.079: INFO: 
Sep  6 15:50:07.079: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 15:50:08.082: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep  6 15:50:08.082: INFO: ss-0  metalk8s-23  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:25 +0000 UTC  }]
Sep  6 15:50:08.082: INFO: ss-1  metalk8s-23  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:45 +0000 UTC  }]
Sep  6 15:50:08.082: INFO: ss-2  metalk8s-23  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:45 +0000 UTC  }]
Sep  6 15:50:08.082: INFO: 
Sep  6 15:50:08.082: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 15:50:09.085: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep  6 15:50:09.085: INFO: ss-0  metalk8s-23  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 15:49:25 +0000 UTC  }]
Sep  6 15:50:09.085: INFO: 
Sep  6 15:50:09.085: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 15:50:10.088: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.991812316s
Sep  6 15:50:11.090: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.988894171s
Sep  6 15:50:12.093: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.986484144s
Sep  6 15:50:13.096: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.983599526s
Sep  6 15:50:14.098: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.981211395s
Sep  6 15:50:15.101: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.978847646s
Sep  6 15:50:16.103: INFO: Verifying statefulset ss doesn't scale past 0 for another 976.259256ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8720
Sep  6 15:50:17.106: INFO: Scaling statefulset ss to 0
Sep  6 15:50:17.113: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 15:50:17.115: INFO: Deleting all statefulset in ns statefulset-8720
Sep  6 15:50:17.116: INFO: Scaling statefulset ss to 0
Sep  6 15:50:17.122: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 15:50:17.123: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:50:17.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8720" for this suite.
Sep  6 15:50:23.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:50:23.206: INFO: namespace statefulset-8720 deletion completed in 6.072105374s

• [SLOW TEST:57.821 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:50:23.206: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep  6 15:50:23.230: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 15:50:23.233: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 15:50:23.235: INFO: 
Logging pods the kubelet thinks is on node metalk8s-23 before test
Sep  6 15:50:23.250: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 14:42:19 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 15:50:23.250: INFO: kube-apiserver-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 15:50:23.250: INFO: grafana-5468d6ff9b-dkqdl from metalk8s-monitoring started at 2019-09-06 14:40:08 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container grafana ready: true, restart count 0
Sep  6 15:50:23.250: INFO: nginx-ingress-controller-97jbb from metalk8s-ingress started at 2019-09-06 14:40:08 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  6 15:50:23.250: INFO: prometheus-k8s-1 from metalk8s-monitoring started at 2019-09-06 14:40:13 +0000 UTC (3 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 15:50:23.250: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 15:50:23.250: INFO: kube-scheduler-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 15:50:23.250: INFO: prometheus-operator-6d89bf894c-tzxp9 from metalk8s-monitoring started at 2019-09-06 14:39:47 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  6 15:50:23.250: INFO: repositories-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 15:50:23.250: INFO: coredns-55d57d558b-9z5x6 from kube-system started at 2019-09-06 14:39:43 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container coredns ready: true, restart count 0
Sep  6 15:50:23.250: INFO: nginx-ingress-default-backend-7d98759d5f-z8zsq from metalk8s-ingress started at 2019-09-06 14:40:08 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep  6 15:50:23.250: INFO: calico-kube-controllers-75579b5cb4-kzvqd from kube-system started at 2019-09-06 14:39:41 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 15:50:23.250: INFO: prometheus-k8s-0 from metalk8s-monitoring started at 2019-09-06 14:40:13 +0000 UTC (3 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 15:50:23.250: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 15:50:23.250: INFO: coredns-55d57d558b-fr555 from kube-system started at 2019-09-06 14:39:43 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container coredns ready: true, restart count 0
Sep  6 15:50:23.250: INFO: etcd-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 15:50:23.250: INFO: kube-proxy-smw98 from kube-system started at 2019-09-06 14:39:34 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 15:50:23.250: INFO: kube-state-metrics-685fc7775f-nbg55 from metalk8s-monitoring started at 2019-09-06 14:40:00 +0000 UTC (4 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  6 15:50:23.250: INFO: sonobuoy-e2e-job-b2dd9a0f0ff64346 from heptio-sonobuoy started at 2019-09-06 14:42:24 +0000 UTC (2 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container e2e ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 15:50:23.250: INFO: alertmanager-main-1 from metalk8s-monitoring started at 2019-09-06 14:40:17 +0000 UTC (2 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 15:50:23.250: INFO: salt-master-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 15:50:23.250: INFO: kube-controller-manager-metalk8s-23 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 15:50:23.250: INFO: prometheus-adapter-7cfccdc649-nh8nw from metalk8s-monitoring started at 2019-09-06 14:39:54 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep  6 15:50:23.250: INFO: alertmanager-main-2 from metalk8s-monitoring started at 2019-09-06 14:40:25 +0000 UTC (2 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 15:50:23.250: INFO: node-exporter-lwh4f from metalk8s-monitoring started at 2019-09-06 14:40:01 +0000 UTC (2 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 15:50:23.250: INFO: alertmanager-main-0 from metalk8s-monitoring started at 2019-09-06 14:40:07 +0000 UTC (2 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 15:50:23.250: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 15:50:23.250: INFO: sonobuoy-systemd-logs-daemon-set-e00271a3bb5e42b1-t5zmh from heptio-sonobuoy started at 2019-09-06 14:42:24 +0000 UTC (2 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 15:50:23.250: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 15:50:23.250: INFO: calico-node-xlrzb from kube-system started at 2019-09-06 14:39:41 +0000 UTC (1 container statuses recorded)
Sep  6 15:50:23.250: INFO: 	Container calico-node ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod sonobuoy requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod sonobuoy-e2e-job-b2dd9a0f0ff64346 requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod sonobuoy-systemd-logs-daemon-set-e00271a3bb5e42b1-t5zmh requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod calico-kube-controllers-75579b5cb4-kzvqd requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod calico-node-xlrzb requesting resource cpu=250m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod coredns-55d57d558b-9z5x6 requesting resource cpu=100m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod coredns-55d57d558b-fr555 requesting resource cpu=100m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod etcd-metalk8s-23 requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod kube-apiserver-metalk8s-23 requesting resource cpu=250m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod kube-controller-manager-metalk8s-23 requesting resource cpu=200m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod kube-proxy-smw98 requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod kube-scheduler-metalk8s-23 requesting resource cpu=100m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod repositories-metalk8s-23 requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod salt-master-metalk8s-23 requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod nginx-ingress-controller-97jbb requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod nginx-ingress-default-backend-7d98759d5f-z8zsq requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod alertmanager-main-0 requesting resource cpu=50m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod alertmanager-main-1 requesting resource cpu=50m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod alertmanager-main-2 requesting resource cpu=50m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod grafana-5468d6ff9b-dkqdl requesting resource cpu=100m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod kube-state-metrics-685fc7775f-nbg55 requesting resource cpu=130m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod node-exporter-lwh4f requesting resource cpu=112m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod prometheus-adapter-7cfccdc649-nh8nw requesting resource cpu=0m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node metalk8s-23
Sep  6 15:50:23.272: INFO: Pod prometheus-operator-6d89bf894c-tzxp9 requesting resource cpu=100m on Node metalk8s-23
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0977e57e-d0be-11e9-9ba3-f679ea11f394.15c1e42c14c7a708], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3641/filler-pod-0977e57e-d0be-11e9-9ba3-f679ea11f394 to metalk8s-23]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0977e57e-d0be-11e9-9ba3-f679ea11f394.15c1e42c375bc21c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0977e57e-d0be-11e9-9ba3-f679ea11f394.15c1e42c39ea1345], Reason = [Created], Message = [Created container filler-pod-0977e57e-d0be-11e9-9ba3-f679ea11f394]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0977e57e-d0be-11e9-9ba3-f679ea11f394.15c1e42c409716a1], Reason = [Started], Message = [Started container filler-pod-0977e57e-d0be-11e9-9ba3-f679ea11f394]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c1e42c8c902379], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 Insufficient cpu.]
STEP: removing the label node off the node metalk8s-23
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:50:26.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3641" for this suite.
Sep  6 15:50:32.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:50:32.383: INFO: namespace sched-pred-3641 deletion completed in 6.071476928s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.177 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:50:32.384: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Sep  6 15:50:32.405: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-262345838 proxy --unix-socket=/tmp/kubectl-proxy-unix844515573/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:50:32.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4249" for this suite.
Sep  6 15:50:38.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:50:38.521: INFO: namespace kubectl-4249 deletion completed in 6.062776797s

• [SLOW TEST:6.138 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:50:38.521: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Sep  6 15:50:38.547: INFO: Waiting up to 5m0s for pod "var-expansion-1291ebc2-d0be-11e9-9ba3-f679ea11f394" in namespace "var-expansion-1740" to be "success or failure"
Sep  6 15:50:38.553: INFO: Pod "var-expansion-1291ebc2-d0be-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 5.708245ms
Sep  6 15:50:40.555: INFO: Pod "var-expansion-1291ebc2-d0be-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008260349s
STEP: Saw pod success
Sep  6 15:50:40.556: INFO: Pod "var-expansion-1291ebc2-d0be-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:50:40.558: INFO: Trying to get logs from node metalk8s-23 pod var-expansion-1291ebc2-d0be-11e9-9ba3-f679ea11f394 container dapi-container: <nil>
STEP: delete the pod
Sep  6 15:50:40.568: INFO: Waiting for pod var-expansion-1291ebc2-d0be-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:50:40.571: INFO: Pod var-expansion-1291ebc2-d0be-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:50:40.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1740" for this suite.
Sep  6 15:50:46.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:50:46.645: INFO: namespace var-expansion-1740 deletion completed in 6.071864651s

• [SLOW TEST:8.124 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:50:46.645: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  6 15:50:46.683: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:50:58.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7737" for this suite.
Sep  6 15:51:04.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:51:04.703: INFO: namespace pods-7737 deletion completed in 6.079315134s

• [SLOW TEST:18.057 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:51:04.703: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Sep  6 15:51:05.382: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  6 15:51:07.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 15:51:09.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 15:51:11.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703381865, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 15:51:14.779: INFO: Waited 1.317397402s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:51:15.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4949" for this suite.
Sep  6 15:51:21.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:51:21.526: INFO: namespace aggregator-4949 deletion completed in 6.156148574s

• [SLOW TEST:16.823 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:51:21.526: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0906 15:51:31.563038      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 15:51:31.563: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:51:31.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5485" for this suite.
Sep  6 15:51:37.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:51:37.637: INFO: namespace gc-5485 deletion completed in 6.07267922s

• [SLOW TEST:16.111 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:51:37.638: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:51:37.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35ce6fba-d0be-11e9-9ba3-f679ea11f394" in namespace "projected-2867" to be "success or failure"
Sep  6 15:51:37.669: INFO: Pod "downwardapi-volume-35ce6fba-d0be-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.329955ms
Sep  6 15:51:39.671: INFO: Pod "downwardapi-volume-35ce6fba-d0be-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006872738s
STEP: Saw pod success
Sep  6 15:51:39.671: INFO: Pod "downwardapi-volume-35ce6fba-d0be-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:51:39.673: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-35ce6fba-d0be-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:51:39.684: INFO: Waiting for pod downwardapi-volume-35ce6fba-d0be-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:51:39.690: INFO: Pod downwardapi-volume-35ce6fba-d0be-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:51:39.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2867" for this suite.
Sep  6 15:51:45.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:51:45.760: INFO: namespace projected-2867 deletion completed in 6.067656529s

• [SLOW TEST:8.122 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:51:45.760: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 15:51:45.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7469'
Sep  6 15:51:45.939: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 15:51:45.939: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
Sep  6 15:51:45.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete jobs e2e-test-nginx-job --namespace=kubectl-7469'
Sep  6 15:51:46.040: INFO: stderr: ""
Sep  6 15:51:46.041: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:51:46.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7469" for this suite.
Sep  6 15:51:52.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:51:52.111: INFO: namespace kubectl-7469 deletion completed in 6.068098982s

• [SLOW TEST:6.351 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:51:52.112: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Sep  6 15:51:52.149: INFO: Waiting up to 5m0s for pod "var-expansion-3e700cda-d0be-11e9-9ba3-f679ea11f394" in namespace "var-expansion-5136" to be "success or failure"
Sep  6 15:51:52.155: INFO: Pod "var-expansion-3e700cda-d0be-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 5.878096ms
Sep  6 15:51:54.158: INFO: Pod "var-expansion-3e700cda-d0be-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00884136s
STEP: Saw pod success
Sep  6 15:51:54.158: INFO: Pod "var-expansion-3e700cda-d0be-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:51:54.160: INFO: Trying to get logs from node metalk8s-23 pod var-expansion-3e700cda-d0be-11e9-9ba3-f679ea11f394 container dapi-container: <nil>
STEP: delete the pod
Sep  6 15:51:54.172: INFO: Waiting for pod var-expansion-3e700cda-d0be-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:51:54.174: INFO: Pod var-expansion-3e700cda-d0be-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:51:54.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5136" for this suite.
Sep  6 15:52:00.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:52:00.245: INFO: namespace var-expansion-5136 deletion completed in 6.069160635s

• [SLOW TEST:8.133 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:52:00.245: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  6 15:52:00.272: INFO: Waiting up to 5m0s for pod "pod-43483482-d0be-11e9-9ba3-f679ea11f394" in namespace "emptydir-2161" to be "success or failure"
Sep  6 15:52:00.275: INFO: Pod "pod-43483482-d0be-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250849ms
Sep  6 15:52:02.278: INFO: Pod "pod-43483482-d0be-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005977619s
STEP: Saw pod success
Sep  6 15:52:02.278: INFO: Pod "pod-43483482-d0be-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:52:02.280: INFO: Trying to get logs from node metalk8s-23 pod pod-43483482-d0be-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:52:02.294: INFO: Waiting for pod pod-43483482-d0be-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:52:02.296: INFO: Pod pod-43483482-d0be-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:52:02.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2161" for this suite.
Sep  6 15:52:08.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:52:08.373: INFO: namespace emptydir-2161 deletion completed in 6.074958933s

• [SLOW TEST:8.128 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:52:08.373: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:52:11.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5062" for this suite.
Sep  6 15:52:33.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:52:33.500: INFO: namespace replication-controller-5062 deletion completed in 22.081048749s

• [SLOW TEST:25.127 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:52:33.500: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-571a9911-d0be-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume configMaps
Sep  6 15:52:33.531: INFO: Waiting up to 5m0s for pod "pod-configmaps-571b2db3-d0be-11e9-9ba3-f679ea11f394" in namespace "configmap-6044" to be "success or failure"
Sep  6 15:52:33.532: INFO: Pod "pod-configmaps-571b2db3-d0be-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 1.395872ms
Sep  6 15:52:35.535: INFO: Pod "pod-configmaps-571b2db3-d0be-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004024537s
STEP: Saw pod success
Sep  6 15:52:35.535: INFO: Pod "pod-configmaps-571b2db3-d0be-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:52:35.537: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-571b2db3-d0be-11e9-9ba3-f679ea11f394 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 15:52:35.550: INFO: Waiting for pod pod-configmaps-571b2db3-d0be-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:52:35.552: INFO: Pod pod-configmaps-571b2db3-d0be-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:52:35.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6044" for this suite.
Sep  6 15:52:41.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:52:41.628: INFO: namespace configmap-6044 deletion completed in 6.07233166s

• [SLOW TEST:8.128 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:52:41.628: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 15:52:41.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8993'
Sep  6 15:52:41.721: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 15:52:41.721: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
Sep  6 15:52:43.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8993'
Sep  6 15:52:43.808: INFO: stderr: ""
Sep  6 15:52:43.808: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:52:43.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8993" for this suite.
Sep  6 15:53:05.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:53:05.885: INFO: namespace kubectl-8993 deletion completed in 22.073577398s

• [SLOW TEST:24.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:53:05.885: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5715
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5715
STEP: Creating statefulset with conflicting port in namespace statefulset-5715
STEP: Waiting until pod test-pod will start running in namespace statefulset-5715
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5715
Sep  6 15:53:07.938: INFO: Observed stateful pod in namespace: statefulset-5715, name: ss-0, uid: 6b4380de-d0be-11e9-b447-fa163e0a078f, status phase: Pending. Waiting for statefulset controller to delete.
Sep  6 15:53:08.331: INFO: Observed stateful pod in namespace: statefulset-5715, name: ss-0, uid: 6b4380de-d0be-11e9-b447-fa163e0a078f, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 15:53:08.336: INFO: Observed stateful pod in namespace: statefulset-5715, name: ss-0, uid: 6b4380de-d0be-11e9-b447-fa163e0a078f, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 15:53:08.339: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5715
STEP: Removing pod with conflicting port in namespace statefulset-5715
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5715 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 15:53:12.364: INFO: Deleting all statefulset in ns statefulset-5715
Sep  6 15:53:12.366: INFO: Scaling statefulset ss to 0
Sep  6 15:53:22.377: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 15:53:22.379: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:53:22.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5715" for this suite.
Sep  6 15:53:28.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:53:28.477: INFO: namespace statefulset-5715 deletion completed in 6.090361292s

• [SLOW TEST:22.592 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:53:28.478: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-77e043e7-d0be-11e9-9ba3-f679ea11f394
Sep  6 15:53:28.510: INFO: Pod name my-hostname-basic-77e043e7-d0be-11e9-9ba3-f679ea11f394: Found 0 pods out of 1
Sep  6 15:53:33.513: INFO: Pod name my-hostname-basic-77e043e7-d0be-11e9-9ba3-f679ea11f394: Found 1 pods out of 1
Sep  6 15:53:33.513: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-77e043e7-d0be-11e9-9ba3-f679ea11f394" are running
Sep  6 15:53:33.515: INFO: Pod "my-hostname-basic-77e043e7-d0be-11e9-9ba3-f679ea11f394-zr9rs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 15:53:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 15:53:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 15:53:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 15:53:28 +0000 UTC Reason: Message:}])
Sep  6 15:53:33.515: INFO: Trying to dial the pod
Sep  6 15:53:38.522: INFO: Controller my-hostname-basic-77e043e7-d0be-11e9-9ba3-f679ea11f394: Got expected result from replica 1 [my-hostname-basic-77e043e7-d0be-11e9-9ba3-f679ea11f394-zr9rs]: "my-hostname-basic-77e043e7-d0be-11e9-9ba3-f679ea11f394-zr9rs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:53:38.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8016" for this suite.
Sep  6 15:53:44.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:53:44.689: INFO: namespace replication-controller-8016 deletion completed in 6.164049024s

• [SLOW TEST:16.211 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:53:44.689: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Sep  6 15:53:44.730: INFO: Waiting up to 5m0s for pod "client-containers-818a3aff-d0be-11e9-9ba3-f679ea11f394" in namespace "containers-2423" to be "success or failure"
Sep  6 15:53:44.744: INFO: Pod "client-containers-818a3aff-d0be-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 13.462817ms
Sep  6 15:53:46.748: INFO: Pod "client-containers-818a3aff-d0be-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01755158s
STEP: Saw pod success
Sep  6 15:53:46.748: INFO: Pod "client-containers-818a3aff-d0be-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:53:46.754: INFO: Trying to get logs from node metalk8s-23 pod client-containers-818a3aff-d0be-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:53:46.773: INFO: Waiting for pod client-containers-818a3aff-d0be-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:53:46.776: INFO: Pod client-containers-818a3aff-d0be-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:53:46.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2423" for this suite.
Sep  6 15:53:52.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:53:52.843: INFO: namespace containers-2423 deletion completed in 6.064981453s

• [SLOW TEST:8.154 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:53:52.844: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:53:52.867: INFO: Creating ReplicaSet my-hostname-basic-866587e6-d0be-11e9-9ba3-f679ea11f394
Sep  6 15:53:52.872: INFO: Pod name my-hostname-basic-866587e6-d0be-11e9-9ba3-f679ea11f394: Found 0 pods out of 1
Sep  6 15:53:57.875: INFO: Pod name my-hostname-basic-866587e6-d0be-11e9-9ba3-f679ea11f394: Found 1 pods out of 1
Sep  6 15:53:57.875: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-866587e6-d0be-11e9-9ba3-f679ea11f394" is running
Sep  6 15:53:57.877: INFO: Pod "my-hostname-basic-866587e6-d0be-11e9-9ba3-f679ea11f394-qszj2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 15:53:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 15:53:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 15:53:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 15:53:52 +0000 UTC Reason: Message:}])
Sep  6 15:53:57.877: INFO: Trying to dial the pod
Sep  6 15:54:02.885: INFO: Controller my-hostname-basic-866587e6-d0be-11e9-9ba3-f679ea11f394: Got expected result from replica 1 [my-hostname-basic-866587e6-d0be-11e9-9ba3-f679ea11f394-qszj2]: "my-hostname-basic-866587e6-d0be-11e9-9ba3-f679ea11f394-qszj2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:54:02.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9268" for this suite.
Sep  6 15:54:08.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:54:08.962: INFO: namespace replicaset-9268 deletion completed in 6.074146306s

• [SLOW TEST:16.118 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:54:08.962: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 15:54:08.995: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9001694c-d0be-11e9-9ba3-f679ea11f394" in namespace "downward-api-9635" to be "success or failure"
Sep  6 15:54:08.998: INFO: Pod "downwardapi-volume-9001694c-d0be-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.646062ms
Sep  6 15:54:11.001: INFO: Pod "downwardapi-volume-9001694c-d0be-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006073933s
STEP: Saw pod success
Sep  6 15:54:11.002: INFO: Pod "downwardapi-volume-9001694c-d0be-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:54:11.004: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-9001694c-d0be-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 15:54:11.024: INFO: Waiting for pod downwardapi-volume-9001694c-d0be-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:54:11.027: INFO: Pod downwardapi-volume-9001694c-d0be-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:54:11.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9635" for this suite.
Sep  6 15:54:17.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:54:17.113: INFO: namespace downward-api-9635 deletion completed in 6.082460802s

• [SLOW TEST:8.150 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:54:17.113: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 15:54:17.148: INFO: Waiting up to 5m0s for pod "pod-94ddb055-d0be-11e9-9ba3-f679ea11f394" in namespace "emptydir-9096" to be "success or failure"
Sep  6 15:54:17.156: INFO: Pod "pod-94ddb055-d0be-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 8.179411ms
Sep  6 15:54:19.159: INFO: Pod "pod-94ddb055-d0be-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010996917s
STEP: Saw pod success
Sep  6 15:54:19.159: INFO: Pod "pod-94ddb055-d0be-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 15:54:19.161: INFO: Trying to get logs from node metalk8s-23 pod pod-94ddb055-d0be-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 15:54:19.176: INFO: Waiting for pod pod-94ddb055-d0be-11e9-9ba3-f679ea11f394 to disappear
Sep  6 15:54:19.179: INFO: Pod pod-94ddb055-d0be-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:54:19.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9096" for this suite.
Sep  6 15:54:25.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:54:25.253: INFO: namespace emptydir-9096 deletion completed in 6.071422942s

• [SLOW TEST:8.140 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:54:25.253: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8072
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep  6 15:54:25.303: INFO: Found 0 stateful pods, waiting for 3
Sep  6 15:54:35.306: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:54:35.306: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:54:35.306: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:54:35.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8072 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:54:35.575: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:54:35.575: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:54:35.575: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  6 15:54:45.602: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  6 15:54:55.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8072 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 15:54:55.801: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 15:54:55.801: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 15:54:55.801: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 15:55:05.814: INFO: Waiting for StatefulSet statefulset-8072/ss2 to complete update
Sep  6 15:55:05.814: INFO: Waiting for Pod statefulset-8072/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 15:55:05.814: INFO: Waiting for Pod statefulset-8072/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 15:55:05.814: INFO: Waiting for Pod statefulset-8072/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 15:55:15.819: INFO: Waiting for StatefulSet statefulset-8072/ss2 to complete update
Sep  6 15:55:15.819: INFO: Waiting for Pod statefulset-8072/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 15:55:15.819: INFO: Waiting for Pod statefulset-8072/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Sep  6 15:55:25.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8072 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 15:55:26.039: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 15:55:26.039: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 15:55:26.039: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 15:55:36.066: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  6 15:55:46.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-262345838 exec --namespace=statefulset-8072 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 15:55:46.269: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 15:55:46.269: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 15:55:46.269: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 15:55:56.282: INFO: Waiting for StatefulSet statefulset-8072/ss2 to complete update
Sep  6 15:55:56.282: INFO: Waiting for Pod statefulset-8072/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 15:56:06.288: INFO: Deleting all statefulset in ns statefulset-8072
Sep  6 15:56:06.291: INFO: Scaling statefulset ss2 to 0
Sep  6 15:56:46.303: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 15:56:46.305: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:56:46.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8072" for this suite.
Sep  6 15:56:52.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:56:52.391: INFO: namespace statefulset-8072 deletion completed in 6.075002178s

• [SLOW TEST:147.138 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:56:52.391: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 15:56:52.413: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:56:54.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9490" for this suite.
Sep  6 15:57:44.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:57:44.652: INFO: namespace pods-9490 deletion completed in 50.21257985s

• [SLOW TEST:52.261 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:57:44.652: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2253
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep  6 15:57:44.714: INFO: Found 0 stateful pods, waiting for 3
Sep  6 15:57:54.717: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:57:54.717: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:57:54.717: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  6 15:57:54.742: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  6 15:58:04.774: INFO: Updating stateful set ss2
Sep  6 15:58:04.781: INFO: Waiting for Pod statefulset-2253/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Sep  6 15:58:14.887: INFO: Found 2 stateful pods, waiting for 3
Sep  6 15:58:24.890: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:58:24.890: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 15:58:24.890: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  6 15:58:24.916: INFO: Updating stateful set ss2
Sep  6 15:58:24.924: INFO: Waiting for Pod statefulset-2253/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 15:58:34.948: INFO: Updating stateful set ss2
Sep  6 15:58:34.956: INFO: Waiting for StatefulSet statefulset-2253/ss2 to complete update
Sep  6 15:58:34.956: INFO: Waiting for Pod statefulset-2253/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 15:58:44.962: INFO: Deleting all statefulset in ns statefulset-2253
Sep  6 15:58:44.964: INFO: Scaling statefulset ss2 to 0
Sep  6 15:58:54.976: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 15:58:54.979: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 15:58:54.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2253" for this suite.
Sep  6 15:59:01.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 15:59:01.071: INFO: namespace statefulset-2253 deletion completed in 6.070709178s

• [SLOW TEST:76.419 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 15:59:01.071: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-3259
Sep  6 15:59:03.106: INFO: Started pod liveness-exec in namespace container-probe-3259
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 15:59:03.109: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 16:03:04.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3259" for this suite.
Sep  6 16:03:10.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 16:03:10.451: INFO: namespace container-probe-3259 deletion completed in 6.089182904s

• [SLOW TEST:249.379 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 16:03:10.451: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 16:03:10.491: INFO: Waiting up to 5m0s for pod "pod-d2c2bd29-d0bf-11e9-9ba3-f679ea11f394" in namespace "emptydir-2869" to be "success or failure"
Sep  6 16:03:10.494: INFO: Pod "pod-d2c2bd29-d0bf-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 3.060024ms
Sep  6 16:03:12.496: INFO: Pod "pod-d2c2bd29-d0bf-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005133323s
STEP: Saw pod success
Sep  6 16:03:12.496: INFO: Pod "pod-d2c2bd29-d0bf-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 16:03:12.497: INFO: Trying to get logs from node metalk8s-23 pod pod-d2c2bd29-d0bf-11e9-9ba3-f679ea11f394 container test-container: <nil>
STEP: delete the pod
Sep  6 16:03:12.508: INFO: Waiting for pod pod-d2c2bd29-d0bf-11e9-9ba3-f679ea11f394 to disappear
Sep  6 16:03:12.511: INFO: Pod pod-d2c2bd29-d0bf-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 16:03:12.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2869" for this suite.
Sep  6 16:03:18.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 16:03:18.581: INFO: namespace emptydir-2869 deletion completed in 6.068220178s

• [SLOW TEST:8.130 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 16:03:18.581: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 16:03:18.614: INFO: Waiting up to 5m0s for pod "downward-api-d79a4db4-d0bf-11e9-9ba3-f679ea11f394" in namespace "downward-api-6698" to be "success or failure"
Sep  6 16:03:18.621: INFO: Pod "downward-api-d79a4db4-d0bf-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 7.589409ms
Sep  6 16:03:20.624: INFO: Pod "downward-api-d79a4db4-d0bf-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010549186s
STEP: Saw pod success
Sep  6 16:03:20.624: INFO: Pod "downward-api-d79a4db4-d0bf-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 16:03:20.626: INFO: Trying to get logs from node metalk8s-23 pod downward-api-d79a4db4-d0bf-11e9-9ba3-f679ea11f394 container dapi-container: <nil>
STEP: delete the pod
Sep  6 16:03:20.646: INFO: Waiting for pod downward-api-d79a4db4-d0bf-11e9-9ba3-f679ea11f394 to disappear
Sep  6 16:03:20.647: INFO: Pod downward-api-d79a4db4-d0bf-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 16:03:20.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6698" for this suite.
Sep  6 16:03:26.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 16:03:26.740: INFO: namespace downward-api-6698 deletion completed in 6.090472344s

• [SLOW TEST:8.158 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 16:03:26.740: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 16:03:30.822: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:30.824: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 16:03:32.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:32.826: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 16:03:34.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:34.826: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 16:03:36.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:36.826: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 16:03:38.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:38.827: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 16:03:40.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:40.826: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 16:03:42.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:42.827: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 16:03:44.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:44.827: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 16:03:46.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:46.826: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 16:03:48.824: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 16:03:48.826: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 16:03:48.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1039" for this suite.
Sep  6 16:04:10.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 16:04:10.902: INFO: namespace container-lifecycle-hook-1039 deletion completed in 22.073650438s

• [SLOW TEST:44.162 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 16:04:10.902: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 16:04:10.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6c98b41-d0bf-11e9-9ba3-f679ea11f394" in namespace "projected-4967" to be "success or failure"
Sep  6 16:04:10.931: INFO: Pod "downwardapi-volume-f6c98b41-d0bf-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020918ms
Sep  6 16:04:12.933: INFO: Pod "downwardapi-volume-f6c98b41-d0bf-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004607251s
STEP: Saw pod success
Sep  6 16:04:12.933: INFO: Pod "downwardapi-volume-f6c98b41-d0bf-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 16:04:12.935: INFO: Trying to get logs from node metalk8s-23 pod downwardapi-volume-f6c98b41-d0bf-11e9-9ba3-f679ea11f394 container client-container: <nil>
STEP: delete the pod
Sep  6 16:04:12.946: INFO: Waiting for pod downwardapi-volume-f6c98b41-d0bf-11e9-9ba3-f679ea11f394 to disappear
Sep  6 16:04:12.950: INFO: Pod downwardapi-volume-f6c98b41-d0bf-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 16:04:12.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4967" for this suite.
Sep  6 16:04:18.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 16:04:19.028: INFO: namespace projected-4967 deletion completed in 6.075606427s

• [SLOW TEST:8.126 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 16:04:19.029: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fba22117-d0bf-11e9-9ba3-f679ea11f394
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fba22117-d0bf-11e9-9ba3-f679ea11f394
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 16:04:23.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4482" for this suite.
Sep  6 16:04:41.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 16:04:41.203: INFO: namespace projected-4482 deletion completed in 18.109088867s

• [SLOW TEST:22.174 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 16:04:41.203: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-7741/secret-test-08d93783-d0c0-11e9-9ba3-f679ea11f394
STEP: Creating a pod to test consume secrets
Sep  6 16:04:41.233: INFO: Waiting up to 5m0s for pod "pod-configmaps-08d98c1d-d0c0-11e9-9ba3-f679ea11f394" in namespace "secrets-7741" to be "success or failure"
Sep  6 16:04:41.238: INFO: Pod "pod-configmaps-08d98c1d-d0c0-11e9-9ba3-f679ea11f394": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914208ms
Sep  6 16:04:43.241: INFO: Pod "pod-configmaps-08d98c1d-d0c0-11e9-9ba3-f679ea11f394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007713934s
STEP: Saw pod success
Sep  6 16:04:43.241: INFO: Pod "pod-configmaps-08d98c1d-d0c0-11e9-9ba3-f679ea11f394" satisfied condition "success or failure"
Sep  6 16:04:43.243: INFO: Trying to get logs from node metalk8s-23 pod pod-configmaps-08d98c1d-d0c0-11e9-9ba3-f679ea11f394 container env-test: <nil>
STEP: delete the pod
Sep  6 16:04:43.253: INFO: Waiting for pod pod-configmaps-08d98c1d-d0c0-11e9-9ba3-f679ea11f394 to disappear
Sep  6 16:04:43.255: INFO: Pod pod-configmaps-08d98c1d-d0c0-11e9-9ba3-f679ea11f394 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 16:04:43.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7741" for this suite.
Sep  6 16:04:49.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 16:04:49.326: INFO: namespace secrets-7741 deletion completed in 6.068693577s

• [SLOW TEST:8.123 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 16:04:49.326: INFO: >>> kubeConfig: /tmp/kubeconfig-262345838
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 16:04:51.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8363" for this suite.
Sep  6 16:05:41.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 16:05:41.467: INFO: namespace kubelet-test-8363 deletion completed in 50.089302129s

• [SLOW TEST:52.141 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSep  6 16:05:41.467: INFO: Running AfterSuite actions on all nodes
Sep  6 16:05:41.467: INFO: Running AfterSuite actions on node 1
Sep  6 16:05:41.467: INFO: Skipping dumping logs from cluster

Ran 204 of 3586 Specs in 4968.187 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3382 Skipped PASS

Ginkgo ran 1 suite in 1h22m49.360235172s
Test Suite Passed
