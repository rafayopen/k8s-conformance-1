I0702 19:13:40.111519      15 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-362853403
I0702 19:13:40.111667      15 e2e.go:240] Starting e2e run "7f000675-9cfd-11e9-808f-7ea354b18644" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1562094818 - Will randomize all specs
Will run 204 of 3584 specs

Jul  2 19:13:40.451: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 19:13:40.457: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul  2 19:13:40.486: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul  2 19:13:40.589: INFO: 26 / 26 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul  2 19:13:40.589: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jul  2 19:13:40.589: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul  2 19:13:40.607: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul  2 19:13:40.607: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Jul  2 19:13:40.607: INFO: e2e test version: v1.14.1
Jul  2 19:13:40.611: INFO: kube-apiserver version: v1.14.1
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:13:40.625: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
Jul  2 19:13:43.958: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jul  2 19:13:43.960: INFO: namespace kubectl-3434
Jul  2 19:13:43.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-3434'
Jul  2 19:13:47.012: INFO: stderr: ""
Jul  2 19:13:47.013: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  2 19:13:48.027: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 19:13:48.027: INFO: Found 0 / 1
Jul  2 19:13:49.024: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 19:13:49.024: INFO: Found 0 / 1
Jul  2 19:13:50.019: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 19:13:50.019: INFO: Found 0 / 1
Jul  2 19:13:51.020: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 19:13:51.020: INFO: Found 0 / 1
Jul  2 19:13:52.020: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 19:13:52.021: INFO: Found 1 / 1
Jul  2 19:13:52.021: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  2 19:13:52.027: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 19:13:52.027: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  2 19:13:52.027: INFO: wait on redis-master startup in kubectl-3434 
Jul  2 19:13:52.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 logs redis-master-ppgz7 redis-master --namespace=kubectl-3434'
Jul  2 19:13:52.232: INFO: stderr: ""
Jul  2 19:13:52.232: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 02 Jul 19:14:20.417 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 Jul 19:14:20.417 # Server started, Redis version 3.2.12\n1:M 02 Jul 19:14:20.417 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 Jul 19:14:20.417 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul  2 19:13:52.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3434'
Jul  2 19:13:52.460: INFO: stderr: ""
Jul  2 19:13:52.460: INFO: stdout: "service/rm2 exposed\n"
Jul  2 19:13:52.467: INFO: Service rm2 in namespace kubectl-3434 found.
STEP: exposing service
Jul  2 19:13:54.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3434'
Jul  2 19:13:54.740: INFO: stderr: ""
Jul  2 19:13:54.740: INFO: stdout: "service/rm3 exposed\n"
Jul  2 19:13:54.747: INFO: Service rm3 in namespace kubectl-3434 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:13:56.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3434" for this suite.
Jul  2 19:14:20.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:14:21.022: INFO: namespace kubectl-3434 deletion completed in 24.256220163s

â€¢ [SLOW TEST:40.398 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:14:21.024: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  2 19:14:21.114: INFO: Waiting up to 5m0s for pod "pod-9881d37b-9cfd-11e9-808f-7ea354b18644" in namespace "emptydir-3681" to be "success or failure"
Jul  2 19:14:21.126: INFO: Pod "pod-9881d37b-9cfd-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 11.547373ms
Jul  2 19:14:23.133: INFO: Pod "pod-9881d37b-9cfd-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018333065s
Jul  2 19:14:25.141: INFO: Pod "pod-9881d37b-9cfd-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026458327s
STEP: Saw pod success
Jul  2 19:14:25.141: INFO: Pod "pod-9881d37b-9cfd-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:14:25.147: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-9881d37b-9cfd-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 19:14:25.194: INFO: Waiting for pod pod-9881d37b-9cfd-11e9-808f-7ea354b18644 to disappear
Jul  2 19:14:25.198: INFO: Pod pod-9881d37b-9cfd-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:14:25.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3681" for this suite.
Jul  2 19:14:31.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:14:31.421: INFO: namespace emptydir-3681 deletion completed in 6.214881187s

â€¢ [SLOW TEST:10.397 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:14:31.424: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9635
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  2 19:14:31.492: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  2 19:15:45.887: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.0.6:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9635 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 19:15:45.887: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 19:15:46.236: INFO: Found all expected endpoints: [netserver-0]
Jul  2 19:15:46.241: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.44.0.5:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9635 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 19:15:46.241: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 19:15:46.586: INFO: Found all expected endpoints: [netserver-1]
Jul  2 19:15:46.590: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.40.0.5:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9635 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 19:15:46.590: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 19:15:46.853: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:15:46.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9635" for this suite.
Jul  2 19:16:14.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:16:15.072: INFO: namespace pod-network-test-9635 deletion completed in 28.208911731s

â€¢ [SLOW TEST:59.515 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:16:15.073: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  2 19:16:15.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2066'
Jul  2 19:16:15.642: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  2 19:16:15.642: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Jul  2 19:16:15.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete jobs e2e-test-nginx-job --namespace=kubectl-2066'
Jul  2 19:16:15.775: INFO: stderr: ""
Jul  2 19:16:15.775: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:16:15.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2066" for this suite.
Jul  2 19:16:27.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:16:28.014: INFO: namespace kubectl-2066 deletion completed in 12.233904417s

â€¢ [SLOW TEST:12.941 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:16:28.015: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul  2 19:16:34.685: INFO: Successfully updated pod "annotationupdatee436935d-9cfd-11e9-808f-7ea354b18644"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:16:36.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2003" for this suite.
Jul  2 19:17:10.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:17:10.676: INFO: namespace projected-2003 deletion completed in 33.954631559s

â€¢ [SLOW TEST:42.662 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:17:10.677: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3240/configmap-test-fdc088b6-9cfd-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 19:17:10.972: INFO: Waiting up to 5m0s for pod "pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644" in namespace "configmap-3240" to be "success or failure"
Jul  2 19:17:10.982: INFO: Pod "pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 10.35647ms
Jul  2 19:17:13.158: INFO: Pod "pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185962428s
Jul  2 19:17:15.908: INFO: Pod "pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936201234s
Jul  2 19:17:18.608: INFO: Pod "pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.63674997s
Jul  2 19:17:20.618: INFO: Pod "pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.646339675s
Jul  2 19:17:22.626: INFO: Pod "pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.65472482s
STEP: Saw pod success
Jul  2 19:17:22.627: INFO: Pod "pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:17:22.640: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644 container env-test: <nil>
STEP: delete the pod
Jul  2 19:17:22.700: INFO: Waiting for pod pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644 to disappear
Jul  2 19:17:22.706: INFO: Pod pod-configmaps-fdc291bd-9cfd-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:17:22.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3240" for this suite.
Jul  2 19:17:40.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:17:40.906: INFO: namespace configmap-3240 deletion completed in 18.191913202s

â€¢ [SLOW TEST:30.229 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:17:40.907: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0702 19:17:51.488113      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  2 19:17:51.488: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:17:51.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5612" for this suite.
Jul  2 19:18:03.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:18:03.823: INFO: namespace gc-5612 deletion completed in 12.327999583s

â€¢ [SLOW TEST:22.917 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:18:03.835: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-1e9e9a23-9cfe-11e9-808f-7ea354b18644
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:18:14.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9956" for this suite.
Jul  2 19:18:36.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:18:37.079: INFO: namespace configmap-9956 deletion completed in 22.287773205s

â€¢ [SLOW TEST:33.244 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:18:37.080: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  2 19:18:37.176: INFO: Waiting up to 5m0s for pod "downward-api-31233f07-9cfe-11e9-808f-7ea354b18644" in namespace "downward-api-1229" to be "success or failure"
Jul  2 19:18:37.186: INFO: Pod "downward-api-31233f07-9cfe-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.720322ms
Jul  2 19:18:39.194: INFO: Pod "downward-api-31233f07-9cfe-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018048415s
Jul  2 19:18:41.214: INFO: Pod "downward-api-31233f07-9cfe-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038105497s
Jul  2 19:18:43.222: INFO: Pod "downward-api-31233f07-9cfe-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046008958s
STEP: Saw pod success
Jul  2 19:18:43.222: INFO: Pod "downward-api-31233f07-9cfe-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:18:43.230: INFO: Trying to get logs from node ah-kres-worker-1 pod downward-api-31233f07-9cfe-11e9-808f-7ea354b18644 container dapi-container: <nil>
STEP: delete the pod
Jul  2 19:18:43.279: INFO: Waiting for pod downward-api-31233f07-9cfe-11e9-808f-7ea354b18644 to disappear
Jul  2 19:18:43.286: INFO: Pod downward-api-31233f07-9cfe-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:18:43.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1229" for this suite.
Jul  2 19:18:49.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:18:49.556: INFO: namespace downward-api-1229 deletion completed in 6.261027916s

â€¢ [SLOW TEST:12.476 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:18:49.558: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 19:18:49.664: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul  2 19:18:54.671: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  2 19:18:54.672: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul  2 19:18:56.680: INFO: Creating deployment "test-rollover-deployment"
Jul  2 19:18:56.701: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul  2 19:18:58.721: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul  2 19:18:58.741: INFO: Ensure that both replica sets have 1 created replica
Jul  2 19:18:58.764: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul  2 19:18:58.787: INFO: Updating deployment test-rollover-deployment
Jul  2 19:18:58.787: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul  2 19:19:00.801: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul  2 19:19:00.814: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul  2 19:19:00.826: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:00.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691939, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:02.842: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:02.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691942, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:04.843: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:04.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691942, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:06.841: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:06.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691942, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:08.842: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:08.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691942, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:10.838: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:10.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691942, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:16.762: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:16.763: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691942, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:16.851: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:16.851: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691942, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:18.844: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:18.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691942, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:20.838: INFO: all replica sets need to contain the pod-template-hash label
Jul  2 19:19:20.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691942, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697691936, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 19:19:22.841: INFO: 
Jul  2 19:19:22.841: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  2 19:19:22.853: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5138,SelfLink:/apis/apps/v1/namespaces/deployment-5138/deployments/test-rollover-deployment,UID:4b7a142d-9cfe-11e9-ab09-0050569fbfc5,ResourceVersion:632546,Generation:2,CreationTimestamp:2019-07-02 19:19:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-02 19:18:56 +0000 UTC 2019-07-02 19:18:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-02 19:19:22 +0000 UTC 2019-07-02 19:18:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  2 19:19:22.859: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-5138,SelfLink:/apis/apps/v1/namespaces/deployment-5138/replicasets/test-rollover-deployment-766b4d6c9d,UID:424612d7-9cfe-11e9-9994-0050569f2898,ResourceVersion:632535,Generation:2,CreationTimestamp:2019-07-02 19:19:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4b7a142d-9cfe-11e9-ab09-0050569fbfc5 0xc000c03ef7 0xc000c03ef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  2 19:19:22.859: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul  2 19:19:22.860: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-5138,SelfLink:/apis/apps/v1/namespaces/deployment-5138/replicasets/test-rollover-deployment-6455657675,UID:40f5f2f6-9cfe-11e9-9994-0050569f2898,ResourceVersion:632484,Generation:2,CreationTimestamp:2019-07-02 19:19:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4b7a142d-9cfe-11e9-ab09-0050569fbfc5 0xc000c03e17 0xc000c03e18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  2 19:19:22.860: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5138,SelfLink:/apis/apps/v1/namespaces/deployment-5138/replicasets/test-rollover-controller,UID:47473622-9cfe-11e9-ab09-0050569fbfc5,ResourceVersion:632544,Generation:2,CreationTimestamp:2019-07-02 19:19:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4b7a142d-9cfe-11e9-ab09-0050569fbfc5 0xc000c03d47 0xc000c03d48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  2 19:19:22.866: INFO: Pod "test-rollover-deployment-766b4d6c9d-f9vpt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-f9vpt,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-5138,SelfLink:/api/v1/namespaces/deployment-5138/pods/test-rollover-deployment-766b4d6c9d-f9vpt,UID:4254fbb7-9cfe-11e9-9994-0050569f2898,ResourceVersion:632495,Generation:0,CreationTimestamp:2019-07-02 19:19:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 424612d7-9cfe-11e9-9994-0050569f2898 0xc000926c87 0xc000926c88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9sjj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9sjj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-9sjj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000926d00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000926d20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:19:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:19:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:19:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:19:06 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:10.44.0.6,StartTime:2019-07-02 19:19:04 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-02 19:19:07 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9efbd914f9ccd7fefbdb5e9382447a30a073bdb292f3623e2721d46d5b295014}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:19:22.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5138" for this suite.
Jul  2 19:19:34.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:19:35.079: INFO: namespace deployment-5138 deletion completed in 12.206540595s

â€¢ [SLOW TEST:45.521 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:19:35.081: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  2 19:19:35.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3345'
Jul  2 19:19:36.014: INFO: stderr: ""
Jul  2 19:19:36.014: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Jul  2 19:19:36.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete pods e2e-test-nginx-pod --namespace=kubectl-3345'
Jul  2 19:19:39.239: INFO: stderr: ""
Jul  2 19:19:39.239: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:19:39.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3345" for this suite.
Jul  2 19:19:47.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:19:47.500: INFO: namespace kubectl-3345 deletion completed in 8.249947952s

â€¢ [SLOW TEST:12.419 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:19:47.504: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Jul  2 19:19:47.590: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul  2 19:19:47.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-3926'
Jul  2 19:19:48.471: INFO: stderr: ""
Jul  2 19:19:48.471: INFO: stdout: "service/redis-slave created\n"
Jul  2 19:19:48.471: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul  2 19:19:48.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-3926'
Jul  2 19:19:49.023: INFO: stderr: ""
Jul  2 19:19:49.023: INFO: stdout: "service/redis-master created\n"
Jul  2 19:19:49.023: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul  2 19:19:49.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-3926'
Jul  2 19:19:49.259: INFO: stderr: ""
Jul  2 19:19:49.259: INFO: stdout: "service/frontend created\n"
Jul  2 19:19:49.261: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul  2 19:19:49.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-3926'
Jul  2 19:19:49.487: INFO: stderr: ""
Jul  2 19:19:49.487: INFO: stdout: "deployment.apps/frontend created\n"
Jul  2 19:19:49.488: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul  2 19:19:49.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-3926'
Jul  2 19:19:51.661: INFO: stderr: ""
Jul  2 19:19:51.661: INFO: stdout: "deployment.apps/redis-master created\n"
Jul  2 19:19:51.661: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul  2 19:19:51.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-3926'
Jul  2 19:19:51.991: INFO: stderr: ""
Jul  2 19:19:51.991: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul  2 19:19:51.991: INFO: Waiting for all frontend pods to be Running.
Jul  2 19:20:42.046: INFO: Waiting for frontend to serve content.
Jul  2 19:20:46.814: INFO: Trying to add a new entry to the guestbook.
Jul  2 19:20:46.846: INFO: Verifying that added entry can be retrieved.
Jul  2 19:20:49.130: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Jul  2 19:20:54.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-3926'
Jul  2 19:20:55.159: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 19:20:55.159: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul  2 19:20:55.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-3926'
Jul  2 19:20:56.211: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 19:20:56.211: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul  2 19:20:56.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-3926'
Jul  2 19:20:56.451: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 19:20:56.451: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  2 19:20:56.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-3926'
Jul  2 19:20:58.484: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 19:20:58.484: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  2 19:20:58.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-3926'
Jul  2 19:20:58.680: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 19:20:58.680: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul  2 19:20:58.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-3926'
Jul  2 19:20:58.952: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 19:20:58.952: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:20:58.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3926" for this suite.
Jul  2 19:22:00.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:22:01.166: INFO: namespace kubectl-3926 deletion completed in 1m2.205482949s

â€¢ [SLOW TEST:133.663 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:22:01.167: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Jul  2 19:22:01.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-394'
Jul  2 19:22:46.359: INFO: stderr: ""
Jul  2 19:22:46.359: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  2 19:22:46.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-394'
Jul  2 19:22:46.910: INFO: stderr: ""
Jul  2 19:22:46.910: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Jul  2 19:22:51.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-394'
Jul  2 19:22:52.119: INFO: stderr: ""
Jul  2 19:22:52.119: INFO: stdout: "update-demo-nautilus-6l6sf update-demo-nautilus-sqplq "
Jul  2 19:22:52.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-6l6sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jul  2 19:22:52.292: INFO: stderr: ""
Jul  2 19:22:52.292: INFO: stdout: ""
Jul  2 19:22:52.292: INFO: update-demo-nautilus-6l6sf is created but not running
Jul  2 19:22:57.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-394'
Jul  2 19:23:04.125: INFO: stderr: ""
Jul  2 19:23:04.125: INFO: stdout: "update-demo-nautilus-6l6sf update-demo-nautilus-sqplq "
Jul  2 19:23:04.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-6l6sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jul  2 19:23:04.648: INFO: stderr: ""
Jul  2 19:23:04.648: INFO: stdout: "true"
Jul  2 19:23:04.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-6l6sf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jul  2 19:23:04.816: INFO: stderr: ""
Jul  2 19:23:04.816: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 19:23:04.816: INFO: validating pod update-demo-nautilus-6l6sf
Jul  2 19:23:04.828: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 19:23:04.829: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 19:23:04.829: INFO: update-demo-nautilus-6l6sf is verified up and running
Jul  2 19:23:04.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-sqplq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jul  2 19:23:04.994: INFO: stderr: ""
Jul  2 19:23:04.994: INFO: stdout: "true"
Jul  2 19:23:04.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-sqplq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jul  2 19:23:05.170: INFO: stderr: ""
Jul  2 19:23:05.170: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 19:23:05.170: INFO: validating pod update-demo-nautilus-sqplq
Jul  2 19:23:05.181: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 19:23:05.181: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 19:23:05.181: INFO: update-demo-nautilus-sqplq is verified up and running
STEP: rolling-update to new replication controller
Jul  2 19:23:05.185: INFO: scanned /root for discovery docs: <nil>
Jul  2 19:23:05.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-394'
Jul  2 19:23:55.465: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul  2 19:23:55.465: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  2 19:23:55.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-394'
Jul  2 19:23:55.823: INFO: stderr: ""
Jul  2 19:23:55.823: INFO: stdout: "update-demo-kitten-hbldb update-demo-kitten-hw6nx "
Jul  2 19:23:55.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-kitten-hbldb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jul  2 19:23:56.004: INFO: stderr: ""
Jul  2 19:23:56.004: INFO: stdout: "true"
Jul  2 19:23:56.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-kitten-hbldb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jul  2 19:23:56.215: INFO: stderr: ""
Jul  2 19:23:56.216: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul  2 19:23:56.216: INFO: validating pod update-demo-kitten-hbldb
Jul  2 19:23:56.224: INFO: got data: {
  "image": "kitten.jpg"
}

Jul  2 19:23:56.225: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul  2 19:23:56.225: INFO: update-demo-kitten-hbldb is verified up and running
Jul  2 19:23:56.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-kitten-hw6nx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jul  2 19:23:56.368: INFO: stderr: ""
Jul  2 19:23:56.368: INFO: stdout: "true"
Jul  2 19:23:56.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-kitten-hw6nx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-394'
Jul  2 19:23:56.570: INFO: stderr: ""
Jul  2 19:23:56.570: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul  2 19:23:56.570: INFO: validating pod update-demo-kitten-hw6nx
Jul  2 19:23:56.583: INFO: got data: {
  "image": "kitten.jpg"
}

Jul  2 19:23:56.583: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul  2 19:23:56.583: INFO: update-demo-kitten-hw6nx is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:23:56.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-394" for this suite.
Jul  2 19:24:30.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:24:30.827: INFO: namespace kubectl-394 deletion completed in 34.23572107s

â€¢ [SLOW TEST:106.319 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:24:30.828: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-03fd1482-9cff-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 19:24:31.277: INFO: Waiting up to 5m0s for pod "pod-secrets-0411b5d7-9cff-11e9-808f-7ea354b18644" in namespace "secrets-3849" to be "success or failure"
Jul  2 19:24:31.283: INFO: Pod "pod-secrets-0411b5d7-9cff-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.319638ms
Jul  2 19:24:33.291: INFO: Pod "pod-secrets-0411b5d7-9cff-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014196326s
Jul  2 19:24:41.725: INFO: Pod "pod-secrets-0411b5d7-9cff-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.448279836s
STEP: Saw pod success
Jul  2 19:24:41.725: INFO: Pod "pod-secrets-0411b5d7-9cff-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:24:41.735: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-secrets-0411b5d7-9cff-11e9-808f-7ea354b18644 container secret-volume-test: <nil>
STEP: delete the pod
Jul  2 19:24:41.804: INFO: Waiting for pod pod-secrets-0411b5d7-9cff-11e9-808f-7ea354b18644 to disappear
Jul  2 19:24:41.808: INFO: Pod pod-secrets-0411b5d7-9cff-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:24:41.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3849" for this suite.
Jul  2 19:24:47.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:24:48.104: INFO: namespace secrets-3849 deletion completed in 6.289135656s

â€¢ [SLOW TEST:17.277 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:24:48.114: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Jul  2 19:24:48.268: INFO: Waiting up to 5m0s for pod "var-expansion-0e5318c6-9cff-11e9-808f-7ea354b18644" in namespace "var-expansion-4924" to be "success or failure"
Jul  2 19:24:48.278: INFO: Pod "var-expansion-0e5318c6-9cff-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 10.343242ms
Jul  2 19:24:50.283: INFO: Pod "var-expansion-0e5318c6-9cff-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015026661s
Jul  2 19:24:56.532: INFO: Pod "var-expansion-0e5318c6-9cff-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.264470796s
STEP: Saw pod success
Jul  2 19:24:56.533: INFO: Pod "var-expansion-0e5318c6-9cff-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:24:56.558: INFO: Trying to get logs from node ah-kres-worker-2 pod var-expansion-0e5318c6-9cff-11e9-808f-7ea354b18644 container dapi-container: <nil>
STEP: delete the pod
Jul  2 19:24:56.643: INFO: Waiting for pod var-expansion-0e5318c6-9cff-11e9-808f-7ea354b18644 to disappear
Jul  2 19:24:56.658: INFO: Pod var-expansion-0e5318c6-9cff-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:24:56.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4924" for this suite.
Jul  2 19:25:02.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:25:02.889: INFO: namespace var-expansion-4924 deletion completed in 6.217540997s

â€¢ [SLOW TEST:14.775 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:25:02.890: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2684
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-2684
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2684
Jul  2 19:25:03.049: INFO: Found 0 stateful pods, waiting for 1
Jul  2 19:25:13.520: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jul  2 19:25:23.055: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul  2 19:25:23.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 19:25:23.617: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 19:25:23.617: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 19:25:23.617: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  2 19:25:23.624: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  2 19:25:33.633: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  2 19:25:33.633: INFO: Waiting for statefulset status.replicas updated to 0
Jul  2 19:25:33.761: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:25:33.761: INFO: ss-0  ah-kres-worker-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:25:33.761: INFO: 
Jul  2 19:25:33.761: INFO: StatefulSet ss has not reached scale 3, at 1
Jul  2 19:25:34.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.905218897s
Jul  2 19:25:36.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.894960151s
Jul  2 19:25:37.225: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.45188307s
Jul  2 19:25:38.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.440851432s
Jul  2 19:25:39.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.42911217s
Jul  2 19:25:40.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.420555688s
Jul  2 19:25:41.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.413372885s
Jul  2 19:25:42.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.405140736s
Jul  2 19:25:43.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 383.724014ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2684
Jul  2 19:25:44.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:25:45.043: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  2 19:25:45.043: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  2 19:25:45.043: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  2 19:25:45.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:25:47.267: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  2 19:25:47.267: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  2 19:25:47.267: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  2 19:25:47.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:25:48.147: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  2 19:25:48.147: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  2 19:25:48.147: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  2 19:25:48.159: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 19:25:48.159: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 19:25:48.159: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul  2 19:25:48.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 19:25:48.745: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 19:25:48.745: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 19:25:48.745: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  2 19:25:48.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 19:25:49.243: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 19:25:49.243: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 19:25:49.243: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  2 19:25:49.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 19:25:49.665: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 19:25:49.665: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 19:25:49.665: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  2 19:25:49.665: INFO: Waiting for statefulset status.replicas updated to 0
Jul  2 19:25:49.672: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul  2 19:25:59.821: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  2 19:25:59.822: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  2 19:25:59.823: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  2 19:25:59.861: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:25:59.861: INFO: ss-0  ah-kres-worker-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:25:59.861: INFO: ss-1  ah-kres-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:25:59.861: INFO: ss-2  ah-kres-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:25:59.861: INFO: 
Jul  2 19:25:59.861: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  2 19:26:00.877: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:26:00.877: INFO: ss-0  ah-kres-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:26:00.877: INFO: ss-1  ah-kres-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:00.877: INFO: ss-2  ah-kres-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:00.877: INFO: 
Jul  2 19:26:00.878: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  2 19:26:01.883: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:26:01.883: INFO: ss-0  ah-kres-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:26:01.884: INFO: ss-1  ah-kres-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:01.884: INFO: ss-2  ah-kres-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:01.884: INFO: 
Jul  2 19:26:01.884: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  2 19:26:02.891: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:26:02.891: INFO: ss-0  ah-kres-worker-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:26:02.891: INFO: ss-1  ah-kres-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:02.891: INFO: ss-2  ah-kres-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:02.891: INFO: 
Jul  2 19:26:02.891: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  2 19:26:03.896: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:26:03.897: INFO: ss-0  ah-kres-worker-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:26:03.897: INFO: ss-1  ah-kres-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:03.897: INFO: ss-2  ah-kres-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:03.897: INFO: 
Jul  2 19:26:03.897: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  2 19:26:04.906: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:26:04.907: INFO: ss-0  ah-kres-worker-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:26:04.907: INFO: ss-2  ah-kres-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:04.907: INFO: 
Jul  2 19:26:04.908: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  2 19:26:07.664: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:26:07.665: INFO: ss-0  ah-kres-worker-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:26:07.666: INFO: ss-2  ah-kres-worker-1  Pending  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:32 +0000 UTC  }]
Jul  2 19:26:07.666: INFO: 
Jul  2 19:26:07.666: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  2 19:26:08.674: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:26:08.674: INFO: ss-0  ah-kres-worker-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:26:08.674: INFO: 
Jul  2 19:26:08.674: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  2 19:26:09.682: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jul  2 19:26:09.683: INFO: ss-0  ah-kres-worker-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:25:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 19:24:58 +0000 UTC  }]
Jul  2 19:26:09.683: INFO: 
Jul  2 19:26:09.683: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2684
Jul  2 19:26:10.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:26:11.059: INFO: rc: 1
Jul  2 19:26:11.059: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00182aba0 exit status 1 <nil> <nil> true [0xc0007bbdf8 0xc0007bbe18 0xc0007bbe68] [0xc0007bbdf8 0xc0007bbe18 0xc0007bbe68] [0xc0007bbe08 0xc0007bbe48] [0x9bf9f0 0x9bf9f0] 0xc002a161e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Jul  2 19:26:21.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:26:21.232: INFO: rc: 1
Jul  2 19:26:21.232: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d33da0 exit status 1 <nil> <nil> true [0xc0019cee90 0xc0019ceec8 0xc0019cef00] [0xc0019cee90 0xc0019ceec8 0xc0019cef00] [0xc0019ceea8 0xc0019ceef0] [0x9bf9f0 0x9bf9f0] 0xc0030dd0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:26:31.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:26:31.382: INFO: rc: 1
Jul  2 19:26:31.382: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001094300 exit status 1 <nil> <nil> true [0xc0019cef20 0xc0019cef50 0xc0019cef88] [0xc0019cef20 0xc0019cef50 0xc0019cef88] [0xc0019cef40 0xc0019cef78] [0x9bf9f0 0x9bf9f0] 0xc0030dd4a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:26:41.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:26:41.576: INFO: rc: 1
Jul  2 19:26:41.576: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00182af60 exit status 1 <nil> <nil> true [0xc0007bbe78 0xc0007bbed8 0xc0007bbf00] [0xc0007bbe78 0xc0007bbed8 0xc0007bbf00] [0xc0007bbea8 0xc0007bbef8] [0x9bf9f0 0x9bf9f0] 0xc002a16660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:26:51.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:26:52.239: INFO: rc: 1
Jul  2 19:26:52.239: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001094660 exit status 1 <nil> <nil> true [0xc0019cefa8 0xc0019cefe8 0xc0019cf030] [0xc0019cefa8 0xc0019cefe8 0xc0019cf030] [0xc0019cefc8 0xc0019cf028] [0x9bf9f0 0x9bf9f0] 0xc0030dd8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:27:02.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:27:03.008: INFO: rc: 1
Jul  2 19:27:03.008: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00182b290 exit status 1 <nil> <nil> true [0xc0007bbf10 0xc0007bbf70 0xc0007bbfa0] [0xc0007bbf10 0xc0007bbf70 0xc0007bbfa0] [0xc0007bbf50 0xc0007bbf90] [0x9bf9f0 0x9bf9f0] 0xc002a16a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:27:13.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:27:13.192: INFO: rc: 1
Jul  2 19:27:13.192: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0010946c0 exit status 1 <nil> <nil> true [0xc0007bbfb8 0xc0000c5e90 0xc0000c5f80] [0xc0007bbfb8 0xc0000c5e90 0xc0000c5f80] [0xc0000c5e60 0xc0000c5f30] [0x9bf9f0 0x9bf9f0] 0xc0030dd980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:27:23.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:27:23.347: INFO: rc: 1
Jul  2 19:27:23.347: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f16270 exit status 1 <nil> <nil> true [0xc000918018 0xc000918848 0xc000918d58] [0xc000918018 0xc000918848 0xc000918d58] [0xc0009181c8 0xc000918c38] [0x9bf9f0 0x9bf9f0] 0xc0030d42a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:27:33.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:27:33.502: INFO: rc: 1
Jul  2 19:27:33.502: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f16720 exit status 1 <nil> <nil> true [0xc000918d88 0xc000919160 0xc0009196d0] [0xc000918d88 0xc000919160 0xc0009196d0] [0xc000918ff0 0xc000919480] [0x9bf9f0 0x9bf9f0] 0xc0030d4600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:27:43.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:27:43.661: INFO: rc: 1
Jul  2 19:27:43.661: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f16a50 exit status 1 <nil> <nil> true [0xc0009198a8 0xc000919b80 0xc000919c58] [0xc0009198a8 0xc000919b80 0xc000919c58] [0xc000919b20 0xc000919c00] [0x9bf9f0 0x9bf9f0] 0xc0030d4960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:27:53.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:27:53.807: INFO: rc: 1
Jul  2 19:27:53.807: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d324e0 exit status 1 <nil> <nil> true [0xc0007ba1b0 0xc0007ba418 0xc0007ba648] [0xc0007ba1b0 0xc0007ba418 0xc0007ba648] [0xc0007ba400 0xc0007ba5b0] [0x9bf9f0 0x9bf9f0] 0xc0010e6840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:28:03.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:28:05.216: INFO: rc: 1
Jul  2 19:28:05.216: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d32840 exit status 1 <nil> <nil> true [0xc0007ba7c0 0xc0007baa30 0xc0007bab80] [0xc0007ba7c0 0xc0007baa30 0xc0007bab80] [0xc0007ba978 0xc0007bab60] [0x9bf9f0 0x9bf9f0] 0xc0010e6e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:28:15.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:28:16.455: INFO: rc: 1
Jul  2 19:28:16.455: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d32ba0 exit status 1 <nil> <nil> true [0xc0007bab88 0xc0007badd8 0xc0007bb048] [0xc0007bab88 0xc0007badd8 0xc0007bb048] [0xc0007baca8 0xc0007baf88] [0x9bf9f0 0x9bf9f0] 0xc0010e7200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:28:26.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:28:27.083: INFO: rc: 1
Jul  2 19:28:27.083: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d32f00 exit status 1 <nil> <nil> true [0xc0007bb128 0xc0007bb2b8 0xc0007bb390] [0xc0007bb128 0xc0007bb2b8 0xc0007bb390] [0xc0007bb258 0xc0007bb380] [0x9bf9f0 0x9bf9f0] 0xc0010e7560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:28:37.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:28:37.284: INFO: rc: 1
Jul  2 19:28:37.284: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d33260 exit status 1 <nil> <nil> true [0xc0007bb3e8 0xc0007bb5c0 0xc0007bb760] [0xc0007bb3e8 0xc0007bb5c0 0xc0007bb760] [0xc0007bb598 0xc0007bb718] [0x9bf9f0 0x9bf9f0] 0xc0010e78c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:28:47.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:28:53.651: INFO: rc: 1
Jul  2 19:28:53.652: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d335c0 exit status 1 <nil> <nil> true [0xc0007bb7b8 0xc0007bb8a0 0xc0007bb958] [0xc0007bb7b8 0xc0007bb8a0 0xc0007bb958] [0xc0007bb870 0xc0007bb8e0] [0x9bf9f0 0x9bf9f0] 0xc0010e7c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:29:03.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:29:04.352: INFO: rc: 1
Jul  2 19:29:04.352: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f16db0 exit status 1 <nil> <nil> true [0xc000919cf0 0xc000919d50 0xc000919e50] [0xc000919cf0 0xc000919d50 0xc000919e50] [0xc000919d38 0xc000919d80] [0x9bf9f0 0x9bf9f0] 0xc0030d4cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:29:14.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:29:14.543: INFO: rc: 1
Jul  2 19:29:14.543: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d338f0 exit status 1 <nil> <nil> true [0xc0007bba60 0xc0007bbb10 0xc0007bbbc0] [0xc0007bba60 0xc0007bbb10 0xc0007bbbc0] [0xc0007bbae0 0xc0007bbb70] [0x9bf9f0 0x9bf9f0] 0xc0028f2000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:29:24.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:29:29.883: INFO: rc: 1
Jul  2 19:29:29.883: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f162d0 exit status 1 <nil> <nil> true [0xc000918018 0xc000918848 0xc000918d58] [0xc000918018 0xc000918848 0xc000918d58] [0xc0009181c8 0xc000918c38] [0x9bf9f0 0x9bf9f0] 0xc0010e6840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:29:39.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:29:40.042: INFO: rc: 1
Jul  2 19:29:40.042: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d32480 exit status 1 <nil> <nil> true [0xc0007ba098 0xc0007ba400 0xc0007ba5b0] [0xc0007ba098 0xc0007ba400 0xc0007ba5b0] [0xc0007ba380 0xc0007ba448] [0x9bf9f0 0x9bf9f0] 0xc0030d42a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:29:50.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:29:51.494: INFO: rc: 1
Jul  2 19:29:51.494: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f16780 exit status 1 <nil> <nil> true [0xc000918d88 0xc000919160 0xc0009196d0] [0xc000918d88 0xc000919160 0xc0009196d0] [0xc000918ff0 0xc000919480] [0x9bf9f0 0x9bf9f0] 0xc0010e6e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:30:01.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:30:58.183: INFO: rc: 1
Jul  2 19:30:58.183: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d32810 exit status 1 <nil> <nil> true [0xc0007ba648 0xc0007ba978 0xc0007bab60] [0xc0007ba648 0xc0007ba978 0xc0007bab60] [0xc0007ba850 0xc0007bab00] [0x9bf9f0 0x9bf9f0] 0xc0030d4600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:31:08.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:31:10.049: INFO: rc: 1
Jul  2 19:31:10.050: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d32bd0 exit status 1 <nil> <nil> true [0xc0007bab80 0xc0007baca8 0xc0007baf88] [0xc0007bab80 0xc0007baca8 0xc0007baf88] [0xc0007babb8 0xc0007baec8] [0x9bf9f0 0x9bf9f0] 0xc0030d4960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:31:20.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:31:22.494: INFO: rc: 1
Jul  2 19:31:22.494: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f16b10 exit status 1 <nil> <nil> true [0xc0009198a8 0xc000919b80 0xc000919c58] [0xc0009198a8 0xc000919b80 0xc000919c58] [0xc000919b20 0xc000919c00] [0x9bf9f0 0x9bf9f0] 0xc0010e7200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:31:32.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:31:32.653: INFO: rc: 1
Jul  2 19:31:32.653: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f16ed0 exit status 1 <nil> <nil> true [0xc000919cf0 0xc000919d50 0xc000919e50] [0xc000919cf0 0xc000919d50 0xc000919e50] [0xc000919d38 0xc000919d80] [0x9bf9f0 0x9bf9f0] 0xc0010e7560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:31:42.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:31:42.840: INFO: rc: 1
Jul  2 19:31:42.840: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f17230 exit status 1 <nil> <nil> true [0xc000919e78 0xc000919f18 0xc0019ce090] [0xc000919e78 0xc000919f18 0xc0019ce090] [0xc000919ed8 0xc0019ce000] [0x9bf9f0 0x9bf9f0] 0xc0010e78c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:31:52.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:31:53.243: INFO: rc: 1
Jul  2 19:31:53.243: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f17590 exit status 1 <nil> <nil> true [0xc0019ce0a0 0xc0019ce150 0xc0019ce1b0] [0xc0019ce0a0 0xc0019ce150 0xc0019ce1b0] [0xc0019ce140 0xc0019ce168] [0x9bf9f0 0x9bf9f0] 0xc0010e7c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:32:03.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:32:03.399: INFO: rc: 1
Jul  2 19:32:03.399: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f178f0 exit status 1 <nil> <nil> true [0xc0019ce1f0 0xc0019ce258 0xc0019ce2a0] [0xc0019ce1f0 0xc0019ce258 0xc0019ce2a0] [0xc0019ce250 0xc0019ce290] [0x9bf9f0 0x9bf9f0] 0xc0028f25a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jul  2 19:32:13.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2684 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:32:13.769: INFO: rc: 1
Jul  2 19:32:13.769: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Jul  2 19:32:13.770: INFO: Scaling statefulset ss to 0
Jul  2 19:32:13.843: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  2 19:32:13.853: INFO: Deleting all statefulset in ns statefulset-2684
Jul  2 19:32:13.871: INFO: Scaling statefulset ss to 0
Jul  2 19:32:13.891: INFO: Waiting for statefulset status.replicas updated to 0
Jul  2 19:32:13.900: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:32:13.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2684" for this suite.
Jul  2 19:32:20.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:32:20.345: INFO: namespace statefulset-2684 deletion completed in 6.369737777s

â€¢ [SLOW TEST:384.660 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:32:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-1bd4dc72-9d00-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 19:32:20.428: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644" in namespace "projected-1264" to be "success or failure"
Jul  2 19:32:20.438: INFO: Pod "pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.904717ms
Jul  2 19:32:22.454: INFO: Pod "pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025534594s
Jul  2 19:32:30.746: INFO: Pod "pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 10.317880017s
Jul  2 19:32:32.753: INFO: Pod "pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 12.325170189s
Jul  2 19:32:37.748: INFO: Pod "pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 17.319822159s
Jul  2 19:32:39.785: INFO: Pod "pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 19.357146453s
STEP: Saw pod success
Jul  2 19:32:39.785: INFO: Pod "pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:32:39.796: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 19:32:39.928: INFO: Waiting for pod pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644 to disappear
Jul  2 19:32:39.955: INFO: Pod pod-projected-configmaps-1bd6b21c-9d00-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:32:39.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1264" for this suite.
Jul  2 19:33:06.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:33:06.325: INFO: namespace projected-1264 deletion completed in 26.326419999s

â€¢ [SLOW TEST:45.979 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:33:06.326: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul  2 19:33:06.417: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:33:23.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8328" for this suite.
Jul  2 19:33:45.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:33:45.593: INFO: namespace init-container-8328 deletion completed in 22.235435869s

â€¢ [SLOW TEST:39.268 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:33:45.594: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-zk96
STEP: Creating a pod to test atomic-volume-subpath
Jul  2 19:33:45.699: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zk96" in namespace "subpath-1514" to be "success or failure"
Jul  2 19:33:45.712: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Pending", Reason="", readiness=false. Elapsed: 12.269967ms
Jul  2 19:33:47.719: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01948656s
Jul  2 19:33:49.949: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Running", Reason="", readiness=true. Elapsed: 4.249735455s
Jul  2 19:33:51.957: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Running", Reason="", readiness=true. Elapsed: 6.257042919s
Jul  2 19:33:54.552: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Running", Reason="", readiness=true. Elapsed: 8.852204279s
Jul  2 19:33:57.033: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Running", Reason="", readiness=true. Elapsed: 11.333603996s
Jul  2 19:33:59.041: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Running", Reason="", readiness=true. Elapsed: 13.341477536s
Jul  2 19:34:01.048: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Running", Reason="", readiness=true. Elapsed: 15.348054032s
Jul  2 19:34:03.056: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Running", Reason="", readiness=true. Elapsed: 17.356348213s
Jul  2 19:34:05.169: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Running", Reason="", readiness=true. Elapsed: 19.46973115s
Jul  2 19:34:07.177: INFO: Pod "pod-subpath-test-downwardapi-zk96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 21.477051218s
STEP: Saw pod success
Jul  2 19:34:07.177: INFO: Pod "pod-subpath-test-downwardapi-zk96" satisfied condition "success or failure"
Jul  2 19:34:07.183: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-subpath-test-downwardapi-zk96 container test-container-subpath-downwardapi-zk96: <nil>
STEP: delete the pod
Jul  2 19:34:07.230: INFO: Waiting for pod pod-subpath-test-downwardapi-zk96 to disappear
Jul  2 19:34:07.235: INFO: Pod pod-subpath-test-downwardapi-zk96 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-zk96
Jul  2 19:34:07.235: INFO: Deleting pod "pod-subpath-test-downwardapi-zk96" in namespace "subpath-1514"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:34:07.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1514" for this suite.
Jul  2 19:34:13.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:34:13.506: INFO: namespace subpath-1514 deletion completed in 6.256978313s

â€¢ [SLOW TEST:27.912 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:34:13.507: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-lrjc
STEP: Creating a pod to test atomic-volume-subpath
Jul  2 19:34:13.624: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lrjc" in namespace "subpath-9845" to be "success or failure"
Jul  2 19:34:13.634: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.381166ms
Jul  2 19:34:15.802: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Running", Reason="", readiness=true. Elapsed: 2.177302838s
Jul  2 19:34:17.809: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Running", Reason="", readiness=true. Elapsed: 4.184726195s
Jul  2 19:34:19.816: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Running", Reason="", readiness=true. Elapsed: 6.191535996s
Jul  2 19:34:21.823: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Running", Reason="", readiness=true. Elapsed: 8.198719961s
Jul  2 19:34:23.833: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Running", Reason="", readiness=true. Elapsed: 10.208353699s
Jul  2 19:34:25.840: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Running", Reason="", readiness=true. Elapsed: 12.215284887s
Jul  2 19:34:28.037: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Running", Reason="", readiness=true. Elapsed: 14.412475058s
Jul  2 19:34:30.045: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Running", Reason="", readiness=true. Elapsed: 16.420483506s
Jul  2 19:34:32.660: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Running", Reason="", readiness=true. Elapsed: 19.035160526s
Jul  2 19:34:34.669: INFO: Pod "pod-subpath-test-secret-lrjc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 21.044196741s
STEP: Saw pod success
Jul  2 19:34:34.669: INFO: Pod "pod-subpath-test-secret-lrjc" satisfied condition "success or failure"
Jul  2 19:34:34.675: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-subpath-test-secret-lrjc container test-container-subpath-secret-lrjc: <nil>
STEP: delete the pod
Jul  2 19:34:34.769: INFO: Waiting for pod pod-subpath-test-secret-lrjc to disappear
Jul  2 19:34:34.778: INFO: Pod pod-subpath-test-secret-lrjc no longer exists
STEP: Deleting pod pod-subpath-test-secret-lrjc
Jul  2 19:34:34.778: INFO: Deleting pod "pod-subpath-test-secret-lrjc" in namespace "subpath-9845"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:34:34.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9845" for this suite.
Jul  2 19:34:40.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:34:41.108: INFO: namespace subpath-9845 deletion completed in 6.313943111s

â€¢ [SLOW TEST:27.601 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:34:41.109: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-6ff02fb8-9d00-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 19:34:41.543: INFO: Waiting up to 5m0s for pod "pod-secrets-6ff21cf6-9d00-11e9-808f-7ea354b18644" in namespace "secrets-4335" to be "success or failure"
Jul  2 19:34:41.564: INFO: Pod "pod-secrets-6ff21cf6-9d00-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 20.248908ms
Jul  2 19:34:43.571: INFO: Pod "pod-secrets-6ff21cf6-9d00-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027484081s
Jul  2 19:34:45.960: INFO: Pod "pod-secrets-6ff21cf6-9d00-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.416612201s
Jul  2 19:34:47.969: INFO: Pod "pod-secrets-6ff21cf6-9d00-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.425185545s
STEP: Saw pod success
Jul  2 19:34:47.969: INFO: Pod "pod-secrets-6ff21cf6-9d00-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:34:47.978: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-secrets-6ff21cf6-9d00-11e9-808f-7ea354b18644 container secret-volume-test: <nil>
STEP: delete the pod
Jul  2 19:34:48.024: INFO: Waiting for pod pod-secrets-6ff21cf6-9d00-11e9-808f-7ea354b18644 to disappear
Jul  2 19:34:48.031: INFO: Pod pod-secrets-6ff21cf6-9d00-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:34:48.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4335" for this suite.
Jul  2 19:34:54.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:34:54.314: INFO: namespace secrets-4335 deletion completed in 6.27430713s

â€¢ [SLOW TEST:13.206 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:34:54.315: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2424
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jul  2 19:34:54.500: INFO: Found 0 stateful pods, waiting for 3
Jul  2 19:35:04.513: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 19:35:04.513: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 19:35:04.513: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 19:35:04.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2424 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 19:35:05.103: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 19:35:05.104: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 19:35:05.104: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul  2 19:35:15.161: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul  2 19:35:25.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2424 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:35:25.770: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  2 19:35:25.770: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  2 19:35:25.770: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  2 19:35:45.831: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Jul  2 19:35:45.832: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul  2 19:35:55.848: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Jul  2 19:35:55.848: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul  2 19:36:13.408: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
STEP: Rolling back to a previous revision
Jul  2 19:36:16.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2424 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 19:36:16.732: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 19:36:16.732: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 19:36:16.732: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  2 19:36:16.788: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul  2 19:36:27.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-2424 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 19:36:28.481: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  2 19:36:28.481: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  2 19:36:28.481: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  2 19:37:08.530: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Jul  2 19:37:08.530: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jul  2 19:38:07.399: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Jul  2 19:38:07.399: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  2 19:38:14.631: INFO: Deleting all statefulset in ns statefulset-2424
Jul  2 19:38:14.639: INFO: Scaling statefulset ss2 to 0
Jul  2 19:38:44.693: INFO: Waiting for statefulset status.replicas updated to 0
Jul  2 19:38:44.698: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:38:44.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2424" for this suite.
Jul  2 19:38:50.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:38:50.926: INFO: namespace statefulset-2424 deletion completed in 6.186542982s

â€¢ [SLOW TEST:192.765 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:38:50.927: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  2 19:38:51.286: INFO: Waiting up to 5m0s for pod "downward-api-04ccd4ad-9d01-11e9-808f-7ea354b18644" in namespace "downward-api-9007" to be "success or failure"
Jul  2 19:38:51.294: INFO: Pod "downward-api-04ccd4ad-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.75529ms
Jul  2 19:38:53.305: INFO: Pod "downward-api-04ccd4ad-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019108298s
Jul  2 19:38:55.317: INFO: Pod "downward-api-04ccd4ad-9d01-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031294988s
STEP: Saw pod success
Jul  2 19:38:55.318: INFO: Pod "downward-api-04ccd4ad-9d01-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:38:55.330: INFO: Trying to get logs from node ah-kres-worker-2 pod downward-api-04ccd4ad-9d01-11e9-808f-7ea354b18644 container dapi-container: <nil>
STEP: delete the pod
Jul  2 19:38:55.383: INFO: Waiting for pod downward-api-04ccd4ad-9d01-11e9-808f-7ea354b18644 to disappear
Jul  2 19:38:55.389: INFO: Pod downward-api-04ccd4ad-9d01-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:38:55.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9007" for this suite.
Jul  2 19:39:01.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:39:02.106: INFO: namespace downward-api-9007 deletion completed in 6.709894269s

â€¢ [SLOW TEST:11.179 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:39:02.106: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jul  2 19:39:02.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-8491'
Jul  2 19:39:04.241: INFO: stderr: ""
Jul  2 19:39:04.241: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  2 19:39:04.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8491'
Jul  2 19:39:04.465: INFO: stderr: ""
Jul  2 19:39:04.465: INFO: stdout: "update-demo-nautilus-nfp9q update-demo-nautilus-nxhkk "
Jul  2 19:39:04.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-nfp9q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8491'
Jul  2 19:39:04.651: INFO: stderr: ""
Jul  2 19:39:04.651: INFO: stdout: ""
Jul  2 19:39:04.651: INFO: update-demo-nautilus-nfp9q is created but not running
Jul  2 19:39:09.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8491'
Jul  2 19:39:09.857: INFO: stderr: ""
Jul  2 19:39:09.857: INFO: stdout: "update-demo-nautilus-nfp9q update-demo-nautilus-nxhkk "
Jul  2 19:39:09.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-nfp9q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8491'
Jul  2 19:39:10.040: INFO: stderr: ""
Jul  2 19:39:10.040: INFO: stdout: "true"
Jul  2 19:39:10.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-nfp9q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8491'
Jul  2 19:39:10.413: INFO: stderr: ""
Jul  2 19:39:10.413: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 19:39:10.413: INFO: validating pod update-demo-nautilus-nfp9q
Jul  2 19:39:10.478: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 19:39:10.478: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 19:39:10.479: INFO: update-demo-nautilus-nfp9q is verified up and running
Jul  2 19:39:10.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-nxhkk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8491'
Jul  2 19:39:10.976: INFO: stderr: ""
Jul  2 19:39:10.976: INFO: stdout: "true"
Jul  2 19:39:10.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-nxhkk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8491'
Jul  2 19:39:11.147: INFO: stderr: ""
Jul  2 19:39:11.147: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 19:39:11.148: INFO: validating pod update-demo-nautilus-nxhkk
Jul  2 19:39:11.158: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 19:39:11.158: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 19:39:11.158: INFO: update-demo-nautilus-nxhkk is verified up and running
STEP: using delete to clean up resources
Jul  2 19:39:11.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-8491'
Jul  2 19:39:11.749: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 19:39:11.749: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  2 19:39:11.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8491'
Jul  2 19:39:11.904: INFO: stderr: "No resources found.\n"
Jul  2 19:39:11.904: INFO: stdout: ""
Jul  2 19:39:11.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -l name=update-demo --namespace=kubectl-8491 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  2 19:39:12.034: INFO: stderr: ""
Jul  2 19:39:12.034: INFO: stdout: "update-demo-nautilus-nfp9q\nupdate-demo-nautilus-nxhkk\n"
Jul  2 19:39:12.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8491'
Jul  2 19:39:12.798: INFO: stderr: "No resources found.\n"
Jul  2 19:39:12.798: INFO: stdout: ""
Jul  2 19:39:12.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -l name=update-demo --namespace=kubectl-8491 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  2 19:39:13.001: INFO: stderr: ""
Jul  2 19:39:13.001: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:39:13.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8491" for this suite.
Jul  2 19:39:35.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:39:35.280: INFO: namespace kubectl-8491 deletion completed in 22.267629231s

â€¢ [SLOW TEST:33.174 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:39:35.284: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 19:39:35.409: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f1a3a58-9d01-11e9-808f-7ea354b18644" in namespace "downward-api-5803" to be "success or failure"
Jul  2 19:39:35.424: INFO: Pod "downwardapi-volume-1f1a3a58-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 14.757149ms
Jul  2 19:39:37.433: INFO: Pod "downwardapi-volume-1f1a3a58-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023030318s
Jul  2 19:39:39.441: INFO: Pod "downwardapi-volume-1f1a3a58-9d01-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031541142s
STEP: Saw pod success
Jul  2 19:39:39.441: INFO: Pod "downwardapi-volume-1f1a3a58-9d01-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:39:39.448: INFO: Trying to get logs from node ah-kres-worker-1 pod downwardapi-volume-1f1a3a58-9d01-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 19:39:39.537: INFO: Waiting for pod downwardapi-volume-1f1a3a58-9d01-11e9-808f-7ea354b18644 to disappear
Jul  2 19:39:39.545: INFO: Pod downwardapi-volume-1f1a3a58-9d01-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:39:39.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5803" for this suite.
Jul  2 19:39:47.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:39:47.783: INFO: namespace downward-api-5803 deletion completed in 8.227749542s

â€¢ [SLOW TEST:12.500 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:39:47.784: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3209.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3209.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  2 19:40:29.923: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-268926ff-9d01-11e9-808f-7ea354b18644)
Jul  2 19:40:29.931: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-268926ff-9d01-11e9-808f-7ea354b18644)
Jul  2 19:40:29.938: INFO: Unable to read wheezy_udp@PodARecord from pod dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-268926ff-9d01-11e9-808f-7ea354b18644)
Jul  2 19:40:29.948: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-268926ff-9d01-11e9-808f-7ea354b18644)
Jul  2 19:40:29.955: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-268926ff-9d01-11e9-808f-7ea354b18644)
Jul  2 19:40:29.961: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-268926ff-9d01-11e9-808f-7ea354b18644)
Jul  2 19:40:29.967: INFO: Unable to read jessie_udp@PodARecord from pod dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-268926ff-9d01-11e9-808f-7ea354b18644)
Jul  2 19:40:29.974: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-268926ff-9d01-11e9-808f-7ea354b18644)
Jul  2 19:40:29.974: INFO: Lookups using dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul  2 19:40:35.028: INFO: DNS probes using dns-3209/dns-test-268926ff-9d01-11e9-808f-7ea354b18644 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:40:41.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3209" for this suite.
Jul  2 19:40:48.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:40:48.356: INFO: namespace dns-3209 deletion completed in 6.485087076s

â€¢ [SLOW TEST:60.573 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:40:48.357: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 19:40:48.471: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul  2 19:40:48.495: INFO: Number of nodes with available pods: 0
Jul  2 19:40:48.495: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul  2 19:40:48.536: INFO: Number of nodes with available pods: 0
Jul  2 19:40:48.536: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 19:40:49.544: INFO: Number of nodes with available pods: 0
Jul  2 19:40:49.544: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 19:40:51.253: INFO: Number of nodes with available pods: 1
Jul  2 19:40:51.253: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul  2 19:40:51.295: INFO: Number of nodes with available pods: 1
Jul  2 19:40:51.295: INFO: Number of running nodes: 0, number of available pods: 1
Jul  2 19:40:52.301: INFO: Number of nodes with available pods: 0
Jul  2 19:40:52.301: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul  2 19:40:52.320: INFO: Number of nodes with available pods: 0
Jul  2 19:40:52.320: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 19:40:53.331: INFO: Number of nodes with available pods: 0
Jul  2 19:40:53.331: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 19:40:54.332: INFO: Number of nodes with available pods: 0
Jul  2 19:40:54.332: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 19:40:55.328: INFO: Number of nodes with available pods: 0
Jul  2 19:40:55.328: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 19:40:56.586: INFO: Number of nodes with available pods: 1
Jul  2 19:40:56.586: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-691, will wait for the garbage collector to delete the pods
Jul  2 19:40:56.700: INFO: Deleting DaemonSet.extensions daemon-set took: 14.799082ms
Jul  2 19:40:57.100: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.374124ms
Jul  2 19:41:01.008: INFO: Number of nodes with available pods: 0
Jul  2 19:41:01.008: INFO: Number of running nodes: 0, number of available pods: 0
Jul  2 19:41:01.022: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-691/daemonsets","resourceVersion":"636042"},"items":null}

Jul  2 19:41:01.029: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-691/pods","resourceVersion":"636042"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:41:01.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-691" for this suite.
Jul  2 19:41:07.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:41:07.373: INFO: namespace daemonsets-691 deletion completed in 6.23637071s

â€¢ [SLOW TEST:19.017 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:41:07.374: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 19:41:07.457: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55f7fa26-9d01-11e9-808f-7ea354b18644" in namespace "downward-api-6953" to be "success or failure"
Jul  2 19:41:07.487: INFO: Pod "downwardapi-volume-55f7fa26-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 29.793606ms
Jul  2 19:41:09.493: INFO: Pod "downwardapi-volume-55f7fa26-9d01-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036097871s
STEP: Saw pod success
Jul  2 19:41:09.493: INFO: Pod "downwardapi-volume-55f7fa26-9d01-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:41:09.500: INFO: Trying to get logs from node ah-kres-worker-1 pod downwardapi-volume-55f7fa26-9d01-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 19:41:09.551: INFO: Waiting for pod downwardapi-volume-55f7fa26-9d01-11e9-808f-7ea354b18644 to disappear
Jul  2 19:41:09.559: INFO: Pod downwardapi-volume-55f7fa26-9d01-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:41:09.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6953" for this suite.
Jul  2 19:41:15.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:41:15.838: INFO: namespace downward-api-6953 deletion completed in 6.267894285s

â€¢ [SLOW TEST:8.464 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:41:15.839: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Jul  2 19:41:15.934: INFO: Waiting up to 5m0s for pod "var-expansion-5b05c44d-9d01-11e9-808f-7ea354b18644" in namespace "var-expansion-4524" to be "success or failure"
Jul  2 19:41:15.945: INFO: Pod "var-expansion-5b05c44d-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 10.885663ms
Jul  2 19:41:17.953: INFO: Pod "var-expansion-5b05c44d-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019243014s
Jul  2 19:41:19.961: INFO: Pod "var-expansion-5b05c44d-9d01-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026576975s
STEP: Saw pod success
Jul  2 19:41:19.961: INFO: Pod "var-expansion-5b05c44d-9d01-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:41:19.969: INFO: Trying to get logs from node ah-kres-worker-2 pod var-expansion-5b05c44d-9d01-11e9-808f-7ea354b18644 container dapi-container: <nil>
STEP: delete the pod
Jul  2 19:41:20.017: INFO: Waiting for pod var-expansion-5b05c44d-9d01-11e9-808f-7ea354b18644 to disappear
Jul  2 19:41:20.024: INFO: Pod var-expansion-5b05c44d-9d01-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:41:20.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4524" for this suite.
Jul  2 19:41:30.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:41:30.416: INFO: namespace var-expansion-4524 deletion completed in 10.38192882s

â€¢ [SLOW TEST:14.577 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:41:30.416: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Jul  2 19:41:30.525: INFO: Waiting up to 5m0s for pod "client-containers-63b653d1-9d01-11e9-808f-7ea354b18644" in namespace "containers-332" to be "success or failure"
Jul  2 19:41:30.552: INFO: Pod "client-containers-63b653d1-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 26.881081ms
Jul  2 19:41:33.032: INFO: Pod "client-containers-63b653d1-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507668363s
Jul  2 19:41:35.047: INFO: Pod "client-containers-63b653d1-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.522744719s
Jul  2 19:41:37.055: INFO: Pod "client-containers-63b653d1-9d01-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.530475197s
STEP: Saw pod success
Jul  2 19:41:37.055: INFO: Pod "client-containers-63b653d1-9d01-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:41:37.061: INFO: Trying to get logs from node ah-kres-worker-3 pod client-containers-63b653d1-9d01-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 19:41:37.111: INFO: Waiting for pod client-containers-63b653d1-9d01-11e9-808f-7ea354b18644 to disappear
Jul  2 19:41:37.117: INFO: Pod client-containers-63b653d1-9d01-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:41:37.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-332" for this suite.
Jul  2 19:41:47.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:41:47.309: INFO: namespace containers-332 deletion completed in 10.183940502s

â€¢ [SLOW TEST:16.893 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:41:47.310: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Jul  2 19:41:52.534: INFO: Pod pod-hostip-6e6d5552-9d01-11e9-808f-7ea354b18644 has hostIP: 10.30.20.166
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:41:52.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8376" for this suite.
Jul  2 19:42:18.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:42:18.818: INFO: namespace pods-8376 deletion completed in 26.274339622s

â€¢ [SLOW TEST:31.508 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:42:18.818: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-8092d1fb-9d01-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 19:42:18.974: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8096bec5-9d01-11e9-808f-7ea354b18644" in namespace "projected-9990" to be "success or failure"
Jul  2 19:42:19.043: INFO: Pod "pod-projected-configmaps-8096bec5-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 68.651725ms
Jul  2 19:42:21.052: INFO: Pod "pod-projected-configmaps-8096bec5-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0772981s
Jul  2 19:42:23.057: INFO: Pod "pod-projected-configmaps-8096bec5-9d01-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082919809s
STEP: Saw pod success
Jul  2 19:42:23.059: INFO: Pod "pod-projected-configmaps-8096bec5-9d01-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:42:23.064: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-projected-configmaps-8096bec5-9d01-11e9-808f-7ea354b18644 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 19:42:23.105: INFO: Waiting for pod pod-projected-configmaps-8096bec5-9d01-11e9-808f-7ea354b18644 to disappear
Jul  2 19:42:23.110: INFO: Pod pod-projected-configmaps-8096bec5-9d01-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:42:23.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9990" for this suite.
Jul  2 19:42:29.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:42:29.327: INFO: namespace projected-9990 deletion completed in 6.209801102s

â€¢ [SLOW TEST:10.509 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:42:29.328: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 19:42:29.426: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86d20dd3-9d01-11e9-808f-7ea354b18644" in namespace "projected-3475" to be "success or failure"
Jul  2 19:42:29.436: INFO: Pod "downwardapi-volume-86d20dd3-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 10.303076ms
Jul  2 19:42:31.443: INFO: Pod "downwardapi-volume-86d20dd3-9d01-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017515923s
Jul  2 19:42:33.452: INFO: Pod "downwardapi-volume-86d20dd3-9d01-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026326s
STEP: Saw pod success
Jul  2 19:42:33.452: INFO: Pod "downwardapi-volume-86d20dd3-9d01-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:42:33.459: INFO: Trying to get logs from node ah-kres-worker-3 pod downwardapi-volume-86d20dd3-9d01-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 19:42:33.543: INFO: Waiting for pod downwardapi-volume-86d20dd3-9d01-11e9-808f-7ea354b18644 to disappear
Jul  2 19:42:33.553: INFO: Pod downwardapi-volume-86d20dd3-9d01-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:42:33.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3475" for this suite.
Jul  2 19:42:39.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:42:39.843: INFO: namespace projected-3475 deletion completed in 6.276885954s

â€¢ [SLOW TEST:10.516 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:42:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul  2 19:42:41.772: INFO: Pod name wrapped-volume-race-8e1adb16-9d01-11e9-808f-7ea354b18644: Found 0 pods out of 5
Jul  2 19:42:46.856: INFO: Pod name wrapped-volume-race-8e1adb16-9d01-11e9-808f-7ea354b18644: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8e1adb16-9d01-11e9-808f-7ea354b18644 in namespace emptydir-wrapper-1097, will wait for the garbage collector to delete the pods
Jul  2 19:43:01.927: INFO: Deleting ReplicationController wrapped-volume-race-8e1adb16-9d01-11e9-808f-7ea354b18644 took: 22.797076ms
Jul  2 19:43:02.327: INFO: Terminating ReplicationController wrapped-volume-race-8e1adb16-9d01-11e9-808f-7ea354b18644 pods took: 400.236843ms
STEP: Creating RC which spawns configmap-volume pods
Jul  2 19:43:37.873: INFO: Pod name wrapped-volume-race-af9c1eb3-9d01-11e9-808f-7ea354b18644: Found 0 pods out of 5
Jul  2 19:43:48.810: INFO: Pod name wrapped-volume-race-af9c1eb3-9d01-11e9-808f-7ea354b18644: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-af9c1eb3-9d01-11e9-808f-7ea354b18644 in namespace emptydir-wrapper-1097, will wait for the garbage collector to delete the pods
Jul  2 19:44:51.390: INFO: Deleting ReplicationController wrapped-volume-race-af9c1eb3-9d01-11e9-808f-7ea354b18644 took: 23.776844ms
Jul  2 19:44:51.807: INFO: Terminating ReplicationController wrapped-volume-race-af9c1eb3-9d01-11e9-808f-7ea354b18644 pods took: 416.426963ms
STEP: Creating RC which spawns configmap-volume pods
Jul  2 19:45:37.121: INFO: Pod name wrapped-volume-race-f60ce861-9d01-11e9-808f-7ea354b18644: Found 0 pods out of 5
Jul  2 19:45:42.131: INFO: Pod name wrapped-volume-race-f60ce861-9d01-11e9-808f-7ea354b18644: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f60ce861-9d01-11e9-808f-7ea354b18644 in namespace emptydir-wrapper-1097, will wait for the garbage collector to delete the pods
Jul  2 19:45:54.751: INFO: Deleting ReplicationController wrapped-volume-race-f60ce861-9d01-11e9-808f-7ea354b18644 took: 17.740006ms
Jul  2 19:45:55.251: INFO: Terminating ReplicationController wrapped-volume-race-f60ce861-9d01-11e9-808f-7ea354b18644 pods took: 500.334263ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:46:47.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1097" for this suite.
Jul  2 19:46:58.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:47:00.616: INFO: namespace emptydir-wrapper-1097 deletion completed in 12.647605841s

â€¢ [SLOW TEST:219.649 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:47:00.619: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul  2 19:47:00.707: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  2 19:47:00.735: INFO: Waiting for terminating namespaces to be deleted...
Jul  2 19:47:00.740: INFO: 
Logging pods the kubelet thinks is on node ah-kres-worker-1 before test
Jul  2 19:47:00.759: INFO: fluent-bit-xdm26 from logging started at 2019-06-28 14:28:59 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.759: INFO: 	Container fluent-bit ready: true, restart count 0
Jul  2 19:47:00.759: INFO: openebs-ndm-tdw4h from openebs started at 2019-06-28 14:28:59 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.759: INFO: 	Container node-disk-manager ready: true, restart count 0
Jul  2 19:47:00.759: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-02 19:14:00 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.759: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  2 19:47:00.759: INFO: sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-mmbft from heptio-sonobuoy started at 2019-07-02 19:14:11 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  2 19:47:00.759: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  2 19:47:00.759: INFO: kube-proxy-c24q8 from kube-system started at 2019-06-28 14:23:42 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.759: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  2 19:47:00.759: INFO: weave-net-2bg4v from kube-system started at 2019-06-28 14:28:25 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.759: INFO: 	Container weave ready: true, restart count 0
Jul  2 19:47:00.759: INFO: 	Container weave-npc ready: true, restart count 0
Jul  2 19:47:00.759: INFO: maya-apiserver-d9589fbc6-7zhct from openebs started at 2019-06-28 14:28:59 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.759: INFO: 	Container maya-apiserver ready: true, restart count 0
Jul  2 19:47:00.759: INFO: cstor-sparse-pool-og2i-5899855dd6-ggnms from openebs started at 2019-06-28 14:30:45 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.759: INFO: 	Container cstor-pool ready: true, restart count 0
Jul  2 19:47:00.759: INFO: 	Container cstor-pool-mgmt ready: true, restart count 0
Jul  2 19:47:00.759: INFO: 
Logging pods the kubelet thinks is on node ah-kres-worker-2 before test
Jul  2 19:47:00.789: INFO: weave-net-487f5 from kube-system started at 2019-06-28 14:27:30 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.789: INFO: 	Container weave ready: true, restart count 0
Jul  2 19:47:00.789: INFO: 	Container weave-npc ready: true, restart count 0
Jul  2 19:47:00.789: INFO: cstor-sparse-pool-h1oa-bf78bcdf-8vqh9 from openebs started at 2019-06-28 14:31:01 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.789: INFO: 	Container cstor-pool ready: true, restart count 0
Jul  2 19:47:00.789: INFO: 	Container cstor-pool-mgmt ready: true, restart count 0
Jul  2 19:47:00.789: INFO: sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-fzrq6 from heptio-sonobuoy started at 2019-07-02 19:13:30 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.789: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  2 19:47:00.789: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  2 19:47:00.789: INFO: coredns-fb8b8dccf-rfjqz from kube-system started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.789: INFO: 	Container coredns ready: true, restart count 8
Jul  2 19:47:00.789: INFO: openebs-ndm-hp9tr from openebs started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.792: INFO: 	Container node-disk-manager ready: true, restart count 0
Jul  2 19:47:00.792: INFO: kube-proxy-6wbkn from kube-system started at 2019-06-28 14:27:15 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.792: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  2 19:47:00.792: INFO: openebs-provisioner-65fd45cf47-nxhwk from openebs started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.792: INFO: 	Container openebs-provisioner ready: true, restart count 0
Jul  2 19:47:00.792: INFO: fluent-bit-58gks from logging started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.792: INFO: 	Container fluent-bit ready: true, restart count 0
Jul  2 19:47:00.792: INFO: 
Logging pods the kubelet thinks is on node ah-kres-worker-3 before test
Jul  2 19:47:00.805: INFO: openebs-snapshot-operator-86996d865f-chsgj from openebs started at 2019-06-28 14:29:11 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.805: INFO: 	Container snapshot-controller ready: true, restart count 0
Jul  2 19:47:00.805: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Jul  2 19:47:00.805: INFO: openebs-ndm-nnrhf from openebs started at 2019-06-28 14:29:11 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.805: INFO: 	Container node-disk-manager ready: true, restart count 0
Jul  2 19:47:00.805: INFO: fluent-bit-vxc9c from logging started at 2019-06-28 14:29:11 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.805: INFO: 	Container fluent-bit ready: true, restart count 0
Jul  2 19:47:00.805: INFO: kube-proxy-jl8vq from kube-system started at 2019-06-28 14:25:42 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.805: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  2 19:47:00.805: INFO: weave-net-5rvbm from kube-system started at 2019-06-28 14:27:43 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.805: INFO: 	Container weave ready: true, restart count 0
Jul  2 19:47:00.805: INFO: 	Container weave-npc ready: true, restart count 0
Jul  2 19:47:00.805: INFO: coredns-fb8b8dccf-2vq7w from kube-system started at 2019-06-28 14:29:11 +0000 UTC (1 container statuses recorded)
Jul  2 19:47:00.805: INFO: 	Container coredns ready: true, restart count 7
Jul  2 19:47:00.805: INFO: cstor-sparse-pool-j6op-559d6f6997-867v4 from openebs started at 2019-06-28 14:30:56 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.805: INFO: 	Container cstor-pool ready: true, restart count 0
Jul  2 19:47:00.805: INFO: 	Container cstor-pool-mgmt ready: true, restart count 0
Jul  2 19:47:00.805: INFO: sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-sw8ks from heptio-sonobuoy started at 2019-07-02 19:14:01 +0000 UTC (2 container statuses recorded)
Jul  2 19:47:00.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  2 19:47:00.805: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2b04981a-9d02-11e9-808f-7ea354b18644 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2b04981a-9d02-11e9-808f-7ea354b18644 off the node ah-kres-worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2b04981a-9d02-11e9-808f-7ea354b18644
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:47:12.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5837" for this suite.
Jul  2 19:47:25.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:47:25.200: INFO: namespace sched-pred-5837 deletion completed in 12.22343233s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:24.581 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:47:25.201: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-37560555-9d02-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 19:47:28.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-3923b566-9d02-11e9-808f-7ea354b18644" in namespace "configmap-8876" to be "success or failure"
Jul  2 19:47:28.760: INFO: Pod "pod-configmaps-3923b566-9d02-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.030073ms
Jul  2 19:47:36.544: INFO: Pod "pod-configmaps-3923b566-9d02-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.792664421s
STEP: Saw pod success
Jul  2 19:47:36.544: INFO: Pod "pod-configmaps-3923b566-9d02-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 19:47:36.548: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-configmaps-3923b566-9d02-11e9-808f-7ea354b18644 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 19:47:36.580: INFO: Waiting for pod pod-configmaps-3923b566-9d02-11e9-808f-7ea354b18644 to disappear
Jul  2 19:47:36.585: INFO: Pod pod-configmaps-3923b566-9d02-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:47:36.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8876" for this suite.
Jul  2 19:47:48.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:47:48.818: INFO: namespace configmap-8876 deletion completed in 12.227944004s

â€¢ [SLOW TEST:23.618 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:47:48.818: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 19:47:48.952: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4359bdef-9d02-11e9-ab09-0050569fbfc5", Controller:(*bool)(0xc0020cafea), BlockOwnerDeletion:(*bool)(0xc0020cafeb)}}
Jul  2 19:47:48.981: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"435510a9-9d02-11e9-ab09-0050569fbfc5", Controller:(*bool)(0xc0020cb1ba), BlockOwnerDeletion:(*bool)(0xc0020cb1bb)}}
Jul  2 19:47:48.999: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4357a4c6-9d02-11e9-ab09-0050569fbfc5", Controller:(*bool)(0xc00246aa22), BlockOwnerDeletion:(*bool)(0xc00246aa23)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:47:54.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9071" for this suite.
Jul  2 19:48:02.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:48:02.275: INFO: namespace gc-9071 deletion completed in 8.239306767s

â€¢ [SLOW TEST:13.457 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:48:02.277: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:48:02.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3592" for this suite.
Jul  2 19:48:22.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:48:22.661: INFO: namespace kubelet-test-3592 deletion completed in 20.245973144s

â€¢ [SLOW TEST:20.385 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:48:22.664: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul  2 19:48:34.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  2 19:48:34.605: INFO: Pod pod-with-prestop-http-hook still exists
Jul  2 19:48:36.608: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  2 19:48:36.615: INFO: Pod pod-with-prestop-http-hook still exists
Jul  2 19:48:38.609: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  2 19:48:38.617: INFO: Pod pod-with-prestop-http-hook still exists
Jul  2 19:48:40.608: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  2 19:48:40.615: INFO: Pod pod-with-prestop-http-hook still exists
Jul  2 19:48:42.608: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  2 19:48:42.615: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:48:42.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9566" for this suite.
Jul  2 19:49:04.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:49:04.896: INFO: namespace container-lifecycle-hook-9566 deletion completed in 22.252428708s

â€¢ [SLOW TEST:42.231 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:49:04.897: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul  2 19:49:08.239: INFO: Pod name pod-release: Found 0 pods out of 1
Jul  2 19:49:16.927: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:49:16.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3905" for this suite.
Jul  2 19:49:25.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:49:25.240: INFO: namespace replication-controller-3905 deletion completed in 8.235475616s

â€¢ [SLOW TEST:20.343 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:49:25.241: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  2 19:52:53.623: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  2 19:52:53.629: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  2 19:52:55.629: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  2 19:52:55.642: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  2 19:52:57.629: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  2 19:52:57.636: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  2 19:52:59.629: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  2 19:52:59.636: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  2 19:53:01.629: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  2 19:53:01.636: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  2 19:53:03.629: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  2 19:53:22.935: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:53:22.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9504" for this suite.
Jul  2 19:54:21.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:54:21.830: INFO: namespace container-lifecycle-hook-9504 deletion completed in 58.813344421s

â€¢ [SLOW TEST:249.478 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:54:21.832: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  2 19:54:38.919: INFO: Successfully updated pod "pod-update-2f829017-9d03-11e9-808f-7ea354b18644"
STEP: verifying the updated pod is in kubernetes
Jul  2 19:54:38.987: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:54:38.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8861" for this suite.
Jul  2 19:55:23.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:55:23.287: INFO: namespace pods-8861 deletion completed in 44.266552556s

â€¢ [SLOW TEST:61.456 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:55:23.291: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:55:50.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4618" for this suite.
Jul  2 19:55:56.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:55:56.493: INFO: namespace namespaces-4618 deletion completed in 6.261221513s
STEP: Destroying namespace "nsdeletetest-3577" for this suite.
Jul  2 19:55:56.497: INFO: Namespace nsdeletetest-3577 was already deleted
STEP: Destroying namespace "nsdeletetest-8709" for this suite.
Jul  2 19:56:03.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:56:03.210: INFO: namespace nsdeletetest-8709 deletion completed in 6.713110068s

â€¢ [SLOW TEST:39.920 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:56:03.211: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  2 19:56:15.272: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  2 19:56:15.291: INFO: Pod pod-with-poststart-http-hook still exists
Jul  2 19:56:17.292: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  2 19:56:17.698: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:56:17.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2816" for this suite.
Jul  2 19:56:39.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:56:39.950: INFO: namespace container-lifecycle-hook-2816 deletion completed in 22.237865448s

â€¢ [SLOW TEST:36.740 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:56:39.951: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul  2 19:56:41.337: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6247,SelfLink:/api/v1/namespaces/watch-6247/configmaps/e2e-watch-test-label-changed,UID:919fab73-9d03-11e9-ab09-0050569fbfc5,ResourceVersion:639076,Generation:0,CreationTimestamp:2019-07-02 19:57:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  2 19:56:41.338: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6247,SelfLink:/api/v1/namespaces/watch-6247/configmaps/e2e-watch-test-label-changed,UID:919fab73-9d03-11e9-ab09-0050569fbfc5,ResourceVersion:639077,Generation:0,CreationTimestamp:2019-07-02 19:57:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul  2 19:56:41.339: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6247,SelfLink:/api/v1/namespaces/watch-6247/configmaps/e2e-watch-test-label-changed,UID:919fab73-9d03-11e9-ab09-0050569fbfc5,ResourceVersion:639078,Generation:0,CreationTimestamp:2019-07-02 19:57:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul  2 19:56:52.187: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6247,SelfLink:/api/v1/namespaces/watch-6247/configmaps/e2e-watch-test-label-changed,UID:919fab73-9d03-11e9-ab09-0050569fbfc5,ResourceVersion:639097,Generation:0,CreationTimestamp:2019-07-02 19:57:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  2 19:56:52.187: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6247,SelfLink:/api/v1/namespaces/watch-6247/configmaps/e2e-watch-test-label-changed,UID:919fab73-9d03-11e9-ab09-0050569fbfc5,ResourceVersion:639100,Generation:0,CreationTimestamp:2019-07-02 19:57:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul  2 19:56:52.191: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6247,SelfLink:/api/v1/namespaces/watch-6247/configmaps/e2e-watch-test-label-changed,UID:919fab73-9d03-11e9-ab09-0050569fbfc5,ResourceVersion:639104,Generation:0,CreationTimestamp:2019-07-02 19:57:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:56:52.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6247" for this suite.
Jul  2 19:56:58.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:56:58.440: INFO: namespace watch-6247 deletion completed in 6.231464065s

â€¢ [SLOW TEST:18.489 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:56:58.441: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:57:35.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7025" for this suite.
Jul  2 19:57:42.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:57:43.829: INFO: namespace container-runtime-7025 deletion completed in 8.117771844s

â€¢ [SLOW TEST:45.389 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:57:43.830: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:57:47.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1323" for this suite.
Jul  2 19:58:32.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:58:32.201: INFO: namespace kubelet-test-1323 deletion completed in 44.223304348s

â€¢ [SLOW TEST:48.371 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:58:32.202: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0702 19:59:03.184047      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  2 19:59:03.184: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:59:03.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7062" for this suite.
Jul  2 19:59:09.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 19:59:09.419: INFO: namespace gc-7062 deletion completed in 6.226823777s

â€¢ [SLOW TEST:37.218 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 19:59:09.420: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 19:59:11.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9270" for this suite.
Jul  2 20:01:02.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:01:03.094: INFO: namespace kubelet-test-9270 deletion completed in 1m6.222863273s

â€¢ [SLOW TEST:68.395 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:01:03.096: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul  2 20:01:03.186: INFO: Waiting up to 5m0s for pod "pod-1eae094e-9d04-11e9-808f-7ea354b18644" in namespace "emptydir-1663" to be "success or failure"
Jul  2 20:01:03.195: INFO: Pod "pod-1eae094e-9d04-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.140284ms
Jul  2 20:01:05.203: INFO: Pod "pod-1eae094e-9d04-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016680067s
Jul  2 20:01:07.475: INFO: Pod "pod-1eae094e-9d04-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.2889108s
STEP: Saw pod success
Jul  2 20:01:07.475: INFO: Pod "pod-1eae094e-9d04-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:01:08.994: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-1eae094e-9d04-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 20:01:09.290: INFO: Waiting for pod pod-1eae094e-9d04-11e9-808f-7ea354b18644 to disappear
Jul  2 20:01:09.298: INFO: Pod pod-1eae094e-9d04-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:01:09.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1663" for this suite.
Jul  2 20:01:15.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:01:15.513: INFO: namespace emptydir-1663 deletion completed in 6.204942478s

â€¢ [SLOW TEST:12.417 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:01:15.514: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul  2 20:01:21.719: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:21.719: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:22.047: INFO: Exec stderr: ""
Jul  2 20:01:22.047: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:22.047: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:22.425: INFO: Exec stderr: ""
Jul  2 20:01:22.425: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:22.426: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:23.377: INFO: Exec stderr: ""
Jul  2 20:01:23.377: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:23.377: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:23.743: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul  2 20:01:23.743: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:23.743: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:24.058: INFO: Exec stderr: ""
Jul  2 20:01:24.059: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:24.059: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:24.418: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul  2 20:01:24.418: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:24.418: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:28.177: INFO: Exec stderr: ""
Jul  2 20:01:28.177: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:28.177: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:28.543: INFO: Exec stderr: ""
Jul  2 20:01:28.544: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:28.544: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:28.892: INFO: Exec stderr: ""
Jul  2 20:01:28.892: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9596 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:01:28.892: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:01:29.310: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:01:29.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9596" for this suite.
Jul  2 20:02:17.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:02:17.898: INFO: namespace e2e-kubelet-etc-hosts-9596 deletion completed in 48.141251791s

â€¢ [SLOW TEST:62.383 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:02:17.898: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:02:17.983: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul  2 20:02:22.990: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  2 20:02:22.991: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  2 20:02:27.048: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-9699,SelfLink:/apis/apps/v1/namespaces/deployment-9699/deployments/test-cleanup-deployment,UID:494940d2-9d04-11e9-ab09-0050569fbfc5,ResourceVersion:639956,Generation:1,CreationTimestamp:2019-07-02 20:02:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-02 20:02:14 +0000 UTC 2019-07-02 20:02:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-02 20:02:17 +0000 UTC 2019-07-02 20:02:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55cbfbc8f5" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  2 20:02:27.053: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-9699,SelfLink:/apis/apps/v1/namespaces/deployment-9699/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:4a0a0eec-9d04-11e9-9994-0050569f2898,ResourceVersion:639945,Generation:1,CreationTimestamp:2019-07-02 20:02:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 494940d2-9d04-11e9-ab09-0050569fbfc5 0xc00246a9e7 0xc00246a9e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  2 20:02:27.059: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-cx7sn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-cx7sn,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-9699,SelfLink:/api/v1/namespaces/deployment-9699/pods/test-cleanup-deployment-55cbfbc8f5-cx7sn,UID:4a0b7331-9d04-11e9-9994-0050569f2898,ResourceVersion:639944,Generation:0,CreationTimestamp:2019-07-02 20:02:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 4a0a0eec-9d04-11e9-9994-0050569f2898 0xc0020cb7c7 0xc0020cb7c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hpbct {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hpbct,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-hpbct true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020cb840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020cb860}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:01:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:01:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:01:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:02:15 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:10.44.0.5,StartTime:2019-07-02 20:01:56 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-02 20:01:58 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://30991986c86721365c5ae26dcd4119770f1e2e08c4ed89d707be62837db9b240}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:02:27.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9699" for this suite.
Jul  2 20:02:35.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:02:35.747: INFO: namespace deployment-9699 deletion completed in 8.681504902s

â€¢ [SLOW TEST:17.849 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:02:35.748: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-55e9438d-9d04-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 20:02:35.893: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-55ebe64d-9d04-11e9-808f-7ea354b18644" in namespace "projected-9687" to be "success or failure"
Jul  2 20:02:35.903: INFO: Pod "pod-projected-secrets-55ebe64d-9d04-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.682774ms
Jul  2 20:02:37.910: INFO: Pod "pod-projected-secrets-55ebe64d-9d04-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016846881s
Jul  2 20:02:39.916: INFO: Pod "pod-projected-secrets-55ebe64d-9d04-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022979061s
STEP: Saw pod success
Jul  2 20:02:39.916: INFO: Pod "pod-projected-secrets-55ebe64d-9d04-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:02:39.923: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-projected-secrets-55ebe64d-9d04-11e9-808f-7ea354b18644 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  2 20:02:39.966: INFO: Waiting for pod pod-projected-secrets-55ebe64d-9d04-11e9-808f-7ea354b18644 to disappear
Jul  2 20:02:39.970: INFO: Pod pod-projected-secrets-55ebe64d-9d04-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:02:39.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9687" for this suite.
Jul  2 20:02:46.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:02:46.174: INFO: namespace projected-9687 deletion completed in 6.19593233s

â€¢ [SLOW TEST:10.426 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:02:46.176: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:02:46.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5426" for this suite.
Jul  2 20:03:10.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:03:10.450: INFO: namespace pods-5426 deletion completed in 24.148783236s

â€¢ [SLOW TEST:24.274 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:03:10.451: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1123
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  2 20:03:10.500: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  2 20:03:41.079: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.44.0.6:8080/dial?request=hostName&protocol=udp&host=10.44.0.5&port=8081&tries=1'] Namespace:pod-network-test-1123 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:03:41.079: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:03:45.117: INFO: Waiting for endpoints: map[]
Jul  2 20:03:45.129: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.44.0.6:8080/dial?request=hostName&protocol=udp&host=10.32.0.6&port=8081&tries=1'] Namespace:pod-network-test-1123 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:03:45.129: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:03:45.487: INFO: Waiting for endpoints: map[]
Jul  2 20:03:45.495: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.44.0.6:8080/dial?request=hostName&protocol=udp&host=10.40.0.5&port=8081&tries=1'] Namespace:pod-network-test-1123 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:03:45.495: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:03:45.892: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:03:45.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1123" for this suite.
Jul  2 20:04:07.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:04:07.955: INFO: namespace pod-network-test-1123 deletion completed in 22.052642352s

â€¢ [SLOW TEST:57.504 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:04:07.958: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-4361
Jul  2 20:04:12.079: INFO: Started pod liveness-exec in namespace container-probe-4361
STEP: checking the pod's current state and verifying that restartCount is present
Jul  2 20:04:12.084: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:09:05.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4361" for this suite.
Jul  2 20:09:25.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:09:25.779: INFO: namespace container-probe-4361 deletion completed in 20.294692423s

â€¢ [SLOW TEST:275.144 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:09:25.781: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-4a4dcb34-9d05-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 20:09:25.878: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4a4faf2e-9d05-11e9-808f-7ea354b18644" in namespace "projected-1429" to be "success or failure"
Jul  2 20:09:25.896: INFO: Pod "pod-projected-secrets-4a4faf2e-9d05-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 18.50687ms
Jul  2 20:09:27.904: INFO: Pod "pod-projected-secrets-4a4faf2e-9d05-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026367694s
STEP: Saw pod success
Jul  2 20:09:27.904: INFO: Pod "pod-projected-secrets-4a4faf2e-9d05-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:09:27.916: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-projected-secrets-4a4faf2e-9d05-11e9-808f-7ea354b18644 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  2 20:09:27.985: INFO: Waiting for pod pod-projected-secrets-4a4faf2e-9d05-11e9-808f-7ea354b18644 to disappear
Jul  2 20:09:27.991: INFO: Pod pod-projected-secrets-4a4faf2e-9d05-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:09:27.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1429" for this suite.
Jul  2 20:09:34.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:09:34.272: INFO: namespace projected-1429 deletion completed in 6.255037313s

â€¢ [SLOW TEST:8.491 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:09:34.278: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  2 20:09:34.387: INFO: Waiting up to 5m0s for pod "downward-api-4f609555-9d05-11e9-808f-7ea354b18644" in namespace "downward-api-4559" to be "success or failure"
Jul  2 20:09:34.408: INFO: Pod "downward-api-4f609555-9d05-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 21.354315ms
Jul  2 20:09:36.413: INFO: Pod "downward-api-4f609555-9d05-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026328904s
STEP: Saw pod success
Jul  2 20:09:36.413: INFO: Pod "downward-api-4f609555-9d05-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:09:36.418: INFO: Trying to get logs from node ah-kres-worker-3 pod downward-api-4f609555-9d05-11e9-808f-7ea354b18644 container dapi-container: <nil>
STEP: delete the pod
Jul  2 20:09:36.463: INFO: Waiting for pod downward-api-4f609555-9d05-11e9-808f-7ea354b18644 to disappear
Jul  2 20:09:36.469: INFO: Pod downward-api-4f609555-9d05-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:09:36.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4559" for this suite.
Jul  2 20:09:42.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:09:42.727: INFO: namespace downward-api-4559 deletion completed in 6.248603872s

â€¢ [SLOW TEST:8.450 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:09:42.728: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-546a525a-9d05-11e9-808f-7ea354b18644
STEP: Creating configMap with name cm-test-opt-upd-546a52cb-9d05-11e9-808f-7ea354b18644
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-546a525a-9d05-11e9-808f-7ea354b18644
STEP: Updating configmap cm-test-opt-upd-546a52cb-9d05-11e9-808f-7ea354b18644
STEP: Creating configMap with name cm-test-opt-create-546a5307-9d05-11e9-808f-7ea354b18644
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:10:55.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1420" for this suite.
Jul  2 20:11:21.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:11:21.906: INFO: namespace projected-1420 deletion completed in 26.838081286s

â€¢ [SLOW TEST:99.178 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:11:21.909: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:11:21.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 version --client'
Jul  2 20:11:22.140: INFO: stderr: ""
Jul  2 20:11:22.140: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jul  2 20:11:22.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-6034'
Jul  2 20:11:22.828: INFO: stderr: ""
Jul  2 20:11:22.828: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul  2 20:11:22.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-6034'
Jul  2 20:11:23.297: INFO: stderr: ""
Jul  2 20:11:23.297: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  2 20:11:24.305: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:11:24.305: INFO: Found 0 / 1
Jul  2 20:11:25.303: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:11:25.304: INFO: Found 0 / 1
Jul  2 20:11:26.302: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:11:26.302: INFO: Found 1 / 1
Jul  2 20:11:26.302: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  2 20:11:26.306: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:11:26.306: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  2 20:11:26.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 describe pod redis-master-7sf92 --namespace=kubectl-6034'
Jul  2 20:11:26.447: INFO: stderr: ""
Jul  2 20:11:26.447: INFO: stdout: "Name:               redis-master-7sf92\nNamespace:          kubectl-6034\nPriority:           0\nPriorityClassName:  <none>\nNode:               ah-kres-worker-2/10.30.20.155\nStart Time:         Tue, 02 Jul 2019 20:11:01 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.44.0.5\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://8baabcc37e4a693c4adecb3d0eaf13edd1570c24e1b8a4783e1fa9ed5e0ff612\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 02 Jul 2019 20:11:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-g59sv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-g59sv:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-g59sv\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                       Message\n  ----    ------     ----       ----                       -------\n  Normal  Scheduled  0s         default-scheduler          Successfully assigned kubectl-6034/redis-master-7sf92 to ah-kres-worker-2\n  Normal  Pulled     <invalid>  kubelet, ah-kres-worker-2  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    <invalid>  kubelet, ah-kres-worker-2  Created container redis-master\n  Normal  Started    <invalid>  kubelet, ah-kres-worker-2  Started container redis-master\n"
Jul  2 20:11:26.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 describe rc redis-master --namespace=kubectl-6034'
Jul  2 20:11:26.592: INFO: stderr: ""
Jul  2 20:11:26.592: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-6034\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-7sf92\n"
Jul  2 20:11:26.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 describe service redis-master --namespace=kubectl-6034'
Jul  2 20:11:26.775: INFO: stderr: ""
Jul  2 20:11:26.775: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-6034\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                192.168.0.191\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.44.0.5:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul  2 20:11:26.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 describe node ah-kres-master-1'
Jul  2 20:11:27.266: INFO: stderr: ""
Jul  2 20:11:27.266: INFO: stdout: "Name:               ah-kres-master-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ah-kres-master-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 28 Jun 2019 13:43:21 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 02 Jul 2019 14:14:03 +0000   Tue, 02 Jul 2019 14:14:03 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Tue, 02 Jul 2019 20:10:50 +0000   Fri, 28 Jun 2019 13:43:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 02 Jul 2019 20:10:50 +0000   Fri, 28 Jun 2019 13:43:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 02 Jul 2019 20:10:50 +0000   Fri, 28 Jun 2019 13:43:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 02 Jul 2019 20:10:50 +0000   Tue, 02 Jul 2019 18:53:33 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.30.20.197\n  Hostname:    ah-kres-master-1\nCapacity:\n cpu:                2\n ephemeral-storage:  41152812Ki\n hugepages-2Mi:      0\n memory:             4046392Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  37926431477\n hugepages-2Mi:      0\n memory:             3943992Ki\n pods:               110\nSystem Info:\n Machine ID:                 3fb6a8b14bbcf6b57bd1ba775c5b69a2\n System UUID:                421FC957-7645-3D8F-5985-BA88F79AEF72\n Boot ID:                    0bcce12b-7b9d-4224-bd42-c6ab73a9ca24\n Kernel Version:             4.4.0-131-generic\n OS Image:                   Ubuntu 16.04.5 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.3\n Kubelet Version:            v1.14.1\n Kube-Proxy Version:         v1.14.1\nPodCIDR:                     192.168.0.0/24\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-cvq85    0 (0%)        0 (0%)      0 (0%)           0 (0%)         57m\n  kube-system                etcd-ah-kres-master-1                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d6h\n  kube-system                kube-apiserver-ah-kres-master-1                            250m (12%)    0 (0%)      0 (0%)           0 (0%)         4d6h\n  kube-system                kube-controller-manager-ah-kres-master-1                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         4d6h\n  kube-system                kube-proxy-86bvk                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d6h\n  kube-system                kube-scheduler-ah-kres-master-1                            100m (5%)     0 (0%)      0 (0%)           0 (0%)         4d6h\n  kube-system                weave-net-qjg8p                                            20m (1%)      0 (0%)      0 (0%)           0 (0%)         4d5h\n  logging                    fluent-bit-tbddb                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d5h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                570m (28%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                    From                          Message\n  ----    ------                   ----                   ----                          -------\n  Normal  Starting                 5h58m                  kubelet, ah-kres-master-1     Starting kubelet.\n  Normal  NodeAllocatableEnforced  5h58m                  kubelet, ah-kres-master-1     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientPID     5h58m (x7 over 5h58m)  kubelet, ah-kres-master-1     Node ah-kres-master-1 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  5h58m (x8 over 5h58m)  kubelet, ah-kres-master-1     Node ah-kres-master-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    5h58m (x8 over 5h58m)  kubelet, ah-kres-master-1     Node ah-kres-master-1 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 5h57m                  kube-proxy, ah-kres-master-1  Starting kube-proxy.\n  Normal  Starting                 77m                    kubelet, ah-kres-master-1     Starting kubelet.\n  Normal  NodeHasSufficientMemory  77m                    kubelet, ah-kres-master-1     Node ah-kres-master-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    77m                    kubelet, ah-kres-master-1     Node ah-kres-master-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     77m                    kubelet, ah-kres-master-1     Node ah-kres-master-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             77m                    kubelet, ah-kres-master-1     Node ah-kres-master-1 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  77m                    kubelet, ah-kres-master-1     Updated Node Allocatable limit across pods\n  Normal  NodeReady                77m                    kubelet, ah-kres-master-1     Node ah-kres-master-1 status is now: NodeReady\n"
Jul  2 20:11:27.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 describe namespace kubectl-6034'
Jul  2 20:11:27.460: INFO: stderr: ""
Jul  2 20:11:27.460: INFO: stdout: "Name:         kubectl-6034\nLabels:       e2e-framework=kubectl\n              e2e-run=7f000675-9cfd-11e9-808f-7ea354b18644\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:11:27.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6034" for this suite.
Jul  2 20:11:53.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:11:53.619: INFO: namespace kubectl-6034 deletion completed in 26.15167411s

â€¢ [SLOW TEST:31.711 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:11:53.619: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 20:11:53.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a26cd765-9d05-11e9-808f-7ea354b18644" in namespace "projected-3357" to be "success or failure"
Jul  2 20:11:53.715: INFO: Pod "downwardapi-volume-a26cd765-9d05-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.777876ms
Jul  2 20:11:55.724: INFO: Pod "downwardapi-volume-a26cd765-9d05-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015225367s
Jul  2 20:11:57.734: INFO: Pod "downwardapi-volume-a26cd765-9d05-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02495054s
STEP: Saw pod success
Jul  2 20:11:57.734: INFO: Pod "downwardapi-volume-a26cd765-9d05-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:11:57.745: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-a26cd765-9d05-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 20:11:58.436: INFO: Waiting for pod downwardapi-volume-a26cd765-9d05-11e9-808f-7ea354b18644 to disappear
Jul  2 20:11:58.464: INFO: Pod downwardapi-volume-a26cd765-9d05-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:11:58.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3357" for this suite.
Jul  2 20:12:04.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:12:04.682: INFO: namespace projected-3357 deletion completed in 6.200916602s

â€¢ [SLOW TEST:11.063 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:12:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  2 20:12:04.806: INFO: Waiting up to 5m0s for pod "pod-a908cc9c-9d05-11e9-808f-7ea354b18644" in namespace "emptydir-1722" to be "success or failure"
Jul  2 20:12:04.814: INFO: Pod "pod-a908cc9c-9d05-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.953556ms
Jul  2 20:12:06.824: INFO: Pod "pod-a908cc9c-9d05-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017927245s
Jul  2 20:12:08.828: INFO: Pod "pod-a908cc9c-9d05-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022566798s
STEP: Saw pod success
Jul  2 20:12:08.828: INFO: Pod "pod-a908cc9c-9d05-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:12:08.832: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-a908cc9c-9d05-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 20:12:08.885: INFO: Waiting for pod pod-a908cc9c-9d05-11e9-808f-7ea354b18644 to disappear
Jul  2 20:12:08.889: INFO: Pod pod-a908cc9c-9d05-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:12:08.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1722" for this suite.
Jul  2 20:12:14.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:12:15.057: INFO: namespace emptydir-1722 deletion completed in 6.162775998s

â€¢ [SLOW TEST:10.372 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:12:15.057: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-af3a235a-9d05-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 20:12:15.237: INFO: Waiting up to 5m0s for pod "pod-secrets-af4215f5-9d05-11e9-808f-7ea354b18644" in namespace "secrets-8620" to be "success or failure"
Jul  2 20:12:15.246: INFO: Pod "pod-secrets-af4215f5-9d05-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.086007ms
Jul  2 20:12:17.845: INFO: Pod "pod-secrets-af4215f5-9d05-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.608532264s
Jul  2 20:12:19.979: INFO: Pod "pod-secrets-af4215f5-9d05-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.742088717s
STEP: Saw pod success
Jul  2 20:12:19.979: INFO: Pod "pod-secrets-af4215f5-9d05-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:12:19.984: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-secrets-af4215f5-9d05-11e9-808f-7ea354b18644 container secret-volume-test: <nil>
STEP: delete the pod
Jul  2 20:12:20.278: INFO: Waiting for pod pod-secrets-af4215f5-9d05-11e9-808f-7ea354b18644 to disappear
Jul  2 20:12:20.285: INFO: Pod pod-secrets-af4215f5-9d05-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:12:20.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8620" for this suite.
Jul  2 20:12:26.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:12:26.476: INFO: namespace secrets-8620 deletion completed in 6.181703351s

â€¢ [SLOW TEST:11.419 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:12:26.478: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul  2 20:12:26.910: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:12:32.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9160" for this suite.
Jul  2 20:12:46.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:12:46.993: INFO: namespace init-container-9160 deletion completed in 14.16852342s

â€¢ [SLOW TEST:20.515 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:12:46.996: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-qg2k
STEP: Creating a pod to test atomic-volume-subpath
Jul  2 20:12:47.085: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qg2k" in namespace "subpath-9503" to be "success or failure"
Jul  2 20:12:47.090: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58285ms
Jul  2 20:12:49.099: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013681513s
Jul  2 20:12:51.106: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Running", Reason="", readiness=true. Elapsed: 4.020487228s
Jul  2 20:12:55.212: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Running", Reason="", readiness=true. Elapsed: 8.126245087s
Jul  2 20:12:57.987: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Running", Reason="", readiness=true. Elapsed: 10.901876202s
Jul  2 20:12:59.995: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Running", Reason="", readiness=true. Elapsed: 12.909823976s
Jul  2 20:13:02.002: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Running", Reason="", readiness=true. Elapsed: 14.916951788s
Jul  2 20:13:04.007: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Running", Reason="", readiness=true. Elapsed: 16.921888975s
Jul  2 20:13:06.014: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Running", Reason="", readiness=true. Elapsed: 18.928470475s
Jul  2 20:13:09.151: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Running", Reason="", readiness=true. Elapsed: 22.065502368s
Jul  2 20:13:15.212: INFO: Pod "pod-subpath-test-configmap-qg2k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.126253675s
STEP: Saw pod success
Jul  2 20:13:15.212: INFO: Pod "pod-subpath-test-configmap-qg2k" satisfied condition "success or failure"
Jul  2 20:13:15.248: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-subpath-test-configmap-qg2k container test-container-subpath-configmap-qg2k: <nil>
STEP: delete the pod
Jul  2 20:13:15.893: INFO: Waiting for pod pod-subpath-test-configmap-qg2k to disappear
Jul  2 20:13:15.900: INFO: Pod pod-subpath-test-configmap-qg2k no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qg2k
Jul  2 20:13:15.900: INFO: Deleting pod "pod-subpath-test-configmap-qg2k" in namespace "subpath-9503"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:13:15.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9503" for this suite.
Jul  2 20:13:21.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:13:22.148: INFO: namespace subpath-9503 deletion completed in 6.228678758s

â€¢ [SLOW TEST:35.152 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:13:22.149: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4937
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  2 20:13:22.217: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  2 20:13:46.394: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.44.0.6:8080/dial?request=hostName&protocol=http&host=10.32.0.6&port=8080&tries=1'] Namespace:pod-network-test-4937 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:13:46.394: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:13:46.734: INFO: Waiting for endpoints: map[]
Jul  2 20:13:46.740: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.44.0.6:8080/dial?request=hostName&protocol=http&host=10.44.0.5&port=8080&tries=1'] Namespace:pod-network-test-4937 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:13:46.740: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:13:47.014: INFO: Waiting for endpoints: map[]
Jul  2 20:13:47.018: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.44.0.6:8080/dial?request=hostName&protocol=http&host=10.40.0.5&port=8080&tries=1'] Namespace:pod-network-test-4937 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:13:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:13:47.430: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:13:47.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4937" for this suite.
Jul  2 20:14:56.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:14:56.688: INFO: namespace pod-network-test-4937 deletion completed in 24.663955901s

â€¢ [SLOW TEST:49.953 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:14:56.689: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Jul  2 20:14:56.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-5501'
Jul  2 20:14:57.198: INFO: stderr: ""
Jul  2 20:14:57.199: INFO: stdout: "pod/pause created\n"
Jul  2 20:14:57.199: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul  2 20:14:57.199: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5501" to be "running and ready"
Jul  2 20:14:57.206: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.212112ms
Jul  2 20:14:59.213: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014026568s
Jul  2 20:15:01.219: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.020251139s
Jul  2 20:15:01.219: INFO: Pod "pause" satisfied condition "running and ready"
Jul  2 20:15:01.219: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Jul  2 20:15:01.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 label pods pause testing-label=testing-label-value --namespace=kubectl-5501'
Jul  2 20:15:01.447: INFO: stderr: ""
Jul  2 20:15:01.447: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul  2 20:15:01.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pod pause -L testing-label --namespace=kubectl-5501'
Jul  2 20:15:01.628: INFO: stderr: ""
Jul  2 20:15:01.628: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul  2 20:15:01.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 label pods pause testing-label- --namespace=kubectl-5501'
Jul  2 20:15:01.804: INFO: stderr: ""
Jul  2 20:15:01.805: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul  2 20:15:01.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pod pause -L testing-label --namespace=kubectl-5501'
Jul  2 20:15:01.977: INFO: stderr: ""
Jul  2 20:15:01.977: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Jul  2 20:15:01.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-5501'
Jul  2 20:15:02.169: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 20:15:02.169: INFO: stdout: "pod \"pause\" force deleted\n"
Jul  2 20:15:02.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get rc,svc -l name=pause --no-headers --namespace=kubectl-5501'
Jul  2 20:15:02.921: INFO: stderr: "No resources found.\n"
Jul  2 20:15:02.921: INFO: stdout: ""
Jul  2 20:15:02.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -l name=pause --namespace=kubectl-5501 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  2 20:15:03.086: INFO: stderr: ""
Jul  2 20:15:03.087: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:15:03.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5501" for this suite.
Jul  2 20:15:09.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:15:09.280: INFO: namespace kubectl-5501 deletion completed in 6.182551544s

â€¢ [SLOW TEST:12.591 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:15:09.280: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Jul  2 20:15:09.900: INFO: created pod pod-service-account-defaultsa
Jul  2 20:15:09.900: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul  2 20:15:09.911: INFO: created pod pod-service-account-mountsa
Jul  2 20:15:09.911: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul  2 20:15:09.925: INFO: created pod pod-service-account-nomountsa
Jul  2 20:15:09.925: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul  2 20:15:09.941: INFO: created pod pod-service-account-defaultsa-mountspec
Jul  2 20:15:09.942: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul  2 20:15:09.951: INFO: created pod pod-service-account-mountsa-mountspec
Jul  2 20:15:09.951: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul  2 20:15:09.969: INFO: created pod pod-service-account-nomountsa-mountspec
Jul  2 20:15:09.970: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul  2 20:15:09.984: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul  2 20:15:09.984: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul  2 20:15:09.996: INFO: created pod pod-service-account-mountsa-nomountspec
Jul  2 20:15:09.996: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul  2 20:15:10.008: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul  2 20:15:10.008: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:15:10.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6762" for this suite.
Jul  2 20:15:34.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:15:34.363: INFO: namespace svcaccounts-6762 deletion completed in 24.346127889s

â€¢ [SLOW TEST:25.083 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:15:34.364: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9409
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9409
STEP: Creating statefulset with conflicting port in namespace statefulset-9409
STEP: Waiting until pod test-pod will start running in namespace statefulset-9409
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9409
Jul  2 20:15:38.561: INFO: Observed stateful pod in namespace: statefulset-9409, name: ss-0, uid: 1d9f0bf5-9d06-11e9-9994-0050569f2898, status phase: Pending. Waiting for statefulset controller to delete.
Jul  2 20:15:41.325: INFO: Observed stateful pod in namespace: statefulset-9409, name: ss-0, uid: 1d9f0bf5-9d06-11e9-9994-0050569f2898, status phase: Failed. Waiting for statefulset controller to delete.
Jul  2 20:15:41.350: INFO: Observed stateful pod in namespace: statefulset-9409, name: ss-0, uid: 1d9f0bf5-9d06-11e9-9994-0050569f2898, status phase: Failed. Waiting for statefulset controller to delete.
Jul  2 20:15:41.362: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9409
STEP: Removing pod with conflicting port in namespace statefulset-9409
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9409 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  2 20:15:57.273: INFO: Deleting all statefulset in ns statefulset-9409
Jul  2 20:15:57.332: INFO: Scaling statefulset ss to 0
Jul  2 20:16:18.267: INFO: Waiting for statefulset status.replicas updated to 0
Jul  2 20:16:18.276: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:16:18.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9409" for this suite.
Jul  2 20:16:26.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:16:26.522: INFO: namespace statefulset-9409 deletion completed in 8.199984766s

â€¢ [SLOW TEST:52.158 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:16:26.524: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:16:26.617: INFO: (0) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.911687ms)
Jul  2 20:16:26.624: INFO: (1) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.530156ms)
Jul  2 20:16:26.631: INFO: (2) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.685537ms)
Jul  2 20:16:26.637: INFO: (3) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.856651ms)
Jul  2 20:16:26.644: INFO: (4) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.407373ms)
Jul  2 20:16:26.650: INFO: (5) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.095121ms)
Jul  2 20:16:26.656: INFO: (6) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.964583ms)
Jul  2 20:16:26.662: INFO: (7) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.777205ms)
Jul  2 20:16:26.670: INFO: (8) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.476092ms)
Jul  2 20:16:26.677: INFO: (9) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.928588ms)
Jul  2 20:16:26.683: INFO: (10) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.554433ms)
Jul  2 20:16:26.690: INFO: (11) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.860243ms)
Jul  2 20:16:26.697: INFO: (12) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.444784ms)
Jul  2 20:16:26.713: INFO: (13) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 16.326781ms)
Jul  2 20:16:26.720: INFO: (14) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.668387ms)
Jul  2 20:16:26.726: INFO: (15) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.17398ms)
Jul  2 20:16:26.732: INFO: (16) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.912871ms)
Jul  2 20:16:26.739: INFO: (17) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.890127ms)
Jul  2 20:16:26.745: INFO: (18) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.886878ms)
Jul  2 20:16:26.752: INFO: (19) /api/v1/nodes/ah-kres-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.395241ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:16:26.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4756" for this suite.
Jul  2 20:16:34.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:16:34.917: INFO: namespace proxy-4756 deletion completed in 8.157692791s

â€¢ [SLOW TEST:8.393 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:16:34.919: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul  2 20:16:34.971: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:16:40.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-635" for this suite.
Jul  2 20:16:46.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:16:46.841: INFO: namespace init-container-635 deletion completed in 6.175856972s

â€¢ [SLOW TEST:11.921 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:16:46.844: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Jul  2 20:16:46.927: INFO: Waiting up to 5m0s for pod "client-containers-513207f1-9d06-11e9-808f-7ea354b18644" in namespace "containers-4224" to be "success or failure"
Jul  2 20:16:46.939: INFO: Pod "client-containers-513207f1-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 11.669523ms
Jul  2 20:16:48.945: INFO: Pod "client-containers-513207f1-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018193659s
Jul  2 20:16:50.950: INFO: Pod "client-containers-513207f1-9d06-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022683748s
STEP: Saw pod success
Jul  2 20:16:50.950: INFO: Pod "client-containers-513207f1-9d06-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:16:50.953: INFO: Trying to get logs from node ah-kres-worker-3 pod client-containers-513207f1-9d06-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 20:16:50.982: INFO: Waiting for pod client-containers-513207f1-9d06-11e9-808f-7ea354b18644 to disappear
Jul  2 20:16:50.986: INFO: Pod client-containers-513207f1-9d06-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:16:50.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4224" for this suite.
Jul  2 20:16:57.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:16:57.184: INFO: namespace containers-4224 deletion completed in 6.193612911s

â€¢ [SLOW TEST:10.341 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:16:57.185: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:17:01.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1573" for this suite.
Jul  2 20:17:07.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:17:07.870: INFO: namespace emptydir-wrapper-1573 deletion completed in 6.216909705s

â€¢ [SLOW TEST:10.685 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:17:07.871: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Jul  2 20:17:08.012: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-362853403 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:17:08.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7176" for this suite.
Jul  2 20:17:14.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:17:14.407: INFO: namespace kubectl-7176 deletion completed in 6.213644032s

â€¢ [SLOW TEST:6.536 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:17:14.407: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Jul  2 20:17:14.492: INFO: Waiting up to 5m0s for pod "client-containers-619f9eaa-9d06-11e9-808f-7ea354b18644" in namespace "containers-4610" to be "success or failure"
Jul  2 20:17:14.509: INFO: Pod "client-containers-619f9eaa-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 16.469569ms
Jul  2 20:17:16.746: INFO: Pod "client-containers-619f9eaa-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253449835s
Jul  2 20:17:18.752: INFO: Pod "client-containers-619f9eaa-9d06-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.259846413s
STEP: Saw pod success
Jul  2 20:17:18.752: INFO: Pod "client-containers-619f9eaa-9d06-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:17:18.757: INFO: Trying to get logs from node ah-kres-worker-2 pod client-containers-619f9eaa-9d06-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 20:17:18.793: INFO: Waiting for pod client-containers-619f9eaa-9d06-11e9-808f-7ea354b18644 to disappear
Jul  2 20:17:18.801: INFO: Pod client-containers-619f9eaa-9d06-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:17:18.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4610" for this suite.
Jul  2 20:17:30.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:17:31.006: INFO: namespace containers-4610 deletion completed in 12.195773191s

â€¢ [SLOW TEST:16.599 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:17:31.007: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul  2 20:17:39.473: INFO: Successfully updated pod "annotationupdate6b859422-9d06-11e9-808f-7ea354b18644"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:17:41.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-826" for this suite.
Jul  2 20:18:09.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:18:09.764: INFO: namespace downward-api-826 deletion completed in 28.180531814s

â€¢ [SLOW TEST:38.758 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:18:09.766: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-829b6de4-9d06-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 20:18:09.829: INFO: Waiting up to 5m0s for pod "pod-configmaps-829cc889-9d06-11e9-808f-7ea354b18644" in namespace "configmap-6227" to be "success or failure"
Jul  2 20:18:09.835: INFO: Pod "pod-configmaps-829cc889-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.642426ms
Jul  2 20:18:11.842: INFO: Pod "pod-configmaps-829cc889-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013057355s
Jul  2 20:18:13.848: INFO: Pod "pod-configmaps-829cc889-9d06-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019698025s
STEP: Saw pod success
Jul  2 20:18:13.849: INFO: Pod "pod-configmaps-829cc889-9d06-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:18:13.853: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-configmaps-829cc889-9d06-11e9-808f-7ea354b18644 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 20:18:14.008: INFO: Waiting for pod pod-configmaps-829cc889-9d06-11e9-808f-7ea354b18644 to disappear
Jul  2 20:18:14.016: INFO: Pod pod-configmaps-829cc889-9d06-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:18:14.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6227" for this suite.
Jul  2 20:18:20.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:18:20.232: INFO: namespace configmap-6227 deletion completed in 6.205931371s

â€¢ [SLOW TEST:10.467 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:18:20.234: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-1650
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1650 to expose endpoints map[]
Jul  2 20:18:20.760: INFO: successfully validated that service endpoint-test2 in namespace services-1650 exposes endpoints map[] (8.237494ms elapsed)
STEP: Creating pod pod1 in namespace services-1650
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1650 to expose endpoints map[pod1:[80]]
Jul  2 20:18:23.810: INFO: successfully validated that service endpoint-test2 in namespace services-1650 exposes endpoints map[pod1:[80]] (3.038046528s elapsed)
STEP: Creating pod pod2 in namespace services-1650
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1650 to expose endpoints map[pod1:[80] pod2:[80]]
Jul  2 20:18:26.879: INFO: successfully validated that service endpoint-test2 in namespace services-1650 exposes endpoints map[pod1:[80] pod2:[80]] (3.058259135s elapsed)
STEP: Deleting pod pod1 in namespace services-1650
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1650 to expose endpoints map[pod2:[80]]
Jul  2 20:18:27.922: INFO: successfully validated that service endpoint-test2 in namespace services-1650 exposes endpoints map[pod2:[80]] (1.035458827s elapsed)
STEP: Deleting pod pod2 in namespace services-1650
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1650 to expose endpoints map[]
Jul  2 20:18:27.960: INFO: successfully validated that service endpoint-test2 in namespace services-1650 exposes endpoints map[] (14.383984ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:18:28.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1650" for this suite.
Jul  2 20:18:36.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:18:36.263: INFO: namespace services-1650 deletion completed in 8.199391861s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:16.030 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:18:36.267: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-926baa72-9d06-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 20:18:36.438: INFO: Waiting up to 5m0s for pod "pod-secrets-92784d1e-9d06-11e9-808f-7ea354b18644" in namespace "secrets-5021" to be "success or failure"
Jul  2 20:18:36.447: INFO: Pod "pod-secrets-92784d1e-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 8.576372ms
Jul  2 20:18:38.454: INFO: Pod "pod-secrets-92784d1e-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015532802s
Jul  2 20:18:40.460: INFO: Pod "pod-secrets-92784d1e-9d06-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021812271s
STEP: Saw pod success
Jul  2 20:18:40.460: INFO: Pod "pod-secrets-92784d1e-9d06-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:18:40.464: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-secrets-92784d1e-9d06-11e9-808f-7ea354b18644 container secret-volume-test: <nil>
STEP: delete the pod
Jul  2 20:18:40.503: INFO: Waiting for pod pod-secrets-92784d1e-9d06-11e9-808f-7ea354b18644 to disappear
Jul  2 20:18:40.509: INFO: Pod pod-secrets-92784d1e-9d06-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:18:40.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5021" for this suite.
Jul  2 20:18:46.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:18:46.714: INFO: namespace secrets-5021 deletion completed in 6.191863582s
STEP: Destroying namespace "secret-namespace-4" for this suite.
Jul  2 20:18:57.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:18:58.030: INFO: namespace secret-namespace-4 deletion completed in 11.315827718s

â€¢ [SLOW TEST:21.763 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:18:58.030: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-9f627fa1-9d06-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 20:18:58.117: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f643f22-9d06-11e9-808f-7ea354b18644" in namespace "configmap-3058" to be "success or failure"
Jul  2 20:18:58.134: INFO: Pod "pod-configmaps-9f643f22-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 17.731024ms
Jul  2 20:19:00.141: INFO: Pod "pod-configmaps-9f643f22-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024201257s
Jul  2 20:19:02.147: INFO: Pod "pod-configmaps-9f643f22-9d06-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030119654s
STEP: Saw pod success
Jul  2 20:19:02.147: INFO: Pod "pod-configmaps-9f643f22-9d06-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:19:02.152: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-configmaps-9f643f22-9d06-11e9-808f-7ea354b18644 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 20:19:02.189: INFO: Waiting for pod pod-configmaps-9f643f22-9d06-11e9-808f-7ea354b18644 to disappear
Jul  2 20:19:02.198: INFO: Pod pod-configmaps-9f643f22-9d06-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:19:02.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3058" for this suite.
Jul  2 20:19:08.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:19:08.453: INFO: namespace configmap-3058 deletion completed in 6.247546736s

â€¢ [SLOW TEST:10.423 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:19:08.454: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Jul  2 20:19:08.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 --namespace=kubectl-6340 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul  2 20:19:11.006: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul  2 20:19:11.006: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:19:13.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6340" for this suite.
Jul  2 20:19:19.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:19:19.219: INFO: namespace kubectl-6340 deletion completed in 6.195475952s

â€¢ [SLOW TEST:10.765 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:19:19.220: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-ac081601-9d06-11e9-808f-7ea354b18644
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-ac081601-9d06-11e9-808f-7ea354b18644
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:19:25.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6808" for this suite.
Jul  2 20:19:47.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:19:47.700: INFO: namespace configmap-6808 deletion completed in 22.268183239s

â€¢ [SLOW TEST:28.480 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:19:47.701: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 20:19:47.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644" in namespace "downward-api-7751" to be "success or failure"
Jul  2 20:19:47.804: INFO: Pod "downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.026314ms
Jul  2 20:19:51.651: INFO: Pod "downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 3.851860366s
Jul  2 20:19:53.694: INFO: Pod "downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.89524037s
Jul  2 20:19:55.703: INFO: Pod "downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.903739597s
Jul  2 20:19:57.710: INFO: Pod "downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.911409629s
STEP: Saw pod success
Jul  2 20:19:57.711: INFO: Pod "downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:19:57.716: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 20:19:57.794: INFO: Waiting for pod downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644 to disappear
Jul  2 20:19:57.803: INFO: Pod downwardapi-volume-bcff224e-9d06-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:19:57.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7751" for this suite.
Jul  2 20:20:59.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:21:00.000: INFO: namespace downward-api-7751 deletion completed in 1m2.187055604s

â€¢ [SLOW TEST:72.299 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:21:00.001: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  2 20:21:00.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1661'
Jul  2 20:21:00.293: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  2 20:21:00.293: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul  2 20:21:00.310: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-dzc94]
Jul  2 20:21:00.311: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-dzc94" in namespace "kubectl-1661" to be "running and ready"
Jul  2 20:21:00.325: INFO: Pod "e2e-test-nginx-rc-dzc94": Phase="Pending", Reason="", readiness=false. Elapsed: 13.615797ms
Jul  2 20:21:02.333: INFO: Pod "e2e-test-nginx-rc-dzc94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022353314s
Jul  2 20:21:04.340: INFO: Pod "e2e-test-nginx-rc-dzc94": Phase="Running", Reason="", readiness=true. Elapsed: 4.029466264s
Jul  2 20:21:04.340: INFO: Pod "e2e-test-nginx-rc-dzc94" satisfied condition "running and ready"
Jul  2 20:21:04.340: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-dzc94]
Jul  2 20:21:04.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 logs rc/e2e-test-nginx-rc --namespace=kubectl-1661'
Jul  2 20:21:04.585: INFO: stderr: ""
Jul  2 20:21:04.585: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Jul  2 20:21:04.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete rc e2e-test-nginx-rc --namespace=kubectl-1661'
Jul  2 20:21:04.804: INFO: stderr: ""
Jul  2 20:21:04.804: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:21:04.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1661" for this suite.
Jul  2 20:21:23.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:21:23.954: INFO: namespace kubectl-1661 deletion completed in 19.139381077s

â€¢ [SLOW TEST:23.953 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:21:23.959: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:21:24.007: INFO: Creating deployment "nginx-deployment"
Jul  2 20:21:24.021: INFO: Waiting for observed generation 1
Jul  2 20:21:26.035: INFO: Waiting for all required pods to come up
Jul  2 20:21:26.041: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul  2 20:21:41.515: INFO: Waiting for deployment "nginx-deployment" to complete
Jul  2 20:21:42.350: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul  2 20:21:42.521: INFO: Updating deployment nginx-deployment
Jul  2 20:21:42.521: INFO: Waiting for observed generation 2
Jul  2 20:22:02.556: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul  2 20:22:02.561: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul  2 20:22:02.565: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul  2 20:22:02.584: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul  2 20:22:02.584: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul  2 20:22:02.593: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul  2 20:22:02.605: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul  2 20:22:02.605: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul  2 20:22:02.617: INFO: Updating deployment nginx-deployment
Jul  2 20:22:02.618: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul  2 20:22:02.627: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul  2 20:22:02.633: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  2 20:22:04.651: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1052,SelfLink:/apis/apps/v1/namespaces/deployment-1052/deployments/nginx-deployment,UID:10e1f400-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:643581,Generation:3,CreationTimestamp:2019-07-02 20:22:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-07-02 20:22:02 +0000 UTC 2019-07-02 20:22:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-02 20:22:03 +0000 UTC 2019-07-02 20:22:02 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul  2 20:22:04.659: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-1052,SelfLink:/apis/apps/v1/namespaces/deployment-1052/replicasets/nginx-deployment-5f9595f595,UID:23e3635f-9d07-11e9-9994-0050569f2898,ResourceVersion:643580,Generation:3,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 10e1f400-9d07-11e9-ab09-0050569fbfc5 0xc00197c4b7 0xc00197c4b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  2 20:22:04.659: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul  2 20:22:04.659: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-1052,SelfLink:/apis/apps/v1/namespaces/deployment-1052/replicasets/nginx-deployment-6f478d8d8,UID:0d22d5b4-9d07-11e9-9994-0050569f2898,ResourceVersion:643566,Generation:3,CreationTimestamp:2019-07-02 20:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 10e1f400-9d07-11e9-ab09-0050569fbfc5 0xc00197c667 0xc00197c668}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul  2 20:22:04.671: INFO: Pod "nginx-deployment-5f9595f595-4qtkf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-4qtkf,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-4qtkf,UID:23e7c67e-9d07-11e9-9994-0050569f2898,ResourceVersion:643444,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a88797 0xc001a88798}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a88810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a88830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:,StartTime:2019-07-02 20:22:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.672: INFO: Pod "nginx-deployment-5f9595f595-8j5bm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-8j5bm,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-8j5bm,UID:24863933-9d07-11e9-9994-0050569f2898,ResourceVersion:643585,Generation:0,CreationTimestamp:2019-07-02 20:22:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a88920 0xc001a88921}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a889a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a889d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:,StartTime:2019-07-02 20:22:29 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.672: INFO: Pod "nginx-deployment-5f9595f595-b284w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-b284w,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-b284w,UID:24033781-9d07-11e9-9994-0050569f2898,ResourceVersion:643471,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a88ab0 0xc001a88ab1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a88b30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a88b50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:,StartTime:2019-07-02 20:22:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.673: INFO: Pod "nginx-deployment-5f9595f595-cjtnk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-cjtnk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-cjtnk,UID:243b4cf9-9d07-11e9-9994-0050569f2898,ResourceVersion:643594,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a88c60 0xc001a88c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a88ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a88d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:,StartTime:2019-07-02 20:22:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.673: INFO: Pod "nginx-deployment-5f9595f595-ghwl7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-ghwl7,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-ghwl7,UID:2460343c-9d07-11e9-9994-0050569f2898,ResourceVersion:643573,Generation:0,CreationTimestamp:2019-07-02 20:22:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a88dd0 0xc001a88dd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a88e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a88e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:,StartTime:2019-07-02 20:22:29 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.674: INFO: Pod "nginx-deployment-5f9595f595-jz52d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jz52d,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-jz52d,UID:23e91b9a-9d07-11e9-9994-0050569f2898,ResourceVersion:643443,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a88f40 0xc001a88f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a88fc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a88fe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:,StartTime:2019-07-02 20:22:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.675: INFO: Pod "nginx-deployment-5f9595f595-kdvw8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kdvw8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-kdvw8,UID:23e98cee-9d07-11e9-9994-0050569f2898,ResourceVersion:643445,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a890b0 0xc001a890b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a89130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a89150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:,StartTime:2019-07-02 20:22:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.675: INFO: Pod "nginx-deployment-5f9595f595-lp8tl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lp8tl,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-lp8tl,UID:2467b29a-9d07-11e9-9994-0050569f2898,ResourceVersion:643604,Generation:0,CreationTimestamp:2019-07-02 20:22:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a89220 0xc001a89221}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a892a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a892c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:,StartTime:2019-07-02 20:22:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.676: INFO: Pod "nginx-deployment-5f9595f595-qx928" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-qx928,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-qx928,UID:243b6cbb-9d07-11e9-9994-0050569f2898,ResourceVersion:643555,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a89390 0xc001a89391}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a89410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a89430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:,StartTime:2019-07-02 20:22:29 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.676: INFO: Pod "nginx-deployment-5f9595f595-rx8r6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-rx8r6,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-rx8r6,UID:2450d838-9d07-11e9-9994-0050569f2898,ResourceVersion:643561,Generation:0,CreationTimestamp:2019-07-02 20:22:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a89500 0xc001a89501}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a89580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a895a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:,StartTime:2019-07-02 20:22:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.677: INFO: Pod "nginx-deployment-5f9595f595-skwdd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-skwdd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-skwdd,UID:2430deee-9d07-11e9-9994-0050569f2898,ResourceVersion:643520,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a89670 0xc001a89671}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a896f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a89710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:,StartTime:2019-07-02 20:22:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.677: INFO: Pod "nginx-deployment-5f9595f595-w2nxh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-w2nxh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-w2nxh,UID:23ff86b3-9d07-11e9-9994-0050569f2898,ResourceVersion:643463,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a897e0 0xc001a897e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a89860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a89880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:,StartTime:2019-07-02 20:22:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.678: INFO: Pod "nginx-deployment-5f9595f595-zmr4h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-zmr4h,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-5f9595f595-zmr4h,UID:24719095-9d07-11e9-9994-0050569f2898,ResourceVersion:643578,Generation:0,CreationTimestamp:2019-07-02 20:22:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 23e3635f-9d07-11e9-9994-0050569f2898 0xc001a89950 0xc001a89951}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a899d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a899f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:,StartTime:2019-07-02 20:22:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.678: INFO: Pod "nginx-deployment-6f478d8d8-22rt8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-22rt8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-22rt8,UID:0d29a889-9d07-11e9-9994-0050569f2898,ResourceVersion:643335,Generation:0,CreationTimestamp:2019-07-02 20:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001a89ac0 0xc001a89ac1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a89b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a89b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:10.40.0.5,StartTime:2019-07-02 20:21:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-02 20:21:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6f2b68fa89ef8f8647903158b3861cafa5699b4bc956667001f85668b9bfd257}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.679: INFO: Pod "nginx-deployment-6f478d8d8-4hfd6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4hfd6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-4hfd6,UID:24326338-9d07-11e9-9994-0050569f2898,ResourceVersion:643535,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001a89c30 0xc001a89c31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a89ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a89cd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:,StartTime:2019-07-02 20:22:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.679: INFO: Pod "nginx-deployment-6f478d8d8-752q2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-752q2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-752q2,UID:0d2c85c0-9d07-11e9-9994-0050569f2898,ResourceVersion:643341,Generation:0,CreationTimestamp:2019-07-02 20:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001a89d90 0xc001a89d91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a89e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a89e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:10.32.0.9,StartTime:2019-07-02 20:21:48 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-02 20:21:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://030cb995019f8bb2876834380b7124329f9d3ca5f1c464bdc50cc7c5601e6473}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.680: INFO: Pod "nginx-deployment-6f478d8d8-7lljf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7lljf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-7lljf,UID:24406af5-9d07-11e9-9994-0050569f2898,ResourceVersion:643563,Generation:0,CreationTimestamp:2019-07-02 20:22:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001a89f00 0xc001a89f01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a89f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a89f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:,StartTime:2019-07-02 20:22:29 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.680: INFO: Pod "nginx-deployment-6f478d8d8-c475b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-c475b,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-c475b,UID:0d2ce7ca-9d07-11e9-9994-0050569f2898,ResourceVersion:643332,Generation:0,CreationTimestamp:2019-07-02 20:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f34057 0xc001f34058}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f340d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f340f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:10.40.0.6,StartTime:2019-07-02 20:21:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-02 20:21:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e818b7ea2e8990cbfcbed67cd6c270456904fde218cbec3decaf479d977e13c2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.681: INFO: Pod "nginx-deployment-6f478d8d8-ckg75" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ckg75,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-ckg75,UID:243fb5c5-9d07-11e9-9994-0050569f2898,ResourceVersion:643545,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f341e0 0xc001f341e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f34270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f34290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:,StartTime:2019-07-02 20:22:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.681: INFO: Pod "nginx-deployment-6f478d8d8-cqtww" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-cqtww,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-cqtww,UID:2432c7fc-9d07-11e9-9994-0050569f2898,ResourceVersion:643539,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f34357 0xc001f34358}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f343e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f34400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:,StartTime:2019-07-02 20:22:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.682: INFO: Pod "nginx-deployment-6f478d8d8-fwkqs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fwkqs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-fwkqs,UID:242ae43d-9d07-11e9-9994-0050569f2898,ResourceVersion:643505,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f344c7 0xc001f344c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f34540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f34560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:,StartTime:2019-07-02 20:22:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.683: INFO: Pod "nginx-deployment-6f478d8d8-g8c2c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-g8c2c,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-g8c2c,UID:242d00af-9d07-11e9-9994-0050569f2898,ResourceVersion:643528,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f34627 0xc001f34628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f346a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f346c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:,StartTime:2019-07-02 20:22:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.683: INFO: Pod "nginx-deployment-6f478d8d8-k4c5t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-k4c5t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-k4c5t,UID:242de0af-9d07-11e9-9994-0050569f2898,ResourceVersion:643537,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f34787 0xc001f34788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f34800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f34820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:,StartTime:2019-07-02 20:22:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.684: INFO: Pod "nginx-deployment-6f478d8d8-kcktb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-kcktb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-kcktb,UID:0d334a35-9d07-11e9-9994-0050569f2898,ResourceVersion:643329,Generation:0,CreationTimestamp:2019-07-02 20:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f348e7 0xc001f348e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f34960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f34980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:10.40.0.7,StartTime:2019-07-02 20:21:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-02 20:21:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4460e08a7e0d14750ffdaf6fc272d918129e0cba44fd7283fe0a32b626ad936e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.684: INFO: Pod "nginx-deployment-6f478d8d8-kxc9g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-kxc9g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-kxc9g,UID:0d33396f-9d07-11e9-9994-0050569f2898,ResourceVersion:643344,Generation:0,CreationTimestamp:2019-07-02 20:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f34a50 0xc001f34a51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f34ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f34ae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:10.32.0.7,StartTime:2019-07-02 20:21:48 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-02 20:21:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2238add8b9a29f74b131f0fabe316c0d5364bb20bd1d999c0df7a5c1fc836aa6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.684: INFO: Pod "nginx-deployment-6f478d8d8-m5j6g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-m5j6g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-m5j6g,UID:0d2767df-9d07-11e9-9994-0050569f2898,ResourceVersion:643355,Generation:0,CreationTimestamp:2019-07-02 20:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f34bb0 0xc001f34bb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f34c20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f34c40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:10.44.0.5,StartTime:2019-07-02 20:21:58 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-02 20:22:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://737dd0e78aae5e2d864cef3869c467553ab5af08480e4b4bbfa2b7cbbe68dba2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.685: INFO: Pod "nginx-deployment-6f478d8d8-mtk29" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mtk29,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-mtk29,UID:0d331eda-9d07-11e9-9994-0050569f2898,ResourceVersion:643338,Generation:0,CreationTimestamp:2019-07-02 20:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f34d10 0xc001f34d11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f34d90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f34db0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:10.32.0.8,StartTime:2019-07-02 20:21:48 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-02 20:21:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d81e249949c038b83a01ce6b8c640c105b2eaabdc25e395628a687e01901b76f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.685: INFO: Pod "nginx-deployment-6f478d8d8-mz625" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mz625,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-mz625,UID:24401cca-9d07-11e9-9994-0050569f2898,ResourceVersion:643582,Generation:0,CreationTimestamp:2019-07-02 20:22:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f34e80 0xc001f34e81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f34ef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f34f10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:,StartTime:2019-07-02 20:22:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.686: INFO: Pod "nginx-deployment-6f478d8d8-nz5n7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-nz5n7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-nz5n7,UID:0d2974c7-9d07-11e9-9994-0050569f2898,ResourceVersion:643347,Generation:0,CreationTimestamp:2019-07-02 20:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f34fd7 0xc001f34fd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f35050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f35070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:21:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:02 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.166,PodIP:10.32.0.6,StartTime:2019-07-02 20:21:48 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-02 20:21:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4ddb31af79262e3407c95d9127ef28384168fb3fe6b3b524e0b2b89fd134ca11}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.686: INFO: Pod "nginx-deployment-6f478d8d8-rd66w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rd66w,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-rd66w,UID:2432df4c-9d07-11e9-9994-0050569f2898,ResourceVersion:643562,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f35160 0xc001f35161}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f351e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f35200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:,StartTime:2019-07-02 20:22:37 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.687: INFO: Pod "nginx-deployment-6f478d8d8-rhdn9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rhdn9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-rhdn9,UID:24423de3-9d07-11e9-9994-0050569f2898,ResourceVersion:643553,Generation:0,CreationTimestamp:2019-07-02 20:22:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f352c7 0xc001f352c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f35340} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f35370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:,StartTime:2019-07-02 20:22:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.687: INFO: Pod "nginx-deployment-6f478d8d8-sw7tx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-sw7tx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-sw7tx,UID:2432a781-9d07-11e9-9994-0050569f2898,ResourceVersion:643543,Generation:0,CreationTimestamp:2019-07-02 20:22:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f35460 0xc001f35461}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f354d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f354f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:40 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:,StartTime:2019-07-02 20:22:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  2 20:22:04.688: INFO: Pod "nginx-deployment-6f478d8d8-wjkbf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wjkbf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1052,SelfLink:/api/v1/namespaces/deployment-1052/pods/nginx-deployment-6f478d8d8-wjkbf,UID:24403d4e-9d07-11e9-9994-0050569f2898,ResourceVersion:643547,Generation:0,CreationTimestamp:2019-07-02 20:22:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d22d5b4-9d07-11e9-9994-0050569f2898 0xc001f355b0 0xc001f355b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sp264 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sp264,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sp264 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f35620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f35640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:22:41 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:,StartTime:2019-07-02 20:22:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:22:04.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1052" for this suite.
Jul  2 20:23:01.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:23:02.381: INFO: namespace deployment-1052 deletion completed in 10.100007498s

â€¢ [SLOW TEST:50.839 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:23:02.385: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul  2 20:23:07.847: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:23:07.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6326" for this suite.
Jul  2 20:24:22.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:24:22.504: INFO: namespace replicaset-6326 deletion completed in 1m14.613090174s

â€¢ [SLOW TEST:80.120 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:24:22.504: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0702 20:24:35.651190      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  2 20:24:35.651: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:24:35.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4905" for this suite.
Jul  2 20:24:41.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:24:41.953: INFO: namespace gc-4905 deletion completed in 6.296257183s

â€¢ [SLOW TEST:19.449 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:24:41.954: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul  2 20:24:42.038: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-a,UID:6abfc929-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644317,Generation:0,CreationTimestamp:2019-07-02 20:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  2 20:24:42.039: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-a,UID:6abfc929-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644317,Generation:0,CreationTimestamp:2019-07-02 20:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul  2 20:24:57.985: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-a,UID:6abfc929-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644341,Generation:0,CreationTimestamp:2019-07-02 20:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul  2 20:24:57.985: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-a,UID:6abfc929-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644341,Generation:0,CreationTimestamp:2019-07-02 20:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul  2 20:25:08.055: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-a,UID:6abfc929-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644363,Generation:0,CreationTimestamp:2019-07-02 20:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  2 20:25:08.055: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-a,UID:6abfc929-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644363,Generation:0,CreationTimestamp:2019-07-02 20:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul  2 20:25:18.071: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-a,UID:6abfc929-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644388,Generation:0,CreationTimestamp:2019-07-02 20:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  2 20:25:18.071: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-a,UID:6abfc929-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644388,Generation:0,CreationTimestamp:2019-07-02 20:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul  2 20:25:28.086: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-b,UID:872e15a7-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644410,Generation:0,CreationTimestamp:2019-07-02 20:25:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  2 20:25:28.087: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-b,UID:872e15a7-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644410,Generation:0,CreationTimestamp:2019-07-02 20:25:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul  2 20:25:42.358: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-b,UID:872e15a7-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644429,Generation:0,CreationTimestamp:2019-07-02 20:25:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  2 20:25:42.358: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5088,SelfLink:/api/v1/namespaces/watch-5088/configmaps/e2e-watch-test-configmap-b,UID:872e15a7-9d07-11e9-ab09-0050569fbfc5,ResourceVersion:644429,Generation:0,CreationTimestamp:2019-07-02 20:25:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:25:52.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5088" for this suite.
Jul  2 20:25:59.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:25:59.412: INFO: namespace watch-5088 deletion completed in 6.50255102s

â€¢ [SLOW TEST:77.458 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:25:59.414: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-9a945f72-9d07-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 20:25:59.555: INFO: Waiting up to 5m0s for pod "pod-secrets-9a964aae-9d07-11e9-808f-7ea354b18644" in namespace "secrets-1247" to be "success or failure"
Jul  2 20:25:59.566: INFO: Pod "pod-secrets-9a964aae-9d07-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 10.689869ms
Jul  2 20:26:01.572: INFO: Pod "pod-secrets-9a964aae-9d07-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017177725s
STEP: Saw pod success
Jul  2 20:26:01.572: INFO: Pod "pod-secrets-9a964aae-9d07-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:26:01.577: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-secrets-9a964aae-9d07-11e9-808f-7ea354b18644 container secret-volume-test: <nil>
STEP: delete the pod
Jul  2 20:26:01.614: INFO: Waiting for pod pod-secrets-9a964aae-9d07-11e9-808f-7ea354b18644 to disappear
Jul  2 20:26:01.619: INFO: Pod pod-secrets-9a964aae-9d07-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:26:01.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1247" for this suite.
Jul  2 20:26:07.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:26:07.854: INFO: namespace secrets-1247 deletion completed in 6.227269768s

â€¢ [SLOW TEST:8.440 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:26:07.854: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 20:26:08.099: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f97626e-9d07-11e9-808f-7ea354b18644" in namespace "projected-1438" to be "success or failure"
Jul  2 20:26:08.109: INFO: Pod "downwardapi-volume-9f97626e-9d07-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 10.496699ms
Jul  2 20:26:10.117: INFO: Pod "downwardapi-volume-9f97626e-9d07-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017737908s
Jul  2 20:26:12.125: INFO: Pod "downwardapi-volume-9f97626e-9d07-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026474599s
STEP: Saw pod success
Jul  2 20:26:12.126: INFO: Pod "downwardapi-volume-9f97626e-9d07-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:26:12.136: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-9f97626e-9d07-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 20:26:14.690: INFO: Waiting for pod downwardapi-volume-9f97626e-9d07-11e9-808f-7ea354b18644 to disappear
Jul  2 20:26:14.701: INFO: Pod downwardapi-volume-9f97626e-9d07-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:26:14.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1438" for this suite.
Jul  2 20:26:20.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:26:21.009: INFO: namespace projected-1438 deletion completed in 6.299673569s

â€¢ [SLOW TEST:13.155 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:26:21.013: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4334
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  2 20:26:21.100: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  2 20:26:43.318: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.32.0.6 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4334 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:26:43.318: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:26:44.695: INFO: Found all expected endpoints: [netserver-0]
Jul  2 20:26:44.704: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.40.0.5 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4334 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:26:44.704: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:26:46.134: INFO: Found all expected endpoints: [netserver-1]
Jul  2 20:26:46.141: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.44.0.5 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4334 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  2 20:26:46.141: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
Jul  2 20:26:47.539: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:26:47.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4334" for this suite.
Jul  2 20:27:17.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:27:18.130: INFO: namespace pod-network-test-4334 deletion completed in 30.582307239s

â€¢ [SLOW TEST:57.118 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:27:18.133: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-c9773f75-9d07-11e9-808f-7ea354b18644
STEP: Creating configMap with name cm-test-opt-upd-c9773fe4-9d07-11e9-808f-7ea354b18644
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c9773f75-9d07-11e9-808f-7ea354b18644
STEP: Updating configmap cm-test-opt-upd-c9773fe4-9d07-11e9-808f-7ea354b18644
STEP: Creating configMap with name cm-test-opt-create-c977400d-9d07-11e9-808f-7ea354b18644
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:27:24.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5610" for this suite.
Jul  2 20:27:44.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:27:44.638: INFO: namespace configmap-5610 deletion completed in 20.228444771s

â€¢ [SLOW TEST:26.506 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:27:44.639: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-d9451058-9d07-11e9-808f-7ea354b18644
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:27:44.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9123" for this suite.
Jul  2 20:27:50.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:27:50.952: INFO: namespace configmap-9123 deletion completed in 6.238854265s

â€¢ [SLOW TEST:6.314 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:27:50.953: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 20:27:51.058: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd0b5f76-9d07-11e9-808f-7ea354b18644" in namespace "downward-api-8263" to be "success or failure"
Jul  2 20:27:51.073: INFO: Pod "downwardapi-volume-dd0b5f76-9d07-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 14.597858ms
Jul  2 20:27:53.079: INFO: Pod "downwardapi-volume-dd0b5f76-9d07-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020227342s
STEP: Saw pod success
Jul  2 20:27:53.079: INFO: Pod "downwardapi-volume-dd0b5f76-9d07-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:27:53.084: INFO: Trying to get logs from node ah-kres-worker-1 pod downwardapi-volume-dd0b5f76-9d07-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 20:27:53.128: INFO: Waiting for pod downwardapi-volume-dd0b5f76-9d07-11e9-808f-7ea354b18644 to disappear
Jul  2 20:27:53.134: INFO: Pod downwardapi-volume-dd0b5f76-9d07-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:27:53.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8263" for this suite.
Jul  2 20:27:59.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:27:59.423: INFO: namespace downward-api-8263 deletion completed in 6.283759975s

â€¢ [SLOW TEST:8.471 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:27:59.424: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:28:00.848: INFO: Create a RollingUpdate DaemonSet
Jul  2 20:28:03.511: INFO: Check that daemon pods launch on every node of the cluster
Jul  2 20:28:03.898: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:03.902: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:03.902: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:03.922: INFO: Number of nodes with available pods: 0
Jul  2 20:28:03.922: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 20:28:04.936: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:04.936: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:04.936: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:04.948: INFO: Number of nodes with available pods: 0
Jul  2 20:28:04.948: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 20:28:05.928: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:05.928: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:05.929: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:05.933: INFO: Number of nodes with available pods: 0
Jul  2 20:28:05.933: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 20:28:06.932: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:06.932: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:06.932: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:06.945: INFO: Number of nodes with available pods: 2
Jul  2 20:28:06.945: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 20:28:45.360: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:45.360: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:45.360: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:45.366: INFO: Number of nodes with available pods: 3
Jul  2 20:28:45.366: INFO: Number of running nodes: 3, number of available pods: 3
Jul  2 20:28:45.366: INFO: Update the DaemonSet to trigger a rollout
Jul  2 20:28:45.381: INFO: Updating DaemonSet daemon-set
Jul  2 20:28:49.405: INFO: Roll back the DaemonSet before rollout is complete
Jul  2 20:28:49.420: INFO: Updating DaemonSet daemon-set
Jul  2 20:28:49.420: INFO: Make sure DaemonSet rollback is complete
Jul  2 20:28:49.428: INFO: Wrong image for pod: daemon-set-228s2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  2 20:28:49.429: INFO: Pod daemon-set-228s2 is not available
Jul  2 20:28:49.440: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:49.440: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:49.440: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:50.449: INFO: Wrong image for pod: daemon-set-228s2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  2 20:28:50.449: INFO: Pod daemon-set-228s2 is not available
Jul  2 20:28:50.457: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:50.457: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:50.457: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:51.447: INFO: Wrong image for pod: daemon-set-228s2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  2 20:28:51.447: INFO: Pod daemon-set-228s2 is not available
Jul  2 20:28:51.452: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:51.452: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:51.452: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:52.447: INFO: Pod daemon-set-gxc7r is not available
Jul  2 20:28:52.456: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:52.456: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 20:28:52.456: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9586, will wait for the garbage collector to delete the pods
Jul  2 20:28:52.538: INFO: Deleting DaemonSet.extensions daemon-set took: 15.554763ms
Jul  2 20:28:52.939: INFO: Terminating DaemonSet.extensions daemon-set pods took: 401.029273ms
Jul  2 20:28:57.245: INFO: Number of nodes with available pods: 0
Jul  2 20:28:57.246: INFO: Number of running nodes: 0, number of available pods: 0
Jul  2 20:28:57.253: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9586/daemonsets","resourceVersion":"645066"},"items":null}

Jul  2 20:28:57.258: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9586/pods","resourceVersion":"645066"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:28:57.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9586" for this suite.
Jul  2 20:29:05.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:29:05.495: INFO: namespace daemonsets-9586 deletion completed in 8.195738432s

â€¢ [SLOW TEST:28.642 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:29:05.496: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-0976deed-9d08-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 20:29:05.586: INFO: Waiting up to 5m0s for pod "pod-configmaps-0978b286-9d08-11e9-808f-7ea354b18644" in namespace "configmap-2909" to be "success or failure"
Jul  2 20:29:05.594: INFO: Pod "pod-configmaps-0978b286-9d08-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 8.663756ms
Jul  2 20:29:07.602: INFO: Pod "pod-configmaps-0978b286-9d08-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01627222s
Jul  2 20:29:09.610: INFO: Pod "pod-configmaps-0978b286-9d08-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023827802s
STEP: Saw pod success
Jul  2 20:29:09.610: INFO: Pod "pod-configmaps-0978b286-9d08-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:29:09.615: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-configmaps-0978b286-9d08-11e9-808f-7ea354b18644 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 20:29:09.663: INFO: Waiting for pod pod-configmaps-0978b286-9d08-11e9-808f-7ea354b18644 to disappear
Jul  2 20:29:09.676: INFO: Pod pod-configmaps-0978b286-9d08-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:29:09.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2909" for this suite.
Jul  2 20:29:19.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:29:19.928: INFO: namespace configmap-2909 deletion completed in 10.241951696s

â€¢ [SLOW TEST:14.431 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:29:19.930: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-7m592 in namespace proxy-3220
I0702 20:29:20.026012      15 runners.go:184] Created replication controller with name: proxy-service-7m592, namespace: proxy-3220, replica count: 1
I0702 20:29:21.076806      15 runners.go:184] proxy-service-7m592 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0702 20:29:22.077055      15 runners.go:184] proxy-service-7m592 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0702 20:29:23.077315      15 runners.go:184] proxy-service-7m592 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0702 20:29:24.077565      15 runners.go:184] proxy-service-7m592 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0702 20:29:25.077893      15 runners.go:184] proxy-service-7m592 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0702 20:29:26.078408      15 runners.go:184] proxy-service-7m592 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  2 20:29:26.085: INFO: setup took 6.09404208s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul  2 20:29:26.112: INFO: (0) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 25.601622ms)
Jul  2 20:29:26.113: INFO: (0) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 26.0302ms)
Jul  2 20:29:26.116: INFO: (0) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 29.077312ms)
Jul  2 20:29:26.117: INFO: (0) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 31.318952ms)
Jul  2 20:29:26.117: INFO: (0) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 31.660638ms)
Jul  2 20:29:26.117: INFO: (0) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 31.261276ms)
Jul  2 20:29:26.118: INFO: (0) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 32.478495ms)
Jul  2 20:29:26.135: INFO: (0) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 49.520288ms)
Jul  2 20:29:26.137: INFO: (0) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 51.010587ms)
Jul  2 20:29:26.138: INFO: (0) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 51.461122ms)
Jul  2 20:29:26.139: INFO: (0) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 53.436438ms)
Jul  2 20:29:26.141: INFO: (0) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 54.572543ms)
Jul  2 20:29:26.141: INFO: (0) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 54.55442ms)
Jul  2 20:29:26.144: INFO: (0) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 57.645101ms)
Jul  2 20:29:26.146: INFO: (0) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 60.528817ms)
Jul  2 20:29:26.154: INFO: (0) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 67.248533ms)
Jul  2 20:29:26.167: INFO: (1) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 12.168439ms)
Jul  2 20:29:26.167: INFO: (1) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 11.751145ms)
Jul  2 20:29:26.167: INFO: (1) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 12.792929ms)
Jul  2 20:29:26.180: INFO: (1) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 22.354336ms)
Jul  2 20:29:26.180: INFO: (1) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 24.294386ms)
Jul  2 20:29:26.180: INFO: (1) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 23.861113ms)
Jul  2 20:29:26.182: INFO: (1) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 24.682161ms)
Jul  2 20:29:26.182: INFO: (1) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 25.214302ms)
Jul  2 20:29:26.182: INFO: (1) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 25.762751ms)
Jul  2 20:29:26.182: INFO: (1) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 27.313506ms)
Jul  2 20:29:26.184: INFO: (1) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 26.586401ms)
Jul  2 20:29:26.185: INFO: (1) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 27.427782ms)
Jul  2 20:29:26.188: INFO: (1) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 32.460351ms)
Jul  2 20:29:26.188: INFO: (1) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 32.558067ms)
Jul  2 20:29:26.189: INFO: (1) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 33.18732ms)
Jul  2 20:29:26.189: INFO: (1) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 32.693078ms)
Jul  2 20:29:26.202: INFO: (2) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 12.188126ms)
Jul  2 20:29:26.207: INFO: (2) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 17.024242ms)
Jul  2 20:29:26.208: INFO: (2) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 17.128636ms)
Jul  2 20:29:26.214: INFO: (2) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 22.517227ms)
Jul  2 20:29:26.215: INFO: (2) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 22.010873ms)
Jul  2 20:29:26.219: INFO: (2) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 28.702179ms)
Jul  2 20:29:26.219: INFO: (2) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 26.734536ms)
Jul  2 20:29:26.220: INFO: (2) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 26.787916ms)
Jul  2 20:29:26.221: INFO: (2) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 30.268266ms)
Jul  2 20:29:26.222: INFO: (2) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 30.554454ms)
Jul  2 20:29:26.223: INFO: (2) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 32.143552ms)
Jul  2 20:29:26.223: INFO: (2) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 30.203326ms)
Jul  2 20:29:26.224: INFO: (2) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 32.748002ms)
Jul  2 20:29:26.225: INFO: (2) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 33.349488ms)
Jul  2 20:29:26.226: INFO: (2) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 33.295787ms)
Jul  2 20:29:26.230: INFO: (2) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 38.274247ms)
Jul  2 20:29:26.248: INFO: (3) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 17.107225ms)
Jul  2 20:29:26.249: INFO: (3) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 18.926711ms)
Jul  2 20:29:26.250: INFO: (3) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 18.510916ms)
Jul  2 20:29:26.250: INFO: (3) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 19.204749ms)
Jul  2 20:29:26.250: INFO: (3) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 19.121135ms)
Jul  2 20:29:26.251: INFO: (3) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 20.153034ms)
Jul  2 20:29:26.251: INFO: (3) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 20.48139ms)
Jul  2 20:29:26.250: INFO: (3) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 19.653793ms)
Jul  2 20:29:26.252: INFO: (3) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 22.032551ms)
Jul  2 20:29:26.254: INFO: (3) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 22.565342ms)
Jul  2 20:29:26.255: INFO: (3) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 25.538037ms)
Jul  2 20:29:26.258: INFO: (3) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 27.269609ms)
Jul  2 20:29:26.261: INFO: (3) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 31.177468ms)
Jul  2 20:29:26.263: INFO: (3) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 32.05096ms)
Jul  2 20:29:26.263: INFO: (3) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 32.033722ms)
Jul  2 20:29:26.263: INFO: (3) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 33.220056ms)
Jul  2 20:29:26.276: INFO: (4) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 11.728789ms)
Jul  2 20:29:26.282: INFO: (4) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 14.957197ms)
Jul  2 20:29:26.283: INFO: (4) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 16.956039ms)
Jul  2 20:29:26.283: INFO: (4) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 19.756831ms)
Jul  2 20:29:26.284: INFO: (4) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 16.601378ms)
Jul  2 20:29:26.284: INFO: (4) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 17.651207ms)
Jul  2 20:29:26.285: INFO: (4) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 20.674272ms)
Jul  2 20:29:26.286: INFO: (4) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 21.067163ms)
Jul  2 20:29:26.286: INFO: (4) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 20.383754ms)
Jul  2 20:29:26.290: INFO: (4) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 23.534427ms)
Jul  2 20:29:26.290: INFO: (4) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 26.915483ms)
Jul  2 20:29:26.291: INFO: (4) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 26.414047ms)
Jul  2 20:29:26.291: INFO: (4) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 25.827759ms)
Jul  2 20:29:26.291: INFO: (4) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 27.499364ms)
Jul  2 20:29:26.292: INFO: (4) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 27.389535ms)
Jul  2 20:29:26.293: INFO: (4) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 28.547347ms)
Jul  2 20:29:26.307: INFO: (5) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 13.520527ms)
Jul  2 20:29:26.315: INFO: (5) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 19.124561ms)
Jul  2 20:29:26.316: INFO: (5) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 19.425212ms)
Jul  2 20:29:26.316: INFO: (5) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 20.182113ms)
Jul  2 20:29:26.318: INFO: (5) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 23.266846ms)
Jul  2 20:29:26.319: INFO: (5) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 24.439255ms)
Jul  2 20:29:26.319: INFO: (5) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 23.643088ms)
Jul  2 20:29:26.320: INFO: (5) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 24.970973ms)
Jul  2 20:29:26.320: INFO: (5) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 26.480866ms)
Jul  2 20:29:26.321: INFO: (5) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 27.713562ms)
Jul  2 20:29:26.321: INFO: (5) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 26.934159ms)
Jul  2 20:29:26.323: INFO: (5) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 29.226462ms)
Jul  2 20:29:26.330: INFO: (5) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 34.203096ms)
Jul  2 20:29:26.331: INFO: (5) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 36.664847ms)
Jul  2 20:29:26.332: INFO: (5) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 38.183916ms)
Jul  2 20:29:26.332: INFO: (5) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 36.097466ms)
Jul  2 20:29:26.351: INFO: (6) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 18.504482ms)
Jul  2 20:29:26.362: INFO: (6) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 28.14562ms)
Jul  2 20:29:26.363: INFO: (6) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 29.396871ms)
Jul  2 20:29:26.363: INFO: (6) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 29.990047ms)
Jul  2 20:29:26.363: INFO: (6) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 29.488166ms)
Jul  2 20:29:26.364: INFO: (6) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 30.693925ms)
Jul  2 20:29:26.365: INFO: (6) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 31.801637ms)
Jul  2 20:29:26.365: INFO: (6) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 32.055703ms)
Jul  2 20:29:26.366: INFO: (6) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 33.64958ms)
Jul  2 20:29:26.367: INFO: (6) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 33.694258ms)
Jul  2 20:29:26.374: INFO: (6) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 39.677708ms)
Jul  2 20:29:26.376: INFO: (6) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 42.898734ms)
Jul  2 20:29:26.377: INFO: (6) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 42.895555ms)
Jul  2 20:29:26.378: INFO: (6) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 43.937901ms)
Jul  2 20:29:26.379: INFO: (6) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 45.360169ms)
Jul  2 20:29:26.379: INFO: (6) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 45.613783ms)
Jul  2 20:29:26.396: INFO: (7) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 16.498653ms)
Jul  2 20:29:26.397: INFO: (7) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 17.52064ms)
Jul  2 20:29:26.398: INFO: (7) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 17.755436ms)
Jul  2 20:29:26.399: INFO: (7) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 19.449416ms)
Jul  2 20:29:26.401: INFO: (7) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 20.732935ms)
Jul  2 20:29:26.402: INFO: (7) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 21.0394ms)
Jul  2 20:29:26.404: INFO: (7) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 24.280596ms)
Jul  2 20:29:26.405: INFO: (7) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 24.90222ms)
Jul  2 20:29:26.405: INFO: (7) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 24.475619ms)
Jul  2 20:29:26.406: INFO: (7) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 26.118692ms)
Jul  2 20:29:26.406: INFO: (7) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 26.199987ms)
Jul  2 20:29:26.408: INFO: (7) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 27.737678ms)
Jul  2 20:29:26.411: INFO: (7) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 31.345154ms)
Jul  2 20:29:26.411: INFO: (7) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 30.846067ms)
Jul  2 20:29:26.411: INFO: (7) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 30.836475ms)
Jul  2 20:29:26.412: INFO: (7) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 31.148362ms)
Jul  2 20:29:26.434: INFO: (8) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 19.914979ms)
Jul  2 20:29:26.439: INFO: (8) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 23.538967ms)
Jul  2 20:29:26.439: INFO: (8) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 25.987887ms)
Jul  2 20:29:26.439: INFO: (8) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 25.648748ms)
Jul  2 20:29:26.440: INFO: (8) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 25.065201ms)
Jul  2 20:29:26.440: INFO: (8) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 25.057806ms)
Jul  2 20:29:26.440: INFO: (8) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 26.839264ms)
Jul  2 20:29:26.440: INFO: (8) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 25.914676ms)
Jul  2 20:29:26.441: INFO: (8) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 28.2255ms)
Jul  2 20:29:26.441: INFO: (8) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 29.633181ms)
Jul  2 20:29:26.442: INFO: (8) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 29.007038ms)
Jul  2 20:29:26.442: INFO: (8) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 29.760444ms)
Jul  2 20:29:26.443: INFO: (8) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 30.885567ms)
Jul  2 20:29:26.443: INFO: (8) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 29.536458ms)
Jul  2 20:29:26.444: INFO: (8) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 28.695491ms)
Jul  2 20:29:26.444: INFO: (8) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 30.905675ms)
Jul  2 20:29:26.469: INFO: (9) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 23.249978ms)
Jul  2 20:29:26.476: INFO: (9) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 30.418429ms)
Jul  2 20:29:26.476: INFO: (9) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 30.791492ms)
Jul  2 20:29:26.476: INFO: (9) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 31.71813ms)
Jul  2 20:29:26.477: INFO: (9) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 31.338845ms)
Jul  2 20:29:26.477: INFO: (9) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 31.590944ms)
Jul  2 20:29:26.492: INFO: (9) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 47.192052ms)
Jul  2 20:29:26.492: INFO: (9) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 47.668838ms)
Jul  2 20:29:26.493: INFO: (9) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 47.624495ms)
Jul  2 20:29:26.494: INFO: (9) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 49.272033ms)
Jul  2 20:29:26.497: INFO: (9) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 52.545369ms)
Jul  2 20:29:26.499: INFO: (9) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 53.812789ms)
Jul  2 20:29:26.504: INFO: (9) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 58.643643ms)
Jul  2 20:29:26.505: INFO: (9) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 59.305239ms)
Jul  2 20:29:26.514: INFO: (9) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 69.4927ms)
Jul  2 20:29:26.515: INFO: (9) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 69.267515ms)
Jul  2 20:29:26.533: INFO: (10) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 17.606201ms)
Jul  2 20:29:26.533: INFO: (10) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 18.59648ms)
Jul  2 20:29:26.541: INFO: (10) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 21.666004ms)
Jul  2 20:29:26.541: INFO: (10) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 21.924912ms)
Jul  2 20:29:26.541: INFO: (10) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 26.014641ms)
Jul  2 20:29:26.541: INFO: (10) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 22.264373ms)
Jul  2 20:29:26.544: INFO: (10) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 20.64036ms)
Jul  2 20:29:26.545: INFO: (10) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 21.616385ms)
Jul  2 20:29:26.545: INFO: (10) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 22.734023ms)
Jul  2 20:29:26.549: INFO: (10) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 26.093714ms)
Jul  2 20:29:26.549: INFO: (10) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 17.91214ms)
Jul  2 20:29:26.550: INFO: (10) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 26.796728ms)
Jul  2 20:29:26.550: INFO: (10) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 31.41135ms)
Jul  2 20:29:26.551: INFO: (10) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 19.211107ms)
Jul  2 20:29:26.555: INFO: (10) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 23.156867ms)
Jul  2 20:29:26.555: INFO: (10) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 23.45137ms)
Jul  2 20:29:26.567: INFO: (11) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 12.246793ms)
Jul  2 20:29:26.568: INFO: (11) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 12.538544ms)
Jul  2 20:29:26.572: INFO: (11) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 15.559177ms)
Jul  2 20:29:26.572: INFO: (11) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 16.206151ms)
Jul  2 20:29:26.575: INFO: (11) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 18.401017ms)
Jul  2 20:29:26.575: INFO: (11) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 19.34722ms)
Jul  2 20:29:26.577: INFO: (11) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 21.420784ms)
Jul  2 20:29:26.578: INFO: (11) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 22.072586ms)
Jul  2 20:29:26.581: INFO: (11) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 25.893602ms)
Jul  2 20:29:26.582: INFO: (11) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 26.340233ms)
Jul  2 20:29:26.584: INFO: (11) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 28.17325ms)
Jul  2 20:29:26.585: INFO: (11) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 28.593639ms)
Jul  2 20:29:26.585: INFO: (11) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 29.328504ms)
Jul  2 20:29:26.586: INFO: (11) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 29.878536ms)
Jul  2 20:29:26.586: INFO: (11) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 29.757822ms)
Jul  2 20:29:26.587: INFO: (11) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 31.505893ms)
Jul  2 20:29:26.606: INFO: (12) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 17.624738ms)
Jul  2 20:29:26.607: INFO: (12) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 17.01615ms)
Jul  2 20:29:26.607: INFO: (12) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 19.138377ms)
Jul  2 20:29:26.607: INFO: (12) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 19.042957ms)
Jul  2 20:29:26.608: INFO: (12) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 20.061402ms)
Jul  2 20:29:26.608: INFO: (12) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 19.973985ms)
Jul  2 20:29:26.608: INFO: (12) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 18.743796ms)
Jul  2 20:29:26.614: INFO: (12) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 26.655901ms)
Jul  2 20:29:26.615: INFO: (12) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 27.570289ms)
Jul  2 20:29:26.619: INFO: (12) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 30.7845ms)
Jul  2 20:29:26.620: INFO: (12) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 32.785098ms)
Jul  2 20:29:26.621: INFO: (12) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 33.130011ms)
Jul  2 20:29:26.621: INFO: (12) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 32.239641ms)
Jul  2 20:29:26.622: INFO: (12) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 32.359173ms)
Jul  2 20:29:26.622: INFO: (12) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 34.189891ms)
Jul  2 20:29:26.624: INFO: (12) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 36.284032ms)
Jul  2 20:29:26.643: INFO: (13) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 15.954622ms)
Jul  2 20:29:26.645: INFO: (13) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 19.798305ms)
Jul  2 20:29:26.646: INFO: (13) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 19.724516ms)
Jul  2 20:29:26.646: INFO: (13) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 21.691951ms)
Jul  2 20:29:26.647: INFO: (13) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 20.859576ms)
Jul  2 20:29:26.650: INFO: (13) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 22.594856ms)
Jul  2 20:29:26.651: INFO: (13) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 22.796766ms)
Jul  2 20:29:26.655: INFO: (13) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 27.643442ms)
Jul  2 20:29:26.655: INFO: (13) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 27.994413ms)
Jul  2 20:29:26.655: INFO: (13) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 29.822743ms)
Jul  2 20:29:26.655: INFO: (13) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 28.991078ms)
Jul  2 20:29:26.655: INFO: (13) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 30.312808ms)
Jul  2 20:29:26.655: INFO: (13) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 30.848307ms)
Jul  2 20:29:26.658: INFO: (13) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 32.408071ms)
Jul  2 20:29:26.659: INFO: (13) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 31.016373ms)
Jul  2 20:29:26.659: INFO: (13) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 31.486973ms)
Jul  2 20:29:26.668: INFO: (14) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 9.159542ms)
Jul  2 20:29:26.677: INFO: (14) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 17.676285ms)
Jul  2 20:29:26.677: INFO: (14) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 18.013826ms)
Jul  2 20:29:26.683: INFO: (14) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 23.322232ms)
Jul  2 20:29:26.686: INFO: (14) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 26.212984ms)
Jul  2 20:29:26.687: INFO: (14) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 27.168884ms)
Jul  2 20:29:26.687: INFO: (14) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 26.774749ms)
Jul  2 20:29:26.688: INFO: (14) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 27.826665ms)
Jul  2 20:29:26.688: INFO: (14) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 27.947401ms)
Jul  2 20:29:26.688: INFO: (14) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 27.790484ms)
Jul  2 20:29:26.688: INFO: (14) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 28.109153ms)
Jul  2 20:29:26.689: INFO: (14) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 28.812042ms)
Jul  2 20:29:26.692: INFO: (14) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 32.198423ms)
Jul  2 20:29:26.693: INFO: (14) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 32.862584ms)
Jul  2 20:29:26.695: INFO: (14) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 34.123721ms)
Jul  2 20:29:26.695: INFO: (14) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 35.323462ms)
Jul  2 20:29:26.717: INFO: (15) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 18.512325ms)
Jul  2 20:29:26.719: INFO: (15) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 22.782782ms)
Jul  2 20:29:26.721: INFO: (15) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 22.785873ms)
Jul  2 20:29:26.722: INFO: (15) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 24.568331ms)
Jul  2 20:29:26.723: INFO: (15) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 24.301692ms)
Jul  2 20:29:26.723: INFO: (15) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 23.879879ms)
Jul  2 20:29:26.723: INFO: (15) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 25.68411ms)
Jul  2 20:29:26.723: INFO: (15) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 24.071391ms)
Jul  2 20:29:26.724: INFO: (15) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 26.918413ms)
Jul  2 20:29:26.725: INFO: (15) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 28.621784ms)
Jul  2 20:29:26.726: INFO: (15) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 29.961968ms)
Jul  2 20:29:26.726: INFO: (15) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 29.569153ms)
Jul  2 20:29:26.727: INFO: (15) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 29.495738ms)
Jul  2 20:29:26.728: INFO: (15) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 30.172538ms)
Jul  2 20:29:26.728: INFO: (15) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 31.204258ms)
Jul  2 20:29:26.728: INFO: (15) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 29.8731ms)
Jul  2 20:29:26.743: INFO: (16) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 14.600277ms)
Jul  2 20:29:26.743: INFO: (16) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 14.610068ms)
Jul  2 20:29:26.749: INFO: (16) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 19.325478ms)
Jul  2 20:29:26.749: INFO: (16) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 19.526325ms)
Jul  2 20:29:26.750: INFO: (16) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 19.057274ms)
Jul  2 20:29:26.751: INFO: (16) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 20.299958ms)
Jul  2 20:29:26.752: INFO: (16) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 22.123396ms)
Jul  2 20:29:26.752: INFO: (16) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 22.233918ms)
Jul  2 20:29:26.757: INFO: (16) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 27.612663ms)
Jul  2 20:29:26.758: INFO: (16) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 26.267204ms)
Jul  2 20:29:26.758: INFO: (16) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 27.229176ms)
Jul  2 20:29:26.759: INFO: (16) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 28.770014ms)
Jul  2 20:29:26.759: INFO: (16) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 28.125611ms)
Jul  2 20:29:26.760: INFO: (16) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 31.431075ms)
Jul  2 20:29:26.761: INFO: (16) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 32.095437ms)
Jul  2 20:29:26.761: INFO: (16) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 30.335319ms)
Jul  2 20:29:26.773: INFO: (17) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 11.625732ms)
Jul  2 20:29:26.773: INFO: (17) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 11.818477ms)
Jul  2 20:29:26.779: INFO: (17) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 16.924553ms)
Jul  2 20:29:26.780: INFO: (17) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 17.689157ms)
Jul  2 20:29:26.780: INFO: (17) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 18.476835ms)
Jul  2 20:29:26.785: INFO: (17) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 21.843604ms)
Jul  2 20:29:26.786: INFO: (17) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 24.564345ms)
Jul  2 20:29:26.788: INFO: (17) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 25.171096ms)
Jul  2 20:29:26.788: INFO: (17) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 26.828181ms)
Jul  2 20:29:26.790: INFO: (17) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 28.251712ms)
Jul  2 20:29:26.791: INFO: (17) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 28.016062ms)
Jul  2 20:29:26.794: INFO: (17) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 31.754528ms)
Jul  2 20:29:26.797: INFO: (17) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 34.813162ms)
Jul  2 20:29:26.798: INFO: (17) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 34.921229ms)
Jul  2 20:29:26.798: INFO: (17) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 35.558393ms)
Jul  2 20:29:26.799: INFO: (17) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 35.939914ms)
Jul  2 20:29:26.811: INFO: (18) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 12.082947ms)
Jul  2 20:29:26.818: INFO: (18) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 17.760318ms)
Jul  2 20:29:26.819: INFO: (18) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 18.502362ms)
Jul  2 20:29:26.820: INFO: (18) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 19.418179ms)
Jul  2 20:29:26.820: INFO: (18) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 20.931118ms)
Jul  2 20:29:26.820: INFO: (18) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 20.355746ms)
Jul  2 20:29:26.820: INFO: (18) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 20.77883ms)
Jul  2 20:29:26.820: INFO: (18) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 20.953962ms)
Jul  2 20:29:26.820: INFO: (18) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 21.376632ms)
Jul  2 20:29:26.822: INFO: (18) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 22.886992ms)
Jul  2 20:29:26.827: INFO: (18) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 26.973098ms)
Jul  2 20:29:26.832: INFO: (18) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 32.382642ms)
Jul  2 20:29:26.832: INFO: (18) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 33.397935ms)
Jul  2 20:29:26.833: INFO: (18) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 33.22574ms)
Jul  2 20:29:26.833: INFO: (18) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 34.487268ms)
Jul  2 20:29:26.833: INFO: (18) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 33.853029ms)
Jul  2 20:29:26.847: INFO: (19) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s/proxy/rewriteme">test</a> (200; 13.120711ms)
Jul  2 20:29:26.852: INFO: (19) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:462/proxy/: tls qux (200; 18.863219ms)
Jul  2 20:29:26.854: INFO: (19) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:1080/proxy/rewriteme">... (200; 18.552734ms)
Jul  2 20:29:26.854: INFO: (19) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:443/proxy/tlsrewritem... (200; 18.942935ms)
Jul  2 20:29:26.855: INFO: (19) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:162/proxy/: bar (200; 20.190261ms)
Jul  2 20:29:26.857: INFO: (19) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:160/proxy/: foo (200; 22.432427ms)
Jul  2 20:29:26.861: INFO: (19) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname1/proxy/: foo (200; 25.804264ms)
Jul  2 20:29:26.863: INFO: (19) /api/v1/namespaces/proxy-3220/pods/http:proxy-service-7m592-wbv8s:162/proxy/: bar (200; 27.595544ms)
Jul  2 20:29:26.863: INFO: (19) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname1/proxy/: foo (200; 29.769436ms)
Jul  2 20:29:26.864: INFO: (19) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/: <a href="/api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:1080/proxy/rewriteme">test<... (200; 29.196967ms)
Jul  2 20:29:26.865: INFO: (19) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname1/proxy/: tls baz (200; 30.670237ms)
Jul  2 20:29:26.865: INFO: (19) /api/v1/namespaces/proxy-3220/services/proxy-service-7m592:portname2/proxy/: bar (200; 30.090504ms)
Jul  2 20:29:26.865: INFO: (19) /api/v1/namespaces/proxy-3220/pods/https:proxy-service-7m592-wbv8s:460/proxy/: tls baz (200; 29.663072ms)
Jul  2 20:29:26.865: INFO: (19) /api/v1/namespaces/proxy-3220/services/https:proxy-service-7m592:tlsportname2/proxy/: tls qux (200; 31.014513ms)
Jul  2 20:29:26.866: INFO: (19) /api/v1/namespaces/proxy-3220/services/http:proxy-service-7m592:portname2/proxy/: bar (200; 30.512548ms)
Jul  2 20:29:26.866: INFO: (19) /api/v1/namespaces/proxy-3220/pods/proxy-service-7m592-wbv8s:160/proxy/: foo (200; 30.793728ms)
STEP: deleting ReplicationController proxy-service-7m592 in namespace proxy-3220, will wait for the garbage collector to delete the pods
Jul  2 20:29:27.095: INFO: Deleting ReplicationController proxy-service-7m592 took: 172.452869ms
Jul  2 20:29:27.895: INFO: Terminating ReplicationController proxy-service-7m592 pods took: 800.358921ms
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:29:38.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3220" for this suite.
Jul  2 20:29:44.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:29:44.805: INFO: namespace proxy-3220 deletion completed in 6.293149275s

â€¢ [SLOW TEST:24.874 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:29:44.805: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul  2 20:29:55.437: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4579 pod-service-account-21370000-9d08-11e9-808f-7ea354b18644 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul  2 20:29:55.866: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4579 pod-service-account-21370000-9d08-11e9-808f-7ea354b18644 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul  2 20:29:56.429: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4579 pod-service-account-21370000-9d08-11e9-808f-7ea354b18644 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:29:56.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4579" for this suite.
Jul  2 20:30:02.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:30:04.153: INFO: namespace svcaccounts-4579 deletion completed in 7.267321478s

â€¢ [SLOW TEST:19.347 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:30:04.155: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7211.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7211.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7211.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7211.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7211.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7211.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7211.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7211.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7211.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7211.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7211.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 144.0.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.0.144_udp@PTR;check="$$(dig +tcp +noall +answer +search 144.0.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.0.144_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7211.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7211.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7211.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7211.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7211.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7211.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7211.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7211.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7211.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7211.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7211.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 144.0.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.0.144_udp@PTR;check="$$(dig +tcp +noall +answer +search 144.0.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.0.144_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  2 20:30:28.365: INFO: Unable to read wheezy_udp@dns-test-service.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.374: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.383: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.391: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.396: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.405: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.410: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.414: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.420: INFO: Unable to read 192.168.0.144_udp@PTR from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.425: INFO: Unable to read 192.168.0.144_tcp@PTR from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.432: INFO: Unable to read jessie_udp@dns-test-service.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.437: INFO: Unable to read jessie_tcp@dns-test-service.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.443: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.449: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.455: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.461: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-7211.svc.cluster.local from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.466: INFO: Unable to read jessie_udp@PodARecord from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.472: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.478: INFO: Unable to read 192.168.0.144_udp@PTR from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.484: INFO: Unable to read 192.168.0.144_tcp@PTR from pod dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-2da9af61-9d08-11e9-808f-7ea354b18644)
Jul  2 20:30:28.484: INFO: Lookups using dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644 failed for: [wheezy_udp@dns-test-service.dns-7211.svc.cluster.local wheezy_tcp@dns-test-service.dns-7211.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-7211.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-7211.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord 192.168.0.144_udp@PTR 192.168.0.144_tcp@PTR jessie_udp@dns-test-service.dns-7211.svc.cluster.local jessie_tcp@dns-test-service.dns-7211.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7211.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-7211.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-7211.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord 192.168.0.144_udp@PTR 192.168.0.144_tcp@PTR]

Jul  2 20:30:33.619: INFO: DNS probes using dns-7211/dns-test-2da9af61-9d08-11e9-808f-7ea354b18644 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:30:33.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7211" for this suite.
Jul  2 20:30:45.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:30:45.847: INFO: namespace dns-7211 deletion completed in 12.017666373s

â€¢ [SLOW TEST:41.692 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:30:45.848: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0702 20:30:47.009615      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  2 20:30:47.009: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:30:47.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1128" for this suite.
Jul  2 20:30:53.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:30:53.345: INFO: namespace gc-1128 deletion completed in 6.327850198s

â€¢ [SLOW TEST:7.498 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:30:53.346: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-5444
Jul  2 20:30:55.546: INFO: Started pod liveness-http in namespace container-probe-5444
STEP: checking the pod's current state and verifying that restartCount is present
Jul  2 20:30:55.551: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:35:37.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5444" for this suite.
Jul  2 20:35:45.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:35:45.337: INFO: namespace container-probe-5444 deletion completed in 8.241939369s

â€¢ [SLOW TEST:252.213 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:35:45.338: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-f7ce8113-9d08-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 20:35:45.455: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f7d06d5f-9d08-11e9-808f-7ea354b18644" in namespace "projected-8496" to be "success or failure"
Jul  2 20:35:45.463: INFO: Pod "pod-projected-configmaps-f7d06d5f-9d08-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 8.500797ms
Jul  2 20:35:47.470: INFO: Pod "pod-projected-configmaps-f7d06d5f-9d08-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015684552s
Jul  2 20:35:52.347: INFO: Pod "pod-projected-configmaps-f7d06d5f-9d08-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.891940413s
STEP: Saw pod success
Jul  2 20:35:52.347: INFO: Pod "pod-projected-configmaps-f7d06d5f-9d08-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:35:52.365: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-projected-configmaps-f7d06d5f-9d08-11e9-808f-7ea354b18644 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 20:35:52.435: INFO: Waiting for pod pod-projected-configmaps-f7d06d5f-9d08-11e9-808f-7ea354b18644 to disappear
Jul  2 20:35:52.443: INFO: Pod pod-projected-configmaps-f7d06d5f-9d08-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:35:52.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8496" for this suite.
Jul  2 20:35:58.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:35:58.605: INFO: namespace projected-8496 deletion completed in 6.15071246s

â€¢ [SLOW TEST:13.268 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:35:58.606: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  2 20:35:58.742: INFO: Waiting up to 5m0s for pod "downward-api-ffb7a5ed-9d08-11e9-808f-7ea354b18644" in namespace "downward-api-819" to be "success or failure"
Jul  2 20:35:58.750: INFO: Pod "downward-api-ffb7a5ed-9d08-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.617434ms
Jul  2 20:36:00.757: INFO: Pod "downward-api-ffb7a5ed-9d08-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012925113s
Jul  2 20:36:02.764: INFO: Pod "downward-api-ffb7a5ed-9d08-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020089151s
STEP: Saw pod success
Jul  2 20:36:02.764: INFO: Pod "downward-api-ffb7a5ed-9d08-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:36:02.769: INFO: Trying to get logs from node ah-kres-worker-3 pod downward-api-ffb7a5ed-9d08-11e9-808f-7ea354b18644 container dapi-container: <nil>
STEP: delete the pod
Jul  2 20:36:02.817: INFO: Waiting for pod downward-api-ffb7a5ed-9d08-11e9-808f-7ea354b18644 to disappear
Jul  2 20:36:02.825: INFO: Pod downward-api-ffb7a5ed-9d08-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:36:02.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-819" for this suite.
Jul  2 20:36:14.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:36:14.984: INFO: namespace downward-api-819 deletion completed in 12.150152534s

â€¢ [SLOW TEST:16.379 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:36:14.987: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:36:28.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3994" for this suite.
Jul  2 20:36:34.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:36:34.458: INFO: namespace namespaces-3994 deletion completed in 6.136102763s
STEP: Destroying namespace "nsdeletetest-3592" for this suite.
Jul  2 20:36:34.461: INFO: Namespace nsdeletetest-3592 was already deleted
STEP: Destroying namespace "nsdeletetest-5747" for this suite.
Jul  2 20:36:50.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:36:50.666: INFO: namespace nsdeletetest-5747 deletion completed in 16.204813644s

â€¢ [SLOW TEST:35.679 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:36:50.668: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  2 20:36:50.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9044'
Jul  2 20:36:51.141: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  2 20:36:51.142: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Jul  2 20:36:53.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9044'
Jul  2 20:36:53.658: INFO: stderr: ""
Jul  2 20:36:53.658: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:36:53.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9044" for this suite.
Jul  2 20:37:01.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:37:01.897: INFO: namespace kubectl-9044 deletion completed in 8.229523906s

â€¢ [SLOW TEST:11.230 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:37:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-256b11af-9d09-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 20:37:01.991: INFO: Waiting up to 5m0s for pod "pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644" in namespace "configmap-3225" to be "success or failure"
Jul  2 20:37:01.999: INFO: Pod "pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.468332ms
Jul  2 20:37:04.005: INFO: Pod "pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013834193s
Jul  2 20:37:06.755: INFO: Pod "pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.763159518s
Jul  2 20:37:08.761: INFO: Pod "pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.769519475s
Jul  2 20:37:11.419: INFO: Pod "pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.427133705s
STEP: Saw pod success
Jul  2 20:37:11.419: INFO: Pod "pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:37:11.425: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 20:37:11.494: INFO: Waiting for pod pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644 to disappear
Jul  2 20:37:11.500: INFO: Pod pod-configmaps-256cf4b2-9d09-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:37:11.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3225" for this suite.
Jul  2 20:37:17.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:37:17.702: INFO: namespace configmap-3225 deletion completed in 6.193517252s

â€¢ [SLOW TEST:15.802 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:37:17.703: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:37:21.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1840" for this suite.
Jul  2 20:38:10.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:38:10.408: INFO: namespace kubelet-test-1840 deletion completed in 48.550574776s

â€¢ [SLOW TEST:52.704 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:38:10.408: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:38:10.519: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:38:12.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3419" for this suite.
Jul  2 20:38:23.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:38:24.039: INFO: namespace custom-resource-definition-3419 deletion completed in 11.279299025s

â€¢ [SLOW TEST:13.631 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:38:24.052: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  2 20:38:24.159: INFO: Waiting up to 5m0s for pod "pod-56672f13-9d09-11e9-808f-7ea354b18644" in namespace "emptydir-6759" to be "success or failure"
Jul  2 20:38:24.167: INFO: Pod "pod-56672f13-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.861951ms
Jul  2 20:38:28.084: INFO: Pod "pod-56672f13-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924995434s
Jul  2 20:38:30.089: INFO: Pod "pod-56672f13-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.929959562s
Jul  2 20:38:32.095: INFO: Pod "pod-56672f13-9d09-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.935408161s
STEP: Saw pod success
Jul  2 20:38:32.096: INFO: Pod "pod-56672f13-9d09-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:38:32.101: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-56672f13-9d09-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 20:38:32.132: INFO: Waiting for pod pod-56672f13-9d09-11e9-808f-7ea354b18644 to disappear
Jul  2 20:38:32.137: INFO: Pod pod-56672f13-9d09-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:38:32.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6759" for this suite.
Jul  2 20:38:40.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:38:40.333: INFO: namespace emptydir-6759 deletion completed in 8.189605796s

â€¢ [SLOW TEST:16.281 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:38:40.334: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-8712
Jul  2 20:38:52.090: INFO: Started pod liveness-exec in namespace container-probe-8712
STEP: checking the pod's current state and verifying that restartCount is present
Jul  2 20:38:52.095: INFO: Initial restart count of pod liveness-exec is 0
Jul  2 20:39:36.569: INFO: Restart count of pod container-probe-8712/liveness-exec is now 1 (44.473764447s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:39:36.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8712" for this suite.
Jul  2 20:39:46.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:39:46.932: INFO: namespace container-probe-8712 deletion completed in 10.338984439s

â€¢ [SLOW TEST:66.598 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:39:46.933: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 20:39:46.999: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87c8e1f9-9d09-11e9-808f-7ea354b18644" in namespace "projected-125" to be "success or failure"
Jul  2 20:39:47.005: INFO: Pod "downwardapi-volume-87c8e1f9-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.786049ms
Jul  2 20:39:49.011: INFO: Pod "downwardapi-volume-87c8e1f9-9d09-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011658534s
STEP: Saw pod success
Jul  2 20:39:49.011: INFO: Pod "downwardapi-volume-87c8e1f9-9d09-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:39:49.015: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-87c8e1f9-9d09-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 20:39:49.058: INFO: Waiting for pod downwardapi-volume-87c8e1f9-9d09-11e9-808f-7ea354b18644 to disappear
Jul  2 20:39:49.068: INFO: Pod downwardapi-volume-87c8e1f9-9d09-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:39:49.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-125" for this suite.
Jul  2 20:39:55.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:39:55.209: INFO: namespace projected-125 deletion completed in 6.134719889s

â€¢ [SLOW TEST:8.276 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:39:55.210: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul  2 20:39:55.261: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:40:06.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1785" for this suite.
Jul  2 20:40:12.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:40:13.099: INFO: namespace pods-1785 deletion completed in 6.213370277s

â€¢ [SLOW TEST:17.890 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:40:13.102: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-976386f0-9d09-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 20:40:13.189: INFO: Waiting up to 5m0s for pod "pod-secrets-976510d8-9d09-11e9-808f-7ea354b18644" in namespace "secrets-7742" to be "success or failure"
Jul  2 20:40:13.199: INFO: Pod "pod-secrets-976510d8-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.992296ms
Jul  2 20:40:15.206: INFO: Pod "pod-secrets-976510d8-9d09-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017184062s
Jul  2 20:40:17.213: INFO: Pod "pod-secrets-976510d8-9d09-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024089632s
STEP: Saw pod success
Jul  2 20:40:17.213: INFO: Pod "pod-secrets-976510d8-9d09-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:40:17.218: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-secrets-976510d8-9d09-11e9-808f-7ea354b18644 container secret-volume-test: <nil>
STEP: delete the pod
Jul  2 20:40:18.033: INFO: Waiting for pod pod-secrets-976510d8-9d09-11e9-808f-7ea354b18644 to disappear
Jul  2 20:40:18.038: INFO: Pod pod-secrets-976510d8-9d09-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:40:18.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7742" for this suite.
Jul  2 20:40:24.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:40:24.748: INFO: namespace secrets-7742 deletion completed in 6.311756266s

â€¢ [SLOW TEST:11.646 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:40:24.749: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul  2 20:40:36.896: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-9e5b2ea3-9d09-11e9-808f-7ea354b18644,GenerateName:,Namespace:events-4933,SelfLink:/api/v1/namespaces/events-4933/pods/send-events-9e5b2ea3-9d09-11e9-808f-7ea354b18644,UID:b245f29f-9d09-11e9-ab09-0050569fbfc5,ResourceVersion:646922,Generation:0,CreationTimestamp:2019-07-02 20:40:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 855320950,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cq8tk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cq8tk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-cq8tk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002067c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002067cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:39:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:39:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:39:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:40:45 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.155,PodIP:10.44.0.5,StartTime:2019-07-02 20:39:35 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-02 20:39:44 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://3d8ac78e979ab8747fee19993cfdf37f9329cf874dbe9cbeb6016c154e1e4087}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul  2 20:40:38.905: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul  2 20:40:40.912: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:40:40.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4933" for this suite.
Jul  2 20:41:22.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:41:23.148: INFO: namespace events-4933 deletion completed in 42.205698159s

â€¢ [SLOW TEST:58.399 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:41:23.150: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4134
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jul  2 20:41:23.262: INFO: Found 0 stateful pods, waiting for 3
Jul  2 20:41:33.267: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 20:41:33.267: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 20:41:33.267: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul  2 20:41:43.269: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 20:41:43.269: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 20:41:43.269: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul  2 20:41:43.302: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul  2 20:41:53.369: INFO: Updating stateful set ss2
Jul  2 20:41:53.401: INFO: Waiting for Pod statefulset-4134/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jul  2 20:42:06.875: INFO: Found 2 stateful pods, waiting for 3
Jul  2 20:42:16.884: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 20:42:16.884: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 20:42:16.884: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul  2 20:42:16.919: INFO: Updating stateful set ss2
Jul  2 20:42:16.947: INFO: Waiting for Pod statefulset-4134/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul  2 20:42:28.459: INFO: Updating stateful set ss2
Jul  2 20:42:28.486: INFO: Waiting for StatefulSet statefulset-4134/ss2 to complete update
Jul  2 20:42:28.486: INFO: Waiting for Pod statefulset-4134/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  2 20:42:42.552: INFO: Deleting all statefulset in ns statefulset-4134
Jul  2 20:42:42.569: INFO: Scaling statefulset ss2 to 0
Jul  2 20:44:26.357: INFO: Waiting for statefulset status.replicas updated to 0
Jul  2 20:44:26.378: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:44:26.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4134" for this suite.
Jul  2 20:44:32.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:44:32.722: INFO: namespace statefulset-4134 deletion completed in 6.247940329s

â€¢ [SLOW TEST:138.184 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:44:32.724: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:44:32.804: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul  2 20:44:32.881: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  2 20:44:37.888: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  2 20:44:37.888: INFO: Creating deployment "test-rolling-update-deployment"
Jul  2 20:44:37.899: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul  2 20:44:37.914: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul  2 20:44:39.943: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul  2 20:44:39.957: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697697057, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697697057, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697697057, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697697057, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 20:44:42.125: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  2 20:44:42.148: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4022,SelfLink:/apis/apps/v1/namespaces/deployment-4022/deployments/test-rolling-update-deployment,UID:2b937308-9d0a-11e9-ab09-0050569fbfc5,ResourceVersion:647617,Generation:1,CreationTimestamp:2019-07-02 20:44:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-02 20:44:17 +0000 UTC 2019-07-02 20:44:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-02 20:44:19 +0000 UTC 2019-07-02 20:44:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  2 20:44:42.155: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-4022,SelfLink:/apis/apps/v1/namespaces/deployment-4022/replicasets/test-rolling-update-deployment-67599b4d9,UID:28bdbb37-9d0a-11e9-9994-0050569f2898,ResourceVersion:647607,Generation:1,CreationTimestamp:2019-07-02 20:44:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2b937308-9d0a-11e9-ab09-0050569fbfc5 0xc002b6f430 0xc002b6f431}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  2 20:44:42.155: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul  2 20:44:42.157: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4022,SelfLink:/apis/apps/v1/namespaces/deployment-4022/replicasets/test-rolling-update-controller,UID:285baf99-9d0a-11e9-ab09-0050569fbfc5,ResourceVersion:647616,Generation:2,CreationTimestamp:2019-07-02 20:44:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2b937308-9d0a-11e9-ab09-0050569fbfc5 0xc002b6f197 0xc002b6f198}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  2 20:44:42.164: INFO: Pod "test-rolling-update-deployment-67599b4d9-ndcn4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-ndcn4,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-4022,SelfLink:/api/v1/namespaces/deployment-4022/pods/test-rolling-update-deployment-67599b4d9-ndcn4,UID:28bfe8c4-9d0a-11e9-9994-0050569f2898,ResourceVersion:647606,Generation:0,CreationTimestamp:2019-07-02 20:44:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 28bdbb37-9d0a-11e9-9994-0050569f2898 0xc002b6fe50 0xc002b6fe51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bpdnr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bpdnr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bpdnr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ah-kres-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b6fec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b6fee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:44:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:44:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:44:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-02 20:44:17 +0000 UTC  }],Message:,Reason:,HostIP:10.30.20.90,PodIP:10.40.0.5,StartTime:2019-07-02 20:44:17 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-02 20:44:19 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://b766f3252b76e018c10edb6c3bf5a96c28d27c49fd2b3f6ca36389a828fdc171}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:44:42.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4022" for this suite.
Jul  2 20:44:56.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:44:56.441: INFO: namespace deployment-4022 deletion completed in 14.267594764s

â€¢ [SLOW TEST:23.716 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:44:56.442: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-4046120c-9d0a-11e9-808f-7ea354b18644
STEP: Creating secret with name secret-projected-all-test-volume-404611cb-9d0a-11e9-808f-7ea354b18644
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul  2 20:44:56.540: INFO: Waiting up to 5m0s for pod "projected-volume-40461147-9d0a-11e9-808f-7ea354b18644" in namespace "projected-6904" to be "success or failure"
Jul  2 20:44:56.545: INFO: Pod "projected-volume-40461147-9d0a-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.997465ms
Jul  2 20:44:58.553: INFO: Pod "projected-volume-40461147-9d0a-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012640632s
STEP: Saw pod success
Jul  2 20:44:58.553: INFO: Pod "projected-volume-40461147-9d0a-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:44:58.559: INFO: Trying to get logs from node ah-kres-worker-1 pod projected-volume-40461147-9d0a-11e9-808f-7ea354b18644 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul  2 20:44:58.602: INFO: Waiting for pod projected-volume-40461147-9d0a-11e9-808f-7ea354b18644 to disappear
Jul  2 20:44:58.611: INFO: Pod projected-volume-40461147-9d0a-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:44:58.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6904" for this suite.
Jul  2 20:45:04.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:45:04.887: INFO: namespace projected-6904 deletion completed in 6.267269288s

â€¢ [SLOW TEST:8.445 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:45:04.888: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jul  2 20:45:04.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-4839'
Jul  2 20:45:05.488: INFO: stderr: ""
Jul  2 20:45:05.488: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  2 20:45:06.496: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:45:06.496: INFO: Found 0 / 1
Jul  2 20:45:07.499: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:45:07.499: INFO: Found 0 / 1
Jul  2 20:45:08.496: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:45:08.496: INFO: Found 1 / 1
Jul  2 20:45:08.496: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul  2 20:45:08.502: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:45:08.502: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  2 20:45:08.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 patch pod redis-master-2x495 --namespace=kubectl-4839 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul  2 20:45:08.677: INFO: stderr: ""
Jul  2 20:45:08.677: INFO: stdout: "pod/redis-master-2x495 patched\n"
STEP: checking annotations
Jul  2 20:45:08.682: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:45:08.682: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:45:08.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4839" for this suite.
Jul  2 20:45:28.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:45:29.208: INFO: namespace kubectl-4839 deletion completed in 20.519673411s

â€¢ [SLOW TEST:24.320 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:45:29.209: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:45:29.274: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:45:33.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3899" for this suite.
Jul  2 20:46:13.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:46:14.039: INFO: namespace pods-3899 deletion completed in 40.375610144s

â€¢ [SLOW TEST:44.830 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:46:14.040: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Jul  2 20:46:14.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 api-versions'
Jul  2 20:46:14.303: INFO: stderr: ""
Jul  2 20:46:14.303: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\nopenebs.io/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvolumesnapshot.external-storage.k8s.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:46:14.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9803" for this suite.
Jul  2 20:46:22.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:46:22.590: INFO: namespace kubectl-9803 deletion completed in 8.276839976s

â€¢ [SLOW TEST:8.550 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:46:22.591: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 20:46:22.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644" in namespace "downward-api-8212" to be "success or failure"
Jul  2 20:46:22.678: INFO: Pod "downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.931103ms
Jul  2 20:46:25.206: INFO: Pod "downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534057772s
Jul  2 20:46:27.213: INFO: Pod "downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.540969785s
Jul  2 20:46:29.218: INFO: Pod "downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54604983s
Jul  2 20:46:31.225: INFO: Pod "downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.553272332s
STEP: Saw pod success
Jul  2 20:46:31.226: INFO: Pod "downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:46:31.235: INFO: Trying to get logs from node ah-kres-worker-1 pod downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 20:46:31.290: INFO: Waiting for pod downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644 to disappear
Jul  2 20:46:31.306: INFO: Pod downwardapi-volume-739e7c87-9d0a-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:46:31.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8212" for this suite.
Jul  2 20:46:47.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:46:47.476: INFO: namespace downward-api-8212 deletion completed in 16.1602716s

â€¢ [SLOW TEST:24.885 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:46:47.478: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:46:47.549: INFO: (0) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.689547ms)
Jul  2 20:46:47.554: INFO: (1) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.024051ms)
Jul  2 20:46:47.558: INFO: (2) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.352765ms)
Jul  2 20:46:47.564: INFO: (3) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.290862ms)
Jul  2 20:46:47.569: INFO: (4) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.528987ms)
Jul  2 20:46:47.576: INFO: (5) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.088731ms)
Jul  2 20:46:47.584: INFO: (6) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.850974ms)
Jul  2 20:46:47.589: INFO: (7) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.298165ms)
Jul  2 20:46:47.594: INFO: (8) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.684279ms)
Jul  2 20:46:47.599: INFO: (9) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.565192ms)
Jul  2 20:46:47.603: INFO: (10) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.431477ms)
Jul  2 20:46:47.608: INFO: (11) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.949754ms)
Jul  2 20:46:47.614: INFO: (12) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.71996ms)
Jul  2 20:46:47.620: INFO: (13) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.685106ms)
Jul  2 20:46:47.624: INFO: (14) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.256384ms)
Jul  2 20:46:47.629: INFO: (15) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.421587ms)
Jul  2 20:46:47.633: INFO: (16) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.405014ms)
Jul  2 20:46:47.637: INFO: (17) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.174914ms)
Jul  2 20:46:47.642: INFO: (18) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.350824ms)
Jul  2 20:46:47.646: INFO: (19) /api/v1/nodes/ah-kres-worker-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.296326ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:46:47.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-328" for this suite.
Jul  2 20:46:53.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:46:54.029: INFO: namespace proxy-328 deletion completed in 6.376748489s

â€¢ [SLOW TEST:6.551 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:46:54.029: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 20:46:54.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-865cabed-9d0a-11e9-808f-7ea354b18644" in namespace "downward-api-752" to be "success or failure"
Jul  2 20:46:54.139: INFO: Pod "downwardapi-volume-865cabed-9d0a-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 14.572557ms
Jul  2 20:46:56.146: INFO: Pod "downwardapi-volume-865cabed-9d0a-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021436371s
Jul  2 20:47:06.250: INFO: Pod "downwardapi-volume-865cabed-9d0a-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.126248252s
STEP: Saw pod success
Jul  2 20:47:06.251: INFO: Pod "downwardapi-volume-865cabed-9d0a-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:47:06.579: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-865cabed-9d0a-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 20:47:07.525: INFO: Waiting for pod downwardapi-volume-865cabed-9d0a-11e9-808f-7ea354b18644 to disappear
Jul  2 20:47:07.534: INFO: Pod downwardapi-volume-865cabed-9d0a-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:47:07.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-752" for this suite.
Jul  2 20:47:13.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:47:13.794: INFO: namespace downward-api-752 deletion completed in 6.249009033s

â€¢ [SLOW TEST:19.764 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:47:13.796: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul  2 20:47:13.890: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3492,SelfLink:/api/v1/namespaces/watch-3492/configmaps/e2e-watch-test-watch-closed,UID:90d6bc8b-9d0a-11e9-ab09-0050569fbfc5,ResourceVersion:648080,Generation:0,CreationTimestamp:2019-07-02 20:47:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  2 20:47:13.890: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3492,SelfLink:/api/v1/namespaces/watch-3492/configmaps/e2e-watch-test-watch-closed,UID:90d6bc8b-9d0a-11e9-ab09-0050569fbfc5,ResourceVersion:648081,Generation:0,CreationTimestamp:2019-07-02 20:47:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul  2 20:47:13.919: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3492,SelfLink:/api/v1/namespaces/watch-3492/configmaps/e2e-watch-test-watch-closed,UID:90d6bc8b-9d0a-11e9-ab09-0050569fbfc5,ResourceVersion:648082,Generation:0,CreationTimestamp:2019-07-02 20:47:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  2 20:47:13.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3492,SelfLink:/api/v1/namespaces/watch-3492/configmaps/e2e-watch-test-watch-closed,UID:90d6bc8b-9d0a-11e9-ab09-0050569fbfc5,ResourceVersion:648083,Generation:0,CreationTimestamp:2019-07-02 20:47:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:47:13.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3492" for this suite.
Jul  2 20:47:21.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:47:22.170: INFO: namespace watch-3492 deletion completed in 8.242233209s

â€¢ [SLOW TEST:8.373 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:47:22.170: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-9849618c-9d0a-11e9-808f-7ea354b18644
STEP: Creating secret with name s-test-opt-upd-984963b4-9d0a-11e9-808f-7ea354b18644
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9849618c-9d0a-11e9-808f-7ea354b18644
STEP: Updating secret s-test-opt-upd-984963b4-9d0a-11e9-808f-7ea354b18644
STEP: Creating secret with name s-test-opt-create-9849641e-9d0a-11e9-808f-7ea354b18644
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:49:04.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7888" for this suite.
Jul  2 20:49:36.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:49:37.004: INFO: namespace secrets-7888 deletion completed in 32.94259942s

â€¢ [SLOW TEST:134.834 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:49:37.004: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 20:49:38.929: INFO: Creating ReplicaSet my-hostname-basic-e89c22ac-9d0a-11e9-808f-7ea354b18644
Jul  2 20:49:39.031: INFO: Pod name my-hostname-basic-e89c22ac-9d0a-11e9-808f-7ea354b18644: Found 1 pods out of 1
Jul  2 20:49:39.031: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e89c22ac-9d0a-11e9-808f-7ea354b18644" is running
Jul  2 20:49:45.748: INFO: Pod "my-hostname-basic-e89c22ac-9d0a-11e9-808f-7ea354b18644-8q48v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-02 20:49:53 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-02 20:49:53 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-e89c22ac-9d0a-11e9-808f-7ea354b18644]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-02 20:49:53 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-e89c22ac-9d0a-11e9-808f-7ea354b18644]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-02 20:49:59 +0000 UTC Reason: Message:}])
Jul  2 20:49:45.748: INFO: Trying to dial the pod
Jul  2 20:49:50.780: INFO: Controller my-hostname-basic-e89c22ac-9d0a-11e9-808f-7ea354b18644: Got expected result from replica 1 [my-hostname-basic-e89c22ac-9d0a-11e9-808f-7ea354b18644-8q48v]: "my-hostname-basic-e89c22ac-9d0a-11e9-808f-7ea354b18644-8q48v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:49:50.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8445" for this suite.
Jul  2 20:49:56.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:49:57.029: INFO: namespace replicaset-8445 deletion completed in 6.240961952s

â€¢ [SLOW TEST:20.025 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:49:57.030: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-f370012e-9d0a-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 20:49:57.130: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f372235d-9d0a-11e9-808f-7ea354b18644" in namespace "projected-183" to be "success or failure"
Jul  2 20:49:57.148: INFO: Pod "pod-projected-configmaps-f372235d-9d0a-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 18.906505ms
Jul  2 20:50:00.049: INFO: Pod "pod-projected-configmaps-f372235d-9d0a-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.91938781s
STEP: Saw pod success
Jul  2 20:50:00.049: INFO: Pod "pod-projected-configmaps-f372235d-9d0a-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:50:00.100: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-projected-configmaps-f372235d-9d0a-11e9-808f-7ea354b18644 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 20:50:00.166: INFO: Waiting for pod pod-projected-configmaps-f372235d-9d0a-11e9-808f-7ea354b18644 to disappear
Jul  2 20:50:00.185: INFO: Pod pod-projected-configmaps-f372235d-9d0a-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:50:00.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-183" for this suite.
Jul  2 20:50:14.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:50:14.458: INFO: namespace projected-183 deletion completed in 14.251939443s

â€¢ [SLOW TEST:17.429 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:50:14.459: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:50:18.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5533" for this suite.
Jul  2 20:50:24.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:50:24.842: INFO: namespace kubelet-test-5533 deletion completed in 6.284209128s

â€¢ [SLOW TEST:10.383 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:50:24.843: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  2 20:50:24.914: INFO: Waiting up to 5m0s for pod "pod-0401dd97-9d0b-11e9-808f-7ea354b18644" in namespace "emptydir-817" to be "success or failure"
Jul  2 20:50:24.923: INFO: Pod "pod-0401dd97-9d0b-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.538667ms
Jul  2 20:50:26.933: INFO: Pod "pod-0401dd97-9d0b-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019608212s
STEP: Saw pod success
Jul  2 20:50:26.933: INFO: Pod "pod-0401dd97-9d0b-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:50:26.939: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-0401dd97-9d0b-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 20:50:26.991: INFO: Waiting for pod pod-0401dd97-9d0b-11e9-808f-7ea354b18644 to disappear
Jul  2 20:50:26.998: INFO: Pod pod-0401dd97-9d0b-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:50:26.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-817" for this suite.
Jul  2 20:51:17.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:51:17.952: INFO: namespace emptydir-817 deletion completed in 6.256538899s

â€¢ [SLOW TEST:8.420 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:51:17.954: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-23ac4a7d-9d0b-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 20:51:18.052: INFO: Waiting up to 5m0s for pod "pod-configmaps-23ae9eb3-9d0b-11e9-808f-7ea354b18644" in namespace "configmap-4801" to be "success or failure"
Jul  2 20:51:18.066: INFO: Pod "pod-configmaps-23ae9eb3-9d0b-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 14.094241ms
Jul  2 20:51:20.362: INFO: Pod "pod-configmaps-23ae9eb3-9d0b-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.31051444s
Jul  2 20:51:22.369: INFO: Pod "pod-configmaps-23ae9eb3-9d0b-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.317428024s
STEP: Saw pod success
Jul  2 20:51:22.369: INFO: Pod "pod-configmaps-23ae9eb3-9d0b-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:51:22.375: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-configmaps-23ae9eb3-9d0b-11e9-808f-7ea354b18644 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 20:51:22.448: INFO: Waiting for pod pod-configmaps-23ae9eb3-9d0b-11e9-808f-7ea354b18644 to disappear
Jul  2 20:51:22.460: INFO: Pod pod-configmaps-23ae9eb3-9d0b-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:51:22.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4801" for this suite.
Jul  2 20:51:30.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:51:30.644: INFO: namespace configmap-4801 deletion completed in 8.175479604s

â€¢ [SLOW TEST:12.690 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:51:30.645: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Jul  2 20:51:30.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-6235'
Jul  2 20:51:31.394: INFO: stderr: ""
Jul  2 20:51:31.394: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Jul  2 20:51:32.400: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:51:32.400: INFO: Found 0 / 1
Jul  2 20:51:33.401: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:51:33.401: INFO: Found 0 / 1
Jul  2 20:51:34.550: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:51:34.550: INFO: Found 0 / 1
Jul  2 20:51:35.401: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:51:35.401: INFO: Found 0 / 1
Jul  2 20:51:36.399: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:51:36.399: INFO: Found 1 / 1
Jul  2 20:51:36.399: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  2 20:51:36.402: INFO: Selector matched 1 pods for map[app:redis]
Jul  2 20:51:36.402: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul  2 20:51:36.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 logs redis-master-29zhg redis-master --namespace=kubectl-6235'
Jul  2 20:51:36.626: INFO: stderr: ""
Jul  2 20:51:36.626: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 02 Jul 20:51:32.192 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 Jul 20:51:32.192 # Server started, Redis version 3.2.12\n1:M 02 Jul 20:51:32.192 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 Jul 20:51:32.192 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul  2 20:51:36.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 log redis-master-29zhg redis-master --namespace=kubectl-6235 --tail=1'
Jul  2 20:51:36.818: INFO: stderr: ""
Jul  2 20:51:36.818: INFO: stdout: "1:M 02 Jul 20:51:32.192 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul  2 20:51:36.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 log redis-master-29zhg redis-master --namespace=kubectl-6235 --limit-bytes=1'
Jul  2 20:51:37.040: INFO: stderr: ""
Jul  2 20:51:37.040: INFO: stdout: " "
STEP: exposing timestamps
Jul  2 20:51:37.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 log redis-master-29zhg redis-master --namespace=kubectl-6235 --tail=1 --timestamps'
Jul  2 20:51:37.233: INFO: stderr: ""
Jul  2 20:51:37.234: INFO: stdout: "2019-07-02T20:51:32.193674262Z 1:M 02 Jul 20:51:32.192 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul  2 20:51:39.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 log redis-master-29zhg redis-master --namespace=kubectl-6235 --since=1s'
Jul  2 20:51:41.782: INFO: stderr: ""
Jul  2 20:51:41.782: INFO: stdout: ""
Jul  2 20:51:41.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 log redis-master-29zhg redis-master --namespace=kubectl-6235 --since=24h'
Jul  2 20:51:42.063: INFO: stderr: ""
Jul  2 20:51:42.063: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 02 Jul 20:51:32.192 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 02 Jul 20:51:32.192 # Server started, Redis version 3.2.12\n1:M 02 Jul 20:51:32.192 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 02 Jul 20:51:32.192 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Jul  2 20:51:42.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-6235'
Jul  2 20:51:42.246: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 20:51:42.246: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul  2 20:51:42.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6235'
Jul  2 20:51:42.453: INFO: stderr: "No resources found.\n"
Jul  2 20:51:42.453: INFO: stdout: ""
Jul  2 20:51:42.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -l name=nginx --namespace=kubectl-6235 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  2 20:51:42.636: INFO: stderr: ""
Jul  2 20:51:42.636: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:51:42.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6235" for this suite.
Jul  2 20:51:49.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:51:49.406: INFO: namespace kubectl-6235 deletion completed in 6.758464397s

â€¢ [SLOW TEST:18.761 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:51:49.406: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  2 20:51:49.505: INFO: Waiting up to 5m0s for pod "pod-366d8160-9d0b-11e9-808f-7ea354b18644" in namespace "emptydir-3795" to be "success or failure"
Jul  2 20:51:49.512: INFO: Pod "pod-366d8160-9d0b-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.346586ms
Jul  2 20:51:51.517: INFO: Pod "pod-366d8160-9d0b-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012414809s
Jul  2 20:51:53.528: INFO: Pod "pod-366d8160-9d0b-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022708062s
STEP: Saw pod success
Jul  2 20:51:53.528: INFO: Pod "pod-366d8160-9d0b-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:51:53.533: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-366d8160-9d0b-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 20:51:53.578: INFO: Waiting for pod pod-366d8160-9d0b-11e9-808f-7ea354b18644 to disappear
Jul  2 20:51:53.583: INFO: Pod pod-366d8160-9d0b-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:51:53.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3795" for this suite.
Jul  2 20:51:59.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:52:00.104: INFO: namespace emptydir-3795 deletion completed in 6.514960445s

â€¢ [SLOW TEST:10.698 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:52:00.107: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:52:04.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8497" for this suite.
Jul  2 20:52:28.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:52:28.969: INFO: namespace replication-controller-8497 deletion completed in 24.276859677s

â€¢ [SLOW TEST:28.862 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:52:28.970: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4e015f09-9d0b-11e9-808f-7ea354b18644
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-4e015f09-9d0b-11e9-808f-7ea354b18644
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:52:35.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2398" for this suite.
Jul  2 20:52:59.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:52:59.849: INFO: namespace projected-2398 deletion completed in 24.668146101s

â€¢ [SLOW TEST:30.885 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:52:59.855: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-62b44b88-9d0b-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 20:53:03.878: INFO: Waiting up to 5m0s for pod "pod-secrets-62bbb844-9d0b-11e9-808f-7ea354b18644" in namespace "secrets-3932" to be "success or failure"
Jul  2 20:53:03.889: INFO: Pod "pod-secrets-62bbb844-9d0b-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 11.813705ms
Jul  2 20:53:05.898: INFO: Pod "pod-secrets-62bbb844-9d0b-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020382024s
Jul  2 20:53:07.905: INFO: Pod "pod-secrets-62bbb844-9d0b-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027442729s
STEP: Saw pod success
Jul  2 20:53:07.905: INFO: Pod "pod-secrets-62bbb844-9d0b-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 20:53:07.910: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-secrets-62bbb844-9d0b-11e9-808f-7ea354b18644 container secret-volume-test: <nil>
STEP: delete the pod
Jul  2 20:53:08.949: INFO: Waiting for pod pod-secrets-62bbb844-9d0b-11e9-808f-7ea354b18644 to disappear
Jul  2 20:53:08.962: INFO: Pod pod-secrets-62bbb844-9d0b-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 20:53:08.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3932" for this suite.
Jul  2 20:53:15.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 20:53:16.131: INFO: namespace secrets-3932 deletion completed in 7.161521584s

â€¢ [SLOW TEST:16.276 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 20:53:16.133: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8293
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8293
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8293
Jul  2 20:53:16.242: INFO: Found 0 stateful pods, waiting for 1
Jul  2 20:53:26.417: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul  2 20:53:26.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 20:53:26.954: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 20:53:26.954: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 20:53:26.954: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  2 20:53:26.962: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  2 20:53:37.170: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  2 20:53:37.170: INFO: Waiting for statefulset status.replicas updated to 0
Jul  2 20:53:37.199: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999433s
Jul  2 20:53:38.205: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993938264s
Jul  2 20:53:39.215: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988158103s
Jul  2 20:53:40.220: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977712771s
Jul  2 20:53:41.227: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.972623837s
Jul  2 20:53:42.235: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.965513942s
Jul  2 20:53:43.240: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.957820739s
Jul  2 20:53:44.247: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.952487714s
Jul  2 20:53:51.899: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.945310257s
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8293
Jul  2 20:53:52.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:53:53.423: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  2 20:53:53.423: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  2 20:53:53.423: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  2 20:53:53.440: INFO: Found 1 stateful pods, waiting for 3
Jul  2 20:54:03.448: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 20:54:03.448: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  2 20:54:03.448: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul  2 20:54:03.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 20:54:03.953: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 20:54:03.953: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 20:54:03.954: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  2 20:54:03.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 20:54:04.837: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 20:54:04.837: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 20:54:04.837: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  2 20:54:04.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  2 20:54:05.337: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  2 20:54:05.337: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  2 20:54:05.337: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  2 20:54:05.337: INFO: Waiting for statefulset status.replicas updated to 0
Jul  2 20:54:05.344: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul  2 20:54:15.356: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  2 20:54:15.356: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  2 20:54:15.356: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  2 20:54:15.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999931s
Jul  2 20:54:16.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995145667s
Jul  2 20:54:17.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986737856s
Jul  2 20:54:18.468: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.953467792s
Jul  2 20:54:21.483: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.90580116s
Jul  2 20:54:22.523: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.891166486s
Jul  2 20:54:23.529: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.85043167s
Jul  2 20:54:24.536: INFO: Verifying statefulset ss doesn't scale past 3 for another 844.431368ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8293
Jul  2 20:54:25.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:54:26.050: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  2 20:54:26.050: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  2 20:54:26.050: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  2 20:54:26.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:54:26.593: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  2 20:54:26.593: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  2 20:54:26.593: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  2 20:54:26.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:54:36.602: INFO: rc: 126
Jul  2 20:54:36.603: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "process_linux.go:91: executing setns process caused \"exit status 21\"": unknown
 command terminated with exit code 126
 [] <nil> 0xc00285c330 exit status 126 <nil> <nil> true [0xc00280b6a8 0xc00280b6e8 0xc00280b700] [0xc00280b6a8 0xc00280b6e8 0xc00280b700] [0xc00280b6d0 0xc00280b6f8] [0x9bf9f0 0x9bf9f0] 0xc00255e9c0 <nil>}:
Command stdout:
OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "process_linux.go:91: executing setns process caused \"exit status 21\"": unknown

stderr:
command terminated with exit code 126

error:
exit status 126

Jul  2 20:54:46.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:54:46.762: INFO: rc: 1
Jul  2 20:54:46.762: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194300 exit status 1 <nil> <nil> true [0xc0019ce000 0xc0019ce0f8 0xc0019ce160] [0xc0019ce000 0xc0019ce0f8 0xc0019ce160] [0xc0019ce0a0 0xc0019ce150] [0x9bf9f0 0x9bf9f0] 0xc001352420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:54:56.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:54:56.922: INFO: rc: 1
Jul  2 20:54:56.922: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194630 exit status 1 <nil> <nil> true [0xc0019ce168 0xc0019ce208 0xc0019ce290] [0xc0019ce168 0xc0019ce208 0xc0019ce290] [0xc0019ce1f0 0xc0019ce258] [0x9bf9f0 0x9bf9f0] 0xc001352960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:55:06.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:55:07.076: INFO: rc: 1
Jul  2 20:55:07.076: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194990 exit status 1 <nil> <nil> true [0xc0019ce2a0 0xc0019ce2f8 0xc0019ce360] [0xc0019ce2a0 0xc0019ce2f8 0xc0019ce360] [0xc0019ce2d0 0xc0019ce348] [0x9bf9f0 0x9bf9f0] 0xc001352ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:55:17.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:55:22.523: INFO: rc: 1
Jul  2 20:55:22.523: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194e40 exit status 1 <nil> <nil> true [0xc0019ce380 0xc0019ce3b0 0xc0019ce3f0] [0xc0019ce380 0xc0019ce3b0 0xc0019ce3f0] [0xc0019ce3a8 0xc0019ce3c8] [0x9bf9f0 0x9bf9f0] 0xc001353620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:55:32.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:55:32.680: INFO: rc: 1
Jul  2 20:55:32.681: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002195170 exit status 1 <nil> <nil> true [0xc0019ce410 0xc0019ce4a8 0xc0019ce4e8] [0xc0019ce410 0xc0019ce4a8 0xc0019ce4e8] [0xc0019ce4a0 0xc0019ce4c8] [0x9bf9f0 0x9bf9f0] 0xc001353c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:55:42.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:55:42.825: INFO: rc: 1
Jul  2 20:55:42.825: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f16480 exit status 1 <nil> <nil> true [0xc002906090 0xc002906118 0xc002906160] [0xc002906090 0xc002906118 0xc002906160] [0xc0029060e8 0xc002906138] [0x9bf9f0 0x9bf9f0] 0xc001fa25a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:55:52.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:55:52.974: INFO: rc: 1
Jul  2 20:55:52.974: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021954a0 exit status 1 <nil> <nil> true [0xc0019ce510 0xc0019ce568 0xc0019ce5d0] [0xc0019ce510 0xc0019ce568 0xc0019ce5d0] [0xc0019ce540 0xc0019ce5c0] [0x9bf9f0 0x9bf9f0] 0xc00274c060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:56:02.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:56:03.132: INFO: rc: 1
Jul  2 20:56:03.133: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002195800 exit status 1 <nil> <nil> true [0xc0019ce610 0xc0019ce6e8 0xc0019ce7a8] [0xc0019ce610 0xc0019ce6e8 0xc0019ce7a8] [0xc0019ce6e0 0xc0019ce738] [0x9bf9f0 0x9bf9f0] 0xc00274c420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:56:13.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:56:13.294: INFO: rc: 1
Jul  2 20:56:13.294: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f16900 exit status 1 <nil> <nil> true [0xc002906178 0xc0029061e0 0xc002906208] [0xc002906178 0xc0029061e0 0xc002906208] [0xc0029061c8 0xc002906200] [0x9bf9f0 0x9bf9f0] 0xc001fa3a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:56:23.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:56:23.451: INFO: rc: 1
Jul  2 20:56:23.451: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002195c20 exit status 1 <nil> <nil> true [0xc0019ce7b8 0xc0019ce878 0xc0019ce930] [0xc0019ce7b8 0xc0019ce878 0xc0019ce930] [0xc0019ce808 0xc0019ce8e0] [0x9bf9f0 0x9bf9f0] 0xc00274cc00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:56:33.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:56:35.904: INFO: rc: 1
Jul  2 20:56:35.904: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f16c90 exit status 1 <nil> <nil> true [0xc002906210 0xc002906260 0xc0029062b8] [0xc002906210 0xc002906260 0xc0029062b8] [0xc002906248 0xc002906298] [0x9bf9f0 0x9bf9f0] 0xc0026981e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:56:45.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:56:46.213: INFO: rc: 1
Jul  2 20:56:46.214: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f16360 exit status 1 <nil> <nil> true [0xc002906090 0xc002906118 0xc002906160] [0xc002906090 0xc002906118 0xc002906160] [0xc0029060e8 0xc002906138] [0x9bf9f0 0x9bf9f0] 0xc001fa25a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:56:56.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:56:56.398: INFO: rc: 1
Jul  2 20:56:56.398: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194360 exit status 1 <nil> <nil> true [0xc0019ce000 0xc0019ce0f8 0xc0019ce160] [0xc0019ce000 0xc0019ce0f8 0xc0019ce160] [0xc0019ce0a0 0xc0019ce150] [0x9bf9f0 0x9bf9f0] 0xc001352420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:57:06.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:57:07.424: INFO: rc: 1
Jul  2 20:57:07.424: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f166c0 exit status 1 <nil> <nil> true [0xc002906178 0xc0029061e0 0xc002906208] [0xc002906178 0xc0029061e0 0xc002906208] [0xc0029061c8 0xc002906200] [0x9bf9f0 0x9bf9f0] 0xc001fa3a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:57:17.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:57:17.576: INFO: rc: 1
Jul  2 20:57:17.576: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f16b70 exit status 1 <nil> <nil> true [0xc002906210 0xc002906260 0xc0029062b8] [0xc002906210 0xc002906260 0xc0029062b8] [0xc002906248 0xc002906298] [0x9bf9f0 0x9bf9f0] 0xc0026981e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:58:08.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:58:09.282: INFO: rc: 1
Jul  2 20:58:09.283: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f16f00 exit status 1 <nil> <nil> true [0xc0029062d8 0xc002906300 0xc002906348] [0xc0029062d8 0xc002906300 0xc002906348] [0xc0029062f8 0xc002906328] [0x9bf9f0 0x9bf9f0] 0xc0026986c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:58:19.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:58:19.448: INFO: rc: 1
Jul  2 20:58:19.448: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f178c0 exit status 1 <nil> <nil> true [0xc002906360 0xc0029063b0 0xc0029063d8] [0xc002906360 0xc0029063b0 0xc0029063d8] [0xc002906390 0xc0029063d0] [0x9bf9f0 0x9bf9f0] 0xc002698c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:58:29.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:58:29.606: INFO: rc: 1
Jul  2 20:58:29.606: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194750 exit status 1 <nil> <nil> true [0xc0019ce168 0xc0019ce208 0xc0019ce290] [0xc0019ce168 0xc0019ce208 0xc0019ce290] [0xc0019ce1f0 0xc0019ce258] [0x9bf9f0 0x9bf9f0] 0xc001352960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:58:39.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:58:39.786: INFO: rc: 1
Jul  2 20:58:39.786: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194ae0 exit status 1 <nil> <nil> true [0xc0019ce2a0 0xc0019ce2f8 0xc0019ce360] [0xc0019ce2a0 0xc0019ce2f8 0xc0019ce360] [0xc0019ce2d0 0xc0019ce348] [0x9bf9f0 0x9bf9f0] 0xc001352ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:58:49.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:58:49.938: INFO: rc: 1
Jul  2 20:58:49.938: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194fc0 exit status 1 <nil> <nil> true [0xc0019ce380 0xc0019ce3b0 0xc0019ce3f0] [0xc0019ce380 0xc0019ce3b0 0xc0019ce3f0] [0xc0019ce3a8 0xc0019ce3c8] [0x9bf9f0 0x9bf9f0] 0xc001353620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:58:59.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:59:02.554: INFO: rc: 1
Jul  2 20:59:02.554: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002195350 exit status 1 <nil> <nil> true [0xc0019ce410 0xc0019ce4a8 0xc0019ce4e8] [0xc0019ce410 0xc0019ce4a8 0xc0019ce4e8] [0xc0019ce4a0 0xc0019ce4c8] [0x9bf9f0 0x9bf9f0] 0xc001353c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:59:12.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:59:14.135: INFO: rc: 1
Jul  2 20:59:14.135: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f17c20 exit status 1 <nil> <nil> true [0xc0029063e0 0xc002906408 0xc002906420] [0xc0029063e0 0xc002906408 0xc002906420] [0xc002906400 0xc002906418] [0x9bf9f0 0x9bf9f0] 0xc002699560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:59:24.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:59:27.303: INFO: rc: 1
Jul  2 20:59:27.303: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194300 exit status 1 <nil> <nil> true [0xc0019ce090 0xc0019ce140 0xc0019ce168] [0xc0019ce090 0xc0019ce140 0xc0019ce168] [0xc0019ce0f8 0xc0019ce160] [0x9bf9f0 0x9bf9f0] 0xc001fa25a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:59:37.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:59:37.462: INFO: rc: 1
Jul  2 20:59:37.462: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f163c0 exit status 1 <nil> <nil> true [0xc002906040 0xc0029060e8 0xc002906138] [0xc002906040 0xc0029060e8 0xc002906138] [0xc0029060d0 0xc002906130] [0x9bf9f0 0x9bf9f0] 0xc001352420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:59:47.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:59:48.128: INFO: rc: 1
Jul  2 20:59:48.129: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002194660 exit status 1 <nil> <nil> true [0xc0019ce1b0 0xc0019ce250 0xc0019ce2a0] [0xc0019ce1b0 0xc0019ce250 0xc0019ce2a0] [0xc0019ce208 0xc0019ce290] [0x9bf9f0 0x9bf9f0] 0xc001fa3a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 20:59:58.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 20:59:58.285: INFO: rc: 1
Jul  2 20:59:58.285: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f16780 exit status 1 <nil> <nil> true [0xc002906160 0xc0029061c8 0xc002906200] [0xc002906160 0xc0029061c8 0xc002906200] [0xc0029061a8 0xc0029061e8] [0x9bf9f0 0x9bf9f0] 0xc001352960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul  2 21:00:08.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 exec --namespace=statefulset-8293 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  2 21:00:08.437: INFO: rc: 1
Jul  2 21:00:08.437: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Jul  2 21:00:08.437: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  2 21:00:08.461: INFO: Deleting all statefulset in ns statefulset-8293
Jul  2 21:00:08.468: INFO: Scaling statefulset ss to 0
Jul  2 21:00:08.485: INFO: Waiting for statefulset status.replicas updated to 0
Jul  2 21:00:08.491: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:00:08.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8293" for this suite.
Jul  2 21:00:15.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:00:15.396: INFO: namespace statefulset-8293 deletion completed in 6.8368935s

â€¢ [SLOW TEST:378.708 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:00:15.401: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:01:24.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5803" for this suite.
Jul  2 21:01:46.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:01:46.616: INFO: namespace container-probe-5803 deletion completed in 22.363319112s

â€¢ [SLOW TEST:91.215 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:01:46.619: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 21:01:46.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a647071-9d0c-11e9-808f-7ea354b18644" in namespace "downward-api-5211" to be "success or failure"
Jul  2 21:01:46.723: INFO: Pod "downwardapi-volume-9a647071-9d0c-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.818782ms
Jul  2 21:01:48.730: INFO: Pod "downwardapi-volume-9a647071-9d0c-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013964763s
Jul  2 21:01:50.738: INFO: Pod "downwardapi-volume-9a647071-9d0c-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021496708s
STEP: Saw pod success
Jul  2 21:01:50.738: INFO: Pod "downwardapi-volume-9a647071-9d0c-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:01:50.744: INFO: Trying to get logs from node ah-kres-worker-3 pod downwardapi-volume-9a647071-9d0c-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 21:01:51.133: INFO: Waiting for pod downwardapi-volume-9a647071-9d0c-11e9-808f-7ea354b18644 to disappear
Jul  2 21:01:51.139: INFO: Pod downwardapi-volume-9a647071-9d0c-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:01:51.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5211" for this suite.
Jul  2 21:01:57.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:01:57.378: INFO: namespace downward-api-5211 deletion completed in 6.231314508s

â€¢ [SLOW TEST:10.760 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:01:57.380: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul  2 21:01:57.452: INFO: PodSpec: initContainers in spec.initContainers
Jul  2 21:02:47.885: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a0cdec97-9d0c-11e9-808f-7ea354b18644", GenerateName:"", Namespace:"init-container-7280", SelfLink:"/api/v1/namespaces/init-container-7280/pods/pod-init-a0cdec97-9d0c-11e9-808f-7ea354b18644", UID:"a702d96b-9d0c-11e9-ab09-0050569fbfc5", ResourceVersion:"650269", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63697698127, loc:(*time.Location)(0x8a060e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"452963888"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-624bn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001de3dc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-624bn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-624bn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-624bn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00308e838), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ah-kres-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0024f7f80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00308e9a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00308e9e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00308e9e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00308e9ec)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698127, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698127, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698127, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698116, loc:(*time.Location)(0x8a060e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.30.20.166", PodIP:"10.32.0.6", StartTime:(*v1.Time)(0xc001053880), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008e4070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008e4150)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://7a83eacc71715f95398d85ad7cfe251536153df7ec4f24cd0e0008aad28745f7"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0010538c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0010538a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:02:47.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7280" for this suite.
Jul  2 21:03:47.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:03:47.503: INFO: namespace init-container-7280 deletion completed in 26.193143636s

â€¢ [SLOW TEST:76.714 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:03:47.505: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-e2789c21-9d0c-11e9-808f-7ea354b18644
STEP: Creating secret with name s-test-opt-upd-e2789ca2-9d0c-11e9-808f-7ea354b18644
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e2789c21-9d0c-11e9-808f-7ea354b18644
STEP: Updating secret s-test-opt-upd-e2789ca2-9d0c-11e9-808f-7ea354b18644
STEP: Creating secret with name s-test-opt-create-e2789ce2-9d0c-11e9-808f-7ea354b18644
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:03:53.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5585" for this suite.
Jul  2 21:04:19.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:04:20.058: INFO: namespace projected-5585 deletion completed in 26.224586261s

â€¢ [SLOW TEST:32.554 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:04:20.059: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  2 21:04:20.146: INFO: Waiting up to 5m0s for pod "pod-f5d81548-9d0c-11e9-808f-7ea354b18644" in namespace "emptydir-5559" to be "success or failure"
Jul  2 21:04:20.165: INFO: Pod "pod-f5d81548-9d0c-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 18.700262ms
Jul  2 21:04:22.342: INFO: Pod "pod-f5d81548-9d0c-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.195834729s
Jul  2 21:04:24.467: INFO: Pod "pod-f5d81548-9d0c-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.321136668s
STEP: Saw pod success
Jul  2 21:04:24.467: INFO: Pod "pod-f5d81548-9d0c-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:04:28.542: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-f5d81548-9d0c-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 21:04:31.385: INFO: Waiting for pod pod-f5d81548-9d0c-11e9-808f-7ea354b18644 to disappear
Jul  2 21:04:31.406: INFO: Pod pod-f5d81548-9d0c-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:04:31.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5559" for this suite.
Jul  2 21:04:37.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:04:37.675: INFO: namespace emptydir-5559 deletion completed in 6.250333839s

â€¢ [SLOW TEST:17.617 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:04:37.678: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-00595b2e-9d0d-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 21:04:37.777: INFO: Waiting up to 5m0s for pod "pod-secrets-005b4153-9d0d-11e9-808f-7ea354b18644" in namespace "secrets-4151" to be "success or failure"
Jul  2 21:04:37.784: INFO: Pod "pod-secrets-005b4153-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.701149ms
Jul  2 21:04:40.373: INFO: Pod "pod-secrets-005b4153-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595884715s
Jul  2 21:04:42.467: INFO: Pod "pod-secrets-005b4153-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689537205s
Jul  2 21:04:57.514: INFO: Pod "pod-secrets-005b4153-9d0d-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 19.736283437s
STEP: Saw pod success
Jul  2 21:04:57.516: INFO: Pod "pod-secrets-005b4153-9d0d-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:04:57.581: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-secrets-005b4153-9d0d-11e9-808f-7ea354b18644 container secret-env-test: <nil>
STEP: delete the pod
Jul  2 21:04:57.677: INFO: Waiting for pod pod-secrets-005b4153-9d0d-11e9-808f-7ea354b18644 to disappear
Jul  2 21:04:57.696: INFO: Pod pod-secrets-005b4153-9d0d-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:04:57.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4151" for this suite.
Jul  2 21:05:29.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:05:29.987: INFO: namespace secrets-4151 deletion completed in 32.278682836s

â€¢ [SLOW TEST:52.309 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:05:29.991: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  2 21:05:40.221: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1f87c732-9d0d-11e9-808f-7ea354b18644"
Jul  2 21:05:40.221: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1f87c732-9d0d-11e9-808f-7ea354b18644" in namespace "pods-1008" to be "terminated due to deadline exceeded"
Jul  2 21:05:40.226: INFO: Pod "pod-update-activedeadlineseconds-1f87c732-9d0d-11e9-808f-7ea354b18644": Phase="Running", Reason="", readiness=true. Elapsed: 4.950525ms
Jul  2 21:05:42.234: INFO: Pod "pod-update-activedeadlineseconds-1f87c732-9d0d-11e9-808f-7ea354b18644": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.013045258s
Jul  2 21:05:42.234: INFO: Pod "pod-update-activedeadlineseconds-1f87c732-9d0d-11e9-808f-7ea354b18644" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:05:42.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1008" for this suite.
Jul  2 21:05:48.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:05:48.532: INFO: namespace pods-1008 deletion completed in 6.288547787s

â€¢ [SLOW TEST:18.541 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:05:48.532: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  2 21:05:48.623: INFO: Waiting up to 5m0s for pod "pod-2a95539c-9d0d-11e9-808f-7ea354b18644" in namespace "emptydir-7212" to be "success or failure"
Jul  2 21:05:48.631: INFO: Pod "pod-2a95539c-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 8.466593ms
Jul  2 21:05:50.638: INFO: Pod "pod-2a95539c-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015527592s
Jul  2 21:05:53.924: INFO: Pod "pod-2a95539c-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.301274452s
Jul  2 21:05:55.943: INFO: Pod "pod-2a95539c-9d0d-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.320469301s
STEP: Saw pod success
Jul  2 21:05:55.945: INFO: Pod "pod-2a95539c-9d0d-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:05:55.956: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-2a95539c-9d0d-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 21:05:55.993: INFO: Waiting for pod pod-2a95539c-9d0d-11e9-808f-7ea354b18644 to disappear
Jul  2 21:05:55.997: INFO: Pod pod-2a95539c-9d0d-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:05:55.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7212" for this suite.
Jul  2 21:06:06.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:06:06.234: INFO: namespace emptydir-7212 deletion completed in 10.229117727s

â€¢ [SLOW TEST:17.701 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:06:06.234: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-20
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-20 to expose endpoints map[]
Jul  2 21:06:07.839: INFO: successfully validated that service multi-endpoint-test in namespace services-20 exposes endpoints map[] (22.036546ms elapsed)
STEP: Creating pod pod1 in namespace services-20
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-20 to expose endpoints map[pod1:[100]]
Jul  2 21:06:09.915: INFO: successfully validated that service multi-endpoint-test in namespace services-20 exposes endpoints map[pod1:[100]] (2.049201194s elapsed)
STEP: Creating pod pod2 in namespace services-20
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-20 to expose endpoints map[pod1:[100] pod2:[101]]
Jul  2 21:06:12.997: INFO: successfully validated that service multi-endpoint-test in namespace services-20 exposes endpoints map[pod1:[100] pod2:[101]] (3.07206186s elapsed)
STEP: Deleting pod pod1 in namespace services-20
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-20 to expose endpoints map[pod2:[101]]
Jul  2 21:06:16.264: INFO: successfully validated that service multi-endpoint-test in namespace services-20 exposes endpoints map[pod2:[101]] (1.423251186s elapsed)
STEP: Deleting pod pod2 in namespace services-20
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-20 to expose endpoints map[]
Jul  2 21:06:17.319: INFO: successfully validated that service multi-endpoint-test in namespace services-20 exposes endpoints map[] (1.032300957s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:06:17.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-20" for this suite.
Jul  2 21:06:41.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:06:41.649: INFO: namespace services-20 deletion completed in 24.268188106s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:35.415 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:06:41.649: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul  2 21:06:49.803: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:06:49.810: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:06:51.810: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:06:51.818: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:06:53.811: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:06:56.029: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:06:57.810: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:06:57.818: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:06:59.811: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:06:59.818: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:07:01.811: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:07:01.818: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:07:03.811: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:07:03.818: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:07:05.810: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:07:05.817: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:07:07.810: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:07:07.819: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:07:09.810: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:07:09.817: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:07:11.810: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:07:11.817: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  2 21:07:13.812: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  2 21:07:13.819: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:07:13.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6030" for this suite.
Jul  2 21:07:37.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:07:38.157: INFO: namespace container-lifecycle-hook-6030 deletion completed in 24.307142775s

â€¢ [SLOW TEST:56.508 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:07:38.158: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 21:08:00.398: INFO: Container started at 2019-07-02 21:07:40 +0000 UTC, pod became ready at 2019-07-02 21:08:01 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:08:00.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9647" for this suite.
Jul  2 21:08:34.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:08:34.751: INFO: namespace container-probe-9647 deletion completed in 34.3444388s

â€¢ [SLOW TEST:56.593 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:08:34.753: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 21:08:34.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 version'
Jul  2 21:08:35.516: INFO: stderr: ""
Jul  2 21:08:35.516: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:02:58Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:08:35.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5319" for this suite.
Jul  2 21:08:41.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:08:41.751: INFO: namespace kubectl-5319 deletion completed in 6.225135063s

â€¢ [SLOW TEST:6.999 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:08:41.752: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-2774/secret-test-91d49e24-9d0d-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 21:08:41.855: INFO: Waiting up to 5m0s for pod "pod-configmaps-91d6644d-9d0d-11e9-808f-7ea354b18644" in namespace "secrets-2774" to be "success or failure"
Jul  2 21:08:41.869: INFO: Pod "pod-configmaps-91d6644d-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 13.178087ms
Jul  2 21:08:43.874: INFO: Pod "pod-configmaps-91d6644d-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018232989s
Jul  2 21:08:45.882: INFO: Pod "pod-configmaps-91d6644d-9d0d-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025571443s
STEP: Saw pod success
Jul  2 21:08:45.882: INFO: Pod "pod-configmaps-91d6644d-9d0d-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:08:45.887: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-configmaps-91d6644d-9d0d-11e9-808f-7ea354b18644 container env-test: <nil>
STEP: delete the pod
Jul  2 21:08:52.394: INFO: Waiting for pod pod-configmaps-91d6644d-9d0d-11e9-808f-7ea354b18644 to disappear
Jul  2 21:08:52.419: INFO: Pod pod-configmaps-91d6644d-9d0d-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:08:52.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2774" for this suite.
Jul  2 21:09:00.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:09:00.736: INFO: namespace secrets-2774 deletion completed in 8.269478816s

â€¢ [SLOW TEST:18.984 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:09:00.737: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  2 21:09:00.849: INFO: Waiting up to 5m0s for pod "pod-9d280800-9d0d-11e9-808f-7ea354b18644" in namespace "emptydir-7484" to be "success or failure"
Jul  2 21:09:00.860: INFO: Pod "pod-9d280800-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 10.115502ms
Jul  2 21:09:03.129: INFO: Pod "pod-9d280800-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279769407s
Jul  2 21:09:05.135: INFO: Pod "pod-9d280800-9d0d-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.285735361s
STEP: Saw pod success
Jul  2 21:09:05.135: INFO: Pod "pod-9d280800-9d0d-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:09:05.140: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-9d280800-9d0d-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 21:09:05.179: INFO: Waiting for pod pod-9d280800-9d0d-11e9-808f-7ea354b18644 to disappear
Jul  2 21:09:05.183: INFO: Pod pod-9d280800-9d0d-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:09:05.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7484" for this suite.
Jul  2 21:09:15.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:09:15.409: INFO: namespace emptydir-7484 deletion completed in 10.219374917s

â€¢ [SLOW TEST:14.673 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:09:15.412: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  2 21:09:15.538: INFO: Waiting up to 5m0s for pod "pod-a5e8cb64-9d0d-11e9-808f-7ea354b18644" in namespace "emptydir-5097" to be "success or failure"
Jul  2 21:09:15.546: INFO: Pod "pod-a5e8cb64-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.429138ms
Jul  2 21:09:17.554: INFO: Pod "pod-a5e8cb64-9d0d-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016218298s
STEP: Saw pod success
Jul  2 21:09:17.555: INFO: Pod "pod-a5e8cb64-9d0d-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:09:17.560: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-a5e8cb64-9d0d-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 21:09:17.608: INFO: Waiting for pod pod-a5e8cb64-9d0d-11e9-808f-7ea354b18644 to disappear
Jul  2 21:09:17.614: INFO: Pod pod-a5e8cb64-9d0d-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:09:17.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5097" for this suite.
Jul  2 21:09:27.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:09:28.140: INFO: namespace emptydir-5097 deletion completed in 10.515951834s

â€¢ [SLOW TEST:12.728 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:09:28.140: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Jul  2 21:09:28.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 cluster-info'
Jul  2 21:09:28.537: INFO: stderr: ""
Jul  2 21:09:28.537: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://192.168.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://192.168.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:09:28.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3306" for this suite.
Jul  2 21:09:34.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:09:34.752: INFO: namespace kubectl-3306 deletion completed in 6.206657618s

â€¢ [SLOW TEST:6.612 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:09:34.753: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-654/configmap-test-b16b8b59-9d0d-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 21:09:34.853: INFO: Waiting up to 5m0s for pod "pod-configmaps-b16d6100-9d0d-11e9-808f-7ea354b18644" in namespace "configmap-654" to be "success or failure"
Jul  2 21:09:34.861: INFO: Pod "pod-configmaps-b16d6100-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.87431ms
Jul  2 21:09:36.866: INFO: Pod "pod-configmaps-b16d6100-9d0d-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01333962s
Jul  2 21:09:38.876: INFO: Pod "pod-configmaps-b16d6100-9d0d-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023158796s
STEP: Saw pod success
Jul  2 21:09:38.876: INFO: Pod "pod-configmaps-b16d6100-9d0d-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:09:38.887: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-configmaps-b16d6100-9d0d-11e9-808f-7ea354b18644 container env-test: <nil>
STEP: delete the pod
Jul  2 21:09:38.967: INFO: Waiting for pod pod-configmaps-b16d6100-9d0d-11e9-808f-7ea354b18644 to disappear
Jul  2 21:09:38.976: INFO: Pod pod-configmaps-b16d6100-9d0d-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:09:38.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-654" for this suite.
Jul  2 21:09:45.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:09:45.225: INFO: namespace configmap-654 deletion completed in 6.234255297s

â€¢ [SLOW TEST:10.472 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:09:45.226: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 21:09:45.298: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:09:49.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4034" for this suite.
Jul  2 21:10:43.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:10:44.254: INFO: namespace pods-4034 deletion completed in 54.87864567s

â€¢ [SLOW TEST:59.028 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:10:44.255: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7240.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7240.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7240.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7240.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7240.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7240.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  2 21:10:48.424: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7240/dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644)
Jul  2 21:10:48.431: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7240/dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644)
Jul  2 21:10:48.437: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-7240.svc.cluster.local from pod dns-7240/dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644)
Jul  2 21:10:48.443: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-7240/dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644)
Jul  2 21:10:48.449: INFO: Unable to read jessie_udp@PodARecord from pod dns-7240/dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644)
Jul  2 21:10:48.454: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7240/dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644: the server could not find the requested resource (get pods dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644)
Jul  2 21:10:48.455: INFO: Lookups using dns-7240/dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-1.dns-test-service.dns-7240.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jul  2 21:11:45.233: INFO: DNS probes using dns-7240/dns-test-dada76ca-9d0d-11e9-808f-7ea354b18644 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:11:45.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7240" for this suite.
Jul  2 21:11:55.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:11:56.973: INFO: namespace dns-7240 deletion completed in 11.698140832s

â€¢ [SLOW TEST:24.349 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:11:56.975: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  2 21:11:57.071: INFO: Waiting up to 5m0s for pod "pod-0631f3f6-9d0e-11e9-808f-7ea354b18644" in namespace "emptydir-9474" to be "success or failure"
Jul  2 21:11:57.080: INFO: Pod "pod-0631f3f6-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 8.728493ms
Jul  2 21:11:59.087: INFO: Pod "pod-0631f3f6-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015789747s
Jul  2 21:12:01.092: INFO: Pod "pod-0631f3f6-9d0e-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021266688s
STEP: Saw pod success
Jul  2 21:12:01.092: INFO: Pod "pod-0631f3f6-9d0e-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:12:01.097: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-0631f3f6-9d0e-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 21:12:01.134: INFO: Waiting for pod pod-0631f3f6-9d0e-11e9-808f-7ea354b18644 to disappear
Jul  2 21:12:01.138: INFO: Pod pod-0631f3f6-9d0e-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:12:01.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9474" for this suite.
Jul  2 21:12:07.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:12:07.332: INFO: namespace emptydir-9474 deletion completed in 6.186629019s

â€¢ [SLOW TEST:10.357 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:12:07.333: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-0c6297b8-9d0e-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 21:12:07.475: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c656331-9d0e-11e9-808f-7ea354b18644" in namespace "projected-2289" to be "success or failure"
Jul  2 21:12:07.482: INFO: Pod "pod-projected-secrets-0c656331-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.805922ms
Jul  2 21:12:09.489: INFO: Pod "pod-projected-secrets-0c656331-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013863092s
Jul  2 21:12:11.496: INFO: Pod "pod-projected-secrets-0c656331-9d0e-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020514439s
STEP: Saw pod success
Jul  2 21:12:11.496: INFO: Pod "pod-projected-secrets-0c656331-9d0e-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:12:11.508: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-projected-secrets-0c656331-9d0e-11e9-808f-7ea354b18644 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  2 21:12:11.567: INFO: Waiting for pod pod-projected-secrets-0c656331-9d0e-11e9-808f-7ea354b18644 to disappear
Jul  2 21:12:11.575: INFO: Pod pod-projected-secrets-0c656331-9d0e-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:12:11.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2289" for this suite.
Jul  2 21:12:17.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:12:17.887: INFO: namespace projected-2289 deletion completed in 6.304013177s

â€¢ [SLOW TEST:10.554 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:12:17.888: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul  2 21:12:25.040: INFO: Successfully updated pod "labelsupdate12a84c00-9d0e-11e9-808f-7ea354b18644"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:12:27.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7717" for this suite.
Jul  2 21:12:49.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:12:49.821: INFO: namespace projected-7717 deletion completed in 22.242076615s

â€¢ [SLOW TEST:31.933 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:12:49.822: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul  2 21:12:50.952: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  2 21:12:50.989: INFO: Waiting for terminating namespaces to be deleted...
Jul  2 21:12:50.995: INFO: 
Logging pods the kubelet thinks is on node ah-kres-worker-1 before test
Jul  2 21:12:51.010: INFO: cstor-sparse-pool-og2i-5899855dd6-ggnms from openebs started at 2019-06-28 14:30:45 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.010: INFO: 	Container cstor-pool ready: true, restart count 0
Jul  2 21:12:51.010: INFO: 	Container cstor-pool-mgmt ready: true, restart count 0
Jul  2 21:12:51.010: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-02 19:14:00 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.010: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  2 21:12:51.010: INFO: sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-mmbft from heptio-sonobuoy started at 2019-07-02 19:14:11 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.010: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  2 21:12:51.018: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  2 21:12:51.018: INFO: openebs-ndm-tdw4h from openebs started at 2019-06-28 14:28:59 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.018: INFO: 	Container node-disk-manager ready: true, restart count 0
Jul  2 21:12:51.018: INFO: weave-net-2bg4v from kube-system started at 2019-06-28 14:28:25 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.018: INFO: 	Container weave ready: true, restart count 0
Jul  2 21:12:51.018: INFO: 	Container weave-npc ready: true, restart count 0
Jul  2 21:12:51.018: INFO: maya-apiserver-d9589fbc6-7zhct from openebs started at 2019-06-28 14:28:59 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.018: INFO: 	Container maya-apiserver ready: true, restart count 0
Jul  2 21:12:51.018: INFO: kube-proxy-c24q8 from kube-system started at 2019-06-28 14:23:42 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.018: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  2 21:12:51.018: INFO: fluent-bit-xdm26 from logging started at 2019-06-28 14:28:59 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.018: INFO: 	Container fluent-bit ready: true, restart count 0
Jul  2 21:12:51.018: INFO: 
Logging pods the kubelet thinks is on node ah-kres-worker-2 before test
Jul  2 21:12:51.031: INFO: weave-net-487f5 from kube-system started at 2019-06-28 14:27:30 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.031: INFO: 	Container weave ready: true, restart count 0
Jul  2 21:12:51.031: INFO: 	Container weave-npc ready: true, restart count 0
Jul  2 21:12:51.031: INFO: cstor-sparse-pool-h1oa-bf78bcdf-8vqh9 from openebs started at 2019-06-28 14:31:01 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.031: INFO: 	Container cstor-pool ready: true, restart count 0
Jul  2 21:12:51.031: INFO: 	Container cstor-pool-mgmt ready: true, restart count 0
Jul  2 21:12:51.031: INFO: kube-proxy-6wbkn from kube-system started at 2019-06-28 14:27:15 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.031: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  2 21:12:51.031: INFO: openebs-ndm-hp9tr from openebs started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.031: INFO: 	Container node-disk-manager ready: true, restart count 0
Jul  2 21:12:51.031: INFO: sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-fzrq6 from heptio-sonobuoy started at 2019-07-02 19:13:30 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.031: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  2 21:12:51.031: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  2 21:12:51.031: INFO: openebs-provisioner-65fd45cf47-nxhwk from openebs started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.031: INFO: 	Container openebs-provisioner ready: true, restart count 0
Jul  2 21:12:51.031: INFO: fluent-bit-58gks from logging started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.031: INFO: 	Container fluent-bit ready: true, restart count 0
Jul  2 21:12:51.031: INFO: coredns-fb8b8dccf-rfjqz from kube-system started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.031: INFO: 	Container coredns ready: true, restart count 8
Jul  2 21:12:51.031: INFO: 
Logging pods the kubelet thinks is on node ah-kres-worker-3 before test
Jul  2 21:12:51.044: INFO: openebs-ndm-nnrhf from openebs started at 2019-06-28 14:29:11 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.044: INFO: 	Container node-disk-manager ready: true, restart count 0
Jul  2 21:12:51.044: INFO: sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-sw8ks from heptio-sonobuoy started at 2019-07-02 19:14:01 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.044: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  2 21:12:51.044: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  2 21:12:51.044: INFO: kube-proxy-jl8vq from kube-system started at 2019-06-28 14:25:42 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.044: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  2 21:12:51.044: INFO: weave-net-5rvbm from kube-system started at 2019-06-28 14:27:43 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.044: INFO: 	Container weave ready: true, restart count 0
Jul  2 21:12:51.045: INFO: 	Container weave-npc ready: true, restart count 0
Jul  2 21:12:51.045: INFO: coredns-fb8b8dccf-2vq7w from kube-system started at 2019-06-28 14:29:11 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.045: INFO: 	Container coredns ready: true, restart count 7
Jul  2 21:12:51.045: INFO: openebs-snapshot-operator-86996d865f-chsgj from openebs started at 2019-06-28 14:29:11 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.045: INFO: 	Container snapshot-controller ready: true, restart count 0
Jul  2 21:12:51.045: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Jul  2 21:12:51.045: INFO: fluent-bit-vxc9c from logging started at 2019-06-28 14:29:11 +0000 UTC (1 container statuses recorded)
Jul  2 21:12:51.045: INFO: 	Container fluent-bit ready: true, restart count 0
Jul  2 21:12:51.045: INFO: cstor-sparse-pool-j6op-559d6f6997-867v4 from openebs started at 2019-06-28 14:30:56 +0000 UTC (2 container statuses recorded)
Jul  2 21:12:51.046: INFO: 	Container cstor-pool ready: true, restart count 0
Jul  2 21:12:51.046: INFO: 	Container cstor-pool-mgmt ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15adb37389367cf5], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:12:52.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-64" for this suite.
Jul  2 21:13:04.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:13:04.862: INFO: namespace sched-pred-64 deletion completed in 12.415981314s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:15.040 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:13:04.863: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul  2 21:13:05.015: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6466,SelfLink:/api/v1/namespaces/watch-6466/configmaps/e2e-watch-test-resource-version,UID:257f2fe9-9d0e-11e9-ab09-0050569fbfc5,ResourceVersion:651857,Generation:0,CreationTimestamp:2019-07-02 21:12:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  2 21:13:05.015: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6466,SelfLink:/api/v1/namespaces/watch-6466/configmaps/e2e-watch-test-resource-version,UID:257f2fe9-9d0e-11e9-ab09-0050569fbfc5,ResourceVersion:651858,Generation:0,CreationTimestamp:2019-07-02 21:12:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:13:05.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6466" for this suite.
Jul  2 21:13:11.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:13:16.281: INFO: namespace watch-6466 deletion completed in 11.259316715s

â€¢ [SLOW TEST:11.418 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:13:16.282: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Jul  2 21:13:19.449: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul  2 21:13:22.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 21:13:24.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 21:13:26.568: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 21:13:34.888: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 21:13:41.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 21:13:43.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697698785, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 21:13:54.998: INFO: Waited 10.441740339s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:13:55.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8735" for this suite.
Jul  2 21:14:05.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:14:07.991: INFO: namespace aggregator-8735 deletion completed in 12.422788614s

â€¢ [SLOW TEST:51.709 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:14:07.992: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-8475
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8475
STEP: Deleting pre-stop pod
Jul  2 21:14:38.969: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:14:39.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8475" for this suite.
Jul  2 21:15:17.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:15:17.725: INFO: namespace prestop-8475 deletion completed in 38.518199795s

â€¢ [SLOW TEST:69.734 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:15:17.729: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-hc2m
STEP: Creating a pod to test atomic-volume-subpath
Jul  2 21:15:17.830: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hc2m" in namespace "subpath-5329" to be "success or failure"
Jul  2 21:15:17.842: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Pending", Reason="", readiness=false. Elapsed: 11.460394ms
Jul  2 21:15:19.848: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017530555s
Jul  2 21:15:23.068: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Running", Reason="", readiness=true. Elapsed: 5.237238196s
Jul  2 21:15:28.309: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Running", Reason="", readiness=true. Elapsed: 10.478357322s
Jul  2 21:15:30.467: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Running", Reason="", readiness=true. Elapsed: 12.636453715s
Jul  2 21:15:32.473: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Running", Reason="", readiness=true. Elapsed: 14.642391991s
Jul  2 21:15:35.842: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Running", Reason="", readiness=true. Elapsed: 18.011747387s
Jul  2 21:15:37.850: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Running", Reason="", readiness=true. Elapsed: 20.019461029s
Jul  2 21:15:41.850: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Running", Reason="", readiness=true. Elapsed: 24.019962106s
Jul  2 21:15:46.142: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Running", Reason="", readiness=true. Elapsed: 28.311890551s
Jul  2 21:15:48.160: INFO: Pod "pod-subpath-test-configmap-hc2m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.329735096s
STEP: Saw pod success
Jul  2 21:15:48.161: INFO: Pod "pod-subpath-test-configmap-hc2m" satisfied condition "success or failure"
Jul  2 21:15:48.367: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-subpath-test-configmap-hc2m container test-container-subpath-configmap-hc2m: <nil>
STEP: delete the pod
Jul  2 21:15:48.417: INFO: Waiting for pod pod-subpath-test-configmap-hc2m to disappear
Jul  2 21:15:48.423: INFO: Pod pod-subpath-test-configmap-hc2m no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hc2m
Jul  2 21:15:48.423: INFO: Deleting pod "pod-subpath-test-configmap-hc2m" in namespace "subpath-5329"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:15:48.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5329" for this suite.
Jul  2 21:15:54.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:15:54.652: INFO: namespace subpath-5329 deletion completed in 6.21575606s

â€¢ [SLOW TEST:36.923 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:15:54.653: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  2 21:15:54.830: INFO: Waiting up to 5m0s for pod "pod-93e95a94-9d0e-11e9-808f-7ea354b18644" in namespace "emptydir-4820" to be "success or failure"
Jul  2 21:15:54.837: INFO: Pod "pod-93e95a94-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.046076ms
Jul  2 21:15:58.204: INFO: Pod "pod-93e95a94-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 3.374058406s
Jul  2 21:16:00.460: INFO: Pod "pod-93e95a94-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.630313658s
Jul  2 21:16:02.470: INFO: Pod "pod-93e95a94-9d0e-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.639705897s
STEP: Saw pod success
Jul  2 21:16:02.470: INFO: Pod "pod-93e95a94-9d0e-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:16:02.476: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-93e95a94-9d0e-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 21:16:02.539: INFO: Waiting for pod pod-93e95a94-9d0e-11e9-808f-7ea354b18644 to disappear
Jul  2 21:16:02.546: INFO: Pod pod-93e95a94-9d0e-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:16:02.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4820" for this suite.
Jul  2 21:17:29.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:17:31.368: INFO: namespace emptydir-4820 deletion completed in 1m28.812039502s

â€¢ [SLOW TEST:96.716 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:17:31.369: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Jul  2 21:17:31.662: INFO: Waiting up to 5m0s for pod "var-expansion-cd9f216d-9d0e-11e9-808f-7ea354b18644" in namespace "var-expansion-2526" to be "success or failure"
Jul  2 21:17:31.670: INFO: Pod "var-expansion-cd9f216d-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.437662ms
Jul  2 21:17:33.676: INFO: Pod "var-expansion-cd9f216d-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014197179s
Jul  2 21:17:35.684: INFO: Pod "var-expansion-cd9f216d-9d0e-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021524702s
Jul  2 21:17:47.623: INFO: Pod "var-expansion-cd9f216d-9d0e-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.960308268s
STEP: Saw pod success
Jul  2 21:17:47.624: INFO: Pod "var-expansion-cd9f216d-9d0e-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:17:47.663: INFO: Trying to get logs from node ah-kres-worker-1 pod var-expansion-cd9f216d-9d0e-11e9-808f-7ea354b18644 container dapi-container: <nil>
STEP: delete the pod
Jul  2 21:17:48.470: INFO: Waiting for pod var-expansion-cd9f216d-9d0e-11e9-808f-7ea354b18644 to disappear
Jul  2 21:17:48.475: INFO: Pod var-expansion-cd9f216d-9d0e-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:17:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2526" for this suite.
Jul  2 21:18:33.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:19:20.065: INFO: namespace var-expansion-2526 deletion completed in 45.330435342s

â€¢ [SLOW TEST:62.445 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:19:20.067: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 21:19:23.996: INFO: Creating deployment "test-recreate-deployment"
Jul  2 21:19:24.017: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul  2 21:19:24.035: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul  2 21:19:26.045: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul  2 21:19:26.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697699148, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697699148, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697699148, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697699148, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  2 21:19:34.350: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul  2 21:19:37.479: INFO: Updating deployment test-recreate-deployment
Jul  2 21:19:37.479: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  2 21:20:28.184: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-5011,SelfLink:/apis/apps/v1/namespaces/deployment-5011/deployments/test-recreate-deployment,UID:0457031c-9d0f-11e9-ab09-0050569fbfc5,ResourceVersion:652786,Generation:2,CreationTimestamp:2019-07-02 21:19:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-02 21:20:11 +0000 UTC 2019-07-02 21:20:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-02 21:20:12 +0000 UTC 2019-07-02 21:19:08 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul  2 21:20:28.193: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-5011,SelfLink:/apis/apps/v1/namespaces/deployment-5011/replicasets/test-recreate-deployment-c9cbd8684,UID:2df20d6a-9d0f-11e9-9994-0050569f2898,ResourceVersion:652783,Generation:1,CreationTimestamp:2019-07-02 21:20:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0457031c-9d0f-11e9-ab09-0050569fbfc5 0xc002576ce0 0xc002576ce1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  2 21:20:28.194: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul  2 21:20:28.195: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-5011,SelfLink:/apis/apps/v1/namespaces/deployment-5011/replicasets/test-recreate-deployment-7d57d5ff7c,UID:0759689d-9d0f-11e9-9994-0050569f2898,ResourceVersion:652759,Generation:2,CreationTimestamp:2019-07-02 21:19:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0457031c-9d0f-11e9-ab09-0050569fbfc5 0xc002576c07 0xc002576c08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  2 21:20:28.204: INFO: Pod "test-recreate-deployment-c9cbd8684-h66ls" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-h66ls,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-5011,SelfLink:/api/v1/namespaces/deployment-5011/pods/test-recreate-deployment-c9cbd8684-h66ls,UID:2e170208-9d0f-11e9-9994-0050569f2898,ResourceVersion:652777,Generation:0,CreationTimestamp:2019-07-02 21:20:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 2df20d6a-9d0f-11e9-9994-0050569f2898 0xc002577610 0xc002577611}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lv8vz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lv8vz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lv8vz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002577670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002577690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:20:28.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5011" for this suite.
Jul  2 21:21:40.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:21:40.389: INFO: namespace deployment-5011 deletion completed in 1m12.176799556s

â€¢ [SLOW TEST:140.322 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:21:40.389: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Jul  2 21:21:40.473: INFO: Waiting up to 5m0s for pod "pod-61eea413-9d0f-11e9-808f-7ea354b18644" in namespace "emptydir-3447" to be "success or failure"
Jul  2 21:21:40.477: INFO: Pod "pod-61eea413-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 3.660785ms
Jul  2 21:21:42.483: INFO: Pod "pod-61eea413-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010068546s
Jul  2 21:21:58.113: INFO: Pod "pod-61eea413-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 17.639931683s
Jul  2 21:22:00.370: INFO: Pod "pod-61eea413-9d0f-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 19.896705641s
STEP: Saw pod success
Jul  2 21:22:00.370: INFO: Pod "pod-61eea413-9d0f-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:22:00.401: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-61eea413-9d0f-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 21:22:00.786: INFO: Waiting for pod pod-61eea413-9d0f-11e9-808f-7ea354b18644 to disappear
Jul  2 21:22:00.804: INFO: Pod pod-61eea413-9d0f-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:22:00.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3447" for this suite.
Jul  2 21:22:49.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:22:49.889: INFO: namespace emptydir-3447 deletion completed in 49.072797251s

â€¢ [SLOW TEST:69.500 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:22:49.890: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-8b844e29-9d0f-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 21:22:50.364: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b967049-9d0f-11e9-808f-7ea354b18644" in namespace "configmap-4751" to be "success or failure"
Jul  2 21:22:50.370: INFO: Pod "pod-configmaps-8b967049-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.513461ms
Jul  2 21:22:53.102: INFO: Pod "pod-configmaps-8b967049-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.738157212s
Jul  2 21:22:57.855: INFO: Pod "pod-configmaps-8b967049-9d0f-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.491797801s
STEP: Saw pod success
Jul  2 21:22:57.856: INFO: Pod "pod-configmaps-8b967049-9d0f-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:22:57.937: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-configmaps-8b967049-9d0f-11e9-808f-7ea354b18644 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 21:22:59.075: INFO: Waiting for pod pod-configmaps-8b967049-9d0f-11e9-808f-7ea354b18644 to disappear
Jul  2 21:23:00.291: INFO: Pod pod-configmaps-8b967049-9d0f-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:23:00.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4751" for this suite.
Jul  2 21:23:08.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:23:08.554: INFO: namespace configmap-4751 deletion completed in 8.248224898s

â€¢ [SLOW TEST:18.664 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:23:08.555: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-97e18d58-9d0f-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 21:23:10.998: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-97e338af-9d0f-11e9-808f-7ea354b18644" in namespace "projected-7133" to be "success or failure"
Jul  2 21:23:11.007: INFO: Pod "pod-projected-configmaps-97e338af-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.515441ms
Jul  2 21:23:13.013: INFO: Pod "pod-projected-configmaps-97e338af-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015564575s
Jul  2 21:23:15.019: INFO: Pod "pod-projected-configmaps-97e338af-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020911439s
Jul  2 21:23:17.023: INFO: Pod "pod-projected-configmaps-97e338af-9d0f-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025500004s
STEP: Saw pod success
Jul  2 21:23:17.023: INFO: Pod "pod-projected-configmaps-97e338af-9d0f-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:23:17.026: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-projected-configmaps-97e338af-9d0f-11e9-808f-7ea354b18644 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 21:23:17.193: INFO: Waiting for pod pod-projected-configmaps-97e338af-9d0f-11e9-808f-7ea354b18644 to disappear
Jul  2 21:23:17.198: INFO: Pod pod-projected-configmaps-97e338af-9d0f-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:23:17.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7133" for this suite.
Jul  2 21:23:31.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:23:31.377: INFO: namespace projected-7133 deletion completed in 14.169816237s

â€¢ [SLOW TEST:22.822 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:23:31.378: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 21:23:41.141: INFO: Waiting up to 5m0s for pod "client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644" in namespace "pods-14" to be "success or failure"
Jul  2 21:23:47.598: INFO: Pod "client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.456995134s
Jul  2 21:23:49.605: INFO: Pod "client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 8.463683023s
Jul  2 21:24:05.620: INFO: Pod "client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 24.479148354s
Jul  2 21:24:07.629: INFO: Pod "client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 26.487810876s
Jul  2 21:24:12.771: INFO: Pod "client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 31.629962071s
Jul  2 21:24:14.778: INFO: Pod "client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 33.636940692s
STEP: Saw pod success
Jul  2 21:24:14.778: INFO: Pod "client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:24:14.785: INFO: Trying to get logs from node ah-kres-worker-3 pod client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644 container env3cont: <nil>
STEP: delete the pod
Jul  2 21:24:15.022: INFO: Waiting for pod client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644 to disappear
Jul  2 21:24:15.027: INFO: Pod client-envvars-a984ba4f-9d0f-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:24:15.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-14" for this suite.
Jul  2 21:27:32.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:27:33.006: INFO: namespace pods-14 deletion completed in 2m30.156329602s

â€¢ [SLOW TEST:193.814 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:27:33.007: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:27:35.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9676" for this suite.
Jul  2 21:27:50.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:27:51.043: INFO: namespace services-9676 deletion completed in 15.92244753s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:18.037 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:27:51.044: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  2 21:27:53.571: INFO: Waiting up to 5m0s for pod "downward-api-404f628e-9d10-11e9-808f-7ea354b18644" in namespace "downward-api-726" to be "success or failure"
Jul  2 21:27:53.576: INFO: Pod "downward-api-404f628e-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.144502ms
Jul  2 21:27:55.582: INFO: Pod "downward-api-404f628e-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011634057s
Jul  2 21:27:58.483: INFO: Pod "downward-api-404f628e-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.91194773s
Jul  2 21:28:00.505: INFO: Pod "downward-api-404f628e-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.934575532s
Jul  2 21:28:02.513: INFO: Pod "downward-api-404f628e-9d10-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.942127089s
STEP: Saw pod success
Jul  2 21:28:02.513: INFO: Pod "downward-api-404f628e-9d10-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:28:02.519: INFO: Trying to get logs from node ah-kres-worker-1 pod downward-api-404f628e-9d10-11e9-808f-7ea354b18644 container dapi-container: <nil>
STEP: delete the pod
Jul  2 21:28:02.668: INFO: Waiting for pod downward-api-404f628e-9d10-11e9-808f-7ea354b18644 to disappear
Jul  2 21:28:02.673: INFO: Pod downward-api-404f628e-9d10-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:28:02.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-726" for this suite.
Jul  2 21:28:27.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:28:27.919: INFO: namespace downward-api-726 deletion completed in 25.237551771s

â€¢ [SLOW TEST:36.875 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:28:27.920: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 21:28:28.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644" in namespace "projected-849" to be "success or failure"
Jul  2 21:28:28.044: INFO: Pod "downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 15.444897ms
Jul  2 21:28:30.050: INFO: Pod "downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021515013s
Jul  2 21:28:32.320: INFO: Pod "downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.290736944s
Jul  2 21:28:34.950: INFO: Pod "downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.921257201s
Jul  2 21:28:36.959: INFO: Pod "downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.929687465s
STEP: Saw pod success
Jul  2 21:28:36.959: INFO: Pod "downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:28:36.964: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 21:28:37.009: INFO: Waiting for pod downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644 to disappear
Jul  2 21:28:37.014: INFO: Pod downwardapi-volume-54d90ebc-9d10-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:28:37.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-849" for this suite.
Jul  2 21:28:47.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:28:47.418: INFO: namespace projected-849 deletion completed in 10.395581689s

â€¢ [SLOW TEST:19.498 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:28:47.420: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644
Jul  2 21:28:47.621: INFO: Pod name my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644: Found 0 pods out of 1
Jul  2 21:28:53.698: INFO: Pod name my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644: Found 1 pods out of 1
Jul  2 21:28:53.698: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644" are running
Jul  2 21:29:13.860: INFO: Pod "my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644-rdksq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-02 21:28:34 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-02 21:28:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-02 21:28:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-02 21:28:40 +0000 UTC Reason: Message:}])
Jul  2 21:29:13.861: INFO: Trying to dial the pod
Jul  2 21:29:30.435: INFO: Controller my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644: Got expected result from replica 1 [my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644-rdksq]: "my-hostname-basic-6087b678-9d10-11e9-808f-7ea354b18644-rdksq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:29:30.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7418" for this suite.
Jul  2 21:30:02.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:30:02.682: INFO: namespace replication-controller-7418 deletion completed in 32.235684585s

â€¢ [SLOW TEST:75.262 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:30:02.684: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Jul  2 21:30:21.743: INFO: Waiting up to 5m0s for pod "client-containers-98a02e50-9d10-11e9-808f-7ea354b18644" in namespace "containers-8472" to be "success or failure"
Jul  2 21:30:21.757: INFO: Pod "client-containers-98a02e50-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 13.691136ms
Jul  2 21:30:23.764: INFO: Pod "client-containers-98a02e50-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020691468s
Jul  2 21:30:28.244: INFO: Pod "client-containers-98a02e50-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.50075555s
Jul  2 21:30:30.575: INFO: Pod "client-containers-98a02e50-9d10-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.832320318s
STEP: Saw pod success
Jul  2 21:30:30.575: INFO: Pod "client-containers-98a02e50-9d10-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:30:30.595: INFO: Trying to get logs from node ah-kres-worker-2 pod client-containers-98a02e50-9d10-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 21:30:30.926: INFO: Waiting for pod client-containers-98a02e50-9d10-11e9-808f-7ea354b18644 to disappear
Jul  2 21:30:30.933: INFO: Pod client-containers-98a02e50-9d10-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:30:30.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8472" for this suite.
Jul  2 21:30:49.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:30:54.110: INFO: namespace containers-8472 deletion completed in 23.165542637s

â€¢ [SLOW TEST:51.426 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:30:54.111: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  2 21:30:54.837: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:54.838: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:54.838: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:54.845: INFO: Number of nodes with available pods: 0
Jul  2 21:30:54.845: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:30:55.856: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:55.856: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:55.856: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:55.865: INFO: Number of nodes with available pods: 0
Jul  2 21:30:55.865: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:30:56.866: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:56.866: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:56.866: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:56.883: INFO: Number of nodes with available pods: 0
Jul  2 21:30:56.883: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:30:57.962: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:57.963: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:57.963: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:57.972: INFO: Number of nodes with available pods: 0
Jul  2 21:30:57.972: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:30:58.857: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:58.857: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:58.857: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:58.864: INFO: Number of nodes with available pods: 0
Jul  2 21:30:58.864: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:30:59.907: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:59.907: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:59.907: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:30:59.914: INFO: Number of nodes with available pods: 3
Jul  2 21:30:59.915: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul  2 21:31:00.467: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:31:00.467: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:31:00.467: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:31:00.474: INFO: Number of nodes with available pods: 3
Jul  2 21:31:00.474: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8546, will wait for the garbage collector to delete the pods
Jul  2 21:31:06.140: INFO: Deleting DaemonSet.extensions daemon-set took: 21.05131ms
Jul  2 21:31:06.541: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.326475ms
Jul  2 21:31:28.070: INFO: Number of nodes with available pods: 0
Jul  2 21:31:28.070: INFO: Number of running nodes: 0, number of available pods: 0
Jul  2 21:31:28.095: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8546/daemonsets","resourceVersion":"654138"},"items":null}

Jul  2 21:31:28.124: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8546/pods","resourceVersion":"654139"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:31:28.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8546" for this suite.
Jul  2 21:32:12.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:32:12.482: INFO: namespace daemonsets-8546 deletion completed in 44.253446929s

â€¢ [SLOW TEST:78.371 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:32:12.484: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  2 21:32:12.594: INFO: Waiting up to 5m0s for pod "pod-dab35ab8-9d10-11e9-808f-7ea354b18644" in namespace "emptydir-7566" to be "success or failure"
Jul  2 21:32:12.606: INFO: Pod "pod-dab35ab8-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 11.900788ms
Jul  2 21:32:14.613: INFO: Pod "pod-dab35ab8-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018483266s
Jul  2 21:32:16.619: INFO: Pod "pod-dab35ab8-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024923353s
Jul  2 21:32:19.038: INFO: Pod "pod-dab35ab8-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.444080516s
Jul  2 21:32:21.868: INFO: Pod "pod-dab35ab8-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.273619232s
Jul  2 21:32:23.880: INFO: Pod "pod-dab35ab8-9d10-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.286289106s
STEP: Saw pod success
Jul  2 21:32:23.880: INFO: Pod "pod-dab35ab8-9d10-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:32:23.885: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-dab35ab8-9d10-11e9-808f-7ea354b18644 container test-container: <nil>
STEP: delete the pod
Jul  2 21:32:23.939: INFO: Waiting for pod pod-dab35ab8-9d10-11e9-808f-7ea354b18644 to disappear
Jul  2 21:32:23.944: INFO: Pod pod-dab35ab8-9d10-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:32:23.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7566" for this suite.
Jul  2 21:32:29.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:32:30.127: INFO: namespace emptydir-7566 deletion completed in 6.176061676s

â€¢ [SLOW TEST:17.643 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:32:30.128: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul  2 21:32:30.240: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  2 21:32:30.255: INFO: Waiting for terminating namespaces to be deleted...
Jul  2 21:32:30.271: INFO: 
Logging pods the kubelet thinks is on node ah-kres-worker-1 before test
Jul  2 21:32:30.287: INFO: kube-proxy-c24q8 from kube-system started at 2019-06-28 14:23:42 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.287: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  2 21:32:30.287: INFO: weave-net-2bg4v from kube-system started at 2019-06-28 14:28:25 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.287: INFO: 	Container weave ready: true, restart count 0
Jul  2 21:32:30.287: INFO: 	Container weave-npc ready: true, restart count 0
Jul  2 21:32:30.287: INFO: maya-apiserver-d9589fbc6-7zhct from openebs started at 2019-06-28 14:28:59 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.287: INFO: 	Container maya-apiserver ready: true, restart count 0
Jul  2 21:32:30.287: INFO: fluent-bit-xdm26 from logging started at 2019-06-28 14:28:59 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.287: INFO: 	Container fluent-bit ready: true, restart count 0
Jul  2 21:32:30.288: INFO: cstor-sparse-pool-og2i-5899855dd6-ggnms from openebs started at 2019-06-28 14:30:45 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.288: INFO: 	Container cstor-pool ready: true, restart count 0
Jul  2 21:32:30.288: INFO: 	Container cstor-pool-mgmt ready: true, restart count 0
Jul  2 21:32:30.288: INFO: openebs-ndm-tdw4h from openebs started at 2019-06-28 14:28:59 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.288: INFO: 	Container node-disk-manager ready: true, restart count 0
Jul  2 21:32:30.288: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-02 19:14:00 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.288: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  2 21:32:30.288: INFO: sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-mmbft from heptio-sonobuoy started at 2019-07-02 19:14:11 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.288: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jul  2 21:32:30.288: INFO: 	Container systemd-logs ready: true, restart count 2
Jul  2 21:32:30.288: INFO: 
Logging pods the kubelet thinks is on node ah-kres-worker-2 before test
Jul  2 21:32:30.299: INFO: sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-fzrq6 from heptio-sonobuoy started at 2019-07-02 19:13:30 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.299: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jul  2 21:32:30.299: INFO: 	Container systemd-logs ready: true, restart count 2
Jul  2 21:32:30.299: INFO: openebs-provisioner-65fd45cf47-nxhwk from openebs started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.299: INFO: 	Container openebs-provisioner ready: true, restart count 0
Jul  2 21:32:30.299: INFO: fluent-bit-58gks from logging started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.299: INFO: 	Container fluent-bit ready: true, restart count 0
Jul  2 21:32:30.299: INFO: coredns-fb8b8dccf-rfjqz from kube-system started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.299: INFO: 	Container coredns ready: true, restart count 8
Jul  2 21:32:30.299: INFO: weave-net-487f5 from kube-system started at 2019-06-28 14:27:30 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.299: INFO: 	Container weave ready: true, restart count 0
Jul  2 21:32:30.299: INFO: 	Container weave-npc ready: true, restart count 0
Jul  2 21:32:30.299: INFO: cstor-sparse-pool-h1oa-bf78bcdf-8vqh9 from openebs started at 2019-06-28 14:31:01 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.299: INFO: 	Container cstor-pool ready: true, restart count 0
Jul  2 21:32:30.299: INFO: 	Container cstor-pool-mgmt ready: true, restart count 0
Jul  2 21:32:30.299: INFO: kube-proxy-6wbkn from kube-system started at 2019-06-28 14:27:15 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.299: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  2 21:32:30.299: INFO: openebs-ndm-hp9tr from openebs started at 2019-06-28 14:28:03 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.300: INFO: 	Container node-disk-manager ready: true, restart count 0
Jul  2 21:32:30.300: INFO: 
Logging pods the kubelet thinks is on node ah-kres-worker-3 before test
Jul  2 21:32:30.316: INFO: openebs-ndm-nnrhf from openebs started at 2019-06-28 14:29:11 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.316: INFO: 	Container node-disk-manager ready: true, restart count 0
Jul  2 21:32:30.316: INFO: sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-sw8ks from heptio-sonobuoy started at 2019-07-02 19:14:01 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.316: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jul  2 21:32:30.316: INFO: 	Container systemd-logs ready: true, restart count 2
Jul  2 21:32:30.316: INFO: openebs-snapshot-operator-86996d865f-chsgj from openebs started at 2019-06-28 14:29:11 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.316: INFO: 	Container snapshot-controller ready: true, restart count 0
Jul  2 21:32:30.316: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Jul  2 21:32:30.316: INFO: fluent-bit-vxc9c from logging started at 2019-06-28 14:29:11 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.316: INFO: 	Container fluent-bit ready: true, restart count 0
Jul  2 21:32:30.316: INFO: kube-proxy-jl8vq from kube-system started at 2019-06-28 14:25:42 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.316: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  2 21:32:30.316: INFO: weave-net-5rvbm from kube-system started at 2019-06-28 14:27:43 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.316: INFO: 	Container weave ready: true, restart count 0
Jul  2 21:32:30.316: INFO: 	Container weave-npc ready: true, restart count 0
Jul  2 21:32:30.316: INFO: coredns-fb8b8dccf-2vq7w from kube-system started at 2019-06-28 14:29:11 +0000 UTC (1 container statuses recorded)
Jul  2 21:32:30.316: INFO: 	Container coredns ready: true, restart count 7
Jul  2 21:32:30.316: INFO: cstor-sparse-pool-j6op-559d6f6997-867v4 from openebs started at 2019-06-28 14:30:56 +0000 UTC (2 container statuses recorded)
Jul  2 21:32:30.316: INFO: 	Container cstor-pool ready: true, restart count 0
Jul  2 21:32:30.316: INFO: 	Container cstor-pool-mgmt ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node ah-kres-worker-1
STEP: verifying the node has the label node ah-kres-worker-2
STEP: verifying the node has the label node ah-kres-worker-3
Jul  2 21:32:30.767: INFO: Pod sonobuoy requesting resource cpu=0m on Node ah-kres-worker-1
Jul  2 21:32:30.767: INFO: Pod sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-fzrq6 requesting resource cpu=0m on Node ah-kres-worker-2
Jul  2 21:32:30.768: INFO: Pod sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-mmbft requesting resource cpu=0m on Node ah-kres-worker-1
Jul  2 21:32:30.768: INFO: Pod sonobuoy-systemd-logs-daemon-set-debe46ca423d4bf6-sw8ks requesting resource cpu=0m on Node ah-kres-worker-3
Jul  2 21:32:30.768: INFO: Pod coredns-fb8b8dccf-2vq7w requesting resource cpu=100m on Node ah-kres-worker-3
Jul  2 21:32:30.768: INFO: Pod coredns-fb8b8dccf-rfjqz requesting resource cpu=100m on Node ah-kres-worker-2
Jul  2 21:32:30.769: INFO: Pod kube-proxy-6wbkn requesting resource cpu=0m on Node ah-kres-worker-2
Jul  2 21:32:30.769: INFO: Pod kube-proxy-c24q8 requesting resource cpu=0m on Node ah-kres-worker-1
Jul  2 21:32:30.769: INFO: Pod kube-proxy-jl8vq requesting resource cpu=0m on Node ah-kres-worker-3
Jul  2 21:32:30.769: INFO: Pod weave-net-2bg4v requesting resource cpu=20m on Node ah-kres-worker-1
Jul  2 21:32:30.770: INFO: Pod weave-net-487f5 requesting resource cpu=20m on Node ah-kres-worker-2
Jul  2 21:32:30.770: INFO: Pod weave-net-5rvbm requesting resource cpu=20m on Node ah-kres-worker-3
Jul  2 21:32:30.770: INFO: Pod fluent-bit-58gks requesting resource cpu=0m on Node ah-kres-worker-2
Jul  2 21:32:30.770: INFO: Pod fluent-bit-vxc9c requesting resource cpu=0m on Node ah-kres-worker-3
Jul  2 21:32:30.771: INFO: Pod fluent-bit-xdm26 requesting resource cpu=0m on Node ah-kres-worker-1
Jul  2 21:32:30.771: INFO: Pod cstor-sparse-pool-h1oa-bf78bcdf-8vqh9 requesting resource cpu=100m on Node ah-kres-worker-2
Jul  2 21:32:30.771: INFO: Pod cstor-sparse-pool-j6op-559d6f6997-867v4 requesting resource cpu=100m on Node ah-kres-worker-3
Jul  2 21:32:30.771: INFO: Pod cstor-sparse-pool-og2i-5899855dd6-ggnms requesting resource cpu=100m on Node ah-kres-worker-1
Jul  2 21:32:30.772: INFO: Pod maya-apiserver-d9589fbc6-7zhct requesting resource cpu=0m on Node ah-kres-worker-1
Jul  2 21:32:30.772: INFO: Pod openebs-ndm-hp9tr requesting resource cpu=0m on Node ah-kres-worker-2
Jul  2 21:32:30.772: INFO: Pod openebs-ndm-nnrhf requesting resource cpu=0m on Node ah-kres-worker-3
Jul  2 21:32:30.772: INFO: Pod openebs-ndm-tdw4h requesting resource cpu=0m on Node ah-kres-worker-1
Jul  2 21:32:30.773: INFO: Pod openebs-provisioner-65fd45cf47-nxhwk requesting resource cpu=0m on Node ah-kres-worker-2
Jul  2 21:32:30.773: INFO: Pod openebs-snapshot-operator-86996d865f-chsgj requesting resource cpu=0m on Node ah-kres-worker-3
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58c8169-9d10-11e9-808f-7ea354b18644.15adb48ff64a0a5a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2784/filler-pod-e58c8169-9d10-11e9-808f-7ea354b18644 to ah-kres-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58c8169-9d10-11e9-808f-7ea354b18644.15adb49251b52fc5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58c8169-9d10-11e9-808f-7ea354b18644.15adb49261906572], Reason = [Created], Message = [Created container filler-pod-e58c8169-9d10-11e9-808f-7ea354b18644]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58c8169-9d10-11e9-808f-7ea354b18644.15adb49279ee4930], Reason = [Started], Message = [Started container filler-pod-e58c8169-9d10-11e9-808f-7ea354b18644]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58f93d0-9d10-11e9-808f-7ea354b18644.15adb48ff7ee4754], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2784/filler-pod-e58f93d0-9d10-11e9-808f-7ea354b18644 to ah-kres-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58f93d0-9d10-11e9-808f-7ea354b18644.15adb49133076e63], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58f93d0-9d10-11e9-808f-7ea354b18644.15adb4915a4e6339], Reason = [Created], Message = [Created container filler-pod-e58f93d0-9d10-11e9-808f-7ea354b18644]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58f93d0-9d10-11e9-808f-7ea354b18644.15adb49173f674f3], Reason = [Started], Message = [Started container filler-pod-e58f93d0-9d10-11e9-808f-7ea354b18644]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e59178eb-9d10-11e9-808f-7ea354b18644.15adb48ff7efaa51], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2784/filler-pod-e59178eb-9d10-11e9-808f-7ea354b18644 to ah-kres-worker-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e59178eb-9d10-11e9-808f-7ea354b18644.15adb4929db7907d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e59178eb-9d10-11e9-808f-7ea354b18644.15adb492c5def9ee], Reason = [Created], Message = [Created container filler-pod-e59178eb-9d10-11e9-808f-7ea354b18644]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e59178eb-9d10-11e9-808f-7ea354b18644.15adb492e55f3474], Reason = [Started], Message = [Started container filler-pod-e59178eb-9d10-11e9-808f-7ea354b18644]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15adb4929a1e831e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node ah-kres-worker-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ah-kres-worker-3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ah-kres-worker-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:32:43.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2784" for this suite.
Jul  2 21:32:57.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:32:57.312: INFO: namespace sched-pred-2784 deletion completed in 13.790577311s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:27.185 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:32:57.314: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-f56803dd-9d10-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 21:32:57.411: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f56a053f-9d10-11e9-808f-7ea354b18644" in namespace "projected-2418" to be "success or failure"
Jul  2 21:32:57.421: INFO: Pod "pod-projected-secrets-f56a053f-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 9.743641ms
Jul  2 21:32:59.427: INFO: Pod "pod-projected-secrets-f56a053f-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016462007s
Jul  2 21:33:02.838: INFO: Pod "pod-projected-secrets-f56a053f-9d10-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.427442978s
STEP: Saw pod success
Jul  2 21:33:02.838: INFO: Pod "pod-projected-secrets-f56a053f-9d10-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:33:02.844: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-projected-secrets-f56a053f-9d10-11e9-808f-7ea354b18644 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  2 21:33:02.938: INFO: Waiting for pod pod-projected-secrets-f56a053f-9d10-11e9-808f-7ea354b18644 to disappear
Jul  2 21:33:02.944: INFO: Pod pod-projected-secrets-f56a053f-9d10-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:33:02.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2418" for this suite.
Jul  2 21:33:08.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:33:09.173: INFO: namespace projected-2418 deletion completed in 6.219122794s

â€¢ [SLOW TEST:11.859 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:33:09.175: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-fc7e94c2-9d10-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 21:33:09.295: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fc806efd-9d10-11e9-808f-7ea354b18644" in namespace "projected-1236" to be "success or failure"
Jul  2 21:33:09.300: INFO: Pod "pod-projected-secrets-fc806efd-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073473ms
Jul  2 21:33:11.307: INFO: Pod "pod-projected-secrets-fc806efd-9d10-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011827952s
Jul  2 21:33:13.313: INFO: Pod "pod-projected-secrets-fc806efd-9d10-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017703271s
STEP: Saw pod success
Jul  2 21:33:13.313: INFO: Pod "pod-projected-secrets-fc806efd-9d10-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:33:13.318: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-projected-secrets-fc806efd-9d10-11e9-808f-7ea354b18644 container secret-volume-test: <nil>
STEP: delete the pod
Jul  2 21:33:13.364: INFO: Waiting for pod pod-projected-secrets-fc806efd-9d10-11e9-808f-7ea354b18644 to disappear
Jul  2 21:33:13.369: INFO: Pod pod-projected-secrets-fc806efd-9d10-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:33:13.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1236" for this suite.
Jul  2 21:33:25.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:33:25.517: INFO: namespace projected-1236 deletion completed in 12.139705144s

â€¢ [SLOW TEST:16.342 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:33:25.517: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 21:33:25.595: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0637bebb-9d11-11e9-808f-7ea354b18644" in namespace "downward-api-838" to be "success or failure"
Jul  2 21:33:25.599: INFO: Pod "downwardapi-volume-0637bebb-9d11-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844647ms
Jul  2 21:33:32.060: INFO: Pod "downwardapi-volume-0637bebb-9d11-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.464450446s
Jul  2 21:33:34.065: INFO: Pod "downwardapi-volume-0637bebb-9d11-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.469702668s
STEP: Saw pod success
Jul  2 21:33:34.065: INFO: Pod "downwardapi-volume-0637bebb-9d11-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:33:34.070: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-0637bebb-9d11-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 21:33:34.101: INFO: Waiting for pod downwardapi-volume-0637bebb-9d11-11e9-808f-7ea354b18644 to disappear
Jul  2 21:33:34.111: INFO: Pod downwardapi-volume-0637bebb-9d11-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:33:34.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-838" for this suite.
Jul  2 21:34:32.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:34:32.539: INFO: namespace downward-api-838 deletion completed in 14.60726825s

â€¢ [SLOW TEST:23.207 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:34:32.540: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jul  2 21:34:32.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 create -f - --namespace=kubectl-5470'
Jul  2 21:34:33.561: INFO: stderr: ""
Jul  2 21:34:33.561: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  2 21:34:33.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5470'
Jul  2 21:34:33.758: INFO: stderr: ""
Jul  2 21:34:33.758: INFO: stdout: "update-demo-nautilus-bh26k update-demo-nautilus-jk2lq "
Jul  2 21:34:33.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-bh26k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:34:33.927: INFO: stderr: ""
Jul  2 21:34:33.927: INFO: stdout: ""
Jul  2 21:34:33.927: INFO: update-demo-nautilus-bh26k is created but not running
Jul  2 21:34:38.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5470'
Jul  2 21:34:39.769: INFO: stderr: ""
Jul  2 21:34:39.770: INFO: stdout: "update-demo-nautilus-bh26k update-demo-nautilus-jk2lq "
Jul  2 21:34:39.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-bh26k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:34:42.765: INFO: stderr: ""
Jul  2 21:34:42.765: INFO: stdout: "true"
Jul  2 21:34:42.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-bh26k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:34:43.372: INFO: stderr: ""
Jul  2 21:34:43.372: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 21:34:43.372: INFO: validating pod update-demo-nautilus-bh26k
Jul  2 21:34:43.390: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 21:34:43.390: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 21:34:43.390: INFO: update-demo-nautilus-bh26k is verified up and running
Jul  2 21:34:43.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-jk2lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:34:43.836: INFO: stderr: ""
Jul  2 21:34:43.836: INFO: stdout: "true"
Jul  2 21:34:43.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-jk2lq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:34:44.028: INFO: stderr: ""
Jul  2 21:34:44.028: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 21:34:44.028: INFO: validating pod update-demo-nautilus-jk2lq
Jul  2 21:34:44.039: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 21:34:44.039: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 21:34:44.039: INFO: update-demo-nautilus-jk2lq is verified up and running
STEP: scaling down the replication controller
Jul  2 21:34:44.045: INFO: scanned /root for discovery docs: <nil>
Jul  2 21:34:44.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5470'
Jul  2 21:34:49.127: INFO: stderr: ""
Jul  2 21:34:49.127: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  2 21:34:49.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5470'
Jul  2 21:34:49.314: INFO: stderr: ""
Jul  2 21:34:49.314: INFO: stdout: "update-demo-nautilus-bh26k update-demo-nautilus-jk2lq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  2 21:34:54.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5470'
Jul  2 21:34:54.808: INFO: stderr: ""
Jul  2 21:34:54.808: INFO: stdout: "update-demo-nautilus-bh26k update-demo-nautilus-jk2lq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  2 21:34:59.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5470'
Jul  2 21:35:01.299: INFO: stderr: ""
Jul  2 21:35:01.299: INFO: stdout: "update-demo-nautilus-bh26k update-demo-nautilus-jk2lq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  2 21:35:06.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5470'
Jul  2 21:35:06.475: INFO: stderr: ""
Jul  2 21:35:06.475: INFO: stdout: "update-demo-nautilus-bh26k "
Jul  2 21:35:06.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-bh26k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:35:06.654: INFO: stderr: ""
Jul  2 21:35:06.654: INFO: stdout: "true"
Jul  2 21:35:06.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-bh26k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:35:08.000: INFO: stderr: ""
Jul  2 21:35:08.000: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 21:35:08.000: INFO: validating pod update-demo-nautilus-bh26k
Jul  2 21:35:08.010: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 21:35:08.010: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 21:35:08.010: INFO: update-demo-nautilus-bh26k is verified up and running
STEP: scaling up the replication controller
Jul  2 21:35:08.016: INFO: scanned /root for discovery docs: <nil>
Jul  2 21:35:08.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5470'
Jul  2 21:35:09.268: INFO: stderr: ""
Jul  2 21:35:09.268: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  2 21:35:09.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5470'
Jul  2 21:35:09.460: INFO: stderr: ""
Jul  2 21:35:09.460: INFO: stdout: "update-demo-nautilus-bh26k update-demo-nautilus-c2gbm "
Jul  2 21:35:09.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-bh26k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:35:09.631: INFO: stderr: ""
Jul  2 21:35:09.631: INFO: stdout: "true"
Jul  2 21:35:09.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-bh26k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:35:09.803: INFO: stderr: ""
Jul  2 21:35:09.803: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 21:35:09.803: INFO: validating pod update-demo-nautilus-bh26k
Jul  2 21:35:09.812: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 21:35:09.812: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 21:35:09.812: INFO: update-demo-nautilus-bh26k is verified up and running
Jul  2 21:35:09.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-c2gbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:35:10.000: INFO: stderr: ""
Jul  2 21:35:10.001: INFO: stdout: ""
Jul  2 21:35:10.001: INFO: update-demo-nautilus-c2gbm is created but not running
Jul  2 21:35:15.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5470'
Jul  2 21:35:22.385: INFO: stderr: ""
Jul  2 21:35:22.385: INFO: stdout: "update-demo-nautilus-bh26k update-demo-nautilus-c2gbm "
Jul  2 21:35:22.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-bh26k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:35:22.567: INFO: stderr: ""
Jul  2 21:35:22.567: INFO: stdout: "true"
Jul  2 21:35:22.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-bh26k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:35:22.758: INFO: stderr: ""
Jul  2 21:35:22.758: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 21:35:22.758: INFO: validating pod update-demo-nautilus-bh26k
Jul  2 21:35:22.817: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 21:35:22.818: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 21:35:22.818: INFO: update-demo-nautilus-bh26k is verified up and running
Jul  2 21:35:22.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-c2gbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:35:22.977: INFO: stderr: ""
Jul  2 21:35:22.977: INFO: stdout: "true"
Jul  2 21:35:22.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods update-demo-nautilus-c2gbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5470'
Jul  2 21:35:23.100: INFO: stderr: ""
Jul  2 21:35:23.100: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  2 21:35:23.100: INFO: validating pod update-demo-nautilus-c2gbm
Jul  2 21:35:23.111: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  2 21:35:23.111: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  2 21:35:23.111: INFO: update-demo-nautilus-c2gbm is verified up and running
STEP: using delete to clean up resources
Jul  2 21:35:23.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete --grace-period=0 --force -f - --namespace=kubectl-5470'
Jul  2 21:35:23.288: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  2 21:35:23.288: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  2 21:35:23.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5470'
Jul  2 21:35:23.450: INFO: stderr: "No resources found.\n"
Jul  2 21:35:23.450: INFO: stdout: ""
Jul  2 21:35:23.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -l name=update-demo --namespace=kubectl-5470 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  2 21:35:23.956: INFO: stderr: ""
Jul  2 21:35:23.956: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:35:23.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5470" for this suite.
Jul  2 21:35:45.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:35:46.111: INFO: namespace kubectl-5470 deletion completed in 22.148765068s

â€¢ [SLOW TEST:73.571 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:35:46.112: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-bl5l
STEP: Creating a pod to test atomic-volume-subpath
Jul  2 21:35:46.939: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bl5l" in namespace "subpath-728" to be "success or failure"
Jul  2 21:35:46.951: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Pending", Reason="", readiness=false. Elapsed: 12.496478ms
Jul  2 21:35:48.957: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018444169s
Jul  2 21:35:50.970: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Running", Reason="", readiness=true. Elapsed: 4.031403253s
Jul  2 21:35:52.977: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Running", Reason="", readiness=true. Elapsed: 6.037529266s
Jul  2 21:35:54.986: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Running", Reason="", readiness=true. Elapsed: 8.046952335s
Jul  2 21:36:01.047: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Running", Reason="", readiness=true. Elapsed: 14.108269287s
Jul  2 21:36:03.053: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Running", Reason="", readiness=true. Elapsed: 16.114326334s
Jul  2 21:36:05.060: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Running", Reason="", readiness=true. Elapsed: 18.121262576s
Jul  2 21:36:09.017: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Running", Reason="", readiness=true. Elapsed: 22.078083451s
Jul  2 21:36:13.892: INFO: Pod "pod-subpath-test-projected-bl5l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.952548206s
STEP: Saw pod success
Jul  2 21:36:13.892: INFO: Pod "pod-subpath-test-projected-bl5l" satisfied condition "success or failure"
Jul  2 21:36:13.897: INFO: Trying to get logs from node ah-kres-worker-1 pod pod-subpath-test-projected-bl5l container test-container-subpath-projected-bl5l: <nil>
STEP: delete the pod
Jul  2 21:36:13.959: INFO: Waiting for pod pod-subpath-test-projected-bl5l to disappear
Jul  2 21:36:13.969: INFO: Pod pod-subpath-test-projected-bl5l no longer exists
STEP: Deleting pod pod-subpath-test-projected-bl5l
Jul  2 21:36:13.969: INFO: Deleting pod "pod-subpath-test-projected-bl5l" in namespace "subpath-728"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:36:13.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-728" for this suite.
Jul  2 21:36:20.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:36:20.381: INFO: namespace subpath-728 deletion completed in 6.385040072s

â€¢ [SLOW TEST:34.269 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:36:20.382: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 21:36:20.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ebbb9de-9d11-11e9-808f-7ea354b18644" in namespace "projected-2678" to be "success or failure"
Jul  2 21:36:21.028: INFO: Pod "downwardapi-volume-6ebbb9de-9d11-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 47.078247ms
Jul  2 21:36:23.035: INFO: Pod "downwardapi-volume-6ebbb9de-9d11-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05380802s
Jul  2 21:36:26.465: INFO: Pod "downwardapi-volume-6ebbb9de-9d11-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.484360214s
STEP: Saw pod success
Jul  2 21:36:26.465: INFO: Pod "downwardapi-volume-6ebbb9de-9d11-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:36:26.583: INFO: Trying to get logs from node ah-kres-worker-1 pod downwardapi-volume-6ebbb9de-9d11-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 21:36:27.126: INFO: Waiting for pod downwardapi-volume-6ebbb9de-9d11-11e9-808f-7ea354b18644 to disappear
Jul  2 21:36:27.135: INFO: Pod downwardapi-volume-6ebbb9de-9d11-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:36:27.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2678" for this suite.
Jul  2 21:36:35.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:36:35.391: INFO: namespace projected-2678 deletion completed in 8.245921741s

â€¢ [SLOW TEST:15.009 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:36:35.391: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 21:36:35.492: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7766018c-9d11-11e9-808f-7ea354b18644" in namespace "projected-3629" to be "success or failure"
Jul  2 21:36:35.498: INFO: Pod "downwardapi-volume-7766018c-9d11-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 6.890084ms
Jul  2 21:36:37.505: INFO: Pod "downwardapi-volume-7766018c-9d11-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013351357s
Jul  2 21:36:39.512: INFO: Pod "downwardapi-volume-7766018c-9d11-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020133006s
STEP: Saw pod success
Jul  2 21:36:39.512: INFO: Pod "downwardapi-volume-7766018c-9d11-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:36:39.517: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-7766018c-9d11-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 21:36:40.856: INFO: Waiting for pod downwardapi-volume-7766018c-9d11-11e9-808f-7ea354b18644 to disappear
Jul  2 21:36:40.862: INFO: Pod downwardapi-volume-7766018c-9d11-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:36:40.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3629" for this suite.
Jul  2 21:36:50.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:36:51.139: INFO: namespace projected-3629 deletion completed in 10.269527601s

â€¢ [SLOW TEST:15.748 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:36:51.141: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0702 21:37:35.343289      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  2 21:37:35.343: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:37:35.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7310" for this suite.
Jul  2 21:37:49.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:37:49.520: INFO: namespace gc-7310 deletion completed in 14.169025018s

â€¢ [SLOW TEST:58.379 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:37:49.520: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-5987
Jul  2 21:37:57.212: INFO: Started pod liveness-http in namespace container-probe-5987
STEP: checking the pod's current state and verifying that restartCount is present
Jul  2 21:37:57.235: INFO: Initial restart count of pod liveness-http is 0
Jul  2 21:38:16.506: INFO: Restart count of pod container-probe-5987/liveness-http is now 1 (19.270337685s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:38:16.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5987" for this suite.
Jul  2 21:38:27.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:38:27.323: INFO: namespace container-probe-5987 deletion completed in 10.77288473s

â€¢ [SLOW TEST:37.803 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:38:27.324: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-bb2fd333-9d11-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 21:38:29.225: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bb3189da-9d11-11e9-808f-7ea354b18644" in namespace "projected-655" to be "success or failure"
Jul  2 21:38:29.230: INFO: Pod "pod-projected-configmaps-bb3189da-9d11-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.224455ms
Jul  2 21:38:31.237: INFO: Pod "pod-projected-configmaps-bb3189da-9d11-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011884755s
Jul  2 21:38:34.396: INFO: Pod "pod-projected-configmaps-bb3189da-9d11-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.171423933s
STEP: Saw pod success
Jul  2 21:38:34.396: INFO: Pod "pod-projected-configmaps-bb3189da-9d11-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:38:34.712: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-projected-configmaps-bb3189da-9d11-11e9-808f-7ea354b18644 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 21:38:34.789: INFO: Waiting for pod pod-projected-configmaps-bb3189da-9d11-11e9-808f-7ea354b18644 to disappear
Jul  2 21:38:34.794: INFO: Pod pod-projected-configmaps-bb3189da-9d11-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:38:34.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-655" for this suite.
Jul  2 21:38:46.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:38:46.975: INFO: namespace projected-655 deletion completed in 12.166661834s

â€¢ [SLOW TEST:19.651 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:38:46.976: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Jul  2 21:38:47.049: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-362853403 proxy --unix-socket=/tmp/kubectl-proxy-unix294027986/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:38:47.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4052" for this suite.
Jul  2 21:39:01.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:39:01.985: INFO: namespace kubectl-4052 deletion completed in 14.794608646s

â€¢ [SLOW TEST:15.010 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:39:01.986: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Jul  2 21:39:04.343: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5325" to be "success or failure"
Jul  2 21:39:04.404: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 61.200711ms
Jul  2 21:39:07.310: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.966796845s
Jul  2 21:39:09.317: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.973927052s
STEP: Saw pod success
Jul  2 21:39:09.317: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul  2 21:39:09.322: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul  2 21:39:09.370: INFO: Waiting for pod pod-host-path-test to disappear
Jul  2 21:39:09.380: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:39:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5325" for this suite.
Jul  2 21:40:11.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:40:11.627: INFO: namespace hostpath-5325 deletion completed in 1m2.233196428s

â€¢ [SLOW TEST:69.642 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:40:11.628: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  2 21:40:11.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-6723'
Jul  2 21:40:11.922: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  2 21:40:11.922: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Jul  2 21:40:13.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6723'
Jul  2 21:40:14.139: INFO: stderr: ""
Jul  2 21:40:14.139: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:40:14.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6723" for this suite.
Jul  2 21:40:26.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:40:26.351: INFO: namespace kubectl-6723 deletion completed in 12.203549762s

â€¢ [SLOW TEST:14.723 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:40:26.352: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 21:40:26.419: INFO: Waiting up to 5m0s for pod "downwardapi-volume-010bba5a-9d12-11e9-808f-7ea354b18644" in namespace "projected-5748" to be "success or failure"
Jul  2 21:40:26.434: INFO: Pod "downwardapi-volume-010bba5a-9d12-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 14.91167ms
Jul  2 21:40:30.687: INFO: Pod "downwardapi-volume-010bba5a-9d12-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.267802112s
Jul  2 21:40:32.707: INFO: Pod "downwardapi-volume-010bba5a-9d12-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.28800938s
STEP: Saw pod success
Jul  2 21:40:32.707: INFO: Pod "downwardapi-volume-010bba5a-9d12-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:40:32.712: INFO: Trying to get logs from node ah-kres-worker-1 pod downwardapi-volume-010bba5a-9d12-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 21:40:32.766: INFO: Waiting for pod downwardapi-volume-010bba5a-9d12-11e9-808f-7ea354b18644 to disappear
Jul  2 21:40:32.772: INFO: Pod downwardapi-volume-010bba5a-9d12-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:40:32.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5748" for this suite.
Jul  2 21:40:42.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:40:42.984: INFO: namespace projected-5748 deletion completed in 10.20253104s

â€¢ [SLOW TEST:16.633 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:40:42.985: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  2 21:40:43.370: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:43.370: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:43.370: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:43.383: INFO: Number of nodes with available pods: 0
Jul  2 21:40:43.383: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:40:44.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:44.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:44.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:44.400: INFO: Number of nodes with available pods: 0
Jul  2 21:40:44.400: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:40:45.398: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:45.398: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:45.398: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:45.416: INFO: Number of nodes with available pods: 0
Jul  2 21:40:45.416: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:40:46.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:46.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:46.393: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:46.399: INFO: Number of nodes with available pods: 3
Jul  2 21:40:46.399: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul  2 21:40:46.436: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:46.437: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:46.437: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:46.443: INFO: Number of nodes with available pods: 2
Jul  2 21:40:46.443: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:47.454: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:47.454: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:47.454: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:47.461: INFO: Number of nodes with available pods: 2
Jul  2 21:40:47.461: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:48.562: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:48.562: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:48.563: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:48.569: INFO: Number of nodes with available pods: 2
Jul  2 21:40:48.569: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:49.453: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:49.453: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:49.454: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:49.461: INFO: Number of nodes with available pods: 2
Jul  2 21:40:49.461: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:50.452: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:50.453: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:50.453: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:50.460: INFO: Number of nodes with available pods: 2
Jul  2 21:40:50.460: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:51.843: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:51.843: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:51.843: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:51.853: INFO: Number of nodes with available pods: 2
Jul  2 21:40:51.853: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:52.453: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:52.453: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:52.453: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:52.460: INFO: Number of nodes with available pods: 2
Jul  2 21:40:52.460: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:53.456: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:53.456: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:53.456: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:53.466: INFO: Number of nodes with available pods: 2
Jul  2 21:40:53.466: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:54.450: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:54.450: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:54.451: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:54.455: INFO: Number of nodes with available pods: 2
Jul  2 21:40:54.455: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:55.454: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:55.454: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:55.454: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:55.459: INFO: Number of nodes with available pods: 2
Jul  2 21:40:55.459: INFO: Node ah-kres-worker-3 is running more than one daemon pod
Jul  2 21:40:56.451: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:56.451: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:56.451: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:40:56.456: INFO: Number of nodes with available pods: 3
Jul  2 21:40:56.456: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7074, will wait for the garbage collector to delete the pods
Jul  2 21:40:56.528: INFO: Deleting DaemonSet.extensions daemon-set took: 12.795775ms
Jul  2 21:40:57.028: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.582835ms
Jul  2 21:41:03.341: INFO: Number of nodes with available pods: 0
Jul  2 21:41:03.341: INFO: Number of running nodes: 0, number of available pods: 0
Jul  2 21:41:03.358: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7074/daemonsets","resourceVersion":"655857"},"items":null}

Jul  2 21:41:03.372: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7074/pods","resourceVersion":"655857"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:41:03.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7074" for this suite.
Jul  2 21:41:09.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:41:09.684: INFO: namespace daemonsets-7074 deletion completed in 6.238729332s

â€¢ [SLOW TEST:26.699 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:41:09.686: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  2 21:41:09.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-8266'
Jul  2 21:41:09.958: INFO: stderr: ""
Jul  2 21:41:09.958: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul  2 21:41:15.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pod e2e-test-nginx-pod --namespace=kubectl-8266 -o json'
Jul  2 21:41:15.229: INFO: stderr: ""
Jul  2 21:41:15.229: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-07-02T21:41:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-8266\",\n        \"resourceVersion\": \"655917\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8266/pods/e2e-test-nginx-pod\",\n        \"uid\": \"2f2c7781-9d12-11e9-9994-0050569f2898\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-x4qmv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ah-kres-worker-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-x4qmv\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-x4qmv\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-02T21:41:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-02T21:41:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-02T21:41:34Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-02T21:41:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://8868a414e3deee4487c523bdc2e117049cbd594c93478d766c386914880ace00\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-02T21:41:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.30.20.166\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.32.0.6\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-02T21:41:31Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul  2 21:41:15.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 replace -f - --namespace=kubectl-8266'
Jul  2 21:41:15.679: INFO: stderr: ""
Jul  2 21:41:15.679: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Jul  2 21:41:15.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete pods e2e-test-nginx-pod --namespace=kubectl-8266'
Jul  2 21:41:22.551: INFO: stderr: ""
Jul  2 21:41:22.551: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:41:22.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8266" for this suite.
Jul  2 21:41:28.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:41:28.808: INFO: namespace kubectl-8266 deletion completed in 6.244351748s

â€¢ [SLOW TEST:19.122 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:41:28.809: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-2648a048-9d12-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume configMaps
Jul  2 21:41:28.911: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-264abfcd-9d12-11e9-808f-7ea354b18644" in namespace "projected-1011" to be "success or failure"
Jul  2 21:41:28.923: INFO: Pod "pod-projected-configmaps-264abfcd-9d12-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 11.608653ms
Jul  2 21:41:30.930: INFO: Pod "pod-projected-configmaps-264abfcd-9d12-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018176228s
Jul  2 21:41:32.937: INFO: Pod "pod-projected-configmaps-264abfcd-9d12-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025041483s
STEP: Saw pod success
Jul  2 21:41:32.937: INFO: Pod "pod-projected-configmaps-264abfcd-9d12-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:41:32.944: INFO: Trying to get logs from node ah-kres-worker-2 pod pod-projected-configmaps-264abfcd-9d12-11e9-808f-7ea354b18644 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  2 21:41:33.001: INFO: Waiting for pod pod-projected-configmaps-264abfcd-9d12-11e9-808f-7ea354b18644 to disappear
Jul  2 21:41:33.013: INFO: Pod pod-projected-configmaps-264abfcd-9d12-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:41:33.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1011" for this suite.
Jul  2 21:41:41.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:41:41.239: INFO: namespace projected-1011 deletion completed in 8.185086628s

â€¢ [SLOW TEST:12.430 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:41:41.240: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  2 21:41:46.867: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul  2 21:41:47.227: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:47.227: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:47.227: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:47.245: INFO: Number of nodes with available pods: 0
Jul  2 21:41:47.245: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:41:48.277: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:48.278: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:48.278: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:48.304: INFO: Number of nodes with available pods: 0
Jul  2 21:41:48.304: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:41:49.304: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:49.304: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:49.305: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:49.323: INFO: Number of nodes with available pods: 0
Jul  2 21:41:49.323: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:41:50.255: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:50.255: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:50.256: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:50.263: INFO: Number of nodes with available pods: 0
Jul  2 21:41:50.263: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:41:51.264: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:51.265: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:51.265: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:51.273: INFO: Number of nodes with available pods: 3
Jul  2 21:41:51.273: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul  2 21:41:51.363: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:51.363: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:51.363: INFO: Wrong image for pod: daemon-set-vq962. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:51.371: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:51.372: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:51.372: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:52.380: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:52.380: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:52.380: INFO: Wrong image for pod: daemon-set-vq962. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:52.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:52.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:52.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:53.382: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:53.382: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:53.383: INFO: Wrong image for pod: daemon-set-vq962. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:53.390: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:53.390: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:53.390: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:54.377: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:54.377: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:54.377: INFO: Wrong image for pod: daemon-set-vq962. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:54.377: INFO: Pod daemon-set-vq962 is not available
Jul  2 21:41:54.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:54.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:54.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:58.292: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:58.292: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:58.293: INFO: Wrong image for pod: daemon-set-vq962. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:58.293: INFO: Pod daemon-set-vq962 is not available
Jul  2 21:41:59.159: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:59.159: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:59.159: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:59.593: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:59.593: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:59.593: INFO: Wrong image for pod: daemon-set-vq962. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:41:59.593: INFO: Pod daemon-set-vq962 is not available
Jul  2 21:41:59.602: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:59.602: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:41:59.602: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:02.909: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:02.909: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:02.909: INFO: Wrong image for pod: daemon-set-vq962. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:02.909: INFO: Pod daemon-set-vq962 is not available
Jul  2 21:42:02.984: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:02.984: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:02.985: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:03.960: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:03.960: INFO: Pod daemon-set-k5qnx is not available
Jul  2 21:42:03.960: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:03.967: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:03.968: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:03.968: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:04.379: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:04.379: INFO: Pod daemon-set-k5qnx is not available
Jul  2 21:42:04.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:04.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:04.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:04.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:05.704: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:05.704: INFO: Pod daemon-set-k5qnx is not available
Jul  2 21:42:05.704: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:05.767: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:05.770: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:05.770: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:06.378: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:06.378: INFO: Pod daemon-set-k5qnx is not available
Jul  2 21:42:06.378: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:06.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:06.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:06.384: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:07.930: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:07.930: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:07.949: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:07.949: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:07.949: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:08.378: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:08.378: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:08.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:08.384: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:08.384: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:09.979: INFO: Wrong image for pod: daemon-set-fm2ph. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:09.979: INFO: Pod daemon-set-fm2ph is not available
Jul  2 21:42:09.980: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:10.286: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:10.287: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:10.287: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:10.379: INFO: Pod daemon-set-hfcbd is not available
Jul  2 21:42:10.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:10.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:10.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:10.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:11.379: INFO: Pod daemon-set-hfcbd is not available
Jul  2 21:42:11.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:11.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:11.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:11.388: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:13.904: INFO: Pod daemon-set-hfcbd is not available
Jul  2 21:42:13.907: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:13.947: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:13.947: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:13.947: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:21.012: INFO: Pod daemon-set-hfcbd is not available
Jul  2 21:42:21.013: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:21.043: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:21.044: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:21.044: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:24.205: INFO: Pod daemon-set-hfcbd is not available
Jul  2 21:42:24.205: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:24.926: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:24.926: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:24.926: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:25.977: INFO: Pod daemon-set-hfcbd is not available
Jul  2 21:42:25.978: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:26.377: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:26.377: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:26.377: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:27.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:27.405: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:27.405: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:27.405: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:28.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:28.385: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:28.385: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:28.385: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:29.377: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:29.385: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:29.385: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:29.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:30.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:30.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:30.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:30.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:31.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:31.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:31.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:31.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:32.380: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:32.388: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:32.388: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:32.388: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:33.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:33.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:33.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:33.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:34.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:34.385: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:34.385: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:34.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:37.952: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:37.989: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:37.989: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:37.990: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:38.393: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:38.399: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:38.399: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:38.399: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:39.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:39.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:39.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:39.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:40.567: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:40.577: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:40.577: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:40.578: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:41.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:41.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:41.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:41.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:42.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:42.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:42.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:42.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:43.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:43.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:43.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:43.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:44.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:44.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:44.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:44.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:46.024: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:46.041: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:46.041: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:46.042: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:46.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:46.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:46.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:46.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:47.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:47.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:47.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:47.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:48.377: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:48.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:48.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:48.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:49.377: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:49.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:49.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:49.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:50.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:50.388: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:50.388: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:50.388: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:51.379: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:51.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:51.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:51.389: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:52.893: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:52.903: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:52.903: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:52.904: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:53.384: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:53.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:53.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:53.392: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:54.502: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:54.509: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:54.509: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:54.509: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:57.224: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:57.232: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:57.233: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:57.233: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:57.384: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:57.434: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:57.434: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:57.434: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:58.392: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:58.403: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:58.403: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:58.403: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:59.377: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:42:59.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:59.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:42:59.383: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:01.621: INFO: Wrong image for pod: daemon-set-ptpvb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  2 21:43:01.621: INFO: Pod daemon-set-ptpvb is not available
Jul  2 21:43:08.158: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:08.159: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:08.159: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:08.378: INFO: Pod daemon-set-vwkmh is not available
Jul  2 21:43:08.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:08.386: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:08.387: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul  2 21:43:08.394: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:08.394: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:08.395: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:08.400: INFO: Number of nodes with available pods: 2
Jul  2 21:43:08.401: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:43:09.563: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:09.563: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:09.563: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:09.571: INFO: Number of nodes with available pods: 2
Jul  2 21:43:09.571: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:43:10.411: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:10.411: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:10.411: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:10.416: INFO: Number of nodes with available pods: 2
Jul  2 21:43:10.416: INFO: Node ah-kres-worker-1 is running more than one daemon pod
Jul  2 21:43:11.409: INFO: DaemonSet pods can't tolerate node ah-kres-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:11.409: INFO: DaemonSet pods can't tolerate node ah-kres-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:11.410: INFO: DaemonSet pods can't tolerate node ah-kres-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  2 21:43:11.416: INFO: Number of nodes with available pods: 3
Jul  2 21:43:11.416: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1284, will wait for the garbage collector to delete the pods
Jul  2 21:43:11.506: INFO: Deleting DaemonSet.extensions daemon-set took: 11.814074ms
Jul  2 21:43:11.907: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.257476ms
Jul  2 21:43:23.713: INFO: Number of nodes with available pods: 0
Jul  2 21:43:23.713: INFO: Number of running nodes: 0, number of available pods: 0
Jul  2 21:43:23.718: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1284/daemonsets","resourceVersion":"656316"},"items":null}

Jul  2 21:43:23.723: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1284/pods","resourceVersion":"656316"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:43:23.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1284" for this suite.
Jul  2 21:43:30.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:43:33.662: INFO: namespace daemonsets-1284 deletion completed in 9.906323663s

â€¢ [SLOW TEST:112.423 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:43:33.663: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 21:43:33.761: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70b52af3-9d12-11e9-808f-7ea354b18644" in namespace "downward-api-2342" to be "success or failure"
Jul  2 21:43:33.769: INFO: Pod "downwardapi-volume-70b52af3-9d12-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 8.19587ms
Jul  2 21:43:35.796: INFO: Pod "downwardapi-volume-70b52af3-9d12-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0344315s
Jul  2 21:43:37.801: INFO: Pod "downwardapi-volume-70b52af3-9d12-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040168057s
STEP: Saw pod success
Jul  2 21:43:37.801: INFO: Pod "downwardapi-volume-70b52af3-9d12-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:43:37.806: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-70b52af3-9d12-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 21:43:38.041: INFO: Waiting for pod downwardapi-volume-70b52af3-9d12-11e9-808f-7ea354b18644 to disappear
Jul  2 21:43:38.046: INFO: Pod downwardapi-volume-70b52af3-9d12-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:43:38.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2342" for this suite.
Jul  2 21:43:44.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:43:44.267: INFO: namespace downward-api-2342 deletion completed in 6.212304933s

â€¢ [SLOW TEST:10.604 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:43:44.269: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0702 21:43:58.654636      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  2 21:43:58.654: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:43:58.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6663" for this suite.
Jul  2 21:44:08.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:44:08.841: INFO: namespace gc-6663 deletion completed in 10.180006514s

â€¢ [SLOW TEST:24.573 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:44:08.841: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul  2 21:44:21.735: INFO: Successfully updated pod "labelsupdate85cfcb5e-9d12-11e9-808f-7ea354b18644"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:44:23.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3771" for this suite.
Jul  2 21:46:00.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:46:04.602: INFO: namespace downward-api-3771 deletion completed in 31.131390283s

â€¢ [SLOW TEST:46.062 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:46:04.603: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9622
Jul  2 21:46:12.699: INFO: Started pod liveness-http in namespace container-probe-9622
STEP: checking the pod's current state and verifying that restartCount is present
Jul  2 21:46:12.705: INFO: Initial restart count of pod liveness-http is 0
Jul  2 21:46:29.244: INFO: Restart count of pod container-probe-9622/liveness-http is now 1 (16.53914272s elapsed)
Jul  2 21:46:46.898: INFO: Restart count of pod container-probe-9622/liveness-http is now 2 (34.193841789s elapsed)
Jul  2 21:47:08.854: INFO: Restart count of pod container-probe-9622/liveness-http is now 3 (56.149330009s elapsed)
Jul  2 21:47:42.154: INFO: Restart count of pod container-probe-9622/liveness-http is now 4 (1m29.449904233s elapsed)
Jul  2 21:47:57.506: INFO: Restart count of pod container-probe-9622/liveness-http is now 5 (1m44.801021236s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:47:57.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9622" for this suite.
Jul  2 21:48:34.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:48:36.197: INFO: namespace container-probe-9622 deletion completed in 38.391548793s

â€¢ [SLOW TEST:151.594 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:48:36.197: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-250689fb-9d13-11e9-808f-7ea354b18644
STEP: Creating a pod to test consume secrets
Jul  2 21:48:36.324: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644" in namespace "projected-1484" to be "success or failure"
Jul  2 21:48:36.336: INFO: Pod "pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 11.42966ms
Jul  2 21:48:38.343: INFO: Pod "pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018400193s
Jul  2 21:48:40.350: INFO: Pod "pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025131677s
Jul  2 21:48:43.488: INFO: Pod "pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.16280103s
Jul  2 21:48:51.931: INFO: Pod "pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 15.606121968s
STEP: Saw pod success
Jul  2 21:48:51.931: INFO: Pod "pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:48:52.796: INFO: Trying to get logs from node ah-kres-worker-3 pod pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  2 21:48:52.865: INFO: Waiting for pod pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644 to disappear
Jul  2 21:48:52.872: INFO: Pod pod-projected-secrets-250ce4e3-9d13-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:48:52.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1484" for this suite.
Jul  2 21:48:58.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:48:59.077: INFO: namespace projected-1484 deletion completed in 6.19416269s

â€¢ [SLOW TEST:22.880 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:48:59.081: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  2 21:48:59.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7893'
Jul  2 21:49:09.167: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  2 21:49:09.169: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul  2 21:49:09.189: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul  2 21:49:09.216: INFO: scanned /root for discovery docs: <nil>
Jul  2 21:49:09.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7893'
Jul  2 21:49:28.991: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul  2 21:49:28.992: INFO: stdout: "Created e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188\nScaling up e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul  2 21:49:28.992: INFO: stdout: "Created e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188\nScaling up e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul  2 21:49:28.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7893'
Jul  2 21:49:30.535: INFO: stderr: ""
Jul  2 21:49:30.535: INFO: stdout: "e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188-hc7rl "
Jul  2 21:49:30.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188-hc7rl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7893'
Jul  2 21:49:39.688: INFO: stderr: ""
Jul  2 21:49:39.688: INFO: stdout: "true"
Jul  2 21:49:39.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 get pods e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188-hc7rl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7893'
Jul  2 21:49:40.091: INFO: stderr: ""
Jul  2 21:49:40.091: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul  2 21:49:40.091: INFO: e2e-test-nginx-rc-c55e5077bcf440f828b8295cb49ce188-hc7rl is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Jul  2 21:49:40.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-362853403 delete rc e2e-test-nginx-rc --namespace=kubectl-7893'
Jul  2 21:49:40.506: INFO: stderr: ""
Jul  2 21:49:40.506: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:49:40.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7893" for this suite.
Jul  2 21:49:48.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:49:48.697: INFO: namespace kubectl-7893 deletion completed in 8.18070175s

â€¢ [SLOW TEST:49.617 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:49:48.699: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8406
I0702 21:49:48.776302      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8406, replica count: 1
I0702 21:49:49.826716      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0702 21:49:50.826915      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0702 21:49:51.827310      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  2 21:49:51.942: INFO: Created: latency-svc-z2c4m
Jul  2 21:49:51.965: INFO: Got endpoints: latency-svc-z2c4m [37.446776ms]
Jul  2 21:49:51.987: INFO: Created: latency-svc-kn6fm
Jul  2 21:49:52.005: INFO: Got endpoints: latency-svc-kn6fm [40.682163ms]
Jul  2 21:49:52.023: INFO: Created: latency-svc-s2hxl
Jul  2 21:49:52.035: INFO: Got endpoints: latency-svc-s2hxl [69.816299ms]
Jul  2 21:49:52.041: INFO: Created: latency-svc-l85zm
Jul  2 21:49:52.050: INFO: Got endpoints: latency-svc-l85zm [84.925317ms]
Jul  2 21:49:52.062: INFO: Created: latency-svc-kz27m
Jul  2 21:49:52.072: INFO: Created: latency-svc-lk5p4
Jul  2 21:49:52.084: INFO: Got endpoints: latency-svc-kz27m [118.218808ms]
Jul  2 21:49:52.088: INFO: Got endpoints: latency-svc-lk5p4 [122.146925ms]
Jul  2 21:49:52.099: INFO: Created: latency-svc-54vdh
Jul  2 21:49:52.108: INFO: Got endpoints: latency-svc-54vdh [142.631336ms]
Jul  2 21:49:52.114: INFO: Created: latency-svc-sfmss
Jul  2 21:49:53.576: INFO: Got endpoints: latency-svc-sfmss [1.610417071s]
Jul  2 21:49:53.596: INFO: Created: latency-svc-jl47c
Jul  2 21:49:53.625: INFO: Got endpoints: latency-svc-jl47c [1.659699802s]
Jul  2 21:49:53.650: INFO: Created: latency-svc-hrpdv
Jul  2 21:49:53.668: INFO: Got endpoints: latency-svc-hrpdv [1.702016488s]
Jul  2 21:49:53.681: INFO: Created: latency-svc-jp8gt
Jul  2 21:49:53.946: INFO: Got endpoints: latency-svc-jp8gt [1.979647308s]
Jul  2 21:49:53.989: INFO: Created: latency-svc-t88v9
Jul  2 21:49:54.316: INFO: Got endpoints: latency-svc-t88v9 [2.350006735s]
Jul  2 21:49:55.815: INFO: Created: latency-svc-p7f6l
Jul  2 21:49:56.127: INFO: Got endpoints: latency-svc-p7f6l [4.161068801s]
Jul  2 21:49:56.140: INFO: Created: latency-svc-8n4r6
Jul  2 21:49:56.168: INFO: Created: latency-svc-7ckh8
Jul  2 21:49:56.305: INFO: Got endpoints: latency-svc-8n4r6 [4.338724322s]
Jul  2 21:49:56.364: INFO: Got endpoints: latency-svc-7ckh8 [4.397943332s]
Jul  2 21:49:56.387: INFO: Created: latency-svc-xrj5r
Jul  2 21:49:56.396: INFO: Got endpoints: latency-svc-xrj5r [4.429664531s]
Jul  2 21:49:57.977: INFO: Created: latency-svc-njbgp
Jul  2 21:49:57.992: INFO: Got endpoints: latency-svc-njbgp [5.986459079s]
Jul  2 21:49:58.001: INFO: Created: latency-svc-mdpsl
Jul  2 21:49:58.148: INFO: Got endpoints: latency-svc-mdpsl [6.113064013s]
Jul  2 21:49:58.157: INFO: Created: latency-svc-pf5dz
Jul  2 21:49:58.174: INFO: Created: latency-svc-zqxnj
Jul  2 21:49:58.183: INFO: Got endpoints: latency-svc-pf5dz [6.128884705s]
Jul  2 21:49:58.204: INFO: Got endpoints: latency-svc-zqxnj [6.119655987s]
Jul  2 21:49:58.213: INFO: Created: latency-svc-kvq6v
Jul  2 21:49:58.223: INFO: Got endpoints: latency-svc-kvq6v [6.135132514s]
Jul  2 21:49:58.233: INFO: Created: latency-svc-52m9d
Jul  2 21:49:58.245: INFO: Got endpoints: latency-svc-52m9d [6.137241957s]
Jul  2 21:49:58.256: INFO: Created: latency-svc-x5d6n
Jul  2 21:49:58.272: INFO: Created: latency-svc-4fkbk
Jul  2 21:49:58.285: INFO: Got endpoints: latency-svc-x5d6n [4.709171812s]
Jul  2 21:49:58.288: INFO: Got endpoints: latency-svc-4fkbk [4.661005717s]
Jul  2 21:49:58.306: INFO: Created: latency-svc-n9knh
Jul  2 21:49:58.315: INFO: Created: latency-svc-r9m5w
Jul  2 21:49:58.315: INFO: Got endpoints: latency-svc-n9knh [4.647322444s]
Jul  2 21:49:58.354: INFO: Got endpoints: latency-svc-r9m5w [4.408429672s]
Jul  2 21:49:58.355: INFO: Created: latency-svc-nn2qb
Jul  2 21:49:58.355: INFO: Got endpoints: latency-svc-nn2qb [4.039411381s]
Jul  2 21:49:58.359: INFO: Created: latency-svc-vvjq9
Jul  2 21:49:58.369: INFO: Got endpoints: latency-svc-vvjq9 [2.242035423s]
Jul  2 21:49:58.378: INFO: Created: latency-svc-2xnd6
Jul  2 21:49:58.388: INFO: Got endpoints: latency-svc-2xnd6 [2.082837793s]
Jul  2 21:49:58.392: INFO: Created: latency-svc-97cdt
Jul  2 21:49:58.399: INFO: Got endpoints: latency-svc-97cdt [2.033745091s]
Jul  2 21:49:59.089: INFO: Created: latency-svc-h6dxx
Jul  2 21:49:59.111: INFO: Created: latency-svc-4br6k
Jul  2 21:49:59.193: INFO: Got endpoints: latency-svc-h6dxx [2.796831627s]
Jul  2 21:49:59.233: INFO: Got endpoints: latency-svc-4br6k [1.241194081s]
Jul  2 21:49:59.241: INFO: Created: latency-svc-4vtgm
Jul  2 21:50:00.987: INFO: Got endpoints: latency-svc-4vtgm [2.838423304s]
Jul  2 21:50:00.991: INFO: Created: latency-svc-9wzcx
Jul  2 21:50:01.059: INFO: Created: latency-svc-jhllf
Jul  2 21:50:01.070: INFO: Got endpoints: latency-svc-9wzcx [2.887178295s]
Jul  2 21:50:01.080: INFO: Got endpoints: latency-svc-jhllf [2.875634335s]
Jul  2 21:50:01.083: INFO: Created: latency-svc-5f8z9
Jul  2 21:50:01.124: INFO: Created: latency-svc-nnb96
Jul  2 21:50:01.139: INFO: Created: latency-svc-7zt79
Jul  2 21:50:01.161: INFO: Got endpoints: latency-svc-5f8z9 [2.938268978s]
Jul  2 21:50:01.175: INFO: Created: latency-svc-vct97
Jul  2 21:50:01.179: INFO: Got endpoints: latency-svc-7zt79 [2.894019189s]
Jul  2 21:50:01.180: INFO: Got endpoints: latency-svc-nnb96 [2.933267046s]
Jul  2 21:50:01.200: INFO: Created: latency-svc-qbfnb
Jul  2 21:50:01.205: INFO: Got endpoints: latency-svc-vct97 [2.91721701s]
Jul  2 21:50:01.211: INFO: Got endpoints: latency-svc-qbfnb [2.895421865s]
Jul  2 21:50:01.228: INFO: Created: latency-svc-7k5b7
Jul  2 21:50:01.235: INFO: Created: latency-svc-6s8wx
Jul  2 21:50:01.243: INFO: Got endpoints: latency-svc-7k5b7 [2.888497127s]
Jul  2 21:50:01.264: INFO: Got endpoints: latency-svc-6s8wx [2.908956626s]
Jul  2 21:50:01.269: INFO: Created: latency-svc-qf9pt
Jul  2 21:50:01.327: INFO: Got endpoints: latency-svc-qf9pt [2.957450135s]
Jul  2 21:50:01.330: INFO: Created: latency-svc-s8gm2
Jul  2 21:50:01.347: INFO: Got endpoints: latency-svc-s8gm2 [2.959571469s]
Jul  2 21:50:01.357: INFO: Created: latency-svc-ztzkl
Jul  2 21:50:01.371: INFO: Created: latency-svc-ssh8c
Jul  2 21:50:01.372: INFO: Got endpoints: latency-svc-ztzkl [2.972644072s]
Jul  2 21:50:01.384: INFO: Got endpoints: latency-svc-ssh8c [2.191144392s]
Jul  2 21:50:01.386: INFO: Created: latency-svc-8m2nh
Jul  2 21:50:01.393: INFO: Got endpoints: latency-svc-8m2nh [2.156175527s]
Jul  2 21:50:01.398: INFO: Created: latency-svc-4ljct
Jul  2 21:50:01.410: INFO: Got endpoints: latency-svc-4ljct [423.045229ms]
Jul  2 21:50:01.412: INFO: Created: latency-svc-jn468
Jul  2 21:50:01.425: INFO: Created: latency-svc-4fv42
Jul  2 21:50:01.426: INFO: Got endpoints: latency-svc-jn468 [355.529053ms]
Jul  2 21:50:01.433: INFO: Got endpoints: latency-svc-4fv42 [351.932888ms]
Jul  2 21:50:01.437: INFO: Created: latency-svc-zwm7f
Jul  2 21:50:01.451: INFO: Created: latency-svc-44bh4
Jul  2 21:50:01.451: INFO: Got endpoints: latency-svc-zwm7f [289.667999ms]
Jul  2 21:50:01.460: INFO: Got endpoints: latency-svc-44bh4 [280.307374ms]
Jul  2 21:50:01.462: INFO: Created: latency-svc-l4x8z
Jul  2 21:50:01.475: INFO: Got endpoints: latency-svc-l4x8z [293.54431ms]
Jul  2 21:50:01.479: INFO: Created: latency-svc-2knm5
Jul  2 21:50:01.481: INFO: Got endpoints: latency-svc-2knm5 [275.639936ms]
Jul  2 21:50:01.487: INFO: Created: latency-svc-q6pb5
Jul  2 21:50:01.499: INFO: Got endpoints: latency-svc-q6pb5 [288.505377ms]
Jul  2 21:50:01.501: INFO: Created: latency-svc-8gj4w
Jul  2 21:50:01.512: INFO: Created: latency-svc-l5bhh
Jul  2 21:50:01.513: INFO: Got endpoints: latency-svc-8gj4w [269.935721ms]
Jul  2 21:50:01.532: INFO: Created: latency-svc-ls7l6
Jul  2 21:50:01.533: INFO: Got endpoints: latency-svc-l5bhh [267.788913ms]
Jul  2 21:50:01.545: INFO: Got endpoints: latency-svc-ls7l6 [217.78094ms]
Jul  2 21:50:01.549: INFO: Created: latency-svc-ggvh2
Jul  2 21:50:01.558: INFO: Created: latency-svc-xvzj5
Jul  2 21:50:01.561: INFO: Got endpoints: latency-svc-ggvh2 [213.597546ms]
Jul  2 21:50:01.569: INFO: Got endpoints: latency-svc-xvzj5 [196.966039ms]
Jul  2 21:50:01.570: INFO: Created: latency-svc-9lxbz
Jul  2 21:50:01.578: INFO: Got endpoints: latency-svc-9lxbz [193.398675ms]
Jul  2 21:50:01.585: INFO: Created: latency-svc-v5nbl
Jul  2 21:50:01.595: INFO: Got endpoints: latency-svc-v5nbl [200.547063ms]
Jul  2 21:50:01.596: INFO: Created: latency-svc-s42jg
Jul  2 21:50:01.605: INFO: Got endpoints: latency-svc-s42jg [194.963732ms]
Jul  2 21:50:01.608: INFO: Created: latency-svc-7w5tm
Jul  2 21:50:01.616: INFO: Created: latency-svc-k4jzc
Jul  2 21:50:01.625: INFO: Got endpoints: latency-svc-7w5tm [198.387923ms]
Jul  2 21:50:01.634: INFO: Got endpoints: latency-svc-k4jzc [200.596378ms]
Jul  2 21:50:01.645: INFO: Created: latency-svc-bdxtq
Jul  2 21:50:01.649: INFO: Got endpoints: latency-svc-bdxtq [198.032363ms]
Jul  2 21:50:01.653: INFO: Created: latency-svc-5464f
Jul  2 21:50:01.665: INFO: Created: latency-svc-dtb4s
Jul  2 21:50:01.672: INFO: Created: latency-svc-dchhz
Jul  2 21:50:01.674: INFO: Got endpoints: latency-svc-5464f [213.770785ms]
Jul  2 21:50:01.678: INFO: Got endpoints: latency-svc-dtb4s [202.839683ms]
Jul  2 21:50:01.685: INFO: Got endpoints: latency-svc-dchhz [203.438787ms]
Jul  2 21:50:01.693: INFO: Created: latency-svc-4xdq9
Jul  2 21:50:01.705: INFO: Created: latency-svc-prspq
Jul  2 21:50:01.716: INFO: Created: latency-svc-nppg6
Jul  2 21:50:01.725: INFO: Got endpoints: latency-svc-4xdq9 [225.354674ms]
Jul  2 21:50:01.734: INFO: Got endpoints: latency-svc-prspq [221.079483ms]
Jul  2 21:50:01.735: INFO: Created: latency-svc-p5vtv
Jul  2 21:50:01.748: INFO: Got endpoints: latency-svc-nppg6 [215.100785ms]
Jul  2 21:50:01.756: INFO: Got endpoints: latency-svc-p5vtv [210.857857ms]
Jul  2 21:50:01.759: INFO: Created: latency-svc-9bdwk
Jul  2 21:50:01.769: INFO: Got endpoints: latency-svc-9bdwk [207.538635ms]
Jul  2 21:50:01.769: INFO: Created: latency-svc-vjdfk
Jul  2 21:50:01.784: INFO: Got endpoints: latency-svc-vjdfk [214.235572ms]
Jul  2 21:50:01.803: INFO: Created: latency-svc-t7sk2
Jul  2 21:50:01.822: INFO: Got endpoints: latency-svc-t7sk2 [243.997849ms]
Jul  2 21:50:01.832: INFO: Created: latency-svc-6fkcw
Jul  2 21:50:01.844: INFO: Created: latency-svc-484s6
Jul  2 21:50:01.853: INFO: Created: latency-svc-4v542
Jul  2 21:50:01.856: INFO: Got endpoints: latency-svc-6fkcw [260.727887ms]
Jul  2 21:50:01.863: INFO: Created: latency-svc-sfkmn
Jul  2 21:50:01.878: INFO: Created: latency-svc-cf28f
Jul  2 21:50:01.887: INFO: Got endpoints: latency-svc-sfkmn [252.325199ms]
Jul  2 21:50:01.887: INFO: Got endpoints: latency-svc-484s6 [282.091538ms]
Jul  2 21:50:01.890: INFO: Got endpoints: latency-svc-4v542 [265.38334ms]
Jul  2 21:50:01.897: INFO: Created: latency-svc-p669b
Jul  2 21:50:01.907: INFO: Created: latency-svc-trtbp
Jul  2 21:50:01.915: INFO: Got endpoints: latency-svc-cf28f [265.751217ms]
Jul  2 21:50:01.916: INFO: Created: latency-svc-b28xs
Jul  2 21:50:01.931: INFO: Created: latency-svc-ml6zx
Jul  2 21:50:01.936: INFO: Created: latency-svc-vkmr7
Jul  2 21:50:01.948: INFO: Created: latency-svc-9j9js
Jul  2 21:50:01.962: INFO: Created: latency-svc-s9mqw
Jul  2 21:50:01.966: INFO: Got endpoints: latency-svc-p669b [291.652504ms]
Jul  2 21:50:01.974: INFO: Created: latency-svc-kmlh9
Jul  2 21:50:01.980: INFO: Created: latency-svc-kw2bb
Jul  2 21:50:01.991: INFO: Created: latency-svc-x2qnp
Jul  2 21:50:01.997: INFO: Created: latency-svc-s8cwl
Jul  2 21:50:02.008: INFO: Created: latency-svc-qbh7b
Jul  2 21:50:02.017: INFO: Got endpoints: latency-svc-trtbp [339.436201ms]
Jul  2 21:50:02.023: INFO: Created: latency-svc-nw4jg
Jul  2 21:50:02.031: INFO: Created: latency-svc-qp87c
Jul  2 21:50:02.044: INFO: Created: latency-svc-p5fpp
Jul  2 21:50:02.063: INFO: Created: latency-svc-c5xtr
Jul  2 21:50:02.068: INFO: Created: latency-svc-kbllx
Jul  2 21:50:02.071: INFO: Got endpoints: latency-svc-b28xs [386.005066ms]
Jul  2 21:50:02.084: INFO: Created: latency-svc-bc6hl
Jul  2 21:50:02.115: INFO: Got endpoints: latency-svc-ml6zx [389.929837ms]
Jul  2 21:50:02.139: INFO: Created: latency-svc-5fxcd
Jul  2 21:50:02.163: INFO: Got endpoints: latency-svc-vkmr7 [428.448349ms]
Jul  2 21:50:02.178: INFO: Created: latency-svc-2kmj4
Jul  2 21:50:02.214: INFO: Got endpoints: latency-svc-9j9js [463.249592ms]
Jul  2 21:50:02.228: INFO: Created: latency-svc-hhb7l
Jul  2 21:50:02.263: INFO: Got endpoints: latency-svc-s9mqw [507.69167ms]
Jul  2 21:50:02.286: INFO: Created: latency-svc-lf9rh
Jul  2 21:50:02.315: INFO: Got endpoints: latency-svc-kmlh9 [545.554106ms]
Jul  2 21:50:02.333: INFO: Created: latency-svc-t8xh7
Jul  2 21:50:02.363: INFO: Got endpoints: latency-svc-kw2bb [579.070645ms]
Jul  2 21:50:02.376: INFO: Created: latency-svc-hfh2w
Jul  2 21:50:02.413: INFO: Got endpoints: latency-svc-x2qnp [590.795097ms]
Jul  2 21:50:02.432: INFO: Created: latency-svc-tnsj4
Jul  2 21:50:02.463: INFO: Got endpoints: latency-svc-s8cwl [607.516706ms]
Jul  2 21:50:02.482: INFO: Created: latency-svc-dbpjr
Jul  2 21:50:02.513: INFO: Got endpoints: latency-svc-qbh7b [625.801246ms]
Jul  2 21:50:02.531: INFO: Created: latency-svc-44dtn
Jul  2 21:50:02.565: INFO: Got endpoints: latency-svc-nw4jg [677.70978ms]
Jul  2 21:50:02.579: INFO: Created: latency-svc-v82gb
Jul  2 21:50:02.613: INFO: Got endpoints: latency-svc-qp87c [722.256183ms]
Jul  2 21:50:02.628: INFO: Created: latency-svc-zzcms
Jul  2 21:50:02.662: INFO: Got endpoints: latency-svc-p5fpp [746.762208ms]
Jul  2 21:50:02.675: INFO: Created: latency-svc-9pnzw
Jul  2 21:50:02.721: INFO: Got endpoints: latency-svc-c5xtr [755.513442ms]
Jul  2 21:50:02.744: INFO: Created: latency-svc-gv9bt
Jul  2 21:50:02.769: INFO: Got endpoints: latency-svc-kbllx [751.402215ms]
Jul  2 21:50:02.785: INFO: Created: latency-svc-zbt4j
Jul  2 21:50:02.813: INFO: Got endpoints: latency-svc-bc6hl [741.84983ms]
Jul  2 21:50:02.831: INFO: Created: latency-svc-cdc6n
Jul  2 21:50:02.865: INFO: Got endpoints: latency-svc-5fxcd [749.337086ms]
Jul  2 21:50:02.886: INFO: Created: latency-svc-2sglc
Jul  2 21:50:02.916: INFO: Got endpoints: latency-svc-2kmj4 [753.101016ms]
Jul  2 21:50:02.931: INFO: Created: latency-svc-l2q7s
Jul  2 21:50:02.965: INFO: Got endpoints: latency-svc-hhb7l [751.333801ms]
Jul  2 21:50:02.982: INFO: Created: latency-svc-65dv8
Jul  2 21:50:03.015: INFO: Got endpoints: latency-svc-lf9rh [752.03621ms]
Jul  2 21:50:03.030: INFO: Created: latency-svc-2ln9t
Jul  2 21:50:03.065: INFO: Got endpoints: latency-svc-t8xh7 [749.928976ms]
Jul  2 21:50:03.664: INFO: Got endpoints: latency-svc-hfh2w [1.301369784s]
Jul  2 21:50:03.667: INFO: Created: latency-svc-28f58
Jul  2 21:50:03.670: INFO: Got endpoints: latency-svc-tnsj4 [1.256969659s]
Jul  2 21:50:03.670: INFO: Got endpoints: latency-svc-44dtn [1.157092038s]
Jul  2 21:50:03.670: INFO: Got endpoints: latency-svc-dbpjr [1.207094899s]
Jul  2 21:50:03.673: INFO: Got endpoints: latency-svc-v82gb [1.107857546s]
Jul  2 21:50:03.705: INFO: Got endpoints: latency-svc-zzcms [1.092364748s]
Jul  2 21:50:03.710: INFO: Created: latency-svc-bmwdr
Jul  2 21:50:03.715: INFO: Got endpoints: latency-svc-gv9bt [992.925483ms]
Jul  2 21:50:03.716: INFO: Got endpoints: latency-svc-9pnzw [1.053274403s]
Jul  2 21:50:03.719: INFO: Got endpoints: latency-svc-cdc6n [905.417451ms]
Jul  2 21:50:03.722: INFO: Got endpoints: latency-svc-zbt4j [952.717368ms]
Jul  2 21:50:04.103: INFO: Got endpoints: latency-svc-2sglc [1.238470203s]
Jul  2 21:50:04.104: INFO: Created: latency-svc-x5xwr
Jul  2 21:50:04.105: INFO: Got endpoints: latency-svc-l2q7s [1.188972353s]
Jul  2 21:50:04.107: INFO: Got endpoints: latency-svc-65dv8 [1.142083423s]
Jul  2 21:50:04.411: INFO: Got endpoints: latency-svc-2ln9t [1.395743078s]
Jul  2 21:50:04.413: INFO: Got endpoints: latency-svc-28f58 [1.348284944s]
Jul  2 21:50:04.421: INFO: Got endpoints: latency-svc-bmwdr [747.961347ms]
Jul  2 21:50:04.425: INFO: Got endpoints: latency-svc-x5xwr [760.20954ms]
Jul  2 21:50:04.430: INFO: Created: latency-svc-4v2bx
Jul  2 21:50:04.727: INFO: Got endpoints: latency-svc-4v2bx [1.052817378s]
Jul  2 21:50:04.736: INFO: Created: latency-svc-cjk5q
Jul  2 21:50:04.741: INFO: Got endpoints: latency-svc-cjk5q [1.063451248s]
Jul  2 21:50:04.758: INFO: Created: latency-svc-hvqz4
Jul  2 21:50:04.782: INFO: Created: latency-svc-5kfl5
Jul  2 21:50:04.783: INFO: Got endpoints: latency-svc-hvqz4 [1.107212648s]
Jul  2 21:50:04.807: INFO: Created: latency-svc-6n79l
Jul  2 21:50:04.821: INFO: Got endpoints: latency-svc-5kfl5 [1.115189386s]
Jul  2 21:50:04.829: INFO: Got endpoints: latency-svc-6n79l [1.113996432s]
Jul  2 21:50:04.838: INFO: Created: latency-svc-s6m7x
Jul  2 21:50:04.876: INFO: Got endpoints: latency-svc-s6m7x [1.15999934s]
Jul  2 21:50:04.886: INFO: Created: latency-svc-zd46t
Jul  2 21:50:04.902: INFO: Got endpoints: latency-svc-zd46t [1.18343023s]
Jul  2 21:50:04.903: INFO: Created: latency-svc-bhmvq
Jul  2 21:50:04.910: INFO: Created: latency-svc-l4vsx
Jul  2 21:50:04.910: INFO: Got endpoints: latency-svc-bhmvq [1.188535285s]
Jul  2 21:50:04.961: INFO: Created: latency-svc-wl97s
Jul  2 21:50:04.961: INFO: Created: latency-svc-8xdbv
Jul  2 21:50:04.962: INFO: Got endpoints: latency-svc-l4vsx [858.640655ms]
Jul  2 21:50:04.965: INFO: Created: latency-svc-qz6k8
Jul  2 21:50:04.981: INFO: Created: latency-svc-27xr7
Jul  2 21:50:04.985: INFO: Got endpoints: latency-svc-8xdbv [879.634036ms]
Jul  2 21:50:05.001: INFO: Created: latency-svc-zb8bl
Jul  2 21:50:05.007: INFO: Got endpoints: latency-svc-qz6k8 [899.687478ms]
Jul  2 21:50:05.010: INFO: Got endpoints: latency-svc-wl97s [599.012169ms]
Jul  2 21:50:05.014: INFO: Created: latency-svc-t8bcb
Jul  2 21:50:05.021: INFO: Got endpoints: latency-svc-27xr7 [607.534032ms]
Jul  2 21:50:05.021: INFO: Created: latency-svc-nrfmq
Jul  2 21:50:05.045: INFO: Created: latency-svc-2jbz8
Jul  2 21:50:05.045: INFO: Got endpoints: latency-svc-t8bcb [619.59171ms]
Jul  2 21:50:05.045: INFO: Got endpoints: latency-svc-zb8bl [624.321559ms]
Jul  2 21:50:05.056: INFO: Got endpoints: latency-svc-2jbz8 [314.516527ms]
Jul  2 21:50:05.056: INFO: Got endpoints: latency-svc-nrfmq [329.03264ms]
Jul  2 21:50:05.061: INFO: Created: latency-svc-kqf6d
Jul  2 21:50:05.585: INFO: Got endpoints: latency-svc-kqf6d [801.324879ms]
Jul  2 21:50:05.591: INFO: Created: latency-svc-8mtbz
Jul  2 21:50:05.607: INFO: Created: latency-svc-bpvj9
Jul  2 21:50:05.621: INFO: Created: latency-svc-rg5h6
Jul  2 21:50:05.637: INFO: Got endpoints: latency-svc-bpvj9 [808.689288ms]
Jul  2 21:50:05.639: INFO: Got endpoints: latency-svc-8mtbz [818.213291ms]
Jul  2 21:50:06.452: INFO: Got endpoints: latency-svc-rg5h6 [1.57571224s]
Jul  2 21:50:06.456: INFO: Created: latency-svc-f8n7n
Jul  2 21:50:06.476: INFO: Got endpoints: latency-svc-f8n7n [1.573032914s]
Jul  2 21:50:07.370: INFO: Created: latency-svc-gg6wb
Jul  2 21:50:07.375: INFO: Got endpoints: latency-svc-gg6wb [2.464407448s]
Jul  2 21:50:07.426: INFO: Created: latency-svc-lflnl
Jul  2 21:50:07.447: INFO: Got endpoints: latency-svc-lflnl [2.484863984s]
Jul  2 21:50:07.469: INFO: Created: latency-svc-vj742
Jul  2 21:50:07.494: INFO: Got endpoints: latency-svc-vj742 [2.508354034s]
Jul  2 21:50:07.520: INFO: Created: latency-svc-ctngv
Jul  2 21:50:07.549: INFO: Got endpoints: latency-svc-ctngv [2.542106858s]
Jul  2 21:50:07.557: INFO: Created: latency-svc-l64t2
Jul  2 21:50:07.566: INFO: Got endpoints: latency-svc-l64t2 [2.55582524s]
Jul  2 21:50:07.584: INFO: Created: latency-svc-tvs6t
Jul  2 21:50:07.588: INFO: Got endpoints: latency-svc-tvs6t [2.567432518s]
Jul  2 21:50:07.598: INFO: Created: latency-svc-k4c97
Jul  2 21:50:07.606: INFO: Got endpoints: latency-svc-k4c97 [2.561486147s]
Jul  2 21:50:07.611: INFO: Created: latency-svc-555kq
Jul  2 21:50:07.622: INFO: Created: latency-svc-gch8v
Jul  2 21:50:07.629: INFO: Got endpoints: latency-svc-555kq [2.583711711s]
Jul  2 21:50:07.639: INFO: Created: latency-svc-msr5c
Jul  2 21:50:07.644: INFO: Got endpoints: latency-svc-gch8v [2.587896767s]
Jul  2 21:50:07.677: INFO: Got endpoints: latency-svc-msr5c [2.620349851s]
Jul  2 21:50:07.678: INFO: Created: latency-svc-v4jgg
Jul  2 21:50:07.678: INFO: Got endpoints: latency-svc-v4jgg [2.093279914s]
Jul  2 21:50:07.694: INFO: Created: latency-svc-pxrwf
Jul  2 21:50:07.694: INFO: Created: latency-svc-hxqkn
Jul  2 21:50:07.704: INFO: Got endpoints: latency-svc-pxrwf [2.064671299s]
Jul  2 21:50:07.704: INFO: Got endpoints: latency-svc-hxqkn [2.065811069s]
Jul  2 21:50:07.706: INFO: Created: latency-svc-fj8pr
Jul  2 21:50:07.724: INFO: Created: latency-svc-k5q5t
Jul  2 21:50:07.726: INFO: Got endpoints: latency-svc-fj8pr [1.273400904s]
Jul  2 21:50:07.731: INFO: Got endpoints: latency-svc-k5q5t [1.255671588s]
Jul  2 21:50:07.746: INFO: Created: latency-svc-d9c8p
Jul  2 21:50:07.746: INFO: Got endpoints: latency-svc-d9c8p [371.06695ms]
Jul  2 21:50:07.753: INFO: Created: latency-svc-9h5tc
Jul  2 21:50:07.759: INFO: Got endpoints: latency-svc-9h5tc [311.807847ms]
Jul  2 21:50:07.779: INFO: Created: latency-svc-fb2zf
Jul  2 21:50:07.787: INFO: Got endpoints: latency-svc-fb2zf [293.426324ms]
Jul  2 21:50:07.796: INFO: Created: latency-svc-8lptv
Jul  2 21:50:07.802: INFO: Got endpoints: latency-svc-8lptv [252.365604ms]
Jul  2 21:50:07.813: INFO: Created: latency-svc-vggzs
Jul  2 21:50:07.825: INFO: Created: latency-svc-8p7t8
Jul  2 21:50:07.833: INFO: Got endpoints: latency-svc-vggzs [265.743974ms]
Jul  2 21:50:07.840: INFO: Got endpoints: latency-svc-8p7t8 [249.013743ms]
Jul  2 21:50:07.847: INFO: Created: latency-svc-nwdbn
Jul  2 21:50:07.852: INFO: Got endpoints: latency-svc-nwdbn [243.204561ms]
Jul  2 21:50:07.872: INFO: Created: latency-svc-d97zn
Jul  2 21:50:07.877: INFO: Created: latency-svc-hhcf8
Jul  2 21:50:07.881: INFO: Created: latency-svc-7g7sl
Jul  2 21:50:07.885: INFO: Got endpoints: latency-svc-hhcf8 [255.959337ms]
Jul  2 21:50:07.887: INFO: Got endpoints: latency-svc-d97zn [243.720556ms]
Jul  2 21:50:07.896: INFO: Got endpoints: latency-svc-7g7sl [218.722386ms]
Jul  2 21:50:07.906: INFO: Created: latency-svc-b28q9
Jul  2 21:50:07.908: INFO: Got endpoints: latency-svc-b28q9 [229.866469ms]
Jul  2 21:50:07.911: INFO: Created: latency-svc-x8zfm
Jul  2 21:50:07.922: INFO: Got endpoints: latency-svc-x8zfm [217.607445ms]
Jul  2 21:50:07.923: INFO: Created: latency-svc-h8btp
Jul  2 21:50:07.932: INFO: Got endpoints: latency-svc-h8btp [227.516442ms]
Jul  2 21:50:07.932: INFO: Created: latency-svc-4pxqv
Jul  2 21:50:07.941: INFO: Got endpoints: latency-svc-4pxqv [215.296308ms]
Jul  2 21:50:07.943: INFO: Created: latency-svc-6pgfb
Jul  2 21:50:07.952: INFO: Got endpoints: latency-svc-6pgfb [220.189607ms]
Jul  2 21:50:07.957: INFO: Created: latency-svc-qxzlf
Jul  2 21:50:07.966: INFO: Got endpoints: latency-svc-qxzlf [219.468364ms]
Jul  2 21:50:07.975: INFO: Created: latency-svc-hrtlv
Jul  2 21:50:07.984: INFO: Created: latency-svc-thszd
Jul  2 21:50:07.985: INFO: Got endpoints: latency-svc-hrtlv [225.889813ms]
Jul  2 21:50:07.998: INFO: Got endpoints: latency-svc-thszd [210.332507ms]
Jul  2 21:50:08.004: INFO: Created: latency-svc-25hz2
Jul  2 21:50:08.012: INFO: Got endpoints: latency-svc-25hz2 [209.833201ms]
Jul  2 21:50:08.021: INFO: Created: latency-svc-df4sv
Jul  2 21:50:08.024: INFO: Got endpoints: latency-svc-df4sv [190.748122ms]
Jul  2 21:50:08.032: INFO: Created: latency-svc-65pk2
Jul  2 21:50:08.040: INFO: Got endpoints: latency-svc-65pk2 [199.374563ms]
Jul  2 21:50:08.047: INFO: Created: latency-svc-p26rx
Jul  2 21:50:08.054: INFO: Got endpoints: latency-svc-p26rx [201.153047ms]
Jul  2 21:50:08.057: INFO: Created: latency-svc-vcd2l
Jul  2 21:50:08.072: INFO: Created: latency-svc-jsf4z
Jul  2 21:50:08.077: INFO: Got endpoints: latency-svc-vcd2l [190.93607ms]
Jul  2 21:50:08.088: INFO: Created: latency-svc-wtcln
Jul  2 21:50:08.091: INFO: Got endpoints: latency-svc-jsf4z [203.236929ms]
Jul  2 21:50:08.108: INFO: Created: latency-svc-c542q
Jul  2 21:50:08.108: INFO: Got endpoints: latency-svc-wtcln [212.447326ms]
Jul  2 21:50:08.124: INFO: Created: latency-svc-mvxk2
Jul  2 21:50:08.124: INFO: Got endpoints: latency-svc-c542q [215.894846ms]
Jul  2 21:50:08.128: INFO: Got endpoints: latency-svc-mvxk2 [205.405572ms]
Jul  2 21:50:08.148: INFO: Created: latency-svc-p2cb2
Jul  2 21:50:08.162: INFO: Got endpoints: latency-svc-p2cb2 [229.856491ms]
Jul  2 21:50:08.162: INFO: Created: latency-svc-l2jzs
Jul  2 21:50:08.176: INFO: Got endpoints: latency-svc-l2jzs [234.803938ms]
Jul  2 21:50:08.186: INFO: Created: latency-svc-4n2vs
Jul  2 21:50:08.196: INFO: Got endpoints: latency-svc-4n2vs [244.236939ms]
Jul  2 21:50:08.200: INFO: Created: latency-svc-pgnvl
Jul  2 21:50:08.212: INFO: Got endpoints: latency-svc-pgnvl [245.837922ms]
Jul  2 21:50:08.212: INFO: Created: latency-svc-2jrrk
Jul  2 21:50:08.222: INFO: Created: latency-svc-jjwlp
Jul  2 21:50:08.231: INFO: Created: latency-svc-dk9jg
Jul  2 21:50:08.240: INFO: Got endpoints: latency-svc-2jrrk [254.355756ms]
Jul  2 21:50:08.272: INFO: Created: latency-svc-p2sp7
Jul  2 21:50:08.283: INFO: Got endpoints: latency-svc-jjwlp [284.97593ms]
Jul  2 21:50:08.300: INFO: Created: latency-svc-fg6kl
Jul  2 21:50:08.301: INFO: Created: latency-svc-qmqbm
Jul  2 21:50:08.314: INFO: Created: latency-svc-88vct
Jul  2 21:50:08.321: INFO: Created: latency-svc-nt6ks
Jul  2 21:50:08.499: INFO: Got endpoints: latency-svc-dk9jg [487.088634ms]
Jul  2 21:50:09.334: INFO: Created: latency-svc-gjqjg
Jul  2 21:50:09.542: INFO: Got endpoints: latency-svc-p2sp7 [1.517553038s]
Jul  2 21:50:09.551: INFO: Got endpoints: latency-svc-fg6kl [1.511366434s]
Jul  2 21:50:09.559: INFO: Got endpoints: latency-svc-qmqbm [1.505472816s]
Jul  2 21:50:09.563: INFO: Got endpoints: latency-svc-nt6ks [1.472729953s]
Jul  2 21:50:09.565: INFO: Got endpoints: latency-svc-88vct [1.487727001s]
Jul  2 21:50:09.565: INFO: Created: latency-svc-nqwx5
Jul  2 21:50:09.578: INFO: Got endpoints: latency-svc-gjqjg [1.469313903s]
Jul  2 21:50:09.604: INFO: Created: latency-svc-mppvq
Jul  2 21:50:09.608: INFO: Got endpoints: latency-svc-nqwx5 [1.484093818s]
Jul  2 21:50:10.494: INFO: Created: latency-svc-mfx9j
Jul  2 21:50:10.500: INFO: Got endpoints: latency-svc-mppvq [2.372284158s]
Jul  2 21:50:10.529: INFO: Got endpoints: latency-svc-mfx9j [2.366888388s]
Jul  2 21:50:10.529: INFO: Latencies: [40.682163ms 69.816299ms 84.925317ms 118.218808ms 122.146925ms 142.631336ms 190.748122ms 190.93607ms 193.398675ms 194.963732ms 196.966039ms 198.032363ms 198.387923ms 199.374563ms 200.547063ms 200.596378ms 201.153047ms 202.839683ms 203.236929ms 203.438787ms 205.405572ms 207.538635ms 209.833201ms 210.332507ms 210.857857ms 212.447326ms 213.597546ms 213.770785ms 214.235572ms 215.100785ms 215.296308ms 215.894846ms 217.607445ms 217.78094ms 218.722386ms 219.468364ms 220.189607ms 221.079483ms 225.354674ms 225.889813ms 227.516442ms 229.856491ms 229.866469ms 234.803938ms 243.204561ms 243.720556ms 243.997849ms 244.236939ms 245.837922ms 249.013743ms 252.325199ms 252.365604ms 254.355756ms 255.959337ms 260.727887ms 265.38334ms 265.743974ms 265.751217ms 267.788913ms 269.935721ms 275.639936ms 280.307374ms 282.091538ms 284.97593ms 288.505377ms 289.667999ms 291.652504ms 293.426324ms 293.54431ms 311.807847ms 314.516527ms 329.03264ms 339.436201ms 351.932888ms 355.529053ms 371.06695ms 386.005066ms 389.929837ms 423.045229ms 428.448349ms 463.249592ms 487.088634ms 507.69167ms 545.554106ms 579.070645ms 590.795097ms 599.012169ms 607.516706ms 607.534032ms 619.59171ms 624.321559ms 625.801246ms 677.70978ms 722.256183ms 741.84983ms 746.762208ms 747.961347ms 749.337086ms 749.928976ms 751.333801ms 751.402215ms 752.03621ms 753.101016ms 755.513442ms 760.20954ms 801.324879ms 808.689288ms 818.213291ms 858.640655ms 879.634036ms 899.687478ms 905.417451ms 952.717368ms 992.925483ms 1.052817378s 1.053274403s 1.063451248s 1.092364748s 1.107212648s 1.107857546s 1.113996432s 1.115189386s 1.142083423s 1.157092038s 1.15999934s 1.18343023s 1.188535285s 1.188972353s 1.207094899s 1.238470203s 1.241194081s 1.255671588s 1.256969659s 1.273400904s 1.301369784s 1.348284944s 1.395743078s 1.469313903s 1.472729953s 1.484093818s 1.487727001s 1.505472816s 1.511366434s 1.517553038s 1.573032914s 1.57571224s 1.610417071s 1.659699802s 1.702016488s 1.979647308s 2.033745091s 2.064671299s 2.065811069s 2.082837793s 2.093279914s 2.156175527s 2.191144392s 2.242035423s 2.350006735s 2.366888388s 2.372284158s 2.464407448s 2.484863984s 2.508354034s 2.542106858s 2.55582524s 2.561486147s 2.567432518s 2.583711711s 2.587896767s 2.620349851s 2.796831627s 2.838423304s 2.875634335s 2.887178295s 2.888497127s 2.894019189s 2.895421865s 2.908956626s 2.91721701s 2.933267046s 2.938268978s 2.957450135s 2.959571469s 2.972644072s 4.039411381s 4.161068801s 4.338724322s 4.397943332s 4.408429672s 4.429664531s 4.647322444s 4.661005717s 4.709171812s 5.986459079s 6.113064013s 6.119655987s 6.128884705s 6.135132514s 6.137241957s]
Jul  2 21:50:10.531: INFO: 50 %ile: 751.402215ms
Jul  2 21:50:10.531: INFO: 90 %ile: 2.933267046s
Jul  2 21:50:10.531: INFO: 99 %ile: 6.135132514s
Jul  2 21:50:10.531: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:50:10.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8406" for this suite.
Jul  2 21:51:10.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:51:10.749: INFO: namespace svc-latency-8406 deletion completed in 1m0.203396678s

â€¢ [SLOW TEST:82.050 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  2 21:51:10.751: INFO: >>> kubeConfig: /tmp/kubeconfig-362853403
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  2 21:51:10.881: INFO: Waiting up to 5m0s for pod "downwardapi-volume-812bcaa5-9d13-11e9-808f-7ea354b18644" in namespace "projected-6385" to be "success or failure"
Jul  2 21:51:10.886: INFO: Pod "downwardapi-volume-812bcaa5-9d13-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 5.402303ms
Jul  2 21:51:14.794: INFO: Pod "downwardapi-volume-812bcaa5-9d13-11e9-808f-7ea354b18644": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91309243s
Jul  2 21:51:17.244: INFO: Pod "downwardapi-volume-812bcaa5-9d13-11e9-808f-7ea354b18644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.363196809s
STEP: Saw pod success
Jul  2 21:51:17.244: INFO: Pod "downwardapi-volume-812bcaa5-9d13-11e9-808f-7ea354b18644" satisfied condition "success or failure"
Jul  2 21:51:17.250: INFO: Trying to get logs from node ah-kres-worker-2 pod downwardapi-volume-812bcaa5-9d13-11e9-808f-7ea354b18644 container client-container: <nil>
STEP: delete the pod
Jul  2 21:51:22.240: INFO: Waiting for pod downwardapi-volume-812bcaa5-9d13-11e9-808f-7ea354b18644 to disappear
Jul  2 21:51:22.253: INFO: Pod downwardapi-volume-812bcaa5-9d13-11e9-808f-7ea354b18644 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  2 21:51:22.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6385" for this suite.
Jul  2 21:51:32.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  2 21:51:32.496: INFO: namespace projected-6385 deletion completed in 10.229895422s

â€¢ [SLOW TEST:21.745 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSJul  2 21:51:32.496: INFO: Running AfterSuite actions on all nodes
Jul  2 21:51:32.496: INFO: Running AfterSuite actions on node 1
Jul  2 21:51:32.496: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 8516.378 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 2h21m58.777131851s
Test Suite Passed
