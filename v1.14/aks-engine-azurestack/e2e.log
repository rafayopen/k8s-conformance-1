I0531 18:11:53.521832      15 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-130611839
I0531 18:11:53.521970      15 e2e.go:240] Starting e2e run "90d4ac92-83cf-11e9-a3c9-f265bfc55a07" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1559326312 - Will randomize all specs
Will run 204 of 3585 specs

May 31 18:11:53.751: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:11:53.753: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 31 18:11:53.784: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 31 18:11:53.818: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 31 18:11:53.818: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
May 31 18:11:53.818: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 31 18:11:53.837: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
May 31 18:11:53.837: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
May 31 18:11:53.837: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 31 18:11:53.837: INFO: e2e test version: v1.14.2
May 31 18:11:53.838: INFO: kube-apiserver version: v1.14.2
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:11:53.838: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename dns
May 31 18:11:53.880: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4049.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4049.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4049.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4049.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4049.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4049.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4049.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 95.142.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.142.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.142.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.142.95_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4049.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4049.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4049.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4049.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4049.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4049.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4049.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4049.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4049.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 95.142.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.142.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.142.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.142.95_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 18:12:16.153: INFO: Unable to read wheezy_udp@dns-test-service.dns-4049.svc.cluster.local from pod dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07: the server could not find the requested resource (get pods dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07)
May 31 18:12:16.155: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4049.svc.cluster.local from pod dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07: the server could not find the requested resource (get pods dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07)
May 31 18:12:16.158: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local from pod dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07: the server could not find the requested resource (get pods dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07)
May 31 18:12:16.160: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local from pod dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07: the server could not find the requested resource (get pods dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07)
May 31 18:12:16.177: INFO: Unable to read jessie_udp@dns-test-service.dns-4049.svc.cluster.local from pod dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07: the server could not find the requested resource (get pods dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07)
May 31 18:12:16.180: INFO: Unable to read jessie_tcp@dns-test-service.dns-4049.svc.cluster.local from pod dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07: the server could not find the requested resource (get pods dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07)
May 31 18:12:16.183: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local from pod dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07: the server could not find the requested resource (get pods dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07)
May 31 18:12:16.186: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local from pod dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07: the server could not find the requested resource (get pods dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07)
May 31 18:12:16.203: INFO: Lookups using dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07 failed for: [wheezy_udp@dns-test-service.dns-4049.svc.cluster.local wheezy_tcp@dns-test-service.dns-4049.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local jessie_udp@dns-test-service.dns-4049.svc.cluster.local jessie_tcp@dns-test-service.dns-4049.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4049.svc.cluster.local]

May 31 18:12:21.265: INFO: DNS probes using dns-4049/dns-test-91d3437b-83cf-11e9-a3c9-f265bfc55a07 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:12:21.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4049" for this suite.
May 31 18:12:27.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:12:27.620: INFO: namespace dns-4049 deletion completed in 6.172355507s

• [SLOW TEST:33.782 seconds]
[sig-network] DNS
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:12:27.620: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-a5fb920b-83cf-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 18:12:27.781: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07" in namespace "projected-9629" to be "success or failure"
May 31 18:12:27.834: INFO: Pod "pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 52.271576ms
May 31 18:12:29.837: INFO: Pod "pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055761943s
May 31 18:12:31.840: INFO: Pod "pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058714503s
May 31 18:12:33.844: INFO: Pod "pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062111872s
May 31 18:12:35.897: INFO: Pod "pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115163173s
May 31 18:12:37.900: INFO: Pod "pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.118443942s
STEP: Saw pod success
May 31 18:12:37.900: INFO: Pod "pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:12:37.902: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 18:12:37.952: INFO: Waiting for pod pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:12:37.954: INFO: Pod pod-projected-configmaps-a5fc0ed1-83cf-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:12:37.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9629" for this suite.
May 31 18:12:43.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:12:44.080: INFO: namespace projected-9629 deletion completed in 6.122067895s

• [SLOW TEST:16.460 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:12:44.080: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 18:12:44.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-4588'
May 31 18:12:44.579: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 31 18:12:44.579: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
May 31 18:12:46.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4588'
May 31 18:12:46.737: INFO: stderr: ""
May 31 18:12:46.737: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:12:46.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4588" for this suite.
May 31 18:12:52.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:12:52.839: INFO: namespace kubectl-4588 deletion completed in 6.096071674s

• [SLOW TEST:8.759 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:12:52.839: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 31 18:12:57.468: INFO: Successfully updated pod "labelsupdateb4f4c85b-83cf-11e9-a3c9-f265bfc55a07"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:12:59.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8323" for this suite.
May 31 18:13:21.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:13:21.631: INFO: namespace downward-api-8323 deletion completed in 22.122223798s

• [SLOW TEST:28.791 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:13:21.631: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 31 18:13:21.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-372'
May 31 18:13:22.000: INFO: stderr: ""
May 31 18:13:22.000: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 18:13:22.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-372'
May 31 18:13:22.119: INFO: stderr: ""
May 31 18:13:22.119: INFO: stdout: "update-demo-nautilus-7psgx update-demo-nautilus-j57wk "
May 31 18:13:22.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-7psgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:22.206: INFO: stderr: ""
May 31 18:13:22.206: INFO: stdout: ""
May 31 18:13:22.206: INFO: update-demo-nautilus-7psgx is created but not running
May 31 18:13:27.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-372'
May 31 18:13:27.310: INFO: stderr: ""
May 31 18:13:27.310: INFO: stdout: "update-demo-nautilus-7psgx update-demo-nautilus-j57wk "
May 31 18:13:27.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-7psgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:27.403: INFO: stderr: ""
May 31 18:13:27.404: INFO: stdout: "true"
May 31 18:13:27.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-7psgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:27.505: INFO: stderr: ""
May 31 18:13:27.505: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 18:13:27.505: INFO: validating pod update-demo-nautilus-7psgx
May 31 18:13:27.510: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 18:13:27.510: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 18:13:27.510: INFO: update-demo-nautilus-7psgx is verified up and running
May 31 18:13:27.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-j57wk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:27.589: INFO: stderr: ""
May 31 18:13:27.589: INFO: stdout: "true"
May 31 18:13:27.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-j57wk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:27.686: INFO: stderr: ""
May 31 18:13:27.686: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 18:13:27.686: INFO: validating pod update-demo-nautilus-j57wk
May 31 18:13:27.690: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 18:13:27.690: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 18:13:27.690: INFO: update-demo-nautilus-j57wk is verified up and running
STEP: scaling down the replication controller
May 31 18:13:27.692: INFO: scanned /root for discovery docs: <nil>
May 31 18:13:27.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-372'
May 31 18:13:28.801: INFO: stderr: ""
May 31 18:13:28.801: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 18:13:28.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-372'
May 31 18:13:28.906: INFO: stderr: ""
May 31 18:13:28.906: INFO: stdout: "update-demo-nautilus-7psgx update-demo-nautilus-j57wk "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 31 18:13:33.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-372'
May 31 18:13:33.993: INFO: stderr: ""
May 31 18:13:33.993: INFO: stdout: "update-demo-nautilus-7psgx update-demo-nautilus-j57wk "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 31 18:13:38.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-372'
May 31 18:13:39.086: INFO: stderr: ""
May 31 18:13:39.086: INFO: stdout: "update-demo-nautilus-7psgx "
May 31 18:13:39.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-7psgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:39.180: INFO: stderr: ""
May 31 18:13:39.180: INFO: stdout: "true"
May 31 18:13:39.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-7psgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:39.267: INFO: stderr: ""
May 31 18:13:39.267: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 18:13:39.267: INFO: validating pod update-demo-nautilus-7psgx
May 31 18:13:39.272: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 18:13:39.272: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 18:13:39.272: INFO: update-demo-nautilus-7psgx is verified up and running
STEP: scaling up the replication controller
May 31 18:13:39.273: INFO: scanned /root for discovery docs: <nil>
May 31 18:13:39.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-372'
May 31 18:13:40.404: INFO: stderr: ""
May 31 18:13:40.404: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 18:13:40.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-372'
May 31 18:13:40.491: INFO: stderr: ""
May 31 18:13:40.491: INFO: stdout: "update-demo-nautilus-7psgx update-demo-nautilus-cncz4 "
May 31 18:13:40.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-7psgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:40.587: INFO: stderr: ""
May 31 18:13:40.587: INFO: stdout: "true"
May 31 18:13:40.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-7psgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:40.673: INFO: stderr: ""
May 31 18:13:40.673: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 18:13:40.673: INFO: validating pod update-demo-nautilus-7psgx
May 31 18:13:40.677: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 18:13:40.677: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 18:13:40.677: INFO: update-demo-nautilus-7psgx is verified up and running
May 31 18:13:40.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-cncz4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:40.775: INFO: stderr: ""
May 31 18:13:40.775: INFO: stdout: ""
May 31 18:13:40.775: INFO: update-demo-nautilus-cncz4 is created but not running
May 31 18:13:45.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-372'
May 31 18:13:45.885: INFO: stderr: ""
May 31 18:13:45.885: INFO: stdout: "update-demo-nautilus-7psgx update-demo-nautilus-cncz4 "
May 31 18:13:45.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-7psgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:45.964: INFO: stderr: ""
May 31 18:13:45.964: INFO: stdout: "true"
May 31 18:13:45.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-7psgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:46.040: INFO: stderr: ""
May 31 18:13:46.040: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 18:13:46.041: INFO: validating pod update-demo-nautilus-7psgx
May 31 18:13:46.043: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 18:13:46.043: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 18:13:46.043: INFO: update-demo-nautilus-7psgx is verified up and running
May 31 18:13:46.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-cncz4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:46.127: INFO: stderr: ""
May 31 18:13:46.127: INFO: stdout: "true"
May 31 18:13:46.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-cncz4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-372'
May 31 18:13:46.204: INFO: stderr: ""
May 31 18:13:46.204: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 18:13:46.204: INFO: validating pod update-demo-nautilus-cncz4
May 31 18:13:46.208: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 18:13:46.208: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 18:13:46.208: INFO: update-demo-nautilus-cncz4 is verified up and running
STEP: using delete to clean up resources
May 31 18:13:46.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-372'
May 31 18:13:46.305: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 18:13:46.305: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 31 18:13:46.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-372'
May 31 18:13:46.404: INFO: stderr: "No resources found.\n"
May 31 18:13:46.404: INFO: stdout: ""
May 31 18:13:46.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -l name=update-demo --namespace=kubectl-372 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 18:13:46.488: INFO: stderr: ""
May 31 18:13:46.488: INFO: stdout: "update-demo-nautilus-7psgx\nupdate-demo-nautilus-cncz4\n"
May 31 18:13:46.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-372'
May 31 18:13:47.106: INFO: stderr: "No resources found.\n"
May 31 18:13:47.106: INFO: stdout: ""
May 31 18:13:47.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -l name=update-demo --namespace=kubectl-372 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 18:13:47.216: INFO: stderr: ""
May 31 18:13:47.216: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:13:47.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-372" for this suite.
May 31 18:13:53.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:13:53.314: INFO: namespace kubectl-372 deletion completed in 6.09463853s

• [SLOW TEST:31.683 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:13:53.315: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-5537/secret-test-d8ff3c4e-83cf-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 18:13:53.371: INFO: Waiting up to 5m0s for pod "pod-configmaps-d8ff9d4b-83cf-11e9-a3c9-f265bfc55a07" in namespace "secrets-5537" to be "success or failure"
May 31 18:13:53.373: INFO: Pod "pod-configmaps-d8ff9d4b-83cf-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 1.841531ms
May 31 18:13:55.376: INFO: Pod "pod-configmaps-d8ff9d4b-83cf-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005184936s
May 31 18:13:57.380: INFO: Pod "pod-configmaps-d8ff9d4b-83cf-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008300138s
May 31 18:13:59.383: INFO: Pod "pod-configmaps-d8ff9d4b-83cf-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01142684s
STEP: Saw pod success
May 31 18:13:59.383: INFO: Pod "pod-configmaps-d8ff9d4b-83cf-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:13:59.385: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-configmaps-d8ff9d4b-83cf-11e9-a3c9-f265bfc55a07 container env-test: <nil>
STEP: delete the pod
May 31 18:13:59.402: INFO: Waiting for pod pod-configmaps-d8ff9d4b-83cf-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:13:59.404: INFO: Pod pod-configmaps-d8ff9d4b-83cf-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:13:59.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5537" for this suite.
May 31 18:14:05.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:14:05.508: INFO: namespace secrets-5537 deletion completed in 6.102187769s

• [SLOW TEST:12.194 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:14:05.509: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:14:15.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8311" for this suite.
May 31 18:15:05.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:15:05.726: INFO: namespace kubelet-test-8311 deletion completed in 50.093822401s

• [SLOW TEST:60.217 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:15:05.726: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kpkv6 in namespace proxy-4750
I0531 18:15:05.801388      15 runners.go:184] Created replication controller with name: proxy-service-kpkv6, namespace: proxy-4750, replica count: 1
I0531 18:15:06.851940      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 18:15:07.852609      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 18:15:08.852966      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 18:15:09.853148      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 18:15:10.853314      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 18:15:11.853512      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 18:15:12.853677      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 18:15:13.853891      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 18:15:14.854098      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 18:15:15.854301      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 18:15:16.854510      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 18:15:17.854727      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 18:15:18.854886      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 18:15:19.855070      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0531 18:15:20.855253      15 runners.go:184] proxy-service-kpkv6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 18:15:20.858: INFO: setup took 15.088136937s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 31 18:15:20.869: INFO: (0) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 10.907183ms)
May 31 18:15:20.870: INFO: (0) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 11.261389ms)
May 31 18:15:20.870: INFO: (0) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 11.704696ms)
May 31 18:15:20.874: INFO: (0) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 16.292274ms)
May 31 18:15:20.875: INFO: (0) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 16.279773ms)
May 31 18:15:20.875: INFO: (0) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 16.152371ms)
May 31 18:15:20.875: INFO: (0) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 16.261073ms)
May 31 18:15:20.878: INFO: (0) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 20.064637ms)
May 31 18:15:20.880: INFO: (0) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 21.005053ms)
May 31 18:15:20.884: INFO: (0) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 25.428327ms)
May 31 18:15:20.884: INFO: (0) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 25.270424ms)
May 31 18:15:20.885: INFO: (0) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 26.21134ms)
May 31 18:15:20.885: INFO: (0) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 26.003137ms)
May 31 18:15:20.886: INFO: (0) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 27.657064ms)
May 31 18:15:20.887: INFO: (0) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 28.571479ms)
May 31 18:15:20.887: INFO: (0) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 28.565679ms)
May 31 18:15:20.894: INFO: (1) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 6.348407ms)
May 31 18:15:20.894: INFO: (1) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 6.52851ms)
May 31 18:15:20.899: INFO: (1) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 11.35139ms)
May 31 18:15:20.900: INFO: (1) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 12.241706ms)
May 31 18:15:20.903: INFO: (1) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 15.041153ms)
May 31 18:15:20.903: INFO: (1) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 15.056353ms)
May 31 18:15:20.903: INFO: (1) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 15.095853ms)
May 31 18:15:20.903: INFO: (1) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 15.065453ms)
May 31 18:15:20.903: INFO: (1) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 15.367058ms)
May 31 18:15:20.903: INFO: (1) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 15.069253ms)
May 31 18:15:20.906: INFO: (1) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 18.676813ms)
May 31 18:15:20.906: INFO: (1) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 18.772915ms)
May 31 18:15:20.906: INFO: (1) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 18.47791ms)
May 31 18:15:20.906: INFO: (1) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 18.698113ms)
May 31 18:15:20.906: INFO: (1) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 18.801516ms)
May 31 18:15:20.906: INFO: (1) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 18.869317ms)
May 31 18:15:20.920: INFO: (2) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 11.504893ms)
May 31 18:15:20.920: INFO: (2) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 11.31539ms)
May 31 18:15:20.920: INFO: (2) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 11.566094ms)
May 31 18:15:20.920: INFO: (2) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 12.255806ms)
May 31 18:15:20.920: INFO: (2) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 12.100103ms)
May 31 18:15:20.920: INFO: (2) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 11.905ms)
May 31 18:15:20.922: INFO: (2) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 12.822116ms)
May 31 18:15:20.922: INFO: (2) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 13.585128ms)
May 31 18:15:20.923: INFO: (2) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 14.625845ms)
May 31 18:15:20.926: INFO: (2) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 17.451293ms)
May 31 18:15:20.927: INFO: (2) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 18.50321ms)
May 31 18:15:20.927: INFO: (2) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 18.370008ms)
May 31 18:15:20.928: INFO: (2) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 19.287024ms)
May 31 18:15:20.928: INFO: (2) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 19.585229ms)
May 31 18:15:20.928: INFO: (2) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 19.252423ms)
May 31 18:15:20.928: INFO: (2) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 19.729031ms)
May 31 18:15:20.940: INFO: (3) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 11.429192ms)
May 31 18:15:20.941: INFO: (3) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 12.635512ms)
May 31 18:15:20.947: INFO: (3) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 18.386509ms)
May 31 18:15:20.954: INFO: (3) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 25.64743ms)
May 31 18:15:20.954: INFO: (3) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 25.742732ms)
May 31 18:15:20.955: INFO: (3) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 25.332726ms)
May 31 18:15:20.955: INFO: (3) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 25.923635ms)
May 31 18:15:20.955: INFO: (3) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 26.298441ms)
May 31 18:15:20.955: INFO: (3) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 26.032537ms)
May 31 18:15:20.956: INFO: (3) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 27.964269ms)
May 31 18:15:20.957: INFO: (3) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 27.590463ms)
May 31 18:15:20.957: INFO: (3) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 28.237274ms)
May 31 18:15:20.957: INFO: (3) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 28.090571ms)
May 31 18:15:20.958: INFO: (3) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 29.030888ms)
May 31 18:15:20.958: INFO: (3) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 29.347893ms)
May 31 18:15:20.961: INFO: (3) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 32.16884ms)
May 31 18:15:20.974: INFO: (4) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 13.316223ms)
May 31 18:15:20.974: INFO: (4) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 13.076219ms)
May 31 18:15:20.976: INFO: (4) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 13.899134ms)
May 31 18:15:20.976: INFO: (4) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 13.751231ms)
May 31 18:15:20.976: INFO: (4) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 13.861932ms)
May 31 18:15:20.977: INFO: (4) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 15.134855ms)
May 31 18:15:20.977: INFO: (4) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 15.049152ms)
May 31 18:15:20.979: INFO: (4) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 17.229289ms)
May 31 18:15:20.979: INFO: (4) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 17.528794ms)
May 31 18:15:20.979: INFO: (4) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 17.208388ms)
May 31 18:15:20.979: INFO: (4) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 17.8902ms)
May 31 18:15:20.979: INFO: (4) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 17.527294ms)
May 31 18:15:20.979: INFO: (4) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 18.025403ms)
May 31 18:15:20.979: INFO: (4) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 17.802799ms)
May 31 18:15:20.979: INFO: (4) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 17.352991ms)
May 31 18:15:20.980: INFO: (4) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 18.080004ms)
May 31 18:15:20.984: INFO: (5) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 4.18397ms)
May 31 18:15:20.990: INFO: (5) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 9.291356ms)
May 31 18:15:20.990: INFO: (5) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 9.624362ms)
May 31 18:15:20.992: INFO: (5) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 10.791781ms)
May 31 18:15:20.992: INFO: (5) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 11.268289ms)
May 31 18:15:20.992: INFO: (5) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 10.680379ms)
May 31 18:15:20.992: INFO: (5) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 11.527593ms)
May 31 18:15:20.992: INFO: (5) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 11.286489ms)
May 31 18:15:20.995: INFO: (5) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 14.730447ms)
May 31 18:15:20.995: INFO: (5) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 14.710547ms)
May 31 18:15:20.995: INFO: (5) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 15.206455ms)
May 31 18:15:20.995: INFO: (5) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 14.506443ms)
May 31 18:15:20.995: INFO: (5) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 15.237955ms)
May 31 18:15:20.995: INFO: (5) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 14.837549ms)
May 31 18:15:20.996: INFO: (5) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 14.795148ms)
May 31 18:15:20.996: INFO: (5) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 15.965668ms)
May 31 18:15:21.008: INFO: (6) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 10.787681ms)
May 31 18:15:21.008: INFO: (6) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 11.174987ms)
May 31 18:15:21.008: INFO: (6) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 10.810782ms)
May 31 18:15:21.008: INFO: (6) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 10.465675ms)
May 31 18:15:21.008: INFO: (6) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 10.884883ms)
May 31 18:15:21.008: INFO: (6) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 11.152087ms)
May 31 18:15:21.008: INFO: (6) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 10.914684ms)
May 31 18:15:21.008: INFO: (6) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 10.882683ms)
May 31 18:15:21.008: INFO: (6) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 11.204688ms)
May 31 18:15:21.009: INFO: (6) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 12.115304ms)
May 31 18:15:21.011: INFO: (6) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 14.205938ms)
May 31 18:15:21.011: INFO: (6) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 14.351841ms)
May 31 18:15:21.011: INFO: (6) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 13.899633ms)
May 31 18:15:21.011: INFO: (6) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 13.775831ms)
May 31 18:15:21.011: INFO: (6) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 13.890133ms)
May 31 18:15:21.011: INFO: (6) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 14.493944ms)
May 31 18:15:21.021: INFO: (7) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 9.035552ms)
May 31 18:15:21.021: INFO: (7) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 9.848165ms)
May 31 18:15:21.021: INFO: (7) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 9.122154ms)
May 31 18:15:21.021: INFO: (7) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 9.694763ms)
May 31 18:15:21.021: INFO: (7) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 10.051869ms)
May 31 18:15:21.021: INFO: (7) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 9.425358ms)
May 31 18:15:21.021: INFO: (7) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 9.660362ms)
May 31 18:15:21.022: INFO: (7) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 9.777464ms)
May 31 18:15:21.022: INFO: (7) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 9.671462ms)
May 31 18:15:21.033: INFO: (7) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 21.47336ms)
May 31 18:15:21.033: INFO: (7) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 20.80985ms)
May 31 18:15:21.033: INFO: (7) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 20.813349ms)
May 31 18:15:21.033: INFO: (7) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 21.209956ms)
May 31 18:15:21.033: INFO: (7) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 21.831967ms)
May 31 18:15:21.035: INFO: (7) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 22.860383ms)
May 31 18:15:21.036: INFO: (7) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 23.465993ms)
May 31 18:15:21.048: INFO: (8) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 11.676596ms)
May 31 18:15:21.053: INFO: (8) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 16.810182ms)
May 31 18:15:21.053: INFO: (8) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 16.993286ms)
May 31 18:15:21.053: INFO: (8) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 16.885384ms)
May 31 18:15:21.053: INFO: (8) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 17.222989ms)
May 31 18:15:21.054: INFO: (8) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 17.159188ms)
May 31 18:15:21.054: INFO: (8) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 17.27769ms)
May 31 18:15:21.054: INFO: (8) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 17.188288ms)
May 31 18:15:21.054: INFO: (8) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 17.735898ms)
May 31 18:15:21.054: INFO: (8) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 17.487494ms)
May 31 18:15:21.056: INFO: (8) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 19.244523ms)
May 31 18:15:21.056: INFO: (8) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 19.149922ms)
May 31 18:15:21.056: INFO: (8) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 19.365125ms)
May 31 18:15:21.056: INFO: (8) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 19.648529ms)
May 31 18:15:21.056: INFO: (8) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 19.540128ms)
May 31 18:15:21.056: INFO: (8) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 19.500328ms)
May 31 18:15:21.064: INFO: (9) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 7.457425ms)
May 31 18:15:21.064: INFO: (9) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 7.715229ms)
May 31 18:15:21.064: INFO: (9) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 7.657929ms)
May 31 18:15:21.066: INFO: (9) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 9.040751ms)
May 31 18:15:21.066: INFO: (9) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 9.698663ms)
May 31 18:15:21.068: INFO: (9) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 10.75428ms)
May 31 18:15:21.068: INFO: (9) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 10.960584ms)
May 31 18:15:21.068: INFO: (9) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 11.262689ms)
May 31 18:15:21.071: INFO: (9) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 14.974451ms)
May 31 18:15:21.072: INFO: (9) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 15.270556ms)
May 31 18:15:21.075: INFO: (9) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 17.371891ms)
May 31 18:15:21.076: INFO: (9) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 18.839216ms)
May 31 18:15:21.076: INFO: (9) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 18.968419ms)
May 31 18:15:21.077: INFO: (9) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 19.106321ms)
May 31 18:15:21.077: INFO: (9) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 20.186339ms)
May 31 18:15:21.077: INFO: (9) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 19.491427ms)
May 31 18:15:21.088: INFO: (10) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 10.306273ms)
May 31 18:15:21.088: INFO: (10) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 10.049568ms)
May 31 18:15:21.090: INFO: (10) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 12.483809ms)
May 31 18:15:21.091: INFO: (10) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 13.411425ms)
May 31 18:15:21.091: INFO: (10) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 13.295023ms)
May 31 18:15:21.092: INFO: (10) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 14.494743ms)
May 31 18:15:21.093: INFO: (10) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 15.698564ms)
May 31 18:15:21.094: INFO: (10) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 15.784065ms)
May 31 18:15:21.094: INFO: (10) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 16.931384ms)
May 31 18:15:21.094: INFO: (10) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 16.749381ms)
May 31 18:15:21.094: INFO: (10) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 16.959785ms)
May 31 18:15:21.094: INFO: (10) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 16.577478ms)
May 31 18:15:21.095: INFO: (10) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 16.631679ms)
May 31 18:15:21.095: INFO: (10) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 16.847783ms)
May 31 18:15:21.095: INFO: (10) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 16.782282ms)
May 31 18:15:21.095: INFO: (10) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 17.657997ms)
May 31 18:15:21.105: INFO: (11) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 9.51806ms)
May 31 18:15:21.105: INFO: (11) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 9.787965ms)
May 31 18:15:21.105: INFO: (11) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 9.925267ms)
May 31 18:15:21.105: INFO: (11) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 9.776564ms)
May 31 18:15:21.108: INFO: (11) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 12.625512ms)
May 31 18:15:21.108: INFO: (11) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 13.056919ms)
May 31 18:15:21.112: INFO: (11) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 16.304274ms)
May 31 18:15:21.112: INFO: (11) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 16.66648ms)
May 31 18:15:21.112: INFO: (11) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 16.254373ms)
May 31 18:15:21.112: INFO: (11) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 16.399275ms)
May 31 18:15:21.112: INFO: (11) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 16.327574ms)
May 31 18:15:21.112: INFO: (11) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 16.512277ms)
May 31 18:15:21.114: INFO: (11) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 18.810115ms)
May 31 18:15:21.114: INFO: (11) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 18.459609ms)
May 31 18:15:21.115: INFO: (11) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 19.359325ms)
May 31 18:15:21.117: INFO: (11) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 21.571462ms)
May 31 18:15:21.135: INFO: (12) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 18.235506ms)
May 31 18:15:21.136: INFO: (12) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 18.390609ms)
May 31 18:15:21.136: INFO: (12) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 18.076404ms)
May 31 18:15:21.139: INFO: (12) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 22.120072ms)
May 31 18:15:21.140: INFO: (12) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 22.971786ms)
May 31 18:15:21.147: INFO: (12) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 29.802401ms)
May 31 18:15:21.147: INFO: (12) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 29.914702ms)
May 31 18:15:21.150: INFO: (12) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 31.999237ms)
May 31 18:15:21.155: INFO: (12) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 37.172024ms)
May 31 18:15:21.170: INFO: (12) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 52.655784ms)
May 31 18:15:21.170: INFO: (12) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 52.530682ms)
May 31 18:15:21.170: INFO: (12) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 52.694784ms)
May 31 18:15:21.180: INFO: (12) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 62.009841ms)
May 31 18:15:21.180: INFO: (12) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 62.244945ms)
May 31 18:15:21.180: INFO: (12) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 62.227144ms)
May 31 18:15:21.180: INFO: (12) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 62.138543ms)
May 31 18:15:21.204: INFO: (13) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 23.867801ms)
May 31 18:15:21.204: INFO: (13) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 24.42221ms)
May 31 18:15:21.206: INFO: (13) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 26.352742ms)
May 31 18:15:21.206: INFO: (13) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 26.363242ms)
May 31 18:15:21.206: INFO: (13) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 26.022936ms)
May 31 18:15:21.206: INFO: (13) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 26.136238ms)
May 31 18:15:21.207: INFO: (13) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 26.246741ms)
May 31 18:15:21.207: INFO: (13) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 27.204857ms)
May 31 18:15:21.207: INFO: (13) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 27.275658ms)
May 31 18:15:21.207: INFO: (13) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 27.43706ms)
May 31 18:15:21.207: INFO: (13) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 27.037154ms)
May 31 18:15:21.209: INFO: (13) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 28.57268ms)
May 31 18:15:21.210: INFO: (13) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 29.722799ms)
May 31 18:15:21.210: INFO: (13) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 29.871301ms)
May 31 18:15:21.210: INFO: (13) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 29.15359ms)
May 31 18:15:21.211: INFO: (13) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 31.153723ms)
May 31 18:15:21.224: INFO: (14) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 11.255189ms)
May 31 18:15:21.224: INFO: (14) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 11.9249ms)
May 31 18:15:21.224: INFO: (14) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 12.045103ms)
May 31 18:15:21.224: INFO: (14) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 11.841599ms)
May 31 18:15:21.224: INFO: (14) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 11.738597ms)
May 31 18:15:21.227: INFO: (14) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 15.031552ms)
May 31 18:15:21.227: INFO: (14) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 15.275657ms)
May 31 18:15:21.227: INFO: (14) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 15.155154ms)
May 31 18:15:21.227: INFO: (14) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 14.87185ms)
May 31 18:15:21.230: INFO: (14) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 17.412792ms)
May 31 18:15:21.234: INFO: (14) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 21.818466ms)
May 31 18:15:21.234: INFO: (14) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 22.292574ms)
May 31 18:15:21.234: INFO: (14) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 21.46776ms)
May 31 18:15:21.234: INFO: (14) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 22.132372ms)
May 31 18:15:21.239: INFO: (14) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 27.247358ms)
May 31 18:15:21.240: INFO: (14) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 27.262658ms)
May 31 18:15:21.259: INFO: (15) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 19.935035ms)
May 31 18:15:21.264: INFO: (15) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 23.646097ms)
May 31 18:15:21.265: INFO: (15) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 24.894018ms)
May 31 18:15:21.265: INFO: (15) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 24.236107ms)
May 31 18:15:21.266: INFO: (15) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 26.196339ms)
May 31 18:15:21.266: INFO: (15) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 26.651647ms)
May 31 18:15:21.266: INFO: (15) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 26.249841ms)
May 31 18:15:21.266: INFO: (15) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 25.501628ms)
May 31 18:15:21.266: INFO: (15) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 25.542628ms)
May 31 18:15:21.266: INFO: (15) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 26.363643ms)
May 31 18:15:21.268: INFO: (15) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 26.643347ms)
May 31 18:15:21.274: INFO: (15) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 32.941753ms)
May 31 18:15:21.274: INFO: (15) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 33.523163ms)
May 31 18:15:21.274: INFO: (15) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 32.74735ms)
May 31 18:15:21.274: INFO: (15) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 34.209274ms)
May 31 18:15:21.274: INFO: (15) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 34.55338ms)
May 31 18:15:21.281: INFO: (16) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 6.799714ms)
May 31 18:15:21.283: INFO: (16) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 7.957733ms)
May 31 18:15:21.283: INFO: (16) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 8.506543ms)
May 31 18:15:21.284: INFO: (16) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 8.749447ms)
May 31 18:15:21.286: INFO: (16) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 11.395191ms)
May 31 18:15:21.286: INFO: (16) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 11.862199ms)
May 31 18:15:21.286: INFO: (16) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 11.709797ms)
May 31 18:15:21.287: INFO: (16) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 12.303706ms)
May 31 18:15:21.287: INFO: (16) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 12.091503ms)
May 31 18:15:21.287: INFO: (16) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 12.266005ms)
May 31 18:15:21.289: INFO: (16) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 14.029335ms)
May 31 18:15:21.289: INFO: (16) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 14.103137ms)
May 31 18:15:21.289: INFO: (16) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 13.960334ms)
May 31 18:15:21.289: INFO: (16) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 13.951334ms)
May 31 18:15:21.289: INFO: (16) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 14.050536ms)
May 31 18:15:21.290: INFO: (16) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 15.083653ms)
May 31 18:15:21.298: INFO: (17) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 7.829832ms)
May 31 18:15:21.299: INFO: (17) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 8.655245ms)
May 31 18:15:21.299: INFO: (17) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 9.180654ms)
May 31 18:15:21.303: INFO: (17) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 13.396424ms)
May 31 18:15:21.304: INFO: (17) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 13.850433ms)
May 31 18:15:21.304: INFO: (17) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 13.587528ms)
May 31 18:15:21.304: INFO: (17) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 13.049119ms)
May 31 18:15:21.304: INFO: (17) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 12.868816ms)
May 31 18:15:21.304: INFO: (17) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 13.363624ms)
May 31 18:15:21.304: INFO: (17) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 13.325724ms)
May 31 18:15:21.304: INFO: (17) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 13.70083ms)
May 31 18:15:21.305: INFO: (17) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 14.963051ms)
May 31 18:15:21.305: INFO: (17) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 14.017536ms)
May 31 18:15:21.305: INFO: (17) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 14.164538ms)
May 31 18:15:21.305: INFO: (17) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 15.063653ms)
May 31 18:15:21.305: INFO: (17) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 14.939051ms)
May 31 18:15:21.315: INFO: (18) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 9.278556ms)
May 31 18:15:21.315: INFO: (18) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 9.190055ms)
May 31 18:15:21.319: INFO: (18) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 13.350824ms)
May 31 18:15:21.319: INFO: (18) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 13.408625ms)
May 31 18:15:21.319: INFO: (18) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 13.181322ms)
May 31 18:15:21.323: INFO: (18) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 17.369691ms)
May 31 18:15:21.323: INFO: (18) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 17.429792ms)
May 31 18:15:21.323: INFO: (18) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 17.206589ms)
May 31 18:15:21.323: INFO: (18) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 17.749098ms)
May 31 18:15:21.323: INFO: (18) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 17.24549ms)
May 31 18:15:21.324: INFO: (18) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 17.420493ms)
May 31 18:15:21.324: INFO: (18) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 18.220806ms)
May 31 18:15:21.325: INFO: (18) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 18.46421ms)
May 31 18:15:21.325: INFO: (18) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 18.585712ms)
May 31 18:15:21.325: INFO: (18) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 19.183022ms)
May 31 18:15:21.326: INFO: (18) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 20.445743ms)
May 31 18:15:21.339: INFO: (19) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 12.395208ms)
May 31 18:15:21.340: INFO: (19) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">test<... (200; 13.693929ms)
May 31 18:15:21.340: INFO: (19) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 13.843633ms)
May 31 18:15:21.341: INFO: (19) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk/proxy/rewriteme">test</a> (200; 14.232239ms)
May 31 18:15:21.341: INFO: (19) /api/v1/namespaces/proxy-4750/pods/proxy-service-kpkv6-k7czk:160/proxy/: foo (200; 14.179238ms)
May 31 18:15:21.342: INFO: (19) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:460/proxy/: tls baz (200; 15.417959ms)
May 31 18:15:21.343: INFO: (19) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:462/proxy/: tls qux (200; 16.634479ms)
May 31 18:15:21.345: INFO: (19) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname1/proxy/: foo (200; 19.128421ms)
May 31 18:15:21.346: INFO: (19) /api/v1/namespaces/proxy-4750/services/http:proxy-service-kpkv6:portname2/proxy/: bar (200; 18.979819ms)
May 31 18:15:21.346: INFO: (19) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname2/proxy/: tls qux (200; 19.214923ms)
May 31 18:15:21.346: INFO: (19) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname1/proxy/: foo (200; 19.025919ms)
May 31 18:15:21.346: INFO: (19) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:162/proxy/: bar (200; 19.027319ms)
May 31 18:15:21.346: INFO: (19) /api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/https:proxy-service-kpkv6-k7czk:443/proxy/tlsrewritem... (200; 19.488228ms)
May 31 18:15:21.346: INFO: (19) /api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4750/pods/http:proxy-service-kpkv6-k7czk:1080/proxy/rewriteme">... (200; 19.144721ms)
May 31 18:15:21.346: INFO: (19) /api/v1/namespaces/proxy-4750/services/https:proxy-service-kpkv6:tlsportname1/proxy/: tls baz (200; 19.513927ms)
May 31 18:15:21.347: INFO: (19) /api/v1/namespaces/proxy-4750/services/proxy-service-kpkv6:portname2/proxy/: bar (200; 20.135038ms)
STEP: deleting ReplicationController proxy-service-kpkv6 in namespace proxy-4750, will wait for the garbage collector to delete the pods
May 31 18:15:21.404: INFO: Deleting ReplicationController proxy-service-kpkv6 took: 4.728679ms
May 31 18:15:21.704: INFO: Terminating ReplicationController proxy-service-kpkv6 pods took: 300.314241ms
[AfterEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:15:24.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4750" for this suite.
May 31 18:15:30.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:15:30.494: INFO: namespace proxy-4750 deletion completed in 6.085334643s

• [SLOW TEST:24.768 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:15:30.494: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
May 31 18:15:31.436: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 31 18:15:33.487: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 18:15:35.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 18:15:37.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 18:15:39.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 18:15:41.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923331, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 18:15:44.414: INFO: Waited 919.292231ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:15:44.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3237" for this suite.
May 31 18:15:51.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:15:51.128: INFO: namespace aggregator-3237 deletion completed in 6.190128311s

• [SLOW TEST:20.634 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:15:51.132: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6231.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6231.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6231.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6231.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6231.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6231.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 18:15:55.220: INFO: DNS probes using dns-6231/dns-test-1f383505-83d0-11e9-a3c9-f265bfc55a07 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:15:55.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6231" for this suite.
May 31 18:16:01.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:16:01.332: INFO: namespace dns-6231 deletion completed in 6.087607794s

• [SLOW TEST:10.200 seconds]
[sig-network] DNS
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:16:01.332: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-254c888f-83d0-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 18:16:01.379: INFO: Waiting up to 5m0s for pod "pod-configmaps-254cf973-83d0-11e9-a3c9-f265bfc55a07" in namespace "configmap-534" to be "success or failure"
May 31 18:16:01.386: INFO: Pod "pod-configmaps-254cf973-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.757614ms
May 31 18:16:03.390: INFO: Pod "pod-configmaps-254cf973-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010202446s
May 31 18:16:05.394: INFO: Pod "pod-configmaps-254cf973-83d0-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014026985s
STEP: Saw pod success
May 31 18:16:05.394: INFO: Pod "pod-configmaps-254cf973-83d0-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:16:05.396: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-configmaps-254cf973-83d0-11e9-a3c9-f265bfc55a07 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 18:16:05.417: INFO: Waiting for pod pod-configmaps-254cf973-83d0-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:16:05.420: INFO: Pod pod-configmaps-254cf973-83d0-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:16:05.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-534" for this suite.
May 31 18:16:11.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:16:11.514: INFO: namespace configmap-534 deletion completed in 6.091513464s

• [SLOW TEST:10.182 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:16:11.516: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-5114
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5114 to expose endpoints map[]
May 31 18:16:11.583: INFO: Get endpoints failed (10.048869ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May 31 18:16:12.586: INFO: successfully validated that service endpoint-test2 in namespace services-5114 exposes endpoints map[] (1.012794603s elapsed)
STEP: Creating pod pod1 in namespace services-5114
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5114 to expose endpoints map[pod1:[80]]
May 31 18:16:16.639: INFO: successfully validated that service endpoint-test2 in namespace services-5114 exposes endpoints map[pod1:[80]] (4.042842571s elapsed)
STEP: Creating pod pod2 in namespace services-5114
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5114 to expose endpoints map[pod1:[80] pod2:[80]]
May 31 18:16:20.711: INFO: successfully validated that service endpoint-test2 in namespace services-5114 exposes endpoints map[pod1:[80] pod2:[80]] (4.057666622s elapsed)
STEP: Deleting pod pod1 in namespace services-5114
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5114 to expose endpoints map[pod2:[80]]
May 31 18:16:20.730: INFO: successfully validated that service endpoint-test2 in namespace services-5114 exposes endpoints map[pod2:[80]] (9.810865ms elapsed)
STEP: Deleting pod pod2 in namespace services-5114
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5114 to expose endpoints map[]
May 31 18:16:21.751: INFO: successfully validated that service endpoint-test2 in namespace services-5114 exposes endpoints map[] (1.012947506s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:16:21.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5114" for this suite.
May 31 18:16:43.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:16:43.874: INFO: namespace services-5114 deletion completed in 22.09004907s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:32.358 seconds]
[sig-network] Services
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:16:43.874: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 31 18:16:48.451: INFO: Successfully updated pod "annotationupdate3ea7d2ba-83d0-11e9-a3c9-f265bfc55a07"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:16:50.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-364" for this suite.
May 31 18:17:12.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:17:12.559: INFO: namespace projected-364 deletion completed in 22.087790458s

• [SLOW TEST:28.684 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:17:12.559: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0531 18:17:42.650452      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 18:17:42.650: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:17:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6492" for this suite.
May 31 18:17:48.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:17:48.739: INFO: namespace gc-6492 deletion completed in 6.086686905s

• [SLOW TEST:36.181 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:17:48.742: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-2567
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2567
STEP: Deleting pre-stop pod
May 31 18:18:03.844: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:18:03.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2567" for this suite.
May 31 18:18:41.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:18:41.974: INFO: namespace prestop-2567 deletion completed in 38.117912949s

• [SLOW TEST:53.232 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:18:41.975: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 31 18:18:42.037: INFO: Waiting up to 5m0s for pod "pod-850e7ae8-83d0-11e9-a3c9-f265bfc55a07" in namespace "emptydir-3029" to be "success or failure"
May 31 18:18:42.043: INFO: Pod "pod-850e7ae8-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.992687ms
May 31 18:18:44.046: INFO: Pod "pod-850e7ae8-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009581918s
May 31 18:18:46.050: INFO: Pod "pod-850e7ae8-83d0-11e9-a3c9-f265bfc55a07": Phase="Running", Reason="", readiness=true. Elapsed: 4.012735573s
May 31 18:18:48.053: INFO: Pod "pod-850e7ae8-83d0-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016349166s
STEP: Saw pod success
May 31 18:18:48.053: INFO: Pod "pod-850e7ae8-83d0-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:18:48.055: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-850e7ae8-83d0-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:18:48.078: INFO: Waiting for pod pod-850e7ae8-83d0-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:18:48.083: INFO: Pod pod-850e7ae8-83d0-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:18:48.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3029" for this suite.
May 31 18:18:54.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:18:54.177: INFO: namespace emptydir-3029 deletion completed in 6.090401527s

• [SLOW TEST:12.202 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:18:54.177: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
May 31 18:18:54.222: INFO: Waiting up to 5m0s for pod "pod-8c52a6a7-83d0-11e9-a3c9-f265bfc55a07" in namespace "emptydir-1555" to be "success or failure"
May 31 18:18:54.229: INFO: Pod "pod-8c52a6a7-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.612097ms
May 31 18:18:56.232: INFO: Pod "pod-8c52a6a7-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010079711s
May 31 18:18:58.236: INFO: Pod "pod-8c52a6a7-83d0-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013706957s
STEP: Saw pod success
May 31 18:18:58.236: INFO: Pod "pod-8c52a6a7-83d0-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:18:58.238: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-8c52a6a7-83d0-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:18:58.260: INFO: Waiting for pod pod-8c52a6a7-83d0-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:18:58.263: INFO: Pod pod-8c52a6a7-83d0-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:18:58.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1555" for this suite.
May 31 18:19:04.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:19:04.370: INFO: namespace emptydir-1555 deletion completed in 6.10364398s

• [SLOW TEST:10.193 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:19:04.370: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 18:19:04.467: INFO: Waiting up to 5m0s for pod "downwardapi-volume-926d4a0a-83d0-11e9-a3c9-f265bfc55a07" in namespace "downward-api-5152" to be "success or failure"
May 31 18:19:04.478: INFO: Pod "downwardapi-volume-926d4a0a-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 10.740558ms
May 31 18:19:06.485: INFO: Pod "downwardapi-volume-926d4a0a-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017858976s
May 31 18:19:08.488: INFO: Pod "downwardapi-volume-926d4a0a-83d0-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021420469s
STEP: Saw pod success
May 31 18:19:08.488: INFO: Pod "downwardapi-volume-926d4a0a-83d0-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:19:08.491: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downwardapi-volume-926d4a0a-83d0-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 18:19:08.510: INFO: Waiting for pod downwardapi-volume-926d4a0a-83d0-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:19:08.514: INFO: Pod downwardapi-volume-926d4a0a-83d0-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:19:08.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5152" for this suite.
May 31 18:19:14.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:19:14.608: INFO: namespace downward-api-5152 deletion completed in 6.089351513s

• [SLOW TEST:10.238 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:19:14.609: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 31 18:19:21.677: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:19:22.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2699" for this suite.
May 31 18:19:40.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:19:40.791: INFO: namespace replicaset-2699 deletion completed in 18.091499673s

• [SLOW TEST:26.182 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:19:40.791: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 31 18:19:40.840: INFO: Waiting up to 5m0s for pod "pod-a81c0263-83d0-11e9-a3c9-f265bfc55a07" in namespace "emptydir-7949" to be "success or failure"
May 31 18:19:40.848: INFO: Pod "pod-a81c0263-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 7.236709ms
May 31 18:19:42.851: INFO: Pod "pod-a81c0263-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010651056s
May 31 18:19:44.854: INFO: Pod "pod-a81c0263-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014002726s
May 31 18:19:46.858: INFO: Pod "pod-a81c0263-83d0-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017306621s
STEP: Saw pod success
May 31 18:19:46.858: INFO: Pod "pod-a81c0263-83d0-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:19:46.860: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-a81c0263-83d0-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:19:46.878: INFO: Waiting for pod pod-a81c0263-83d0-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:19:46.881: INFO: Pod pod-a81c0263-83d0-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:19:46.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7949" for this suite.
May 31 18:19:52.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:19:53.009: INFO: namespace emptydir-7949 deletion completed in 6.124956057s

• [SLOW TEST:12.219 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:19:53.014: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1432
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 31 18:19:53.107: INFO: Found 0 stateful pods, waiting for 3
May 31 18:20:03.111: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 18:20:03.111: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 18:20:03.111: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 31 18:20:03.134: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 31 18:20:13.168: INFO: Updating stateful set ss2
May 31 18:20:13.193: INFO: Waiting for Pod statefulset-1432/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 31 18:20:23.248: INFO: Found 2 stateful pods, waiting for 3
May 31 18:20:33.252: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 18:20:33.252: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 18:20:33.252: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 31 18:20:33.277: INFO: Updating stateful set ss2
May 31 18:20:33.287: INFO: Waiting for Pod statefulset-1432/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 31 18:20:43.297: INFO: Waiting for Pod statefulset-1432/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 31 18:20:53.310: INFO: Updating stateful set ss2
May 31 18:20:53.324: INFO: Waiting for StatefulSet statefulset-1432/ss2 to complete update
May 31 18:20:53.324: INFO: Waiting for Pod statefulset-1432/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 31 18:21:03.331: INFO: Waiting for StatefulSet statefulset-1432/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 18:21:13.330: INFO: Deleting all statefulset in ns statefulset-1432
May 31 18:21:13.333: INFO: Scaling statefulset ss2 to 0
May 31 18:21:33.360: INFO: Waiting for statefulset status.replicas updated to 0
May 31 18:21:33.362: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:21:33.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1432" for this suite.
May 31 18:21:39.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:21:39.463: INFO: namespace statefulset-1432 deletion completed in 6.085672142s

• [SLOW TEST:106.452 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:21:39.464: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-eed777c5-83d0-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 18:21:39.514: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eed7f791-83d0-11e9-a3c9-f265bfc55a07" in namespace "projected-2735" to be "success or failure"
May 31 18:21:39.519: INFO: Pod "pod-projected-configmaps-eed7f791-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905476ms
May 31 18:21:41.522: INFO: Pod "pod-projected-configmaps-eed7f791-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008642104s
May 31 18:21:43.526: INFO: Pod "pod-projected-configmaps-eed7f791-83d0-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011765038s
STEP: Saw pod success
May 31 18:21:43.526: INFO: Pod "pod-projected-configmaps-eed7f791-83d0-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:21:43.528: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-projected-configmaps-eed7f791-83d0-11e9-a3c9-f265bfc55a07 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 18:21:43.555: INFO: Waiting for pod pod-projected-configmaps-eed7f791-83d0-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:21:43.558: INFO: Pod pod-projected-configmaps-eed7f791-83d0-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:21:43.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2735" for this suite.
May 31 18:21:49.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:21:49.658: INFO: namespace projected-2735 deletion completed in 6.096004943s

• [SLOW TEST:10.194 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:21:49.659: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 18:21:49.704: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4eaf495-83d0-11e9-a3c9-f265bfc55a07" in namespace "downward-api-9297" to be "success or failure"
May 31 18:21:49.706: INFO: Pod "downwardapi-volume-f4eaf495-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291735ms
May 31 18:21:51.709: INFO: Pod "downwardapi-volume-f4eaf495-83d0-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005556033s
May 31 18:21:53.713: INFO: Pod "downwardapi-volume-f4eaf495-83d0-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009233853s
STEP: Saw pod success
May 31 18:21:53.713: INFO: Pod "downwardapi-volume-f4eaf495-83d0-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:21:53.716: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downwardapi-volume-f4eaf495-83d0-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 18:21:53.737: INFO: Waiting for pod downwardapi-volume-f4eaf495-83d0-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:21:53.740: INFO: Pod downwardapi-volume-f4eaf495-83d0-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:21:53.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9297" for this suite.
May 31 18:21:59.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:21:59.906: INFO: namespace downward-api-9297 deletion completed in 6.163072521s

• [SLOW TEST:10.247 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:21:59.907: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 18:21:59.946: INFO: Creating ReplicaSet my-hostname-basic-fb06ac4a-83d0-11e9-a3c9-f265bfc55a07
May 31 18:21:59.955: INFO: Pod name my-hostname-basic-fb06ac4a-83d0-11e9-a3c9-f265bfc55a07: Found 0 pods out of 1
May 31 18:22:04.959: INFO: Pod name my-hostname-basic-fb06ac4a-83d0-11e9-a3c9-f265bfc55a07: Found 1 pods out of 1
May 31 18:22:04.959: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-fb06ac4a-83d0-11e9-a3c9-f265bfc55a07" is running
May 31 18:22:04.961: INFO: Pod "my-hostname-basic-fb06ac4a-83d0-11e9-a3c9-f265bfc55a07-lw94g" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 18:21:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 18:22:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 18:22:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 18:21:59 +0000 UTC Reason: Message:}])
May 31 18:22:04.961: INFO: Trying to dial the pod
May 31 18:22:09.970: INFO: Controller my-hostname-basic-fb06ac4a-83d0-11e9-a3c9-f265bfc55a07: Got expected result from replica 1 [my-hostname-basic-fb06ac4a-83d0-11e9-a3c9-f265bfc55a07-lw94g]: "my-hostname-basic-fb06ac4a-83d0-11e9-a3c9-f265bfc55a07-lw94g", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:22:09.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5404" for this suite.
May 31 18:22:15.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:22:16.128: INFO: namespace replicaset-5404 deletion completed in 6.155910463s

• [SLOW TEST:16.222 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:22:16.129: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:22:20.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5823" for this suite.
May 31 18:22:26.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:22:26.316: INFO: namespace kubelet-test-5823 deletion completed in 6.125051589s

• [SLOW TEST:10.187 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:22:26.316: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 18:22:26.386: INFO: Creating deployment "test-recreate-deployment"
May 31 18:22:26.391: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 31 18:22:26.401: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 31 18:22:28.406: INFO: Waiting deployment "test-recreate-deployment" to complete
May 31 18:22:28.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 18:22:30.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 18:22:32.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 18:22:34.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694923746, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 18:22:36.411: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 31 18:22:36.419: INFO: Updating deployment test-recreate-deployment
May 31 18:22:36.419: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 18:22:36.515: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-4685,SelfLink:/apis/apps/v1/namespaces/deployment-4685/deployments/test-recreate-deployment,UID:0ac9599b-83d1-11e9-a607-001dd80c001e,ResourceVersion:3357,Generation:2,CreationTimestamp:2019-05-31 18:22:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-31 18:22:36 +0000 UTC 2019-05-31 18:22:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-31 18:22:36 +0000 UTC 2019-05-31 18:22:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 31 18:22:36.523: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-4685,SelfLink:/apis/apps/v1/namespaces/deployment-4685/replicasets/test-recreate-deployment-c9cbd8684,UID:10cab667-83d1-11e9-a607-001dd80c001e,ResourceVersion:3355,Generation:1,CreationTimestamp:2019-05-31 18:22:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0ac9599b-83d1-11e9-a607-001dd80c001e 0xc002f449a0 0xc002f449a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 18:22:36.523: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 31 18:22:36.523: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-4685,SelfLink:/apis/apps/v1/namespaces/deployment-4685/replicasets/test-recreate-deployment-7d57d5ff7c,UID:0ac9fa1b-83d1-11e9-a607-001dd80c001e,ResourceVersion:3347,Generation:2,CreationTimestamp:2019-05-31 18:22:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0ac9599b-83d1-11e9-a607-001dd80c001e 0xc002f448e7 0xc002f448e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 18:22:36.525: INFO: Pod "test-recreate-deployment-c9cbd8684-6487c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-6487c,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-4685,SelfLink:/api/v1/namespaces/deployment-4685/pods/test-recreate-deployment-c9cbd8684-6487c,UID:10cc7136-83d1-11e9-a607-001dd80c001e,ResourceVersion:3358,Generation:0,CreationTimestamp:2019-05-31 18:22:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 10cab667-83d1-11e9-a607-001dd80c001e 0xc002f451d0 0xc002f451d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lthc7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lthc7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lthc7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002f45230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002f45250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:22:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:22:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:22:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:22:36 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2019-05-31 18:22:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:22:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4685" for this suite.
May 31 18:22:42.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:22:42.633: INFO: namespace deployment-4685 deletion completed in 6.103833173s

• [SLOW TEST:16.317 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:22:42.633: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:22:46.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4965" for this suite.
May 31 18:23:26.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:23:26.827: INFO: namespace kubelet-test-4965 deletion completed in 40.086694919s

• [SLOW TEST:44.194 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:23:26.828: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
May 31 18:23:26.878: INFO: Waiting up to 5m0s for pod "pod-2ed68e7c-83d1-11e9-a3c9-f265bfc55a07" in namespace "emptydir-5331" to be "success or failure"
May 31 18:23:26.884: INFO: Pod "pod-2ed68e7c-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.542804ms
May 31 18:23:28.889: INFO: Pod "pod-2ed68e7c-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01183734s
May 31 18:23:30.893: INFO: Pod "pod-2ed68e7c-83d1-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015088752s
STEP: Saw pod success
May 31 18:23:30.893: INFO: Pod "pod-2ed68e7c-83d1-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:23:30.896: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-2ed68e7c-83d1-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:23:30.912: INFO: Waiting for pod pod-2ed68e7c-83d1-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:23:30.913: INFO: Pod pod-2ed68e7c-83d1-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:23:30.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5331" for this suite.
May 31 18:23:36.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:23:37.030: INFO: namespace emptydir-5331 deletion completed in 6.114132659s

• [SLOW TEST:10.202 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:23:37.030: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 31 18:23:37.194: INFO: namespace kubectl-2546
May 31 18:23:37.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-2546'
May 31 18:23:37.954: INFO: stderr: ""
May 31 18:23:37.954: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 31 18:23:38.958: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:23:38.958: INFO: Found 0 / 1
May 31 18:23:39.957: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:23:39.957: INFO: Found 0 / 1
May 31 18:23:40.958: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:23:40.958: INFO: Found 1 / 1
May 31 18:23:40.958: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 31 18:23:40.961: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:23:40.961: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 31 18:23:40.961: INFO: wait on redis-master startup in kubectl-2546 
May 31 18:23:40.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 logs redis-master-v2wtn redis-master --namespace=kubectl-2546'
May 31 18:23:41.067: INFO: stderr: ""
May 31 18:23:41.068: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 31 May 18:23:39.709 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 May 18:23:39.709 # Server started, Redis version 3.2.12\n1:M 31 May 18:23:39.709 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 May 18:23:39.709 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 31 18:23:41.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2546'
May 31 18:23:41.211: INFO: stderr: ""
May 31 18:23:41.211: INFO: stdout: "service/rm2 exposed\n"
May 31 18:23:41.216: INFO: Service rm2 in namespace kubectl-2546 found.
STEP: exposing service
May 31 18:23:43.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2546'
May 31 18:23:43.326: INFO: stderr: ""
May 31 18:23:43.326: INFO: stdout: "service/rm3 exposed\n"
May 31 18:23:43.330: INFO: Service rm3 in namespace kubectl-2546 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:23:45.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2546" for this suite.
May 31 18:24:07.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:24:07.435: INFO: namespace kubectl-2546 deletion completed in 22.096746516s

• [SLOW TEST:30.404 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:24:07.435: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
May 31 18:24:07.482: INFO: Waiting up to 5m0s for pod "client-containers-470a4b3f-83d1-11e9-a3c9-f265bfc55a07" in namespace "containers-1908" to be "success or failure"
May 31 18:24:07.484: INFO: Pod "client-containers-470a4b3f-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 1.835029ms
May 31 18:24:09.488: INFO: Pod "client-containers-470a4b3f-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005057424s
May 31 18:24:11.490: INFO: Pod "client-containers-470a4b3f-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007713917s
May 31 18:24:13.493: INFO: Pod "client-containers-470a4b3f-83d1-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010872128s
STEP: Saw pod success
May 31 18:24:13.493: INFO: Pod "client-containers-470a4b3f-83d1-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:24:13.496: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod client-containers-470a4b3f-83d1-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:24:13.510: INFO: Waiting for pod client-containers-470a4b3f-83d1-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:24:13.514: INFO: Pod client-containers-470a4b3f-83d1-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:24:13.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1908" for this suite.
May 31 18:24:19.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:24:19.605: INFO: namespace containers-1908 deletion completed in 6.087334329s

• [SLOW TEST:12.170 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:24:19.606: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 31 18:24:35.710: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:35.710: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:35.858: INFO: Exec stderr: ""
May 31 18:24:35.858: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:35.858: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:35.976: INFO: Exec stderr: ""
May 31 18:24:35.976: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:35.976: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:36.106: INFO: Exec stderr: ""
May 31 18:24:36.106: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:36.106: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:36.265: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 31 18:24:36.265: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:36.265: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:36.399: INFO: Exec stderr: ""
May 31 18:24:36.399: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:36.399: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:36.537: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 31 18:24:36.537: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:36.537: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:36.657: INFO: Exec stderr: ""
May 31 18:24:36.657: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:36.657: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:36.794: INFO: Exec stderr: ""
May 31 18:24:36.794: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:36.794: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:36.919: INFO: Exec stderr: ""
May 31 18:24:36.920: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-11 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:24:36.920: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:24:37.071: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:24:37.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-11" for this suite.
May 31 18:25:19.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:25:19.162: INFO: namespace e2e-kubelet-etc-hosts-11 deletion completed in 42.086543792s

• [SLOW TEST:59.556 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:25:19.162: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 31 18:25:19.207: INFO: Waiting up to 5m0s for pod "pod-71ca9eb9-83d1-11e9-a3c9-f265bfc55a07" in namespace "emptydir-9114" to be "success or failure"
May 31 18:25:19.217: INFO: Pod "pod-71ca9eb9-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 9.795058ms
May 31 18:25:21.221: INFO: Pod "pod-71ca9eb9-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013752337s
May 31 18:25:23.224: INFO: Pod "pod-71ca9eb9-83d1-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01756702s
STEP: Saw pod success
May 31 18:25:23.224: INFO: Pod "pod-71ca9eb9-83d1-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:25:23.227: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-71ca9eb9-83d1-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:25:23.245: INFO: Waiting for pod pod-71ca9eb9-83d1-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:25:23.248: INFO: Pod pod-71ca9eb9-83d1-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:25:23.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9114" for this suite.
May 31 18:25:29.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:25:29.363: INFO: namespace emptydir-9114 deletion completed in 6.112191613s

• [SLOW TEST:10.201 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:25:29.363: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 31 18:25:33.941: INFO: Successfully updated pod "labelsupdate77deeba0-83d1-11e9-a3c9-f265bfc55a07"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:25:35.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4252" for this suite.
May 31 18:25:57.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:25:58.057: INFO: namespace projected-4252 deletion completed in 22.096279539s

• [SLOW TEST:28.694 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:25:58.058: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
May 31 18:26:02.117: INFO: Pod pod-hostip-88f9def4-83d1-11e9-a3c9-f265bfc55a07 has hostIP: 10.240.0.4
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:26:02.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2419" for this suite.
May 31 18:26:24.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:26:24.199: INFO: namespace pods-2419 deletion completed in 22.077358046s

• [SLOW TEST:26.142 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:26:24.200: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 31 18:26:24.262: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:24.267: INFO: Number of nodes with available pods: 0
May 31 18:26:24.267: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:25.271: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:25.274: INFO: Number of nodes with available pods: 0
May 31 18:26:25.274: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:26.270: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:26.273: INFO: Number of nodes with available pods: 0
May 31 18:26:26.273: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:27.271: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:27.278: INFO: Number of nodes with available pods: 3
May 31 18:26:27.279: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 31 18:26:27.291: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:27.294: INFO: Number of nodes with available pods: 2
May 31 18:26:27.294: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:28.298: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:28.301: INFO: Number of nodes with available pods: 2
May 31 18:26:28.301: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:29.298: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:29.301: INFO: Number of nodes with available pods: 2
May 31 18:26:29.301: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:30.299: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:30.302: INFO: Number of nodes with available pods: 2
May 31 18:26:30.302: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:31.298: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:31.301: INFO: Number of nodes with available pods: 2
May 31 18:26:31.301: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:32.302: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:32.305: INFO: Number of nodes with available pods: 2
May 31 18:26:32.305: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:33.305: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:33.309: INFO: Number of nodes with available pods: 2
May 31 18:26:33.309: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:34.298: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:34.300: INFO: Number of nodes with available pods: 2
May 31 18:26:34.301: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:35.310: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:35.320: INFO: Number of nodes with available pods: 2
May 31 18:26:35.321: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:36.309: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:36.312: INFO: Number of nodes with available pods: 2
May 31 18:26:36.312: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:37.297: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:37.301: INFO: Number of nodes with available pods: 2
May 31 18:26:37.301: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:38.298: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:38.301: INFO: Number of nodes with available pods: 2
May 31 18:26:38.301: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:39.298: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:39.301: INFO: Number of nodes with available pods: 2
May 31 18:26:39.301: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:40.299: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:40.303: INFO: Number of nodes with available pods: 2
May 31 18:26:40.303: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:41.298: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:41.302: INFO: Number of nodes with available pods: 2
May 31 18:26:41.302: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:26:42.298: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:26:42.303: INFO: Number of nodes with available pods: 3
May 31 18:26:42.303: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6442, will wait for the garbage collector to delete the pods
May 31 18:26:42.363: INFO: Deleting DaemonSet.extensions daemon-set took: 5.425388ms
May 31 18:26:42.663: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.493975ms
May 31 18:26:47.566: INFO: Number of nodes with available pods: 0
May 31 18:26:47.566: INFO: Number of running nodes: 0, number of available pods: 0
May 31 18:26:47.569: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6442/daemonsets","resourceVersion":"4131"},"items":null}

May 31 18:26:47.571: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6442/pods","resourceVersion":"4131"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:26:47.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6442" for this suite.
May 31 18:26:53.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:26:53.742: INFO: namespace daemonsets-6442 deletion completed in 6.160049294s

• [SLOW TEST:29.543 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:26:53.743: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-aa2d4c5f-83d1-11e9-a3c9-f265bfc55a07
STEP: Creating configMap with name cm-test-opt-upd-aa2d4c90-83d1-11e9-a3c9-f265bfc55a07
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-aa2d4c5f-83d1-11e9-a3c9-f265bfc55a07
STEP: Updating configmap cm-test-opt-upd-aa2d4c90-83d1-11e9-a3c9-f265bfc55a07
STEP: Creating configMap with name cm-test-opt-create-aa2d4cb0-83d1-11e9-a3c9-f265bfc55a07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:27:01.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5311" for this suite.
May 31 18:27:23.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:27:24.005: INFO: namespace configmap-5311 deletion completed in 22.076398605s

• [SLOW TEST:30.262 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:27:24.006: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 31 18:27:24.057: INFO: Pod name pod-release: Found 0 pods out of 1
May 31 18:27:29.060: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:27:30.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2621" for this suite.
May 31 18:27:36.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:27:36.223: INFO: namespace replication-controller-2621 deletion completed in 6.144545921s

• [SLOW TEST:12.218 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:27:36.225: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-gkxl
STEP: Creating a pod to test atomic-volume-subpath
May 31 18:27:36.306: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gkxl" in namespace "subpath-1111" to be "success or failure"
May 31 18:27:36.313: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.661909ms
May 31 18:27:38.316: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009673822s
May 31 18:27:40.322: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 4.015997793s
May 31 18:27:42.327: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 6.020896144s
May 31 18:27:44.343: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 8.037362288s
May 31 18:27:46.346: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 10.040319115s
May 31 18:27:48.349: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 12.042954841s
May 31 18:27:50.352: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 14.046092979s
May 31 18:27:52.355: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 16.049090918s
May 31 18:27:54.358: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 18.052205762s
May 31 18:27:56.424: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 20.117427523s
May 31 18:27:58.426: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Running", Reason="", readiness=true. Elapsed: 22.120304671s
May 31 18:28:00.430: INFO: Pod "pod-subpath-test-configmap-gkxl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.123632929s
STEP: Saw pod success
May 31 18:28:00.430: INFO: Pod "pod-subpath-test-configmap-gkxl" satisfied condition "success or failure"
May 31 18:28:00.432: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-subpath-test-configmap-gkxl container test-container-subpath-configmap-gkxl: <nil>
STEP: delete the pod
May 31 18:28:00.450: INFO: Waiting for pod pod-subpath-test-configmap-gkxl to disappear
May 31 18:28:00.452: INFO: Pod pod-subpath-test-configmap-gkxl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gkxl
May 31 18:28:00.452: INFO: Deleting pod "pod-subpath-test-configmap-gkxl" in namespace "subpath-1111"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:28:00.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1111" for this suite.
May 31 18:28:06.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:28:06.588: INFO: namespace subpath-1111 deletion completed in 6.13209289s

• [SLOW TEST:30.363 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:28:06.588: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-d597a2ad-83d1-11e9-a3c9-f265bfc55a07
STEP: Creating secret with name secret-projected-all-test-volume-d597a292-83d1-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test Check all projections for projected volume plugin
May 31 18:28:06.657: INFO: Waiting up to 5m0s for pod "projected-volume-d597a0ea-83d1-11e9-a3c9-f265bfc55a07" in namespace "projected-2947" to be "success or failure"
May 31 18:28:06.667: INFO: Pod "projected-volume-d597a0ea-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 10.540372ms
May 31 18:28:08.670: INFO: Pod "projected-volume-d597a0ea-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013365637s
May 31 18:28:10.673: INFO: Pod "projected-volume-d597a0ea-83d1-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016839116s
STEP: Saw pod success
May 31 18:28:10.674: INFO: Pod "projected-volume-d597a0ea-83d1-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:28:10.676: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod projected-volume-d597a0ea-83d1-11e9-a3c9-f265bfc55a07 container projected-all-volume-test: <nil>
STEP: delete the pod
May 31 18:28:10.697: INFO: Waiting for pod projected-volume-d597a0ea-83d1-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:28:10.700: INFO: Pod projected-volume-d597a0ea-83d1-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:28:10.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2947" for this suite.
May 31 18:28:16.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:28:16.803: INFO: namespace projected-2947 deletion completed in 6.099662414s

• [SLOW TEST:10.215 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:28:16.803: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 31 18:28:16.849: INFO: Waiting up to 5m0s for pod "pod-dbaccc4c-83d1-11e9-a3c9-f265bfc55a07" in namespace "emptydir-9921" to be "success or failure"
May 31 18:28:16.854: INFO: Pod "pod-dbaccc4c-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.682477ms
May 31 18:28:18.857: INFO: Pod "pod-dbaccc4c-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007814964s
May 31 18:28:20.862: INFO: Pod "pod-dbaccc4c-83d1-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012562581s
STEP: Saw pod success
May 31 18:28:20.862: INFO: Pod "pod-dbaccc4c-83d1-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:28:20.866: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-dbaccc4c-83d1-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:28:20.884: INFO: Waiting for pod pod-dbaccc4c-83d1-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:28:20.886: INFO: Pod pod-dbaccc4c-83d1-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:28:20.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9921" for this suite.
May 31 18:28:26.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:28:26.977: INFO: namespace emptydir-9921 deletion completed in 6.087942574s

• [SLOW TEST:10.174 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:28:26.977: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-e1bcced1-83d1-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 18:28:27.025: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1bd571b-83d1-11e9-a3c9-f265bfc55a07" in namespace "configmap-7057" to be "success or failure"
May 31 18:28:27.027: INFO: Pod "pod-configmaps-e1bd571b-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.262537ms
May 31 18:28:29.031: INFO: Pod "pod-configmaps-e1bd571b-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006826264s
May 31 18:28:31.034: INFO: Pod "pod-configmaps-e1bd571b-83d1-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009838069s
STEP: Saw pod success
May 31 18:28:31.035: INFO: Pod "pod-configmaps-e1bd571b-83d1-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:28:31.037: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-configmaps-e1bd571b-83d1-11e9-a3c9-f265bfc55a07 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 18:28:31.054: INFO: Waiting for pod pod-configmaps-e1bd571b-83d1-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:28:31.058: INFO: Pod pod-configmaps-e1bd571b-83d1-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:28:31.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7057" for this suite.
May 31 18:28:37.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:28:37.180: INFO: namespace configmap-7057 deletion completed in 6.11839992s

• [SLOW TEST:10.204 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:28:37.182: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
May 31 18:28:37.251: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5252" to be "success or failure"
May 31 18:28:37.254: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915748ms
May 31 18:28:39.257: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005918365s
May 31 18:28:41.261: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009702798s
STEP: Saw pod success
May 31 18:28:41.261: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 31 18:28:41.263: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 31 18:28:41.279: INFO: Waiting for pod pod-host-path-test to disappear
May 31 18:28:41.282: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:28:41.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5252" for this suite.
May 31 18:28:47.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:28:47.381: INFO: namespace hostpath-5252 deletion completed in 6.09224144s

• [SLOW TEST:10.199 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:28:47.381: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 18:28:47.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ede73265-83d1-11e9-a3c9-f265bfc55a07" in namespace "downward-api-8077" to be "success or failure"
May 31 18:28:47.438: INFO: Pod "downwardapi-volume-ede73265-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.29547ms
May 31 18:28:49.442: INFO: Pod "downwardapi-volume-ede73265-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007927712s
May 31 18:28:51.445: INFO: Pod "downwardapi-volume-ede73265-83d1-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011209453s
STEP: Saw pod success
May 31 18:28:51.445: INFO: Pod "downwardapi-volume-ede73265-83d1-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:28:51.448: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downwardapi-volume-ede73265-83d1-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 18:28:51.472: INFO: Waiting for pod downwardapi-volume-ede73265-83d1-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:28:51.476: INFO: Pod downwardapi-volume-ede73265-83d1-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:28:51.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8077" for this suite.
May 31 18:28:57.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:28:57.575: INFO: namespace downward-api-8077 deletion completed in 6.09383251s

• [SLOW TEST:10.194 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:28:57.576: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f3f9bf54-83d1-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 18:28:57.622: INFO: Waiting up to 5m0s for pod "pod-secrets-f3fa31e5-83d1-11e9-a3c9-f265bfc55a07" in namespace "secrets-6756" to be "success or failure"
May 31 18:28:57.626: INFO: Pod "pod-secrets-f3fa31e5-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332571ms
May 31 18:28:59.630: INFO: Pod "pod-secrets-f3fa31e5-83d1-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007493921s
May 31 18:29:01.633: INFO: Pod "pod-secrets-f3fa31e5-83d1-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010608473s
STEP: Saw pod success
May 31 18:29:01.633: INFO: Pod "pod-secrets-f3fa31e5-83d1-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:29:01.635: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-secrets-f3fa31e5-83d1-11e9-a3c9-f265bfc55a07 container secret-env-test: <nil>
STEP: delete the pod
May 31 18:29:01.651: INFO: Waiting for pod pod-secrets-f3fa31e5-83d1-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:29:01.653: INFO: Pod pod-secrets-f3fa31e5-83d1-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:29:01.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6756" for this suite.
May 31 18:29:07.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:29:07.782: INFO: namespace secrets-6756 deletion completed in 6.12603358s

• [SLOW TEST:10.206 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:29:07.783: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5791
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 31 18:29:07.928: INFO: Found 0 stateful pods, waiting for 3
May 31 18:29:17.931: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 18:29:17.931: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 18:29:17.931: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 31 18:29:17.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-5791 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 18:29:18.178: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 18:29:18.178: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 18:29:18.178: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 31 18:29:28.207: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 31 18:29:38.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-5791 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 18:29:38.507: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 31 18:29:38.507: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 18:29:38.507: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
May 31 18:30:08.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-5791 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 18:30:08.836: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 18:30:08.836: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 18:30:08.836: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 18:30:18.869: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 31 18:30:28.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-5791 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 18:30:29.154: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 31 18:30:29.154: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 18:30:29.154: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 18:30:49.177: INFO: Waiting for StatefulSet statefulset-5791/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 18:30:59.183: INFO: Deleting all statefulset in ns statefulset-5791
May 31 18:30:59.185: INFO: Scaling statefulset ss2 to 0
May 31 18:31:19.196: INFO: Waiting for statefulset status.replicas updated to 0
May 31 18:31:19.199: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:31:19.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5791" for this suite.
May 31 18:31:25.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:31:25.295: INFO: namespace statefulset-5791 deletion completed in 6.081113988s

• [SLOW TEST:137.511 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:31:25.295: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-4c097756-83d2-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 18:31:25.365: INFO: Waiting up to 5m0s for pod "pod-configmaps-4c09f605-83d2-11e9-a3c9-f265bfc55a07" in namespace "configmap-9855" to be "success or failure"
May 31 18:31:25.370: INFO: Pod "pod-configmaps-4c09f605-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.260686ms
May 31 18:31:27.374: INFO: Pod "pod-configmaps-4c09f605-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008815999s
May 31 18:31:29.377: INFO: Pod "pod-configmaps-4c09f605-83d2-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011930307s
STEP: Saw pod success
May 31 18:31:29.377: INFO: Pod "pod-configmaps-4c09f605-83d2-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:31:29.379: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-configmaps-4c09f605-83d2-11e9-a3c9-f265bfc55a07 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 18:31:29.398: INFO: Waiting for pod pod-configmaps-4c09f605-83d2-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:31:29.401: INFO: Pod pod-configmaps-4c09f605-83d2-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:31:29.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9855" for this suite.
May 31 18:31:35.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:31:35.525: INFO: namespace configmap-9855 deletion completed in 6.121110869s

• [SLOW TEST:10.231 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:31:35.533: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-52260d4b-83d2-11e9-a3c9-f265bfc55a07
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:31:39.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1966" for this suite.
May 31 18:32:01.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:32:01.819: INFO: namespace configmap-1966 deletion completed in 22.08604682s

• [SLOW TEST:26.287 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:32:01.823: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 31 18:32:01.868: INFO: Waiting up to 5m0s for pod "pod-61cb8317-83d2-11e9-a3c9-f265bfc55a07" in namespace "emptydir-3220" to be "success or failure"
May 31 18:32:01.873: INFO: Pod "pod-61cb8317-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.018482ms
May 31 18:32:03.876: INFO: Pod "pod-61cb8317-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007924811s
May 31 18:32:05.879: INFO: Pod "pod-61cb8317-83d2-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01070924s
STEP: Saw pod success
May 31 18:32:05.879: INFO: Pod "pod-61cb8317-83d2-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:32:05.882: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-61cb8317-83d2-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:32:05.896: INFO: Waiting for pod pod-61cb8317-83d2-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:32:05.898: INFO: Pod pod-61cb8317-83d2-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:32:05.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3220" for this suite.
May 31 18:32:11.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:32:11.995: INFO: namespace emptydir-3220 deletion completed in 6.092250074s

• [SLOW TEST:10.172 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:32:11.995: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 31 18:32:12.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-9383'
May 31 18:32:12.291: INFO: stderr: ""
May 31 18:32:12.291: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 31 18:32:13.293: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:32:13.294: INFO: Found 0 / 1
May 31 18:32:14.294: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:32:14.294: INFO: Found 0 / 1
May 31 18:32:15.296: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:32:15.296: INFO: Found 0 / 1
May 31 18:32:16.294: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:32:16.294: INFO: Found 0 / 1
May 31 18:32:17.296: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:32:17.296: INFO: Found 1 / 1
May 31 18:32:17.296: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 31 18:32:17.301: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:32:17.301: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 31 18:32:17.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 patch pod redis-master-vq5vl --namespace=kubectl-9383 -p {"metadata":{"annotations":{"x":"y"}}}'
May 31 18:32:17.410: INFO: stderr: ""
May 31 18:32:17.410: INFO: stdout: "pod/redis-master-vq5vl patched\n"
STEP: checking annotations
May 31 18:32:17.418: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:32:17.418: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:32:17.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9383" for this suite.
May 31 18:32:39.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:32:39.580: INFO: namespace kubectl-9383 deletion completed in 22.159395903s

• [SLOW TEST:27.585 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:32:39.584: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-785091f0-83d2-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 18:32:39.650: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7850f405-83d2-11e9-a3c9-f265bfc55a07" in namespace "projected-4043" to be "success or failure"
May 31 18:32:39.652: INFO: Pod "pod-projected-secrets-7850f405-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037034ms
May 31 18:32:41.655: INFO: Pod "pod-projected-secrets-7850f405-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004941687s
May 31 18:32:43.658: INFO: Pod "pod-projected-secrets-7850f405-83d2-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007623338s
STEP: Saw pod success
May 31 18:32:43.658: INFO: Pod "pod-projected-secrets-7850f405-83d2-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:32:43.660: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-projected-secrets-7850f405-83d2-11e9-a3c9-f265bfc55a07 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 18:32:43.677: INFO: Waiting for pod pod-projected-secrets-7850f405-83d2-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:32:43.679: INFO: Pod pod-projected-secrets-7850f405-83d2-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:32:43.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4043" for this suite.
May 31 18:32:49.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:32:49.797: INFO: namespace projected-4043 deletion completed in 6.115135822s

• [SLOW TEST:10.213 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:32:49.797: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7718
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-7718
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7718
May 31 18:32:49.861: INFO: Found 0 stateful pods, waiting for 1
May 31 18:32:59.914: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 31 18:32:59.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-7718 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 18:33:00.167: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 18:33:00.167: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 18:33:00.167: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 18:33:00.170: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 31 18:33:10.174: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 18:33:10.174: INFO: Waiting for statefulset status.replicas updated to 0
May 31 18:33:10.201: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 18:33:10.201: INFO: ss-0  k8s-linuxpool-33506892-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  }]
May 31 18:33:10.202: INFO: 
May 31 18:33:10.202: INFO: StatefulSet ss has not reached scale 3, at 1
May 31 18:33:11.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984079477s
May 31 18:33:12.210: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979083933s
May 31 18:33:13.213: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975763417s
May 31 18:33:14.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972272798s
May 31 18:33:15.221: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968824279s
May 31 18:33:16.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965052654s
May 31 18:33:17.228: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961500433s
May 31 18:33:18.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957676507s
May 31 18:33:19.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.57216ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7718
May 31 18:33:20.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-7718 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 18:33:20.572: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 31 18:33:20.572: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 18:33:20.572: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 18:33:20.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-7718 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 18:33:20.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 31 18:33:20.834: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 18:33:20.834: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 18:33:20.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-7718 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 18:33:21.045: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 31 18:33:21.045: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 18:33:21.045: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 18:33:21.049: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 18:33:21.049: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 18:33:21.049: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 31 18:33:21.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-7718 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 18:33:21.286: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 18:33:21.286: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 18:33:21.286: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 18:33:21.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-7718 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 18:33:21.496: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 18:33:21.496: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 18:33:21.496: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 18:33:21.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-7718 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 18:33:21.730: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 18:33:21.730: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 18:33:21.730: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 18:33:21.730: INFO: Waiting for statefulset status.replicas updated to 0
May 31 18:33:21.733: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 31 18:33:31.739: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 18:33:31.739: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 31 18:33:31.739: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 31 18:33:31.748: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 18:33:31.748: INFO: ss-0  k8s-linuxpool-33506892-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  }]
May 31 18:33:31.748: INFO: ss-1  k8s-linuxpool-33506892-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  }]
May 31 18:33:31.748: INFO: ss-2  k8s-linuxpool-33506892-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  }]
May 31 18:33:31.748: INFO: 
May 31 18:33:31.748: INFO: StatefulSet ss has not reached scale 0, at 3
May 31 18:33:32.752: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 18:33:32.752: INFO: ss-0  k8s-linuxpool-33506892-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  }]
May 31 18:33:32.752: INFO: ss-1  k8s-linuxpool-33506892-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  }]
May 31 18:33:32.753: INFO: ss-2  k8s-linuxpool-33506892-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  }]
May 31 18:33:32.753: INFO: 
May 31 18:33:32.753: INFO: StatefulSet ss has not reached scale 0, at 3
May 31 18:33:33.757: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 18:33:33.757: INFO: ss-0  k8s-linuxpool-33506892-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  }]
May 31 18:33:33.757: INFO: ss-1  k8s-linuxpool-33506892-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  }]
May 31 18:33:33.757: INFO: ss-2  k8s-linuxpool-33506892-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  }]
May 31 18:33:33.757: INFO: 
May 31 18:33:33.757: INFO: StatefulSet ss has not reached scale 0, at 3
May 31 18:33:34.761: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 18:33:34.761: INFO: ss-0  k8s-linuxpool-33506892-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  }]
May 31 18:33:34.761: INFO: ss-1  k8s-linuxpool-33506892-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  }]
May 31 18:33:34.761: INFO: 
May 31 18:33:34.761: INFO: StatefulSet ss has not reached scale 0, at 2
May 31 18:33:35.765: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 18:33:35.765: INFO: ss-0  k8s-linuxpool-33506892-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  }]
May 31 18:33:35.765: INFO: ss-1  k8s-linuxpool-33506892-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:10 +0000 UTC  }]
May 31 18:33:35.766: INFO: 
May 31 18:33:35.766: INFO: StatefulSet ss has not reached scale 0, at 2
May 31 18:33:36.788: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
May 31 18:33:36.788: INFO: ss-0  k8s-linuxpool-33506892-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:33:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:32:49 +0000 UTC  }]
May 31 18:33:36.788: INFO: 
May 31 18:33:36.788: INFO: StatefulSet ss has not reached scale 0, at 1
May 31 18:33:37.791: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.957327595s
May 31 18:33:38.794: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.954212175s
May 31 18:33:39.797: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.951255058s
May 31 18:33:40.800: INFO: Verifying statefulset ss doesn't scale past 0 for another 948.307641ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7718
May 31 18:33:41.803: INFO: Scaling statefulset ss to 0
May 31 18:33:41.811: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 18:33:41.813: INFO: Deleting all statefulset in ns statefulset-7718
May 31 18:33:41.815: INFO: Scaling statefulset ss to 0
May 31 18:33:41.822: INFO: Waiting for statefulset status.replicas updated to 0
May 31 18:33:41.824: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:33:41.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7718" for this suite.
May 31 18:33:47.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:33:47.929: INFO: namespace statefulset-7718 deletion completed in 6.090882013s

• [SLOW TEST:58.132 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:33:47.935: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-7556
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7556 to expose endpoints map[]
May 31 18:33:48.001: INFO: successfully validated that service multi-endpoint-test in namespace services-7556 exposes endpoints map[] (7.667026ms elapsed)
STEP: Creating pod pod1 in namespace services-7556
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7556 to expose endpoints map[pod1:[100]]
May 31 18:33:51.031: INFO: successfully validated that service multi-endpoint-test in namespace services-7556 exposes endpoints map[pod1:[100]] (3.024097508s elapsed)
STEP: Creating pod pod2 in namespace services-7556
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7556 to expose endpoints map[pod1:[100] pod2:[101]]
May 31 18:33:54.074: INFO: successfully validated that service multi-endpoint-test in namespace services-7556 exposes endpoints map[pod1:[100] pod2:[101]] (3.038896754s elapsed)
STEP: Deleting pod pod1 in namespace services-7556
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7556 to expose endpoints map[pod2:[101]]
May 31 18:33:54.103: INFO: successfully validated that service multi-endpoint-test in namespace services-7556 exposes endpoints map[pod2:[101]] (20.997146ms elapsed)
STEP: Deleting pod pod2 in namespace services-7556
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7556 to expose endpoints map[]
May 31 18:33:55.117: INFO: successfully validated that service multi-endpoint-test in namespace services-7556 exposes endpoints map[] (1.006630181s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:33:55.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7556" for this suite.
May 31 18:34:17.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:34:17.273: INFO: namespace services-7556 deletion completed in 22.125856804s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.338 seconds]
[sig-network] Services
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:34:17.277: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-2050
May 31 18:34:21.337: INFO: Started pod liveness-exec in namespace container-probe-2050
STEP: checking the pod's current state and verifying that restartCount is present
May 31 18:34:21.339: INFO: Initial restart count of pod liveness-exec is 0
May 31 18:35:15.466: INFO: Restart count of pod container-probe-2050/liveness-exec is now 1 (54.125704311s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:35:15.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2050" for this suite.
May 31 18:35:21.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:35:21.567: INFO: namespace container-probe-2050 deletion completed in 6.090283912s

• [SLOW TEST:64.290 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:35:21.567: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 18:35:25.651: INFO: Waiting up to 5m0s for pod "client-envvars-db41debb-83d2-11e9-a3c9-f265bfc55a07" in namespace "pods-6025" to be "success or failure"
May 31 18:35:25.655: INFO: Pod "client-envvars-db41debb-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.66626ms
May 31 18:35:27.658: INFO: Pod "client-envvars-db41debb-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006522584s
May 31 18:35:29.661: INFO: Pod "client-envvars-db41debb-83d2-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009683332s
STEP: Saw pod success
May 31 18:35:29.661: INFO: Pod "client-envvars-db41debb-83d2-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:35:29.664: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod client-envvars-db41debb-83d2-11e9-a3c9-f265bfc55a07 container env3cont: <nil>
STEP: delete the pod
May 31 18:35:29.684: INFO: Waiting for pod client-envvars-db41debb-83d2-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:35:29.687: INFO: Pod client-envvars-db41debb-83d2-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:35:29.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6025" for this suite.
May 31 18:36:19.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:36:19.800: INFO: namespace pods-6025 deletion completed in 50.110887502s

• [SLOW TEST:58.233 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:36:19.800: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 18:36:19.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb912711-83d2-11e9-a3c9-f265bfc55a07" in namespace "projected-4334" to be "success or failure"
May 31 18:36:19.862: INFO: Pod "downwardapi-volume-fb912711-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 8.729332ms
May 31 18:36:21.866: INFO: Pod "downwardapi-volume-fb912711-83d2-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012871873s
May 31 18:36:23.870: INFO: Pod "downwardapi-volume-fb912711-83d2-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016529516s
STEP: Saw pod success
May 31 18:36:23.870: INFO: Pod "downwardapi-volume-fb912711-83d2-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:36:23.872: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downwardapi-volume-fb912711-83d2-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 18:36:23.887: INFO: Waiting for pod downwardapi-volume-fb912711-83d2-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:36:23.890: INFO: Pod downwardapi-volume-fb912711-83d2-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:36:23.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4334" for this suite.
May 31 18:36:29.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:36:29.975: INFO: namespace projected-4334 deletion completed in 6.081998264s

• [SLOW TEST:10.175 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:36:29.975: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 18:36:30.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01a0d581-83d3-11e9-a3c9-f265bfc55a07" in namespace "downward-api-6259" to be "success or failure"
May 31 18:36:30.027: INFO: Pod "downwardapi-volume-01a0d581-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.652171ms
May 31 18:36:32.030: INFO: Pod "downwardapi-volume-01a0d581-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007639643s
May 31 18:36:34.034: INFO: Pod "downwardapi-volume-01a0d581-83d3-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010960028s
STEP: Saw pod success
May 31 18:36:34.034: INFO: Pod "downwardapi-volume-01a0d581-83d3-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:36:34.037: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downwardapi-volume-01a0d581-83d3-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 18:36:34.054: INFO: Waiting for pod downwardapi-volume-01a0d581-83d3-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:36:34.059: INFO: Pod downwardapi-volume-01a0d581-83d3-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:36:34.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6259" for this suite.
May 31 18:36:40.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:36:40.147: INFO: namespace downward-api-6259 deletion completed in 6.085481761s

• [SLOW TEST:10.172 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:36:40.150: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 31 18:36:40.428: INFO: Pod name wrapped-volume-race-07cb8827-83d3-11e9-a3c9-f265bfc55a07: Found 4 pods out of 5
May 31 18:36:45.436: INFO: Pod name wrapped-volume-race-07cb8827-83d3-11e9-a3c9-f265bfc55a07: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-07cb8827-83d3-11e9-a3c9-f265bfc55a07 in namespace emptydir-wrapper-9309, will wait for the garbage collector to delete the pods
May 31 18:36:59.533: INFO: Deleting ReplicationController wrapped-volume-race-07cb8827-83d3-11e9-a3c9-f265bfc55a07 took: 11.392473ms
May 31 18:36:59.834: INFO: Terminating ReplicationController wrapped-volume-race-07cb8827-83d3-11e9-a3c9-f265bfc55a07 pods took: 300.810481ms
STEP: Creating RC which spawns configmap-volume pods
May 31 18:37:40.949: INFO: Pod name wrapped-volume-race-2be61caf-83d3-11e9-a3c9-f265bfc55a07: Found 0 pods out of 5
May 31 18:37:45.957: INFO: Pod name wrapped-volume-race-2be61caf-83d3-11e9-a3c9-f265bfc55a07: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2be61caf-83d3-11e9-a3c9-f265bfc55a07 in namespace emptydir-wrapper-9309, will wait for the garbage collector to delete the pods
May 31 18:38:00.067: INFO: Deleting ReplicationController wrapped-volume-race-2be61caf-83d3-11e9-a3c9-f265bfc55a07 took: 10.845866ms
May 31 18:38:00.467: INFO: Terminating ReplicationController wrapped-volume-race-2be61caf-83d3-11e9-a3c9-f265bfc55a07 pods took: 400.291147ms
STEP: Creating RC which spawns configmap-volume pods
May 31 18:38:39.989: INFO: Pod name wrapped-volume-race-4f15d3bf-83d3-11e9-a3c9-f265bfc55a07: Found 0 pods out of 5
May 31 18:38:44.997: INFO: Pod name wrapped-volume-race-4f15d3bf-83d3-11e9-a3c9-f265bfc55a07: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4f15d3bf-83d3-11e9-a3c9-f265bfc55a07 in namespace emptydir-wrapper-9309, will wait for the garbage collector to delete the pods
May 31 18:38:59.085: INFO: Deleting ReplicationController wrapped-volume-race-4f15d3bf-83d3-11e9-a3c9-f265bfc55a07 took: 6.919707ms
May 31 18:38:59.385: INFO: Terminating ReplicationController wrapped-volume-race-4f15d3bf-83d3-11e9-a3c9-f265bfc55a07 pods took: 300.241843ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:39:35.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9309" for this suite.
May 31 18:39:41.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:39:41.409: INFO: namespace emptydir-wrapper-9309 deletion completed in 6.096981882s

• [SLOW TEST:181.260 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:39:41.409: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 31 18:39:49.497: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 31 18:39:49.500: INFO: Pod pod-with-poststart-http-hook still exists
May 31 18:39:51.500: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 31 18:39:51.503: INFO: Pod pod-with-poststart-http-hook still exists
May 31 18:39:53.501: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 31 18:39:53.503: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:39:53.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9723" for this suite.
May 31 18:40:15.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:40:15.600: INFO: namespace container-lifecycle-hook-9723 deletion completed in 22.093946919s

• [SLOW TEST:34.191 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:40:15.602: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 18:40:15.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 version'
May 31 18:40:15.728: INFO: stderr: ""
May 31 18:40:15.728: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.2\", GitCommit:\"66049e3b21efe110454d67df4fa62b08ea79a19b\", GitTreeState:\"clean\", BuildDate:\"2019-05-16T16:23:09Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.2\", GitCommit:\"89187e25b767da23d0cdf1118754ec72adb4e4ca\", GitTreeState:\"clean\", BuildDate:\"2019-05-22T01:59:49Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:40:15.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-421" for this suite.
May 31 18:40:21.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:40:21.813: INFO: namespace kubectl-421 deletion completed in 6.080928517s

• [SLOW TEST:6.211 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:40:21.813: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 31 18:40:21.851: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:40:25.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1390" for this suite.
May 31 18:40:31.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:40:31.494: INFO: namespace init-container-1390 deletion completed in 6.098161373s

• [SLOW TEST:9.681 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:40:31.494: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 31 18:40:31.544: INFO: Waiting up to 5m0s for pod "downward-api-9195ebc8-83d3-11e9-a3c9-f265bfc55a07" in namespace "downward-api-7186" to be "success or failure"
May 31 18:40:31.551: INFO: Pod "downward-api-9195ebc8-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.816106ms
May 31 18:40:33.554: INFO: Pod "downward-api-9195ebc8-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009566375s
May 31 18:40:35.557: INFO: Pod "downward-api-9195ebc8-83d3-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012745256s
STEP: Saw pod success
May 31 18:40:35.557: INFO: Pod "downward-api-9195ebc8-83d3-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:40:35.559: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downward-api-9195ebc8-83d3-11e9-a3c9-f265bfc55a07 container dapi-container: <nil>
STEP: delete the pod
May 31 18:40:35.581: INFO: Waiting for pod downward-api-9195ebc8-83d3-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:40:35.583: INFO: Pod downward-api-9195ebc8-83d3-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:40:35.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7186" for this suite.
May 31 18:40:41.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:40:41.680: INFO: namespace downward-api-7186 deletion completed in 6.093523391s

• [SLOW TEST:10.186 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:40:41.680: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-97a7d719-83d3-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 18:40:41.728: INFO: Waiting up to 5m0s for pod "pod-configmaps-97a835f7-83d3-11e9-a3c9-f265bfc55a07" in namespace "configmap-8810" to be "success or failure"
May 31 18:40:41.732: INFO: Pod "pod-configmaps-97a835f7-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.253067ms
May 31 18:40:43.740: INFO: Pod "pod-configmaps-97a835f7-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012353248s
May 31 18:40:45.743: INFO: Pod "pod-configmaps-97a835f7-83d3-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015491758s
STEP: Saw pod success
May 31 18:40:45.743: INFO: Pod "pod-configmaps-97a835f7-83d3-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:40:45.745: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-configmaps-97a835f7-83d3-11e9-a3c9-f265bfc55a07 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 18:40:45.765: INFO: Waiting for pod pod-configmaps-97a835f7-83d3-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:40:45.768: INFO: Pod pod-configmaps-97a835f7-83d3-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:40:45.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8810" for this suite.
May 31 18:40:51.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:40:51.858: INFO: namespace configmap-8810 deletion completed in 6.087120979s

• [SLOW TEST:10.178 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:40:51.858: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-9db9290f-83d3-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 18:40:51.913: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9db9b8a4-83d3-11e9-a3c9-f265bfc55a07" in namespace "projected-2244" to be "success or failure"
May 31 18:40:51.918: INFO: Pod "pod-projected-configmaps-9db9b8a4-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.960462ms
May 31 18:40:53.921: INFO: Pod "pod-projected-configmaps-9db9b8a4-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0074292s
May 31 18:40:55.925: INFO: Pod "pod-projected-configmaps-9db9b8a4-83d3-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01134655s
STEP: Saw pod success
May 31 18:40:55.925: INFO: Pod "pod-projected-configmaps-9db9b8a4-83d3-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:40:55.928: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-projected-configmaps-9db9b8a4-83d3-11e9-a3c9-f265bfc55a07 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 18:40:55.948: INFO: Waiting for pod pod-projected-configmaps-9db9b8a4-83d3-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:40:55.950: INFO: Pod pod-projected-configmaps-9db9b8a4-83d3-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:40:55.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2244" for this suite.
May 31 18:41:01.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:41:02.044: INFO: namespace projected-2244 deletion completed in 6.09125603s

• [SLOW TEST:10.186 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:41:02.044: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 18:41:02.107: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 31 18:41:02.118: INFO: Number of nodes with available pods: 0
May 31 18:41:02.118: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 31 18:41:02.136: INFO: Number of nodes with available pods: 0
May 31 18:41:02.136: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:03.139: INFO: Number of nodes with available pods: 0
May 31 18:41:03.139: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:04.139: INFO: Number of nodes with available pods: 0
May 31 18:41:04.140: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:05.139: INFO: Number of nodes with available pods: 1
May 31 18:41:05.140: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 31 18:41:05.168: INFO: Number of nodes with available pods: 0
May 31 18:41:05.168: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 31 18:41:05.184: INFO: Number of nodes with available pods: 0
May 31 18:41:05.185: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:06.190: INFO: Number of nodes with available pods: 0
May 31 18:41:06.190: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:07.189: INFO: Number of nodes with available pods: 0
May 31 18:41:07.189: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:08.188: INFO: Number of nodes with available pods: 0
May 31 18:41:08.188: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:09.188: INFO: Number of nodes with available pods: 0
May 31 18:41:09.188: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:10.192: INFO: Number of nodes with available pods: 0
May 31 18:41:10.193: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:11.188: INFO: Number of nodes with available pods: 0
May 31 18:41:11.188: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:41:12.188: INFO: Number of nodes with available pods: 1
May 31 18:41:12.189: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7291, will wait for the garbage collector to delete the pods
May 31 18:41:12.255: INFO: Deleting DaemonSet.extensions daemon-set took: 6.249598ms
May 31 18:41:12.557: INFO: Terminating DaemonSet.extensions daemon-set pods took: 301.24012ms
May 31 18:41:16.160: INFO: Number of nodes with available pods: 0
May 31 18:41:16.160: INFO: Number of running nodes: 0, number of available pods: 0
May 31 18:41:16.162: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7291/daemonsets","resourceVersion":"7753"},"items":null}

May 31 18:41:16.164: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7291/pods","resourceVersion":"7753"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:41:16.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7291" for this suite.
May 31 18:41:22.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:41:22.280: INFO: namespace daemonsets-7291 deletion completed in 6.095350562s

• [SLOW TEST:20.235 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:41:22.280: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
May 31 18:41:22.322: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-130611839 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:41:22.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7607" for this suite.
May 31 18:41:28.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:41:28.490: INFO: namespace kubectl-7607 deletion completed in 6.085779962s

• [SLOW TEST:6.210 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:41:28.490: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 31 18:41:28.538: INFO: Waiting up to 5m0s for pod "downward-api-b38e9cb4-83d3-11e9-a3c9-f265bfc55a07" in namespace "downward-api-6711" to be "success or failure"
May 31 18:41:28.543: INFO: Pod "downward-api-b38e9cb4-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.15628ms
May 31 18:41:30.548: INFO: Pod "downward-api-b38e9cb4-83d3-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010120441s
May 31 18:41:32.551: INFO: Pod "downward-api-b38e9cb4-83d3-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013165277s
STEP: Saw pod success
May 31 18:41:32.552: INFO: Pod "downward-api-b38e9cb4-83d3-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:41:32.554: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downward-api-b38e9cb4-83d3-11e9-a3c9-f265bfc55a07 container dapi-container: <nil>
STEP: delete the pod
May 31 18:41:32.573: INFO: Waiting for pod downward-api-b38e9cb4-83d3-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:41:32.575: INFO: Pod downward-api-b38e9cb4-83d3-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:41:32.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6711" for this suite.
May 31 18:41:38.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:41:38.732: INFO: namespace downward-api-6711 deletion completed in 6.154206417s

• [SLOW TEST:10.241 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:41:38.732: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-d29z
STEP: Creating a pod to test atomic-volume-subpath
May 31 18:41:38.860: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d29z" in namespace "subpath-1310" to be "success or failure"
May 31 18:41:38.862: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.598341ms
May 31 18:41:40.865: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005682299s
May 31 18:41:42.869: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 4.008877364s
May 31 18:41:44.871: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 6.011678628s
May 31 18:41:46.874: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 8.014621599s
May 31 18:41:48.878: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 10.018139984s
May 31 18:41:50.881: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 12.021253067s
May 31 18:41:52.883: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 14.023688945s
May 31 18:41:54.886: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 16.026363332s
May 31 18:41:56.890: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 18.029951138s
May 31 18:41:58.893: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 20.033655151s
May 31 18:42:00.897: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Running", Reason="", readiness=true. Elapsed: 22.036860261s
May 31 18:42:02.899: INFO: Pod "pod-subpath-test-configmap-d29z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.039534768s
STEP: Saw pod success
May 31 18:42:02.899: INFO: Pod "pod-subpath-test-configmap-d29z" satisfied condition "success or failure"
May 31 18:42:02.902: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-subpath-test-configmap-d29z container test-container-subpath-configmap-d29z: <nil>
STEP: delete the pod
May 31 18:42:02.917: INFO: Waiting for pod pod-subpath-test-configmap-d29z to disappear
May 31 18:42:02.920: INFO: Pod pod-subpath-test-configmap-d29z no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d29z
May 31 18:42:02.920: INFO: Deleting pod "pod-subpath-test-configmap-d29z" in namespace "subpath-1310"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:42:02.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1310" for this suite.
May 31 18:42:08.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:42:09.012: INFO: namespace subpath-1310 deletion completed in 6.087274197s

• [SLOW TEST:30.280 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:42:09.012: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 18:42:09.064: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 31 18:42:09.074: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:09.076: INFO: Number of nodes with available pods: 0
May 31 18:42:09.076: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:42:10.082: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:10.085: INFO: Number of nodes with available pods: 0
May 31 18:42:10.085: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:42:11.081: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:11.085: INFO: Number of nodes with available pods: 0
May 31 18:42:11.085: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:42:12.080: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:12.083: INFO: Number of nodes with available pods: 2
May 31 18:42:12.083: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:42:13.080: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:13.083: INFO: Number of nodes with available pods: 3
May 31 18:42:13.083: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 31 18:42:13.125: INFO: Wrong image for pod: daemon-set-68xcs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:13.125: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:13.125: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:13.133: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:14.137: INFO: Wrong image for pod: daemon-set-68xcs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:14.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:14.137: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:14.140: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:15.137: INFO: Wrong image for pod: daemon-set-68xcs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:15.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:15.137: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:15.139: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:16.137: INFO: Wrong image for pod: daemon-set-68xcs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:16.137: INFO: Pod daemon-set-68xcs is not available
May 31 18:42:16.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:16.137: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:16.140: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:17.137: INFO: Wrong image for pod: daemon-set-68xcs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:17.138: INFO: Pod daemon-set-68xcs is not available
May 31 18:42:17.138: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:17.138: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:17.141: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:18.165: INFO: Pod daemon-set-bh98s is not available
May 31 18:42:18.165: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:18.165: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:18.168: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:19.137: INFO: Pod daemon-set-bh98s is not available
May 31 18:42:19.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:19.137: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:19.140: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:20.141: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:20.141: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:20.144: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:21.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:21.137: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:21.137: INFO: Pod daemon-set-z48zt is not available
May 31 18:42:21.140: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:22.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:22.138: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:22.138: INFO: Pod daemon-set-z48zt is not available
May 31 18:42:22.141: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:23.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:23.138: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:23.138: INFO: Pod daemon-set-z48zt is not available
May 31 18:42:23.141: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:24.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:24.137: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:24.137: INFO: Pod daemon-set-z48zt is not available
May 31 18:42:24.140: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:25.138: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:25.138: INFO: Wrong image for pod: daemon-set-z48zt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:25.139: INFO: Pod daemon-set-z48zt is not available
May 31 18:42:25.142: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:26.137: INFO: Pod daemon-set-hmw5b is not available
May 31 18:42:26.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:26.140: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:27.137: INFO: Pod daemon-set-hmw5b is not available
May 31 18:42:27.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:27.140: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:28.137: INFO: Pod daemon-set-hmw5b is not available
May 31 18:42:28.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:28.140: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:29.137: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:29.141: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:30.138: INFO: Wrong image for pod: daemon-set-n5zzg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 31 18:42:30.138: INFO: Pod daemon-set-n5zzg is not available
May 31 18:42:30.153: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:31.137: INFO: Pod daemon-set-gstbs is not available
May 31 18:42:31.140: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 31 18:42:31.143: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:31.146: INFO: Number of nodes with available pods: 2
May 31 18:42:31.146: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:42:32.149: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:32.153: INFO: Number of nodes with available pods: 2
May 31 18:42:32.153: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:42:33.150: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:42:33.153: INFO: Number of nodes with available pods: 3
May 31 18:42:33.153: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1537, will wait for the garbage collector to delete the pods
May 31 18:42:33.230: INFO: Deleting DaemonSet.extensions daemon-set took: 6.3319ms
May 31 18:42:33.530: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.226234ms
May 31 18:42:46.033: INFO: Number of nodes with available pods: 0
May 31 18:42:46.033: INFO: Number of running nodes: 0, number of available pods: 0
May 31 18:42:46.035: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1537/daemonsets","resourceVersion":"8101"},"items":null}

May 31 18:42:46.038: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1537/pods","resourceVersion":"8101"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:42:46.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1537" for this suite.
May 31 18:42:52.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:42:52.217: INFO: namespace daemonsets-1537 deletion completed in 6.160350357s

• [SLOW TEST:43.205 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:42:52.222: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 18:42:52.277: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 31 18:42:57.281: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 31 18:42:57.281: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 18:42:57.301: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8324,SelfLink:/apis/apps/v1/namespaces/deployment-8324/deployments/test-cleanup-deployment,UID:e875775a-83d3-11e9-a607-001dd80c001e,ResourceVersion:8181,Generation:1,CreationTimestamp:2019-05-31 18:42:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 31 18:42:57.305: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May 31 18:42:57.305: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 31 18:42:57.305: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-8324,SelfLink:/apis/apps/v1/namespaces/deployment-8324/replicasets/test-cleanup-controller,UID:e5781965-83d3-11e9-a607-001dd80c001e,ResourceVersion:8182,Generation:1,CreationTimestamp:2019-05-31 18:42:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment e875775a-83d3-11e9-a607-001dd80c001e 0xc002f4455f 0xc002f44570}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 31 18:42:57.311: INFO: Pod "test-cleanup-controller-5rf68" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-5rf68,GenerateName:test-cleanup-controller-,Namespace:deployment-8324,SelfLink:/api/v1/namespaces/deployment-8324/pods/test-cleanup-controller-5rf68,UID:e579251d-83d3-11e9-a607-001dd80c001e,ResourceVersion:8177,Generation:0,CreationTimestamp:2019-05-31 18:42:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller e5781965-83d3-11e9-a607-001dd80c001e 0xc0028a4b7f 0xc0028a4b90}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hcl4z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hcl4z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hcl4z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028a4bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028a4c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:42:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:42:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:42:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:42:52 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.45,StartTime:2019-05-31 18:42:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 18:42:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c0c7ac05610f5585e122e066e31392d6849e29acc1f8610ae6809ba90f0750ee}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:42:57.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8324" for this suite.
May 31 18:43:03.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:43:03.415: INFO: namespace deployment-8324 deletion completed in 6.09176665s

• [SLOW TEST:11.193 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:43:03.415: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
May 31 18:43:03.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-7194'
May 31 18:43:04.143: INFO: stderr: ""
May 31 18:43:04.143: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 18:43:04.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7194'
May 31 18:43:04.352: INFO: stderr: ""
May 31 18:43:04.352: INFO: stdout: "update-demo-nautilus-g2csd update-demo-nautilus-q52cl "
May 31 18:43:04.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-g2csd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7194'
May 31 18:43:04.442: INFO: stderr: ""
May 31 18:43:04.442: INFO: stdout: ""
May 31 18:43:04.442: INFO: update-demo-nautilus-g2csd is created but not running
May 31 18:43:09.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7194'
May 31 18:43:09.537: INFO: stderr: ""
May 31 18:43:09.537: INFO: stdout: "update-demo-nautilus-g2csd update-demo-nautilus-q52cl "
May 31 18:43:09.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-g2csd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7194'
May 31 18:43:09.614: INFO: stderr: ""
May 31 18:43:09.614: INFO: stdout: "true"
May 31 18:43:09.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-g2csd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7194'
May 31 18:43:09.694: INFO: stderr: ""
May 31 18:43:09.694: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 18:43:09.694: INFO: validating pod update-demo-nautilus-g2csd
May 31 18:43:09.699: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 18:43:09.699: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 18:43:09.699: INFO: update-demo-nautilus-g2csd is verified up and running
May 31 18:43:09.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-q52cl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7194'
May 31 18:43:09.779: INFO: stderr: ""
May 31 18:43:09.779: INFO: stdout: "true"
May 31 18:43:09.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-q52cl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7194'
May 31 18:43:09.866: INFO: stderr: ""
May 31 18:43:09.866: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 18:43:09.866: INFO: validating pod update-demo-nautilus-q52cl
May 31 18:43:09.870: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 18:43:09.870: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 18:43:09.870: INFO: update-demo-nautilus-q52cl is verified up and running
STEP: rolling-update to new replication controller
May 31 18:43:09.872: INFO: scanned /root for discovery docs: <nil>
May 31 18:43:09.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7194'
May 31 18:43:37.506: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 31 18:43:37.506: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 18:43:37.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7194'
May 31 18:43:37.605: INFO: stderr: ""
May 31 18:43:37.605: INFO: stdout: "update-demo-kitten-b4v6h update-demo-kitten-nt58c "
May 31 18:43:37.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-kitten-b4v6h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7194'
May 31 18:43:37.706: INFO: stderr: ""
May 31 18:43:37.706: INFO: stdout: "true"
May 31 18:43:37.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-kitten-b4v6h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7194'
May 31 18:43:37.858: INFO: stderr: ""
May 31 18:43:37.858: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 31 18:43:37.858: INFO: validating pod update-demo-kitten-b4v6h
May 31 18:43:37.876: INFO: got data: {
  "image": "kitten.jpg"
}

May 31 18:43:37.876: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 31 18:43:37.876: INFO: update-demo-kitten-b4v6h is verified up and running
May 31 18:43:37.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-kitten-nt58c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7194'
May 31 18:43:38.002: INFO: stderr: ""
May 31 18:43:38.002: INFO: stdout: "true"
May 31 18:43:38.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-kitten-nt58c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7194'
May 31 18:43:38.107: INFO: stderr: ""
May 31 18:43:38.107: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 31 18:43:38.107: INFO: validating pod update-demo-kitten-nt58c
May 31 18:43:38.113: INFO: got data: {
  "image": "kitten.jpg"
}

May 31 18:43:38.113: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 31 18:43:38.113: INFO: update-demo-kitten-nt58c is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:43:38.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7194" for this suite.
May 31 18:44:00.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:44:00.200: INFO: namespace kubectl-7194 deletion completed in 22.082041316s

• [SLOW TEST:56.785 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:44:00.201: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3315
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 18:44:00.247: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 31 18:44:26.375: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.49:8080/dial?request=hostName&protocol=http&host=10.244.1.67&port=8080&tries=1'] Namespace:pod-network-test-3315 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:44:26.375: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:44:26.507: INFO: Waiting for endpoints: map[]
May 31 18:44:26.510: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.49:8080/dial?request=hostName&protocol=http&host=10.244.2.19&port=8080&tries=1'] Namespace:pod-network-test-3315 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:44:26.510: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:44:26.635: INFO: Waiting for endpoints: map[]
May 31 18:44:26.637: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.49:8080/dial?request=hostName&protocol=http&host=10.244.3.48&port=8080&tries=1'] Namespace:pod-network-test-3315 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:44:26.637: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:44:26.776: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:44:26.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3315" for this suite.
May 31 18:44:40.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:44:40.862: INFO: namespace pod-network-test-3315 deletion completed in 14.082317613s

• [SLOW TEST:40.662 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:44:40.862: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-2637ddc2-83d4-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 18:44:40.909: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-263852e4-83d4-11e9-a3c9-f265bfc55a07" in namespace "projected-5076" to be "success or failure"
May 31 18:44:40.914: INFO: Pod "pod-projected-secrets-263852e4-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.190083ms
May 31 18:44:42.917: INFO: Pod "pod-projected-secrets-263852e4-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008261033s
May 31 18:44:44.920: INFO: Pod "pod-projected-secrets-263852e4-83d4-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011059482s
STEP: Saw pod success
May 31 18:44:44.920: INFO: Pod "pod-projected-secrets-263852e4-83d4-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:44:44.922: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-projected-secrets-263852e4-83d4-11e9-a3c9-f265bfc55a07 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 18:44:44.937: INFO: Waiting for pod pod-projected-secrets-263852e4-83d4-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:44:44.939: INFO: Pod pod-projected-secrets-263852e4-83d4-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:44:44.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5076" for this suite.
May 31 18:44:50.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:44:51.036: INFO: namespace projected-5076 deletion completed in 6.094254435s

• [SLOW TEST:10.174 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:44:51.037: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 31 18:44:51.095: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9438,SelfLink:/api/v1/namespaces/watch-9438/configmaps/e2e-watch-test-resource-version,UID:2c48c5ef-83d4-11e9-a607-001dd80c001e,ResourceVersion:8667,Generation:0,CreationTimestamp:2019-05-31 18:44:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 18:44:51.095: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9438,SelfLink:/api/v1/namespaces/watch-9438/configmaps/e2e-watch-test-resource-version,UID:2c48c5ef-83d4-11e9-a607-001dd80c001e,ResourceVersion:8668,Generation:0,CreationTimestamp:2019-05-31 18:44:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:44:51.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9438" for this suite.
May 31 18:44:57.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:44:57.178: INFO: namespace watch-9438 deletion completed in 6.079819038s

• [SLOW TEST:6.141 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:44:57.181: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 18:44:57.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ff9615a-83d4-11e9-a3c9-f265bfc55a07" in namespace "projected-3536" to be "success or failure"
May 31 18:44:57.282: INFO: Pod "downwardapi-volume-2ff9615a-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.92661ms
May 31 18:44:59.286: INFO: Pod "downwardapi-volume-2ff9615a-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010650999s
May 31 18:45:01.289: INFO: Pod "downwardapi-volume-2ff9615a-83d4-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013917384s
STEP: Saw pod success
May 31 18:45:01.289: INFO: Pod "downwardapi-volume-2ff9615a-83d4-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:45:01.291: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downwardapi-volume-2ff9615a-83d4-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 18:45:01.306: INFO: Waiting for pod downwardapi-volume-2ff9615a-83d4-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:45:01.308: INFO: Pod downwardapi-volume-2ff9615a-83d4-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:45:01.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3536" for this suite.
May 31 18:45:07.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:45:07.394: INFO: namespace projected-3536 deletion completed in 6.083133244s

• [SLOW TEST:10.213 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:45:07.397: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-4441
May 31 18:45:11.454: INFO: Started pod liveness-exec in namespace container-probe-4441
STEP: checking the pod's current state and verifying that restartCount is present
May 31 18:45:11.457: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:49:11.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4441" for this suite.
May 31 18:49:17.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:49:18.032: INFO: namespace container-probe-4441 deletion completed in 6.085568617s

• [SLOW TEST:250.636 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:49:18.033: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
May 31 18:49:18.087: INFO: Waiting up to 5m0s for pod "client-containers-cb6e073d-83d4-11e9-a3c9-f265bfc55a07" in namespace "containers-5" to be "success or failure"
May 31 18:49:18.092: INFO: Pod "client-containers-cb6e073d-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184283ms
May 31 18:49:20.096: INFO: Pod "client-containers-cb6e073d-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008518921s
May 31 18:49:22.099: INFO: Pod "client-containers-cb6e073d-83d4-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012236967s
STEP: Saw pod success
May 31 18:49:22.099: INFO: Pod "client-containers-cb6e073d-83d4-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:49:22.102: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod client-containers-cb6e073d-83d4-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:49:22.122: INFO: Waiting for pod client-containers-cb6e073d-83d4-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:49:22.124: INFO: Pod client-containers-cb6e073d-83d4-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:49:22.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5" for this suite.
May 31 18:49:28.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:49:28.219: INFO: namespace containers-5 deletion completed in 6.090993036s

• [SLOW TEST:10.186 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:49:28.220: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-d17f8f87-83d4-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 18:49:28.272: INFO: Waiting up to 5m0s for pod "pod-configmaps-d18020ff-83d4-11e9-a3c9-f265bfc55a07" in namespace "configmap-8821" to be "success or failure"
May 31 18:49:28.276: INFO: Pod "pod-configmaps-d18020ff-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105766ms
May 31 18:49:30.279: INFO: Pod "pod-configmaps-d18020ff-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007257411s
May 31 18:49:32.282: INFO: Pod "pod-configmaps-d18020ff-83d4-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010571761s
STEP: Saw pod success
May 31 18:49:32.282: INFO: Pod "pod-configmaps-d18020ff-83d4-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:49:32.285: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-configmaps-d18020ff-83d4-11e9-a3c9-f265bfc55a07 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 18:49:32.303: INFO: Waiting for pod pod-configmaps-d18020ff-83d4-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:49:32.305: INFO: Pod pod-configmaps-d18020ff-83d4-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:49:32.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8821" for this suite.
May 31 18:49:38.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:49:38.402: INFO: namespace configmap-8821 deletion completed in 6.094985833s

• [SLOW TEST:10.183 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:49:38.405: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-d79513e9-83d4-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 18:49:38.480: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d795b4a7-83d4-11e9-a3c9-f265bfc55a07" in namespace "projected-2097" to be "success or failure"
May 31 18:49:38.484: INFO: Pod "pod-projected-secrets-d795b4a7-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.714959ms
May 31 18:49:40.486: INFO: Pod "pod-projected-secrets-d795b4a7-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00656741s
May 31 18:49:42.489: INFO: Pod "pod-projected-secrets-d795b4a7-83d4-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009463164s
STEP: Saw pod success
May 31 18:49:42.489: INFO: Pod "pod-projected-secrets-d795b4a7-83d4-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:49:42.492: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-projected-secrets-d795b4a7-83d4-11e9-a3c9-f265bfc55a07 container secret-volume-test: <nil>
STEP: delete the pod
May 31 18:49:42.510: INFO: Waiting for pod pod-projected-secrets-d795b4a7-83d4-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:49:42.512: INFO: Pod pod-projected-secrets-d795b4a7-83d4-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:49:42.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2097" for this suite.
May 31 18:49:48.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:49:48.656: INFO: namespace projected-2097 deletion completed in 6.140530497s

• [SLOW TEST:10.250 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:49:48.656: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 31 18:49:48.733: INFO: Waiting up to 5m0s for pod "pod-ddb0bbad-83d4-11e9-a3c9-f265bfc55a07" in namespace "emptydir-4887" to be "success or failure"
May 31 18:49:48.764: INFO: Pod "pod-ddb0bbad-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 30.205287ms
May 31 18:49:50.767: INFO: Pod "pod-ddb0bbad-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03322555s
May 31 18:49:52.770: INFO: Pod "pod-ddb0bbad-83d4-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036548821s
STEP: Saw pod success
May 31 18:49:52.770: INFO: Pod "pod-ddb0bbad-83d4-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:49:52.773: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-ddb0bbad-83d4-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:49:52.791: INFO: Waiting for pod pod-ddb0bbad-83d4-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:49:52.793: INFO: Pod pod-ddb0bbad-83d4-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:49:52.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4887" for this suite.
May 31 18:49:58.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:49:58.908: INFO: namespace emptydir-4887 deletion completed in 6.112402674s

• [SLOW TEST:10.252 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:49:58.912: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 31 18:49:59.101: INFO: Waiting up to 5m0s for pod "pod-e3df83b3-83d4-11e9-a3c9-f265bfc55a07" in namespace "emptydir-8022" to be "success or failure"
May 31 18:49:59.106: INFO: Pod "pod-e3df83b3-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.866478ms
May 31 18:50:01.109: INFO: Pod "pod-e3df83b3-83d4-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007879852s
May 31 18:50:03.112: INFO: Pod "pod-e3df83b3-83d4-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010956829s
STEP: Saw pod success
May 31 18:50:03.112: INFO: Pod "pod-e3df83b3-83d4-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:50:03.114: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-e3df83b3-83d4-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:50:03.129: INFO: Waiting for pod pod-e3df83b3-83d4-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:50:03.132: INFO: Pod pod-e3df83b3-83d4-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:50:03.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8022" for this suite.
May 31 18:50:09.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:50:09.371: INFO: namespace emptydir-8022 deletion completed in 6.236317302s

• [SLOW TEST:10.459 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:50:09.372: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
May 31 18:50:09.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-6717'
May 31 18:50:09.674: INFO: stderr: ""
May 31 18:50:09.674: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
May 31 18:50:10.677: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:50:10.677: INFO: Found 0 / 1
May 31 18:50:11.677: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:50:11.677: INFO: Found 0 / 1
May 31 18:50:12.718: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:50:12.718: INFO: Found 1 / 1
May 31 18:50:12.718: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 31 18:50:12.720: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:50:12.720: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 31 18:50:12.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 logs redis-master-kkq9t redis-master --namespace=kubectl-6717'
May 31 18:50:12.823: INFO: stderr: ""
May 31 18:50:12.823: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 31 May 18:50:11.408 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 May 18:50:11.408 # Server started, Redis version 3.2.12\n1:M 31 May 18:50:11.408 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 May 18:50:11.408 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 31 18:50:12.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 log redis-master-kkq9t redis-master --namespace=kubectl-6717 --tail=1'
May 31 18:50:12.923: INFO: stderr: ""
May 31 18:50:12.923: INFO: stdout: "1:M 31 May 18:50:11.408 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 31 18:50:12.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 log redis-master-kkq9t redis-master --namespace=kubectl-6717 --limit-bytes=1'
May 31 18:50:13.016: INFO: stderr: ""
May 31 18:50:13.016: INFO: stdout: " "
STEP: exposing timestamps
May 31 18:50:13.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 log redis-master-kkq9t redis-master --namespace=kubectl-6717 --tail=1 --timestamps'
May 31 18:50:13.112: INFO: stderr: ""
May 31 18:50:13.112: INFO: stdout: "2019-05-31T18:50:11.408243385Z 1:M 31 May 18:50:11.408 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 31 18:50:15.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 log redis-master-kkq9t redis-master --namespace=kubectl-6717 --since=1s'
May 31 18:50:15.734: INFO: stderr: ""
May 31 18:50:15.734: INFO: stdout: ""
May 31 18:50:15.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 log redis-master-kkq9t redis-master --namespace=kubectl-6717 --since=24h'
May 31 18:50:15.873: INFO: stderr: ""
May 31 18:50:15.874: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 31 May 18:50:11.408 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 May 18:50:11.408 # Server started, Redis version 3.2.12\n1:M 31 May 18:50:11.408 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 May 18:50:11.408 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
May 31 18:50:15.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-6717'
May 31 18:50:16.000: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 18:50:16.000: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 31 18:50:16.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6717'
May 31 18:50:16.266: INFO: stderr: "No resources found.\n"
May 31 18:50:16.266: INFO: stdout: ""
May 31 18:50:16.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -l name=nginx --namespace=kubectl-6717 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 18:50:16.352: INFO: stderr: ""
May 31 18:50:16.352: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:50:16.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6717" for this suite.
May 31 18:50:38.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:50:38.487: INFO: namespace kubectl-6717 deletion completed in 22.130139458s

• [SLOW TEST:29.116 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:50:38.488: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1581
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 18:50:38.548: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 31 18:51:00.637: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.20:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1581 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:51:00.637: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:51:00.763: INFO: Found all expected endpoints: [netserver-0]
May 31 18:51:00.765: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.56:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1581 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:51:00.765: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:51:00.902: INFO: Found all expected endpoints: [netserver-1]
May 31 18:51:00.905: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.71:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1581 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:51:00.905: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:51:01.041: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:51:01.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1581" for this suite.
May 31 18:51:23.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:51:23.128: INFO: namespace pod-network-test-1581 deletion completed in 22.083061729s

• [SLOW TEST:44.641 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:51:23.129: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 18:51:23.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 version --client'
May 31 18:51:23.229: INFO: stderr: ""
May 31 18:51:23.229: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.2\", GitCommit:\"66049e3b21efe110454d67df4fa62b08ea79a19b\", GitTreeState:\"clean\", BuildDate:\"2019-05-16T16:23:09Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 31 18:51:23.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-4662'
May 31 18:51:23.510: INFO: stderr: ""
May 31 18:51:23.510: INFO: stdout: "replicationcontroller/redis-master created\n"
May 31 18:51:23.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-4662'
May 31 18:51:23.807: INFO: stderr: ""
May 31 18:51:23.807: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 31 18:51:24.810: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:51:24.810: INFO: Found 0 / 1
May 31 18:51:25.810: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:51:25.810: INFO: Found 0 / 1
May 31 18:51:26.811: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:51:26.811: INFO: Found 1 / 1
May 31 18:51:26.811: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 31 18:51:26.814: INFO: Selector matched 1 pods for map[app:redis]
May 31 18:51:26.814: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 31 18:51:26.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 describe pod redis-master-g9spj --namespace=kubectl-4662'
May 31 18:51:26.941: INFO: stderr: ""
May 31 18:51:26.941: INFO: stdout: "Name:               redis-master-g9spj\nNamespace:          kubectl-4662\nPriority:           0\nPriorityClassName:  <none>\nNode:               k8s-linuxpool-33506892-0/10.240.0.4\nStart Time:         Fri, 31 May 2019 18:51:23 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.244.1.73\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://031eed4368d0627d90dc16ee87aed065e28902242110a9d7fa5cfc995504e5f8\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 31 May 2019 18:51:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bl7gh (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-bl7gh:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-bl7gh\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                               Message\n  ----    ------     ----  ----                               -------\n  Normal  Scheduled  3s    default-scheduler                  Successfully assigned kubectl-4662/redis-master-g9spj to k8s-linuxpool-33506892-0\n  Normal  Pulled     2s    kubelet, k8s-linuxpool-33506892-0  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, k8s-linuxpool-33506892-0  Created container redis-master\n  Normal  Started    1s    kubelet, k8s-linuxpool-33506892-0  Started container redis-master\n"
May 31 18:51:26.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 describe rc redis-master --namespace=kubectl-4662'
May 31 18:51:27.063: INFO: stderr: ""
May 31 18:51:27.063: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4662\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-g9spj\n"
May 31 18:51:27.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 describe service redis-master --namespace=kubectl-4662'
May 31 18:51:27.177: INFO: stderr: ""
May 31 18:51:27.177: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4662\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.60.4\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.1.73:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 31 18:51:27.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 describe node k8s-linuxpool-33506892-0'
May 31 18:51:27.290: INFO: stderr: ""
May 31 18:51:27.290: INFO: stdout: "Name:               k8s-linuxpool-33506892-0\nRoles:              agent\nLabels:             agentpool=linuxpool\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=redmond\n                    failure-domain.beta.kubernetes.io/zone=1\n                    kubernetes.azure.com/cluster=rgk8s-75951\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-linuxpool-33506892-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=agent\n                    node-role.kubernetes.io/agent=\n                    storageprofile=managed\n                    storagetier=Standard_LRS\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"76:d9:78:41:16:c3\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.240.0.4\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 31 May 2019 18:09:15 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 31 May 2019 18:50:41 +0000   Fri, 31 May 2019 18:09:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 31 May 2019 18:50:41 +0000   Fri, 31 May 2019 18:09:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 31 May 2019 18:50:41 +0000   Fri, 31 May 2019 18:09:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 31 May 2019 18:50:41 +0000   Fri, 31 May 2019 18:10:05 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.240.0.4\n  Hostname:    k8s-linuxpool-33506892-0\nCapacity:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              203234980Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7137164Ki\n pods:                           110\nAllocatable:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              187301357258\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         6369164Ki\n pods:                           110\nSystem Info:\n Machine ID:                 3594d51b886b462a8fb4b13092b24f49\n System UUID:                BD6FD9BF-F461-2840-91CD-B83505F42909\n Boot ID:                    d4b70bd8-a210-4139-8c9c-05c752b28123\n Kernel Version:             4.15.0-1022-azure\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://3.0.5\n Kubelet Version:            v1.14.2\n Kube-Proxy Version:         v1.14.2\nPodCIDR:                     10.244.1.0/24\nProviderID:                  azure:///subscriptions/a0cfa6bc-958d-43c6-af3e-3c863ae1ebf5/resourceGroups/rgk8s-75951/providers/Microsoft.Compute/virtualMachines/k8s-linuxpool-33506892-0\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-zhmp4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  kube-system                azure-ip-masq-agent-hffrz                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (4%)     41m\n  kube-system                kube-flannel-ds-5kgxc                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  kube-system                kube-proxy-5bk4d                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         41m\n  kube-system                metrics-server-6bf85bb69b-rc4nd                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  kube-system                tiller-deploy-5d9ffb885b-sm2bb                             50m (2%)      50m (2%)    150Mi (2%)       150Mi (2%)     41m\n  kubectl-4662               redis-master-g9spj                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            200m (10%)  100m (5%)\n  memory                         200Mi (3%)  400Mi (6%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:\n  Type    Reason                   Age                From                                  Message\n  ----    ------                   ----               ----                                  -------\n  Normal  NodeHasSufficientMemory  42m (x7 over 42m)  kubelet, k8s-linuxpool-33506892-0     Node k8s-linuxpool-33506892-0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    42m (x7 over 42m)  kubelet, k8s-linuxpool-33506892-0     Node k8s-linuxpool-33506892-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     42m (x7 over 42m)  kubelet, k8s-linuxpool-33506892-0     Node k8s-linuxpool-33506892-0 status is now: NodeHasSufficientPID\n  Normal  Starting                 41m                kube-proxy, k8s-linuxpool-33506892-0  Starting kube-proxy.\n  Normal  NodeReady                41m                kubelet, k8s-linuxpool-33506892-0     Node k8s-linuxpool-33506892-0 status is now: NodeReady\n"
May 31 18:51:27.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 describe namespace kubectl-4662'
May 31 18:51:27.397: INFO: stderr: ""
May 31 18:51:27.397: INFO: stdout: "Name:         kubectl-4662\nLabels:       e2e-framework=kubectl\n              e2e-run=90d4ac92-83cf-11e9-a3c9-f265bfc55a07\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:51:27.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4662" for this suite.
May 31 18:51:49.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:51:49.501: INFO: namespace kubectl-4662 deletion completed in 22.101363162s

• [SLOW TEST:26.373 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:51:49.502: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 31 18:51:53.606: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-25b99cef-83d5-11e9-a3c9-f265bfc55a07,GenerateName:,Namespace:events-3655,SelfLink:/api/v1/namespaces/events-3655/pods/send-events-25b99cef-83d5-11e9-a3c9-f265bfc55a07,UID:25ba420d-83d5-11e9-a607-001dd80c001e,ResourceVersion:9702,Generation:0,CreationTimestamp:2019-05-31 18:51:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 570486212,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m8lm4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m8lm4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-m8lm4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029e1690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029e16b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:51:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:51:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:51:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 18:51:49 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.57,StartTime:2019-05-31 18:51:49 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-31 18:51:51 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://a2b5e7f063554f71b43a314ba464a56a171e86e0ec44fa0ecbbe8e396aa068d8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 31 18:51:55.610: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 31 18:51:57.614: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:51:57.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3655" for this suite.
May 31 18:52:35.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:52:35.726: INFO: namespace events-3655 deletion completed in 38.096224425s

• [SLOW TEST:46.225 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:52:35.727: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-4143e296-83d5-11e9-a3c9-f265bfc55a07
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-4143e296-83d5-11e9-a3c9-f265bfc55a07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:53:54.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2910" for this suite.
May 31 18:54:16.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:54:16.400: INFO: namespace configmap-2910 deletion completed in 22.08770936s

• [SLOW TEST:100.673 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:54:16.400: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 31 18:54:16.469: INFO: Waiting up to 5m0s for pod "pod-7d47828c-83d5-11e9-a3c9-f265bfc55a07" in namespace "emptydir-2044" to be "success or failure"
May 31 18:54:16.472: INFO: Pod "pod-7d47828c-83d5-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.675044ms
May 31 18:54:18.475: INFO: Pod "pod-7d47828c-83d5-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005360007s
May 31 18:54:20.478: INFO: Pod "pod-7d47828c-83d5-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00859138s
STEP: Saw pod success
May 31 18:54:20.478: INFO: Pod "pod-7d47828c-83d5-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:54:20.480: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-7d47828c-83d5-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 18:54:20.501: INFO: Waiting for pod pod-7d47828c-83d5-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:54:20.503: INFO: Pod pod-7d47828c-83d5-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:54:20.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2044" for this suite.
May 31 18:54:26.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:54:26.684: INFO: namespace emptydir-2044 deletion completed in 6.177266344s

• [SLOW TEST:10.284 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:54:26.685: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 31 18:54:26.737: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:54:39.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1574" for this suite.
May 31 18:54:45.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:54:45.967: INFO: namespace pods-1574 deletion completed in 6.097100477s

• [SLOW TEST:19.282 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:54:45.968: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
May 31 18:54:46.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 cluster-info'
May 31 18:54:46.525: INFO: stderr: ""
May 31 18:54:46.525: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mtiller-deploy\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/tiller-deploy:tiller/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:54:46.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-358" for this suite.
May 31 18:54:52.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:54:52.619: INFO: namespace kubectl-358 deletion completed in 6.090027874s

• [SLOW TEST:6.651 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:54:52.620: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 18:54:52.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92e3c740-83d5-11e9-a3c9-f265bfc55a07" in namespace "projected-8806" to be "success or failure"
May 31 18:54:52.728: INFO: Pod "downwardapi-volume-92e3c740-83d5-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942548ms
May 31 18:54:54.731: INFO: Pod "downwardapi-volume-92e3c740-83d5-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005869535s
May 31 18:54:56.733: INFO: Pod "downwardapi-volume-92e3c740-83d5-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008603621s
STEP: Saw pod success
May 31 18:54:56.733: INFO: Pod "downwardapi-volume-92e3c740-83d5-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:54:56.736: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downwardapi-volume-92e3c740-83d5-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 18:54:56.761: INFO: Waiting for pod downwardapi-volume-92e3c740-83d5-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:54:56.775: INFO: Pod downwardapi-volume-92e3c740-83d5-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:54:56.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8806" for this suite.
May 31 18:55:02.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:55:02.863: INFO: namespace projected-8806 deletion completed in 6.084846207s

• [SLOW TEST:10.243 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:55:02.864: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-98fe3f60-83d5-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 18:55:02.968: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98feaad5-83d5-11e9-a3c9-f265bfc55a07" in namespace "projected-8343" to be "success or failure"
May 31 18:55:02.986: INFO: Pod "pod-projected-secrets-98feaad5-83d5-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 17.517684ms
May 31 18:55:04.989: INFO: Pod "pod-projected-secrets-98feaad5-83d5-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020567779s
May 31 18:55:06.997: INFO: Pod "pod-projected-secrets-98feaad5-83d5-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029078964s
STEP: Saw pod success
May 31 18:55:06.997: INFO: Pod "pod-projected-secrets-98feaad5-83d5-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:55:07.002: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-projected-secrets-98feaad5-83d5-11e9-a3c9-f265bfc55a07 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 18:55:07.030: INFO: Waiting for pod pod-projected-secrets-98feaad5-83d5-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:55:07.034: INFO: Pod pod-projected-secrets-98feaad5-83d5-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:55:07.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8343" for this suite.
May 31 18:55:13.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:55:13.133: INFO: namespace projected-8343 deletion completed in 6.089754702s

• [SLOW TEST:10.269 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:55:13.133: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
May 31 18:55:13.201: INFO: Waiting up to 5m0s for pod "var-expansion-9f1727a8-83d5-11e9-a3c9-f265bfc55a07" in namespace "var-expansion-4919" to be "success or failure"
May 31 18:55:13.207: INFO: Pod "var-expansion-9f1727a8-83d5-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.647992ms
May 31 18:55:15.210: INFO: Pod "var-expansion-9f1727a8-83d5-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008748693s
May 31 18:55:17.213: INFO: Pod "var-expansion-9f1727a8-83d5-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012121s
STEP: Saw pod success
May 31 18:55:17.214: INFO: Pod "var-expansion-9f1727a8-83d5-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:55:17.216: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod var-expansion-9f1727a8-83d5-11e9-a3c9-f265bfc55a07 container dapi-container: <nil>
STEP: delete the pod
May 31 18:55:17.231: INFO: Waiting for pod var-expansion-9f1727a8-83d5-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:55:17.236: INFO: Pod var-expansion-9f1727a8-83d5-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:55:17.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4919" for this suite.
May 31 18:55:23.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:55:23.329: INFO: namespace var-expansion-4919 deletion completed in 6.088739702s

• [SLOW TEST:10.196 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:55:23.329: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a52aa423-83d5-11e9-a3c9-f265bfc55a07
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a52aa423-83d5-11e9-a3c9-f265bfc55a07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:56:57.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5818" for this suite.
May 31 18:57:19.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:57:19.904: INFO: namespace projected-5818 deletion completed in 22.088803791s

• [SLOW TEST:116.575 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:57:19.905: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6021
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 18:57:19.955: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 31 18:57:42.056: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.63:8080/dial?request=hostName&protocol=udp&host=10.244.3.62&port=8081&tries=1'] Namespace:pod-network-test-6021 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:57:42.056: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:57:42.187: INFO: Waiting for endpoints: map[]
May 31 18:57:42.190: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.63:8080/dial?request=hostName&protocol=udp&host=10.244.1.77&port=8081&tries=1'] Namespace:pod-network-test-6021 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:57:42.190: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:57:42.318: INFO: Waiting for endpoints: map[]
May 31 18:57:42.321: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.63:8080/dial?request=hostName&protocol=udp&host=10.244.2.21&port=8081&tries=1'] Namespace:pod-network-test-6021 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 18:57:42.322: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 18:57:42.461: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:57:42.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6021" for this suite.
May 31 18:58:04.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:58:04.548: INFO: namespace pod-network-test-6021 deletion completed in 22.082672187s

• [SLOW TEST:44.643 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:58:04.549: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
May 31 18:58:04.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 api-versions'
May 31 18:58:04.719: INFO: stderr: ""
May 31 18:58:04.719: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:58:04.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2870" for this suite.
May 31 18:58:10.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:58:10.827: INFO: namespace kubectl-2870 deletion completed in 6.104510382s

• [SLOW TEST:6.278 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:58:10.827: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-090015ab-83d6-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 18:58:10.884: INFO: Waiting up to 5m0s for pod "pod-secrets-0900b395-83d6-11e9-a3c9-f265bfc55a07" in namespace "secrets-8052" to be "success or failure"
May 31 18:58:10.891: INFO: Pod "pod-secrets-0900b395-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.608507ms
May 31 18:58:12.894: INFO: Pod "pod-secrets-0900b395-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009637186s
May 31 18:58:14.898: INFO: Pod "pod-secrets-0900b395-83d6-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013712982s
STEP: Saw pod success
May 31 18:58:14.898: INFO: Pod "pod-secrets-0900b395-83d6-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:58:14.900: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-secrets-0900b395-83d6-11e9-a3c9-f265bfc55a07 container secret-volume-test: <nil>
STEP: delete the pod
May 31 18:58:14.944: INFO: Waiting for pod pod-secrets-0900b395-83d6-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:58:14.948: INFO: Pod pod-secrets-0900b395-83d6-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:58:14.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8052" for this suite.
May 31 18:58:20.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:58:21.041: INFO: namespace secrets-8052 deletion completed in 6.089167244s

• [SLOW TEST:10.214 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:58:21.041: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 31 18:58:21.139: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:58:21.141: INFO: Number of nodes with available pods: 0
May 31 18:58:21.141: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:58:22.145: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:58:22.148: INFO: Number of nodes with available pods: 0
May 31 18:58:22.148: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:58:23.144: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:58:23.148: INFO: Number of nodes with available pods: 0
May 31 18:58:23.148: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 18:58:24.145: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:58:24.148: INFO: Number of nodes with available pods: 3
May 31 18:58:24.148: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 31 18:58:24.167: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 18:58:24.171: INFO: Number of nodes with available pods: 3
May 31 18:58:24.172: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8635, will wait for the garbage collector to delete the pods
May 31 18:58:24.254: INFO: Deleting DaemonSet.extensions daemon-set took: 4.714676ms
May 31 18:58:24.555: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.860894ms
May 31 18:58:37.557: INFO: Number of nodes with available pods: 0
May 31 18:58:37.557: INFO: Number of running nodes: 0, number of available pods: 0
May 31 18:58:37.558: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8635/daemonsets","resourceVersion":"10766"},"items":null}

May 31 18:58:37.560: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8635/pods","resourceVersion":"10766"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:58:37.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8635" for this suite.
May 31 18:58:43.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:58:43.670: INFO: namespace daemonsets-8635 deletion completed in 6.098113915s

• [SLOW TEST:22.629 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:58:43.671: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 31 18:58:43.728: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 18:58:43.734: INFO: Waiting for terminating namespaces to be deleted...
May 31 18:58:43.738: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-33506892-0 before test
May 31 18:58:43.746: INFO: kube-flannel-ds-5kgxc from kube-system started at 2019-05-31 18:09:36 +0000 UTC (2 container statuses recorded)
May 31 18:58:43.746: INFO: 	Container install-cni ready: true, restart count 0
May 31 18:58:43.746: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 18:58:43.746: INFO: azure-ip-masq-agent-hffrz from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.746: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 18:58:43.746: INFO: tiller-deploy-5d9ffb885b-sm2bb from kube-system started at 2019-05-31 18:10:05 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.746: INFO: 	Container tiller ready: true, restart count 0
May 31 18:58:43.746: INFO: sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-zhmp4 from heptio-sonobuoy started at 2019-05-31 18:11:14 +0000 UTC (2 container statuses recorded)
May 31 18:58:43.746: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
May 31 18:58:43.746: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 18:58:43.746: INFO: kube-proxy-5bk4d from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.746: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 18:58:43.746: INFO: metrics-server-6bf85bb69b-rc4nd from kube-system started at 2019-05-31 18:10:05 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.746: INFO: 	Container metrics-server ready: true, restart count 0
May 31 18:58:43.746: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-33506892-1 before test
May 31 18:58:43.752: INFO: kube-flannel-ds-9s4jc from kube-system started at 2019-05-31 18:09:35 +0000 UTC (2 container statuses recorded)
May 31 18:58:43.752: INFO: 	Container install-cni ready: true, restart count 0
May 31 18:58:43.752: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 18:58:43.753: INFO: azure-ip-masq-agent-8cg76 from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.753: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 18:58:43.753: INFO: kube-proxy-kkq24 from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.753: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 18:58:43.753: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-31 18:11:05 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.753: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 18:58:43.754: INFO: sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-t5p7z from heptio-sonobuoy started at 2019-05-31 18:11:14 +0000 UTC (2 container statuses recorded)
May 31 18:58:43.754: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
May 31 18:58:43.754: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 18:58:43.754: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-33506892-2 before test
May 31 18:58:43.789: INFO: sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-fnw89 from heptio-sonobuoy started at 2019-05-31 18:11:14 +0000 UTC (2 container statuses recorded)
May 31 18:58:43.789: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
May 31 18:58:43.789: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 18:58:43.789: INFO: kube-flannel-ds-n8nxk from kube-system started at 2019-05-31 18:09:35 +0000 UTC (2 container statuses recorded)
May 31 18:58:43.789: INFO: 	Container install-cni ready: true, restart count 0
May 31 18:58:43.789: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 18:58:43.789: INFO: azure-ip-masq-agent-lnst2 from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.789: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 18:58:43.789: INFO: kube-proxy-49ngv from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.789: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 18:58:43.789: INFO: kubernetes-dashboard-54b795b69b-zsd4t from kube-system started at 2019-05-31 18:10:07 +0000 UTC (1 container statuses recorded)
May 31 18:58:43.789: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node k8s-linuxpool-33506892-0
STEP: verifying the node has the label node k8s-linuxpool-33506892-1
STEP: verifying the node has the label node k8s-linuxpool-33506892-2
May 31 18:58:43.848: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-linuxpool-33506892-1
May 31 18:58:43.848: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-fnw89 requesting resource cpu=0m on Node k8s-linuxpool-33506892-2
May 31 18:58:43.848: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-t5p7z requesting resource cpu=0m on Node k8s-linuxpool-33506892-1
May 31 18:58:43.848: INFO: Pod sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-zhmp4 requesting resource cpu=0m on Node k8s-linuxpool-33506892-0
May 31 18:58:43.848: INFO: Pod azure-ip-masq-agent-8cg76 requesting resource cpu=50m on Node k8s-linuxpool-33506892-1
May 31 18:58:43.848: INFO: Pod azure-ip-masq-agent-hffrz requesting resource cpu=50m on Node k8s-linuxpool-33506892-0
May 31 18:58:43.848: INFO: Pod azure-ip-masq-agent-lnst2 requesting resource cpu=50m on Node k8s-linuxpool-33506892-2
May 31 18:58:43.848: INFO: Pod kube-flannel-ds-5kgxc requesting resource cpu=0m on Node k8s-linuxpool-33506892-0
May 31 18:58:43.848: INFO: Pod kube-flannel-ds-9s4jc requesting resource cpu=0m on Node k8s-linuxpool-33506892-1
May 31 18:58:43.848: INFO: Pod kube-flannel-ds-n8nxk requesting resource cpu=0m on Node k8s-linuxpool-33506892-2
May 31 18:58:43.848: INFO: Pod kube-proxy-49ngv requesting resource cpu=100m on Node k8s-linuxpool-33506892-2
May 31 18:58:43.848: INFO: Pod kube-proxy-5bk4d requesting resource cpu=100m on Node k8s-linuxpool-33506892-0
May 31 18:58:43.848: INFO: Pod kube-proxy-kkq24 requesting resource cpu=100m on Node k8s-linuxpool-33506892-1
May 31 18:58:43.848: INFO: Pod kubernetes-dashboard-54b795b69b-zsd4t requesting resource cpu=300m on Node k8s-linuxpool-33506892-2
May 31 18:58:43.848: INFO: Pod metrics-server-6bf85bb69b-rc4nd requesting resource cpu=0m on Node k8s-linuxpool-33506892-0
May 31 18:58:43.848: INFO: Pod tiller-deploy-5d9ffb885b-sm2bb requesting resource cpu=50m on Node k8s-linuxpool-33506892-0
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca77722-83d6-11e9-a3c9-f265bfc55a07.15a3d9939387df07], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9058/filler-pod-1ca77722-83d6-11e9-a3c9-f265bfc55a07 to k8s-linuxpool-33506892-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca77722-83d6-11e9-a3c9-f265bfc55a07.15a3d993e4725d91], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca77722-83d6-11e9-a3c9-f265bfc55a07.15a3d993f97ebe9b], Reason = [Created], Message = [Created container filler-pod-1ca77722-83d6-11e9-a3c9-f265bfc55a07]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca77722-83d6-11e9-a3c9-f265bfc55a07.15a3d9940609e657], Reason = [Started], Message = [Started container filler-pod-1ca77722-83d6-11e9-a3c9-f265bfc55a07]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca859d5-83d6-11e9-a3c9-f265bfc55a07.15a3d99394693c8b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9058/filler-pod-1ca859d5-83d6-11e9-a3c9-f265bfc55a07 to k8s-linuxpool-33506892-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca859d5-83d6-11e9-a3c9-f265bfc55a07.15a3d993e67284b5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca859d5-83d6-11e9-a3c9-f265bfc55a07.15a3d993fab7cd77], Reason = [Created], Message = [Created container filler-pod-1ca859d5-83d6-11e9-a3c9-f265bfc55a07]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca859d5-83d6-11e9-a3c9-f265bfc55a07.15a3d99407bd91f5], Reason = [Started], Message = [Started container filler-pod-1ca859d5-83d6-11e9-a3c9-f265bfc55a07]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca97c03-83d6-11e9-a3c9-f265bfc55a07.15a3d99394ac3df3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9058/filler-pod-1ca97c03-83d6-11e9-a3c9-f265bfc55a07 to k8s-linuxpool-33506892-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca97c03-83d6-11e9-a3c9-f265bfc55a07.15a3d993e6728091], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca97c03-83d6-11e9-a3c9-f265bfc55a07.15a3d993fbaa6db1], Reason = [Created], Message = [Created container filler-pod-1ca97c03-83d6-11e9-a3c9-f265bfc55a07]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ca97c03-83d6-11e9-a3c9-f265bfc55a07.15a3d99408de5ca1], Reason = [Started], Message = [Started container filler-pod-1ca97c03-83d6-11e9-a3c9-f265bfc55a07]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15a3d9948680c854], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node k8s-linuxpool-33506892-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-linuxpool-33506892-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-linuxpool-33506892-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:58:48.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9058" for this suite.
May 31 18:58:55.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:58:55.080: INFO: namespace sched-pred-9058 deletion completed in 6.082440471s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.409 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:58:55.082: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 18:58:55.143: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2361ed2f-83d6-11e9-a3c9-f265bfc55a07" in namespace "projected-9076" to be "success or failure"
May 31 18:58:55.149: INFO: Pod "downwardapi-volume-2361ed2f-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.254202ms
May 31 18:58:57.152: INFO: Pod "downwardapi-volume-2361ed2f-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009431898s
May 31 18:58:59.156: INFO: Pod "downwardapi-volume-2361ed2f-83d6-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013394408s
STEP: Saw pod success
May 31 18:58:59.156: INFO: Pod "downwardapi-volume-2361ed2f-83d6-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 18:58:59.159: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downwardapi-volume-2361ed2f-83d6-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 18:58:59.175: INFO: Waiting for pod downwardapi-volume-2361ed2f-83d6-11e9-a3c9-f265bfc55a07 to disappear
May 31 18:58:59.178: INFO: Pod downwardapi-volume-2361ed2f-83d6-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:58:59.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9076" for this suite.
May 31 18:59:05.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:59:05.274: INFO: namespace projected-9076 deletion completed in 6.092531346s

• [SLOW TEST:10.192 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:59:05.276: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 18:59:05.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-626'
May 31 18:59:05.459: INFO: stderr: ""
May 31 18:59:05.459: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 31 18:59:10.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pod e2e-test-nginx-pod --namespace=kubectl-626 -o json'
May 31 18:59:10.599: INFO: stderr: ""
May 31 18:59:10.599: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-05-31T18:59:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-626\",\n        \"resourceVersion\": \"10948\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-626/pods/e2e-test-nginx-pod\",\n        \"uid\": \"2986c22f-83d6-11e9-a607-001dd80c001e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-bz45l\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-linuxpool-33506892-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-bz45l\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-bz45l\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-31T18:59:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-31T18:59:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-31T18:59:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-31T18:59:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://5e1c9c57f8844bf367c1d4ab6dc23c06027c390cd2e7f5f47628047383ad04ee\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-31T18:59:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.6\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.67\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-31T18:59:05Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 31 18:59:10.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 replace -f - --namespace=kubectl-626'
May 31 18:59:10.869: INFO: stderr: ""
May 31 18:59:10.869: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
May 31 18:59:10.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete pods e2e-test-nginx-pod --namespace=kubectl-626'
May 31 18:59:17.552: INFO: stderr: ""
May 31 18:59:17.552: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:59:17.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-626" for this suite.
May 31 18:59:23.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:59:23.645: INFO: namespace kubectl-626 deletion completed in 6.090508931s

• [SLOW TEST:18.370 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:59:23.646: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 18:59:28.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1368" for this suite.
May 31 18:59:56.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 18:59:56.811: INFO: namespace replication-controller-1368 deletion completed in 28.089434891s

• [SLOW TEST:33.166 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 18:59:56.811: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-482b6546-83d6-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 18:59:56.864: INFO: Waiting up to 5m0s for pod "pod-secrets-482be484-83d6-11e9-a3c9-f265bfc55a07" in namespace "secrets-2918" to be "success or failure"
May 31 18:59:56.870: INFO: Pod "pod-secrets-482be484-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.298702ms
May 31 18:59:58.874: INFO: Pod "pod-secrets-482be484-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009406517s
May 31 19:00:00.876: INFO: Pod "pod-secrets-482be484-83d6-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012252928s
STEP: Saw pod success
May 31 19:00:00.876: INFO: Pod "pod-secrets-482be484-83d6-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:00:00.879: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-secrets-482be484-83d6-11e9-a3c9-f265bfc55a07 container secret-volume-test: <nil>
STEP: delete the pod
May 31 19:00:00.899: INFO: Waiting for pod pod-secrets-482be484-83d6-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:00:00.905: INFO: Pod pod-secrets-482be484-83d6-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:00:00.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2918" for this suite.
May 31 19:00:06.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:00:07.022: INFO: namespace secrets-2918 deletion completed in 6.114828668s

• [SLOW TEST:10.211 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:00:07.023: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 31 19:00:07.066: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:00:11.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2747" for this suite.
May 31 19:00:33.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:00:33.442: INFO: namespace init-container-2747 deletion completed in 22.090496359s

• [SLOW TEST:26.419 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:00:33.443: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 19:00:33.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7553'
May 31 19:00:33.606: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 31 19:00:33.606: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
May 31 19:00:33.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete jobs e2e-test-nginx-job --namespace=kubectl-7553'
May 31 19:00:33.710: INFO: stderr: ""
May 31 19:00:33.710: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:00:33.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7553" for this suite.
May 31 19:00:39.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:00:39.822: INFO: namespace kubectl-7553 deletion completed in 6.106785766s

• [SLOW TEST:6.380 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:00:39.824: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-61d1a2a5-83d6-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 19:00:39.903: INFO: Waiting up to 5m0s for pod "pod-secrets-61d23685-83d6-11e9-a3c9-f265bfc55a07" in namespace "secrets-1994" to be "success or failure"
May 31 19:00:39.918: INFO: Pod "pod-secrets-61d23685-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 14.11893ms
May 31 19:00:41.982: INFO: Pod "pod-secrets-61d23685-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078930863s
May 31 19:00:43.986: INFO: Pod "pod-secrets-61d23685-83d6-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082484498s
STEP: Saw pod success
May 31 19:00:43.986: INFO: Pod "pod-secrets-61d23685-83d6-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:00:43.988: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-secrets-61d23685-83d6-11e9-a3c9-f265bfc55a07 container secret-volume-test: <nil>
STEP: delete the pod
May 31 19:00:44.006: INFO: Waiting for pod pod-secrets-61d23685-83d6-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:00:44.008: INFO: Pod pod-secrets-61d23685-83d6-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:00:44.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1994" for this suite.
May 31 19:00:50.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:00:50.111: INFO: namespace secrets-1994 deletion completed in 6.09976916s

• [SLOW TEST:10.287 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:00:50.111: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-67f233d7-83d6-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 19:00:50.274: INFO: Waiting up to 5m0s for pod "pod-secrets-68005dad-83d6-11e9-a3c9-f265bfc55a07" in namespace "secrets-1362" to be "success or failure"
May 31 19:00:50.279: INFO: Pod "pod-secrets-68005dad-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820379ms
May 31 19:00:52.284: INFO: Pod "pod-secrets-68005dad-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00986304s
May 31 19:00:54.288: INFO: Pod "pod-secrets-68005dad-83d6-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013294276s
STEP: Saw pod success
May 31 19:00:54.288: INFO: Pod "pod-secrets-68005dad-83d6-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:00:54.290: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-secrets-68005dad-83d6-11e9-a3c9-f265bfc55a07 container secret-volume-test: <nil>
STEP: delete the pod
May 31 19:00:54.318: INFO: Waiting for pod pod-secrets-68005dad-83d6-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:00:54.320: INFO: Pod pod-secrets-68005dad-83d6-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:00:54.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1362" for this suite.
May 31 19:01:00.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:01:00.429: INFO: namespace secrets-1362 deletion completed in 6.105101555s
STEP: Destroying namespace "secret-namespace-1930" for this suite.
May 31 19:01:06.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:01:06.527: INFO: namespace secret-namespace-1930 deletion completed in 6.098720656s

• [SLOW TEST:16.416 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:01:06.528: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-71ba01fb-83d6-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 19:01:06.588: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-71bab06e-83d6-11e9-a3c9-f265bfc55a07" in namespace "projected-6037" to be "success or failure"
May 31 19:01:06.591: INFO: Pod "pod-projected-secrets-71bab06e-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05255ms
May 31 19:01:08.595: INFO: Pod "pod-projected-secrets-71bab06e-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007231702s
May 31 19:01:10.598: INFO: Pod "pod-projected-secrets-71bab06e-83d6-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010567741s
STEP: Saw pod success
May 31 19:01:10.598: INFO: Pod "pod-projected-secrets-71bab06e-83d6-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:01:10.601: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-projected-secrets-71bab06e-83d6-11e9-a3c9-f265bfc55a07 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 19:01:10.622: INFO: Waiting for pod pod-projected-secrets-71bab06e-83d6-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:01:10.624: INFO: Pod pod-projected-secrets-71bab06e-83d6-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:01:10.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6037" for this suite.
May 31 19:01:16.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:01:16.716: INFO: namespace projected-6037 deletion completed in 6.089213609s

• [SLOW TEST:10.188 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:01:16.716: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:01:20.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6405" for this suite.
May 31 19:01:26.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:01:26.922: INFO: namespace emptydir-wrapper-6405 deletion completed in 6.084037533s

• [SLOW TEST:10.206 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:01:26.922: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6451/configmap-test-7de10265-83d6-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 19:01:26.972: INFO: Waiting up to 5m0s for pod "pod-configmaps-7de1616e-83d6-11e9-a3c9-f265bfc55a07" in namespace "configmap-6451" to be "success or failure"
May 31 19:01:26.975: INFO: Pod "pod-configmaps-7de1616e-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802346ms
May 31 19:01:28.979: INFO: Pod "pod-configmaps-7de1616e-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006321792s
May 31 19:01:31.010: INFO: Pod "pod-configmaps-7de1616e-83d6-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037206285s
STEP: Saw pod success
May 31 19:01:31.010: INFO: Pod "pod-configmaps-7de1616e-83d6-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:01:31.013: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-configmaps-7de1616e-83d6-11e9-a3c9-f265bfc55a07 container env-test: <nil>
STEP: delete the pod
May 31 19:01:31.033: INFO: Waiting for pod pod-configmaps-7de1616e-83d6-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:01:31.037: INFO: Pod pod-configmaps-7de1616e-83d6-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:01:31.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6451" for this suite.
May 31 19:01:37.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:01:37.171: INFO: namespace configmap-6451 deletion completed in 6.12874427s

• [SLOW TEST:10.249 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:01:37.174: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 31 19:01:37.281: INFO: Waiting up to 5m0s for pod "pod-84063a24-83d6-11e9-a3c9-f265bfc55a07" in namespace "emptydir-252" to be "success or failure"
May 31 19:01:37.285: INFO: Pod "pod-84063a24-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28267ms
May 31 19:01:39.289: INFO: Pod "pod-84063a24-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008179325s
May 31 19:01:41.293: INFO: Pod "pod-84063a24-83d6-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011732575s
STEP: Saw pod success
May 31 19:01:41.293: INFO: Pod "pod-84063a24-83d6-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:01:41.295: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-84063a24-83d6-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 19:01:41.317: INFO: Waiting for pod pod-84063a24-83d6-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:01:41.320: INFO: Pod pod-84063a24-83d6-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:01:41.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-252" for this suite.
May 31 19:01:47.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:01:47.431: INFO: namespace emptydir-252 deletion completed in 6.107676434s

• [SLOW TEST:10.257 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:01:47.432: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0531 19:01:48.520304      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 19:01:48.520: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:01:48.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6411" for this suite.
May 31 19:01:54.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:01:54.606: INFO: namespace gc-6411 deletion completed in 6.082851034s

• [SLOW TEST:7.175 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:01:54.607: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0531 19:02:00.675993      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 19:02:00.676: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:02:00.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1119" for this suite.
May 31 19:02:06.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:02:06.797: INFO: namespace gc-1119 deletion completed in 6.117873313s

• [SLOW TEST:12.190 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:02:06.798: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 31 19:02:06.883: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5443,SelfLink:/api/v1/namespaces/watch-5443/configmaps/e2e-watch-test-watch-closed,UID:95a9eee6-83d6-11e9-a607-001dd80c001e,ResourceVersion:11807,Generation:0,CreationTimestamp:2019-05-31 19:02:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 19:02:06.883: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5443,SelfLink:/api/v1/namespaces/watch-5443/configmaps/e2e-watch-test-watch-closed,UID:95a9eee6-83d6-11e9-a607-001dd80c001e,ResourceVersion:11808,Generation:0,CreationTimestamp:2019-05-31 19:02:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 31 19:02:06.896: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5443,SelfLink:/api/v1/namespaces/watch-5443/configmaps/e2e-watch-test-watch-closed,UID:95a9eee6-83d6-11e9-a607-001dd80c001e,ResourceVersion:11809,Generation:0,CreationTimestamp:2019-05-31 19:02:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 19:02:06.896: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5443,SelfLink:/api/v1/namespaces/watch-5443/configmaps/e2e-watch-test-watch-closed,UID:95a9eee6-83d6-11e9-a607-001dd80c001e,ResourceVersion:11810,Generation:0,CreationTimestamp:2019-05-31 19:02:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:02:06.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5443" for this suite.
May 31 19:02:12.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:02:13.030: INFO: namespace watch-5443 deletion completed in 6.13026212s

• [SLOW TEST:6.232 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:02:13.030: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-fplp
STEP: Creating a pod to test atomic-volume-subpath
May 31 19:02:13.086: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fplp" in namespace "subpath-4453" to be "success or failure"
May 31 19:02:13.089: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.960149ms
May 31 19:02:15.093: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006466306s
May 31 19:02:17.096: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 4.009283452s
May 31 19:02:19.121: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 6.03422906s
May 31 19:02:21.124: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 8.037751618s
May 31 19:02:23.128: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 10.041349779s
May 31 19:02:25.131: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 12.04435243s
May 31 19:02:27.135: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 14.0484554s
May 31 19:02:29.138: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 16.051830758s
May 31 19:02:31.142: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 18.055201716s
May 31 19:02:33.145: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 20.058460673s
May 31 19:02:35.149: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Running", Reason="", readiness=true. Elapsed: 22.062227039s
May 31 19:02:37.153: INFO: Pod "pod-subpath-test-downwardapi-fplp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.066501114s
STEP: Saw pod success
May 31 19:02:37.153: INFO: Pod "pod-subpath-test-downwardapi-fplp" satisfied condition "success or failure"
May 31 19:02:37.157: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-subpath-test-downwardapi-fplp container test-container-subpath-downwardapi-fplp: <nil>
STEP: delete the pod
May 31 19:02:37.175: INFO: Waiting for pod pod-subpath-test-downwardapi-fplp to disappear
May 31 19:02:37.178: INFO: Pod pod-subpath-test-downwardapi-fplp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fplp
May 31 19:02:37.179: INFO: Deleting pod "pod-subpath-test-downwardapi-fplp" in namespace "subpath-4453"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:02:37.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4453" for this suite.
May 31 19:02:43.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:02:43.279: INFO: namespace subpath-4453 deletion completed in 6.094767162s

• [SLOW TEST:30.249 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:02:43.279: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:02:43.365: INFO: Create a RollingUpdate DaemonSet
May 31 19:02:43.373: INFO: Check that daemon pods launch on every node of the cluster
May 31 19:02:43.380: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 19:02:43.383: INFO: Number of nodes with available pods: 0
May 31 19:02:43.383: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 19:02:44.387: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 19:02:44.390: INFO: Number of nodes with available pods: 0
May 31 19:02:44.390: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 19:02:45.387: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 19:02:45.390: INFO: Number of nodes with available pods: 0
May 31 19:02:45.390: INFO: Node k8s-linuxpool-33506892-0 is running more than one daemon pod
May 31 19:02:46.387: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 19:02:46.390: INFO: Number of nodes with available pods: 3
May 31 19:02:46.390: INFO: Number of running nodes: 3, number of available pods: 3
May 31 19:02:46.391: INFO: Update the DaemonSet to trigger a rollout
May 31 19:02:46.396: INFO: Updating DaemonSet daemon-set
May 31 19:03:00.409: INFO: Roll back the DaemonSet before rollout is complete
May 31 19:03:00.415: INFO: Updating DaemonSet daemon-set
May 31 19:03:00.415: INFO: Make sure DaemonSet rollback is complete
May 31 19:03:00.424: INFO: Wrong image for pod: daemon-set-lc9wl. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 31 19:03:00.424: INFO: Pod daemon-set-lc9wl is not available
May 31 19:03:00.436: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 19:03:01.439: INFO: Wrong image for pod: daemon-set-lc9wl. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 31 19:03:01.439: INFO: Pod daemon-set-lc9wl is not available
May 31 19:03:01.443: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 31 19:03:02.439: INFO: Pod daemon-set-9k4cs is not available
May 31 19:03:02.442: INFO: DaemonSet pods can't tolerate node k8s-master-33506892-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-145, will wait for the garbage collector to delete the pods
May 31 19:03:02.507: INFO: Deleting DaemonSet.extensions daemon-set took: 5.430689ms
May 31 19:03:02.808: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.730803ms
May 31 19:03:09.910: INFO: Number of nodes with available pods: 0
May 31 19:03:09.910: INFO: Number of running nodes: 0, number of available pods: 0
May 31 19:03:09.912: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-145/daemonsets","resourceVersion":"12036"},"items":null}

May 31 19:03:09.915: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-145/pods","resourceVersion":"12036"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:03:09.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-145" for this suite.
May 31 19:03:15.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:03:16.027: INFO: namespace daemonsets-145 deletion completed in 6.094823984s

• [SLOW TEST:32.748 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:03:16.027: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-bee96315-83d6-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 19:03:16.081: INFO: Waiting up to 5m0s for pod "pod-configmaps-bee9e6c8-83d6-11e9-a3c9-f265bfc55a07" in namespace "configmap-255" to be "success or failure"
May 31 19:03:16.085: INFO: Pod "pod-configmaps-bee9e6c8-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07195ms
May 31 19:03:18.088: INFO: Pod "pod-configmaps-bee9e6c8-83d6-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006203614s
May 31 19:03:20.091: INFO: Pod "pod-configmaps-bee9e6c8-83d6-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00935438s
STEP: Saw pod success
May 31 19:03:20.091: INFO: Pod "pod-configmaps-bee9e6c8-83d6-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:03:20.094: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-configmaps-bee9e6c8-83d6-11e9-a3c9-f265bfc55a07 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 19:03:20.116: INFO: Waiting for pod pod-configmaps-bee9e6c8-83d6-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:03:20.119: INFO: Pod pod-configmaps-bee9e6c8-83d6-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:03:20.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-255" for this suite.
May 31 19:03:26.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:03:26.203: INFO: namespace configmap-255 deletion completed in 6.081411272s

• [SLOW TEST:10.176 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:03:26.204: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 31 19:03:26.248: INFO: PodSpec: initContainers in spec.initContainers
May 31 19:04:08.136: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c4fa4ca5-83d6-11e9-a3c9-f265bfc55a07", GenerateName:"", Namespace:"init-container-9648", SelfLink:"/api/v1/namespaces/init-container-9648/pods/pod-init-c4fa4ca5-83d6-11e9-a3c9-f265bfc55a07", UID:"c4facda7-83d6-11e9-a607-001dd80c001e", ResourceVersion:"12221", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63694926206, loc:(*time.Location)(0x8a140e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"248877915"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-h8c98", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002bba000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-h8c98", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-h8c98", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-h8c98", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002c76098), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-linuxpool-33506892-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003544000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c76110)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c76130)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002c76138), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002c7613c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694926206, loc:(*time.Location)(0x8a140e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694926206, loc:(*time.Location)(0x8a140e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694926206, loc:(*time.Location)(0x8a140e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694926206, loc:(*time.Location)(0x8a140e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.4", PodIP:"10.244.1.94", StartTime:(*v1.Time)(0xc001d88060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000dce070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000dce0e0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://c920565a2e321da61561af86caa0c6ba4a922f8ecfad45795438612e97860af5"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001d880a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001d88080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:04:08.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9648" for this suite.
May 31 19:04:30.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:04:30.243: INFO: namespace init-container-9648 deletion completed in 22.101475234s

• [SLOW TEST:64.039 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:04:30.243: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7465
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 19:04:30.285: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 31 19:04:54.369: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.29 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7465 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 19:04:54.369: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 19:04:55.514: INFO: Found all expected endpoints: [netserver-0]
May 31 19:04:55.518: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.80 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7465 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 19:04:55.518: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 19:04:56.644: INFO: Found all expected endpoints: [netserver-1]
May 31 19:04:56.646: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.95 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7465 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 31 19:04:56.646: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
May 31 19:04:57.766: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:04:57.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7465" for this suite.
May 31 19:05:19.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:05:19.898: INFO: namespace pod-network-test-7465 deletion completed in 22.128044361s

• [SLOW TEST:49.654 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:05:19.898: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:05:20.034: INFO: (0) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 6.830811ms)
May 31 19:05:20.038: INFO: (1) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.70386ms)
May 31 19:05:20.041: INFO: (2) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.488457ms)
May 31 19:05:20.045: INFO: (3) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.558858ms)
May 31 19:05:20.048: INFO: (4) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.141451ms)
May 31 19:05:20.051: INFO: (5) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.040249ms)
May 31 19:05:20.055: INFO: (6) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.400156ms)
May 31 19:05:20.072: INFO: (7) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 16.438968ms)
May 31 19:05:20.076: INFO: (8) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.540658ms)
May 31 19:05:20.080: INFO: (9) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.165368ms)
May 31 19:05:20.084: INFO: (10) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.598459ms)
May 31 19:05:20.088: INFO: (11) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.762261ms)
May 31 19:05:20.092: INFO: (12) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.947664ms)
May 31 19:05:20.095: INFO: (13) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.092851ms)
May 31 19:05:20.098: INFO: (14) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.327855ms)
May 31 19:05:20.101: INFO: (15) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.312654ms)
May 31 19:05:20.105: INFO: (16) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.477857ms)
May 31 19:05:20.109: INFO: (17) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.857763ms)
May 31 19:05:20.112: INFO: (18) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.307554ms)
May 31 19:05:20.115: INFO: (19) /api/v1/nodes/k8s-linuxpool-33506892-0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 2.860547ms)
[AfterEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:05:20.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2254" for this suite.
May 31 19:05:26.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:05:26.201: INFO: namespace proxy-2254 deletion completed in 6.082474354s

• [SLOW TEST:6.303 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:05:26.203: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 31 19:05:30.828: INFO: Successfully updated pod "pod-update-activedeadlineseconds-0c878678-83d7-11e9-a3c9-f265bfc55a07"
May 31 19:05:30.828: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-0c878678-83d7-11e9-a3c9-f265bfc55a07" in namespace "pods-8198" to be "terminated due to deadline exceeded"
May 31 19:05:30.830: INFO: Pod "pod-update-activedeadlineseconds-0c878678-83d7-11e9-a3c9-f265bfc55a07": Phase="Running", Reason="", readiness=true. Elapsed: 2.199436ms
May 31 19:05:32.833: INFO: Pod "pod-update-activedeadlineseconds-0c878678-83d7-11e9-a3c9-f265bfc55a07": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.005008319s
May 31 19:05:32.833: INFO: Pod "pod-update-activedeadlineseconds-0c878678-83d7-11e9-a3c9-f265bfc55a07" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:05:32.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8198" for this suite.
May 31 19:05:38.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:05:38.930: INFO: namespace pods-8198 deletion completed in 6.093125434s

• [SLOW TEST:12.727 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:05:38.932: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-14169682-83d7-11e9-a3c9-f265bfc55a07
STEP: Creating configMap with name cm-test-opt-upd-141696be-83d7-11e9-a3c9-f265bfc55a07
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-14169682-83d7-11e9-a3c9-f265bfc55a07
STEP: Updating configmap cm-test-opt-upd-141696be-83d7-11e9-a3c9-f265bfc55a07
STEP: Creating configMap with name cm-test-opt-create-141696e6-83d7-11e9-a3c9-f265bfc55a07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:05:47.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5566" for this suite.
May 31 19:06:09.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:06:09.239: INFO: namespace projected-5566 deletion completed in 22.101721917s

• [SLOW TEST:30.307 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:06:09.239: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 31 19:06:13.812: INFO: Successfully updated pod "annotationupdate26269395-83d7-11e9-a3c9-f265bfc55a07"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:06:15.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9812" for this suite.
May 31 19:06:37.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:06:37.955: INFO: namespace downward-api-9812 deletion completed in 22.119873258s

• [SLOW TEST:28.716 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:06:37.955: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 19:06:38.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3163'
May 31 19:06:39.159: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 31 19:06:39.159: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 31 19:06:39.178: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-fkdwj]
May 31 19:06:39.178: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-fkdwj" in namespace "kubectl-3163" to be "running and ready"
May 31 19:06:39.182: INFO: Pod "e2e-test-nginx-rc-fkdwj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097767ms
May 31 19:06:41.185: INFO: Pod "e2e-test-nginx-rc-fkdwj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007122064s
May 31 19:06:43.189: INFO: Pod "e2e-test-nginx-rc-fkdwj": Phase="Running", Reason="", readiness=true. Elapsed: 4.010488367s
May 31 19:06:43.189: INFO: Pod "e2e-test-nginx-rc-fkdwj" satisfied condition "running and ready"
May 31 19:06:43.189: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-fkdwj]
May 31 19:06:43.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 logs rc/e2e-test-nginx-rc --namespace=kubectl-3163'
May 31 19:06:43.294: INFO: stderr: ""
May 31 19:06:43.294: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
May 31 19:06:43.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete rc e2e-test-nginx-rc --namespace=kubectl-3163'
May 31 19:06:43.396: INFO: stderr: ""
May 31 19:06:43.396: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:06:43.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3163" for this suite.
May 31 19:06:49.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:06:49.493: INFO: namespace kubectl-3163 deletion completed in 6.092980063s

• [SLOW TEST:11.539 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:06:49.494: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:06:49.560: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3e270893-83d7-11e9-a607-001dd80c001e", Controller:(*bool)(0xc002c0fc4e), BlockOwnerDeletion:(*bool)(0xc002c0fc4f)}}
May 31 19:06:49.569: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3e25ece9-83d7-11e9-a607-001dd80c001e", Controller:(*bool)(0xc0022c2e9e), BlockOwnerDeletion:(*bool)(0xc0022c2e9f)}}
May 31 19:06:49.576: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3e2674d6-83d7-11e9-a607-001dd80c001e", Controller:(*bool)(0xc002c0fdfe), BlockOwnerDeletion:(*bool)(0xc002c0fdff)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:06:54.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2576" for this suite.
May 31 19:07:00.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:07:00.695: INFO: namespace gc-2576 deletion completed in 6.100139084s

• [SLOW TEST:11.201 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:07:00.696: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
May 31 19:07:00.735: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 31 19:07:00.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-7624'
May 31 19:07:01.099: INFO: stderr: ""
May 31 19:07:01.099: INFO: stdout: "service/redis-slave created\n"
May 31 19:07:01.099: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 31 19:07:01.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-7624'
May 31 19:07:01.412: INFO: stderr: ""
May 31 19:07:01.412: INFO: stdout: "service/redis-master created\n"
May 31 19:07:01.412: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 31 19:07:01.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-7624'
May 31 19:07:01.695: INFO: stderr: ""
May 31 19:07:01.696: INFO: stdout: "service/frontend created\n"
May 31 19:07:01.696: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 31 19:07:01.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-7624'
May 31 19:07:01.966: INFO: stderr: ""
May 31 19:07:01.966: INFO: stdout: "deployment.apps/frontend created\n"
May 31 19:07:01.966: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 31 19:07:01.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-7624'
May 31 19:07:02.266: INFO: stderr: ""
May 31 19:07:02.266: INFO: stdout: "deployment.apps/redis-master created\n"
May 31 19:07:02.267: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 31 19:07:02.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-7624'
May 31 19:07:02.548: INFO: stderr: ""
May 31 19:07:02.548: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 31 19:07:02.548: INFO: Waiting for all frontend pods to be Running.
May 31 19:07:37.650: INFO: Waiting for frontend to serve content.
May 31 19:07:37.673: INFO: Trying to add a new entry to the guestbook.
May 31 19:07:37.685: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 31 19:07:37.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-7624'
May 31 19:07:37.853: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 19:07:37.853: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 31 19:07:37.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-7624'
May 31 19:07:38.166: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 19:07:38.166: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 31 19:07:38.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-7624'
May 31 19:07:38.287: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 19:07:38.287: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 31 19:07:38.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-7624'
May 31 19:07:38.405: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 19:07:38.405: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 31 19:07:38.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-7624'
May 31 19:07:38.519: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 19:07:38.520: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 31 19:07:38.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-7624'
May 31 19:07:38.629: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 19:07:38.629: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:07:38.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7624" for this suite.
May 31 19:08:20.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:08:20.725: INFO: namespace kubectl-7624 deletion completed in 42.092746521s

• [SLOW TEST:80.029 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:08:20.725: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:08:20.794: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:08:24.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9510" for this suite.
May 31 19:09:02.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:09:02.925: INFO: namespace pods-9510 deletion completed in 38.082764337s

• [SLOW TEST:42.200 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:09:02.926: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 31 19:09:02.979: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7732,SelfLink:/api/v1/namespaces/watch-7732/configmaps/e2e-watch-test-label-changed,UID:8dadab2f-83d7-11e9-a607-001dd80c001e,ResourceVersion:13231,Generation:0,CreationTimestamp:2019-05-31 19:09:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 19:09:02.980: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7732,SelfLink:/api/v1/namespaces/watch-7732/configmaps/e2e-watch-test-label-changed,UID:8dadab2f-83d7-11e9-a607-001dd80c001e,ResourceVersion:13232,Generation:0,CreationTimestamp:2019-05-31 19:09:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 31 19:09:02.980: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7732,SelfLink:/api/v1/namespaces/watch-7732/configmaps/e2e-watch-test-label-changed,UID:8dadab2f-83d7-11e9-a607-001dd80c001e,ResourceVersion:13233,Generation:0,CreationTimestamp:2019-05-31 19:09:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 31 19:09:12.996: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7732,SelfLink:/api/v1/namespaces/watch-7732/configmaps/e2e-watch-test-label-changed,UID:8dadab2f-83d7-11e9-a607-001dd80c001e,ResourceVersion:13252,Generation:0,CreationTimestamp:2019-05-31 19:09:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 19:09:12.996: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7732,SelfLink:/api/v1/namespaces/watch-7732/configmaps/e2e-watch-test-label-changed,UID:8dadab2f-83d7-11e9-a607-001dd80c001e,ResourceVersion:13253,Generation:0,CreationTimestamp:2019-05-31 19:09:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 31 19:09:12.996: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7732,SelfLink:/api/v1/namespaces/watch-7732/configmaps/e2e-watch-test-label-changed,UID:8dadab2f-83d7-11e9-a607-001dd80c001e,ResourceVersion:13254,Generation:0,CreationTimestamp:2019-05-31 19:09:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:09:12.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7732" for this suite.
May 31 19:09:19.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:09:19.092: INFO: namespace watch-7732 deletion completed in 6.093671627s

• [SLOW TEST:16.166 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:09:19.094: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 31 19:09:19.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-6559'
May 31 19:09:19.401: INFO: stderr: ""
May 31 19:09:19.401: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 19:09:19.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6559'
May 31 19:09:19.577: INFO: stderr: ""
May 31 19:09:19.577: INFO: stdout: "update-demo-nautilus-jmdvh update-demo-nautilus-x76k5 "
May 31 19:09:19.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-jmdvh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6559'
May 31 19:09:19.673: INFO: stderr: ""
May 31 19:09:19.673: INFO: stdout: ""
May 31 19:09:19.673: INFO: update-demo-nautilus-jmdvh is created but not running
May 31 19:09:24.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6559'
May 31 19:09:24.762: INFO: stderr: ""
May 31 19:09:24.762: INFO: stdout: "update-demo-nautilus-jmdvh update-demo-nautilus-x76k5 "
May 31 19:09:24.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-jmdvh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6559'
May 31 19:09:24.879: INFO: stderr: ""
May 31 19:09:24.879: INFO: stdout: "true"
May 31 19:09:24.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-jmdvh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6559'
May 31 19:09:24.986: INFO: stderr: ""
May 31 19:09:24.986: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 19:09:24.986: INFO: validating pod update-demo-nautilus-jmdvh
May 31 19:09:24.998: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 19:09:24.998: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 19:09:24.998: INFO: update-demo-nautilus-jmdvh is verified up and running
May 31 19:09:24.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-x76k5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6559'
May 31 19:09:25.120: INFO: stderr: ""
May 31 19:09:25.120: INFO: stdout: "true"
May 31 19:09:25.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods update-demo-nautilus-x76k5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6559'
May 31 19:09:25.238: INFO: stderr: ""
May 31 19:09:25.238: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 31 19:09:25.238: INFO: validating pod update-demo-nautilus-x76k5
May 31 19:09:25.244: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 19:09:25.244: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 19:09:25.244: INFO: update-demo-nautilus-x76k5 is verified up and running
STEP: using delete to clean up resources
May 31 19:09:25.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-6559'
May 31 19:09:25.369: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 19:09:25.369: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 31 19:09:25.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6559'
May 31 19:09:25.633: INFO: stderr: "No resources found.\n"
May 31 19:09:25.633: INFO: stdout: ""
May 31 19:09:25.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -l name=update-demo --namespace=kubectl-6559 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 19:09:25.754: INFO: stderr: ""
May 31 19:09:25.754: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:09:25.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6559" for this suite.
May 31 19:09:39.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:09:39.841: INFO: namespace kubectl-6559 deletion completed in 14.084028977s

• [SLOW TEST:20.747 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:09:39.843: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 31 19:09:39.897: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:09:44.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5537" for this suite.
May 31 19:09:50.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:09:50.356: INFO: namespace init-container-5537 deletion completed in 6.091252042s

• [SLOW TEST:10.513 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:09:50.356: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1149
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1149
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1149
May 31 19:09:50.423: INFO: Found 0 stateful pods, waiting for 1
May 31 19:10:00.426: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 31 19:10:00.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 19:10:00.650: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 19:10:00.650: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 19:10:00.650: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 19:10:00.653: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 31 19:10:10.656: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 19:10:10.657: INFO: Waiting for statefulset status.replicas updated to 0
May 31 19:10:10.673: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
May 31 19:10:11.677: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993185823s
May 31 19:10:12.680: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989554296s
May 31 19:10:13.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986450376s
May 31 19:10:14.696: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981999435s
May 31 19:10:15.699: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970435679s
May 31 19:10:16.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.967222155s
May 31 19:10:17.706: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964039931s
May 31 19:10:18.709: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.961140611s
May 31 19:10:19.714: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.779783ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1149
May 31 19:10:20.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:10:20.957: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 31 19:10:20.957: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 19:10:20.957: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 19:10:20.960: INFO: Found 1 stateful pods, waiting for 3
May 31 19:10:30.963: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 19:10:30.963: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 19:10:30.963: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 31 19:10:30.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 19:10:31.171: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 19:10:31.171: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 19:10:31.171: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 19:10:31.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 19:10:31.407: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 19:10:31.407: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 19:10:31.407: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 19:10:31.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 31 19:10:31.656: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 31 19:10:31.656: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 31 19:10:31.656: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 31 19:10:31.656: INFO: Waiting for statefulset status.replicas updated to 0
May 31 19:10:31.660: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 31 19:10:41.667: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 19:10:41.667: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 31 19:10:41.667: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 31 19:10:41.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999996s
May 31 19:10:42.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99513713s
May 31 19:10:43.686: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991133373s
May 31 19:10:44.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987653823s
May 31 19:10:45.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984247974s
May 31 19:10:46.698: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980021311s
May 31 19:10:47.701: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975551044s
May 31 19:10:48.709: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967728722s
May 31 19:10:49.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963919064s
May 31 19:10:50.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.48211ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1149
May 31 19:10:51.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:10:51.931: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 31 19:10:51.931: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 19:10:51.931: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 19:10:51.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:10:52.197: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 31 19:10:52.197: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 31 19:10:52.197: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 31 19:10:52.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:10:52.439: INFO: rc: 126
May 31 19:10:52.439: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "process_linux.go:91: executing setns process caused \"exit status 21\"": unknown
 command terminated with exit code 126
 [] <nil> 0xc00240a2a0 exit status 126 <nil> <nil> true [0xc002f821f0 0xc002f82208 0xc002f82228] [0xc002f821f0 0xc002f82208 0xc002f82228] [0xc002f82200 0xc002f82218] [0x9c00a0 0x9c00a0] 0xc0034062a0 <nil>}:
Command stdout:
OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "process_linux.go:91: executing setns process caused \"exit status 21\"": unknown

stderr:
command terminated with exit code 126

error:
exit status 126

May 31 19:11:02.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:11:02.512: INFO: rc: 1
May 31 19:11:02.512: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016ba5d0 exit status 1 <nil> <nil> true [0xc001ad64a0 0xc001ad64b8 0xc001ad64d0] [0xc001ad64a0 0xc001ad64b8 0xc001ad64d0] [0xc001ad64b0 0xc001ad64c8] [0x9c00a0 0x9c00a0] 0xc003447500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:11:12.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:11:12.593: INFO: rc: 1
May 31 19:11:12.593: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240a660 exit status 1 <nil> <nil> true [0xc002f82230 0xc002f82248 0xc002f82260] [0xc002f82230 0xc002f82248 0xc002f82260] [0xc002f82240 0xc002f82258] [0x9c00a0 0x9c00a0] 0xc0034069c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:11:22.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:11:22.666: INFO: rc: 1
May 31 19:11:22.666: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240aa80 exit status 1 <nil> <nil> true [0xc002f82268 0xc002f82280 0xc002f82298] [0xc002f82268 0xc002f82280 0xc002f82298] [0xc002f82278 0xc002f82290] [0x9c00a0 0x9c00a0] 0xc003406d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:11:32.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:11:32.741: INFO: rc: 1
May 31 19:11:32.741: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240af90 exit status 1 <nil> <nil> true [0xc002f822a0 0xc002f822b8 0xc002f822d0] [0xc002f822a0 0xc002f822b8 0xc002f822d0] [0xc002f822b0 0xc002f822c8] [0x9c00a0 0x9c00a0] 0xc003407140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:11:42.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:11:42.825: INFO: rc: 1
May 31 19:11:42.825: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240b8f0 exit status 1 <nil> <nil> true [0xc002f822d8 0xc002f822f0 0xc002f82308] [0xc002f822d8 0xc002f822f0 0xc002f82308] [0xc002f822e8 0xc002f82300] [0x9c00a0 0x9c00a0] 0xc0034075c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:11:52.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:11:52.913: INFO: rc: 1
May 31 19:11:52.913: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240bcb0 exit status 1 <nil> <nil> true [0xc002f82310 0xc002f82328 0xc002f82340] [0xc002f82310 0xc002f82328 0xc002f82340] [0xc002f82320 0xc002f82338] [0x9c00a0 0x9c00a0] 0xc0034079e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:12:02.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:12:02.989: INFO: rc: 1
May 31 19:12:02.989: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023d2090 exit status 1 <nil> <nil> true [0xc002f82348 0xc002f82360 0xc002f82378] [0xc002f82348 0xc002f82360 0xc002f82378] [0xc002f82358 0xc002f82370] [0x9c00a0 0x9c00a0] 0xc003407ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:12:12.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:12:13.071: INFO: rc: 1
May 31 19:12:13.071: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240a4b0 exit status 1 <nil> <nil> true [0xc002f82008 0xc002f82020 0xc002f82038] [0xc002f82008 0xc002f82020 0xc002f82038] [0xc002f82018 0xc002f82030] [0x9c00a0 0x9c00a0] 0xc0027707e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:12:23.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:12:23.150: INFO: rc: 1
May 31 19:12:23.150: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240a930 exit status 1 <nil> <nil> true [0xc002f82040 0xc002f82058 0xc002f82070] [0xc002f82040 0xc002f82058 0xc002f82070] [0xc002f82050 0xc002f82068] [0x9c00a0 0x9c00a0] 0xc003544120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:12:33.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:12:33.233: INFO: rc: 1
May 31 19:12:33.233: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c28510 exit status 1 <nil> <nil> true [0xc001ad6000 0xc001ad6040 0xc001ad6058] [0xc001ad6000 0xc001ad6040 0xc001ad6058] [0xc001ad6028 0xc001ad6050] [0x9c00a0 0x9c00a0] 0xc002478480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:12:43.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:12:43.314: INFO: rc: 1
May 31 19:12:43.314: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c28900 exit status 1 <nil> <nil> true [0xc001ad6060 0xc001ad6078 0xc001ad6090] [0xc001ad6060 0xc001ad6078 0xc001ad6090] [0xc001ad6070 0xc001ad6088] [0x9c00a0 0x9c00a0] 0xc0024787e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:12:53.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:12:53.394: INFO: rc: 1
May 31 19:12:53.394: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240acf0 exit status 1 <nil> <nil> true [0xc002f82078 0xc002f82090 0xc002f820a8] [0xc002f82078 0xc002f82090 0xc002f820a8] [0xc002f82088 0xc002f820a0] [0x9c00a0 0x9c00a0] 0xc003544480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:13:03.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:13:03.475: INFO: rc: 1
May 31 19:13:03.475: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240b770 exit status 1 <nil> <nil> true [0xc002f820b0 0xc002f820c8 0xc002f820e0] [0xc002f820b0 0xc002f820c8 0xc002f820e0] [0xc002f820c0 0xc002f820d8] [0x9c00a0 0x9c00a0] 0xc003544840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:13:13.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:13:13.562: INFO: rc: 1
May 31 19:13:13.562: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240bbf0 exit status 1 <nil> <nil> true [0xc002f820e8 0xc002f82100 0xc002f82118] [0xc002f820e8 0xc002f82100 0xc002f82118] [0xc002f820f8 0xc002f82110] [0x9c00a0 0x9c00a0] 0xc003544d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:13:23.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:13:23.639: INFO: rc: 1
May 31 19:13:23.639: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c28de0 exit status 1 <nil> <nil> true [0xc001ad6098 0xc001ad60b0 0xc001ad60c8] [0xc001ad6098 0xc001ad60b0 0xc001ad60c8] [0xc001ad60a8 0xc001ad60c0] [0x9c00a0 0x9c00a0] 0xc002478b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:13:33.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:13:33.714: INFO: rc: 1
May 31 19:13:33.715: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c291d0 exit status 1 <nil> <nil> true [0xc001ad60d0 0xc001ad60e8 0xc001ad6100] [0xc001ad60d0 0xc001ad60e8 0xc001ad6100] [0xc001ad60e0 0xc001ad60f8] [0x9c00a0 0x9c00a0] 0xc002478ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:13:43.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:13:43.789: INFO: rc: 1
May 31 19:13:43.789: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001908000 exit status 1 <nil> <nil> true [0xc002f82120 0xc002f82138 0xc002f82150] [0xc002f82120 0xc002f82138 0xc002f82150] [0xc002f82130 0xc002f82148] [0x9c00a0 0x9c00a0] 0xc003545260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:13:53.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:13:53.893: INFO: rc: 1
May 31 19:13:53.894: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c299e0 exit status 1 <nil> <nil> true [0xc001ad6108 0xc001ad6120 0xc001ad6138] [0xc001ad6108 0xc001ad6120 0xc001ad6138] [0xc001ad6118 0xc001ad6130] [0x9c00a0 0x9c00a0] 0xc002479560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:14:03.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:14:04.028: INFO: rc: 1
May 31 19:14:04.029: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c29da0 exit status 1 <nil> <nil> true [0xc001ad6140 0xc001ad6170 0xc001ad6188] [0xc001ad6140 0xc001ad6170 0xc001ad6188] [0xc001ad6168 0xc001ad6180] [0x9c00a0 0x9c00a0] 0xc002479c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:14:14.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:14:14.120: INFO: rc: 1
May 31 19:14:14.120: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240a480 exit status 1 <nil> <nil> true [0xc002f82008 0xc002f82020 0xc002f82038] [0xc002f82008 0xc002f82020 0xc002f82038] [0xc002f82018 0xc002f82030] [0x9c00a0 0x9c00a0] 0xc00262e1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:14:24.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:14:24.202: INFO: rc: 1
May 31 19:14:24.202: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001908360 exit status 1 <nil> <nil> true [0xc001ad6000 0xc001ad6040 0xc001ad6058] [0xc001ad6000 0xc001ad6040 0xc001ad6058] [0xc001ad6028 0xc001ad6050] [0x9c00a0 0x9c00a0] 0xc0035442a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:14:34.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:14:34.276: INFO: rc: 1
May 31 19:14:34.276: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019087e0 exit status 1 <nil> <nil> true [0xc001ad6060 0xc001ad6078 0xc001ad6090] [0xc001ad6060 0xc001ad6078 0xc001ad6090] [0xc001ad6070 0xc001ad6088] [0x9c00a0 0x9c00a0] 0xc003544600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:14:44.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:14:44.369: INFO: rc: 1
May 31 19:14:44.369: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001908ba0 exit status 1 <nil> <nil> true [0xc001ad6098 0xc001ad60b0 0xc001ad60c8] [0xc001ad6098 0xc001ad60b0 0xc001ad60c8] [0xc001ad60a8 0xc001ad60c0] [0x9c00a0 0x9c00a0] 0xc0035449c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:14:54.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:14:54.447: INFO: rc: 1
May 31 19:14:54.447: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001908f60 exit status 1 <nil> <nil> true [0xc001ad60d0 0xc001ad60e8 0xc001ad6100] [0xc001ad60d0 0xc001ad60e8 0xc001ad6100] [0xc001ad60e0 0xc001ad60f8] [0x9c00a0 0x9c00a0] 0xc003545020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:15:04.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:15:04.529: INFO: rc: 1
May 31 19:15:04.529: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019092f0 exit status 1 <nil> <nil> true [0xc001ad6108 0xc001ad6120 0xc001ad6138] [0xc001ad6108 0xc001ad6120 0xc001ad6138] [0xc001ad6118 0xc001ad6130] [0x9c00a0 0x9c00a0] 0xc0035454a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:15:14.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:15:14.614: INFO: rc: 1
May 31 19:15:14.614: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019096e0 exit status 1 <nil> <nil> true [0xc001ad6140 0xc001ad6170 0xc001ad6188] [0xc001ad6140 0xc001ad6170 0xc001ad6188] [0xc001ad6168 0xc001ad6180] [0x9c00a0 0x9c00a0] 0xc003545920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:15:24.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:15:24.769: INFO: rc: 1
May 31 19:15:24.769: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240a8d0 exit status 1 <nil> <nil> true [0xc002f82040 0xc002f82058 0xc002f82070] [0xc002f82040 0xc002f82058 0xc002f82070] [0xc002f82050 0xc002f82068] [0x9c00a0 0x9c00a0] 0xc002478240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:15:34.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:15:34.860: INFO: rc: 1
May 31 19:15:34.860: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001909aa0 exit status 1 <nil> <nil> true [0xc001ad6190 0xc001ad61a8 0xc001ad61c0] [0xc001ad6190 0xc001ad61a8 0xc001ad61c0] [0xc001ad61a0 0xc001ad61b8] [0x9c00a0 0x9c00a0] 0xc003545e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:15:44.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:15:44.950: INFO: rc: 1
May 31 19:15:44.950: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00240ac90 exit status 1 <nil> <nil> true [0xc002f82078 0xc002f82090 0xc002f820a8] [0xc002f82078 0xc002f82090 0xc002f820a8] [0xc002f82088 0xc002f820a0] [0x9c00a0 0x9c00a0] 0xc002478660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

May 31 19:15:54.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 exec --namespace=statefulset-1149 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 31 19:15:55.036: INFO: rc: 1
May 31 19:15:55.036: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
May 31 19:15:55.036: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 19:15:55.044: INFO: Deleting all statefulset in ns statefulset-1149
May 31 19:15:55.046: INFO: Scaling statefulset ss to 0
May 31 19:15:55.052: INFO: Waiting for statefulset status.replicas updated to 0
May 31 19:15:55.054: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:15:55.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1149" for this suite.
May 31 19:16:01.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:16:01.157: INFO: namespace statefulset-1149 deletion completed in 6.088908947s

• [SLOW TEST:370.801 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:16:01.158: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 31 19:16:01.203: INFO: Waiting up to 5m0s for pod "pod-86f6662b-83d8-11e9-a3c9-f265bfc55a07" in namespace "emptydir-8997" to be "success or failure"
May 31 19:16:01.210: INFO: Pod "pod-86f6662b-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 7.063914ms
May 31 19:16:03.213: INFO: Pod "pod-86f6662b-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009967703s
May 31 19:16:05.217: INFO: Pod "pod-86f6662b-83d8-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013620406s
STEP: Saw pod success
May 31 19:16:05.217: INFO: Pod "pod-86f6662b-83d8-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:16:05.219: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-86f6662b-83d8-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 19:16:05.240: INFO: Waiting for pod pod-86f6662b-83d8-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:16:05.242: INFO: Pod pod-86f6662b-83d8-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:16:05.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8997" for this suite.
May 31 19:16:11.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:16:11.326: INFO: namespace emptydir-8997 deletion completed in 6.080896543s

• [SLOW TEST:10.168 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:16:11.326: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:16:11.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9444" for this suite.
May 31 19:16:17.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:16:17.528: INFO: namespace services-9444 deletion completed in 6.132121382s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.202 seconds]
[sig-network] Services
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:16:17.528: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 31 19:16:22.237: INFO: Successfully updated pod "pod-update-90cd6971-83d8-11e9-a3c9-f265bfc55a07"
STEP: verifying the updated pod is in kubernetes
May 31 19:16:22.242: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:16:22.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9618" for this suite.
May 31 19:16:44.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:16:44.324: INFO: namespace pods-9618 deletion completed in 22.079287312s

• [SLOW TEST:26.796 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:16:44.324: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-a0b135e8-83d8-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 19:16:44.373: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a0b1a988-83d8-11e9-a3c9-f265bfc55a07" in namespace "projected-737" to be "success or failure"
May 31 19:16:44.380: INFO: Pod "pod-projected-configmaps-a0b1a988-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 7.558321ms
May 31 19:16:46.383: INFO: Pod "pod-projected-configmaps-a0b1a988-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01018884s
May 31 19:16:48.386: INFO: Pod "pod-projected-configmaps-a0b1a988-83d8-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013521171s
STEP: Saw pod success
May 31 19:16:48.387: INFO: Pod "pod-projected-configmaps-a0b1a988-83d8-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:16:48.389: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-projected-configmaps-a0b1a988-83d8-11e9-a3c9-f265bfc55a07 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 19:16:48.407: INFO: Waiting for pod pod-projected-configmaps-a0b1a988-83d8-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:16:48.409: INFO: Pod pod-projected-configmaps-a0b1a988-83d8-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:16:48.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-737" for this suite.
May 31 19:16:54.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:16:54.572: INFO: namespace projected-737 deletion completed in 6.158746596s

• [SLOW TEST:10.248 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:16:54.573: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:16:54.675: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6d54029-83d8-11e9-a3c9-f265bfc55a07" in namespace "downward-api-4632" to be "success or failure"
May 31 19:16:54.689: INFO: Pod "downwardapi-volume-a6d54029-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 8.373135ms
May 31 19:16:56.740: INFO: Pod "downwardapi-volume-a6d54029-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059990749s
May 31 19:16:58.743: INFO: Pod "downwardapi-volume-a6d54029-83d8-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062976282s
STEP: Saw pod success
May 31 19:16:58.743: INFO: Pod "downwardapi-volume-a6d54029-83d8-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:16:58.746: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downwardapi-volume-a6d54029-83d8-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:16:58.770: INFO: Waiting for pod downwardapi-volume-a6d54029-83d8-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:16:58.773: INFO: Pod downwardapi-volume-a6d54029-83d8-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:16:58.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4632" for this suite.
May 31 19:17:04.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:17:04.887: INFO: namespace downward-api-4632 deletion completed in 6.108990418s

• [SLOW TEST:10.314 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:17:04.887: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:17:04.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-acf2e007-83d8-11e9-a3c9-f265bfc55a07" in namespace "projected-1947" to be "success or failure"
May 31 19:17:04.947: INFO: Pod "downwardapi-volume-acf2e007-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 11.715089ms
May 31 19:17:06.950: INFO: Pod "downwardapi-volume-acf2e007-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014797729s
May 31 19:17:08.953: INFO: Pod "downwardapi-volume-acf2e007-83d8-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018554582s
STEP: Saw pod success
May 31 19:17:08.953: INFO: Pod "downwardapi-volume-acf2e007-83d8-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:17:08.956: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downwardapi-volume-acf2e007-83d8-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:17:08.971: INFO: Waiting for pod downwardapi-volume-acf2e007-83d8-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:17:08.973: INFO: Pod downwardapi-volume-acf2e007-83d8-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:17:08.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1947" for this suite.
May 31 19:17:14.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:17:15.054: INFO: namespace projected-1947 deletion completed in 6.075456901s

• [SLOW TEST:10.167 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:17:15.058: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 31 19:17:23.143: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:23.145: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:25.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:25.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:27.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:27.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:29.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:29.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:31.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:31.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:33.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:33.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:35.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:35.148: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:37.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:37.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:39.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:39.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:41.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:41.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:43.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:43.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:45.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:45.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:47.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:47.149: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 19:17:49.146: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 19:17:49.148: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:17:49.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6283" for this suite.
May 31 19:18:11.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:18:11.236: INFO: namespace container-lifecycle-hook-6283 deletion completed in 22.078419064s

• [SLOW TEST:56.177 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:18:11.236: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:18:11.277: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 31 19:18:11.285: INFO: Pod name sample-pod: Found 0 pods out of 1
May 31 19:18:16.295: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 31 19:18:16.295: INFO: Creating deployment "test-rolling-update-deployment"
May 31 19:18:16.302: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 31 19:18:16.313: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 31 19:18:18.319: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 31 19:18:18.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694927096, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694927096, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694927096, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694927096, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 19:18:20.329: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 19:18:20.336: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-7462,SelfLink:/apis/apps/v1/namespaces/deployment-7462/deployments/test-rolling-update-deployment,UID:d77d2c07-83d8-11e9-a607-001dd80c001e,ResourceVersion:14728,Generation:1,CreationTimestamp:2019-05-31 19:18:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-31 19:18:16 +0000 UTC 2019-05-31 19:18:16 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-31 19:18:19 +0000 UTC 2019-05-31 19:18:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 31 19:18:20.340: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-7462,SelfLink:/apis/apps/v1/namespaces/deployment-7462/replicasets/test-rolling-update-deployment-67599b4d9,UID:d77ecf35-83d8-11e9-a607-001dd80c001e,ResourceVersion:14717,Generation:1,CreationTimestamp:2019-05-31 19:18:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d77d2c07-83d8-11e9-a607-001dd80c001e 0xc0025ace60 0xc0025ace61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 31 19:18:20.340: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 31 19:18:20.340: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-7462,SelfLink:/apis/apps/v1/namespaces/deployment-7462/replicasets/test-rolling-update-controller,UID:d47f1400-83d8-11e9-a607-001dd80c001e,ResourceVersion:14727,Generation:2,CreationTimestamp:2019-05-31 19:18:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d77d2c07-83d8-11e9-a607-001dd80c001e 0xc0025acd97 0xc0025acd98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 19:18:20.343: INFO: Pod "test-rolling-update-deployment-67599b4d9-zlq4t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-zlq4t,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-7462,SelfLink:/api/v1/namespaces/deployment-7462/pods/test-rolling-update-deployment-67599b4d9-zlq4t,UID:d77f9336-83d8-11e9-a607-001dd80c001e,ResourceVersion:14716,Generation:0,CreationTimestamp:2019-05-31 19:18:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 d77ecf35-83d8-11e9-a607-001dd80c001e 0xc0029e13b0 0xc0029e13b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-j7fkk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-j7fkk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-j7fkk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029e1410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029e1430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:18:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:18:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:18:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:18:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.96,StartTime:2019-05-31 19:18:16 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-31 19:18:18 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9ca4fdbc7b54900c36aea8f5f713b6966f3e3fb9c2a1a4991ef3f6bc02cb2b04}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:18:20.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7462" for this suite.
May 31 19:18:26.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:18:26.463: INFO: namespace deployment-7462 deletion completed in 6.117073713s

• [SLOW TEST:15.227 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:18:26.465: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 31 19:18:26.536: INFO: Waiting up to 5m0s for pod "downward-api-dd95cbf3-83d8-11e9-a3c9-f265bfc55a07" in namespace "downward-api-2335" to be "success or failure"
May 31 19:18:26.542: INFO: Pod "downward-api-dd95cbf3-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.445188ms
May 31 19:18:28.545: INFO: Pod "downward-api-dd95cbf3-83d8-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008598583s
May 31 19:18:30.549: INFO: Pod "downward-api-dd95cbf3-83d8-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012121785s
STEP: Saw pod success
May 31 19:18:30.549: INFO: Pod "downward-api-dd95cbf3-83d8-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:18:30.551: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downward-api-dd95cbf3-83d8-11e9-a3c9-f265bfc55a07 container dapi-container: <nil>
STEP: delete the pod
May 31 19:18:30.566: INFO: Waiting for pod downward-api-dd95cbf3-83d8-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:18:30.569: INFO: Pod downward-api-dd95cbf3-83d8-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:18:30.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2335" for this suite.
May 31 19:18:36.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:18:36.654: INFO: namespace downward-api-2335 deletion completed in 6.081960965s

• [SLOW TEST:10.189 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:18:36.663: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:18:42.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3339" for this suite.
May 31 19:18:48.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:18:48.902: INFO: namespace namespaces-3339 deletion completed in 6.084851134s
STEP: Destroying namespace "nsdeletetest-6910" for this suite.
May 31 19:18:48.904: INFO: Namespace nsdeletetest-6910 was already deleted
STEP: Destroying namespace "nsdeletetest-9863" for this suite.
May 31 19:18:54.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:18:54.995: INFO: namespace nsdeletetest-9863 deletion completed in 6.090295333s

• [SLOW TEST:18.332 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:18:54.996: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-2938
May 31 19:18:59.119: INFO: Started pod liveness-http in namespace container-probe-2938
STEP: checking the pod's current state and verifying that restartCount is present
May 31 19:18:59.122: INFO: Initial restart count of pod liveness-http is 0
May 31 19:19:19.156: INFO: Restart count of pod container-probe-2938/liveness-http is now 1 (20.03378403s elapsed)
May 31 19:19:39.188: INFO: Restart count of pod container-probe-2938/liveness-http is now 2 (40.066450552s elapsed)
May 31 19:19:59.226: INFO: Restart count of pod container-probe-2938/liveness-http is now 3 (1m0.103981458s elapsed)
May 31 19:20:19.263: INFO: Restart count of pod container-probe-2938/liveness-http is now 4 (1m20.140775153s elapsed)
May 31 19:21:21.393: INFO: Restart count of pod container-probe-2938/liveness-http is now 5 (2m22.270837861s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:21:21.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2938" for this suite.
May 31 19:21:27.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:21:27.585: INFO: namespace container-probe-2938 deletion completed in 6.175211137s

• [SLOW TEST:152.589 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:21:27.585: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:21:27.641: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:21:28.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2102" for this suite.
May 31 19:21:34.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:21:34.391: INFO: namespace custom-resource-definition-2102 deletion completed in 6.088457244s

• [SLOW TEST:6.806 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:21:34.392: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 19:21:34.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1244'
May 31 19:21:34.934: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 31 19:21:34.934: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
May 31 19:21:34.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1244'
May 31 19:21:35.049: INFO: stderr: ""
May 31 19:21:35.049: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:21:35.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1244" for this suite.
May 31 19:21:41.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:21:41.149: INFO: namespace kubectl-1244 deletion completed in 6.095623068s

• [SLOW TEST:6.757 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:21:41.150: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 31 19:21:41.189: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 19:21:41.196: INFO: Waiting for terminating namespaces to be deleted...
May 31 19:21:41.199: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-33506892-0 before test
May 31 19:21:41.204: INFO: kube-flannel-ds-5kgxc from kube-system started at 2019-05-31 18:09:36 +0000 UTC (2 container statuses recorded)
May 31 19:21:41.204: INFO: 	Container install-cni ready: true, restart count 0
May 31 19:21:41.204: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 19:21:41.204: INFO: azure-ip-masq-agent-hffrz from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.204: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 19:21:41.204: INFO: tiller-deploy-5d9ffb885b-sm2bb from kube-system started at 2019-05-31 18:10:05 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.204: INFO: 	Container tiller ready: true, restart count 0
May 31 19:21:41.204: INFO: sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-zhmp4 from heptio-sonobuoy started at 2019-05-31 18:11:14 +0000 UTC (2 container statuses recorded)
May 31 19:21:41.204: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 19:21:41.204: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 31 19:21:41.204: INFO: kube-proxy-5bk4d from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.204: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 19:21:41.204: INFO: metrics-server-6bf85bb69b-rc4nd from kube-system started at 2019-05-31 18:10:05 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.204: INFO: 	Container metrics-server ready: true, restart count 0
May 31 19:21:41.204: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-33506892-1 before test
May 31 19:21:41.211: INFO: sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-t5p7z from heptio-sonobuoy started at 2019-05-31 18:11:14 +0000 UTC (2 container statuses recorded)
May 31 19:21:41.211: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 19:21:41.211: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 31 19:21:41.211: INFO: kube-flannel-ds-9s4jc from kube-system started at 2019-05-31 18:09:35 +0000 UTC (2 container statuses recorded)
May 31 19:21:41.211: INFO: 	Container install-cni ready: true, restart count 0
May 31 19:21:41.211: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 19:21:41.211: INFO: azure-ip-masq-agent-8cg76 from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.211: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 19:21:41.211: INFO: kube-proxy-kkq24 from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.211: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 19:21:41.211: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-31 18:11:05 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.211: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 19:21:41.211: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-33506892-2 before test
May 31 19:21:41.218: INFO: kube-flannel-ds-n8nxk from kube-system started at 2019-05-31 18:09:35 +0000 UTC (2 container statuses recorded)
May 31 19:21:41.218: INFO: 	Container install-cni ready: true, restart count 0
May 31 19:21:41.218: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 19:21:41.218: INFO: azure-ip-masq-agent-lnst2 from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.218: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 19:21:41.218: INFO: kube-proxy-49ngv from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.218: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 19:21:41.218: INFO: kubernetes-dashboard-54b795b69b-zsd4t from kube-system started at 2019-05-31 18:10:07 +0000 UTC (1 container statuses recorded)
May 31 19:21:41.218: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 31 19:21:41.218: INFO: sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-fnw89 from heptio-sonobuoy started at 2019-05-31 18:11:14 +0000 UTC (2 container statuses recorded)
May 31 19:21:41.218: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 19:21:41.218: INFO: 	Container sonobuoy-worker ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15a3dad445d7888e], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:21:42.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6852" for this suite.
May 31 19:21:48.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:21:48.323: INFO: namespace sched-pred-6852 deletion completed in 6.077480684s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.173 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:21:48.323: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
May 31 19:21:48.373: INFO: Waiting up to 5m0s for pod "client-containers-55e478dc-83d9-11e9-a3c9-f265bfc55a07" in namespace "containers-7691" to be "success or failure"
May 31 19:21:48.375: INFO: Pod "client-containers-55e478dc-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 1.759228ms
May 31 19:21:50.378: INFO: Pod "client-containers-55e478dc-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005033827s
May 31 19:21:52.381: INFO: Pod "client-containers-55e478dc-83d9-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007869118s
STEP: Saw pod success
May 31 19:21:52.381: INFO: Pod "client-containers-55e478dc-83d9-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:21:52.384: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod client-containers-55e478dc-83d9-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 19:21:52.401: INFO: Waiting for pod client-containers-55e478dc-83d9-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:21:52.404: INFO: Pod client-containers-55e478dc-83d9-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:21:52.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7691" for this suite.
May 31 19:21:58.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:21:58.497: INFO: namespace containers-7691 deletion completed in 6.090520307s

• [SLOW TEST:10.174 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:21:58.499: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:22:24.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-228" for this suite.
May 31 19:22:30.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:22:30.736: INFO: namespace namespaces-228 deletion completed in 6.08520726s
STEP: Destroying namespace "nsdeletetest-8247" for this suite.
May 31 19:22:30.738: INFO: Namespace nsdeletetest-8247 was already deleted
STEP: Destroying namespace "nsdeletetest-2180" for this suite.
May 31 19:22:36.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:22:36.875: INFO: namespace nsdeletetest-2180 deletion completed in 6.137150507s

• [SLOW TEST:38.376 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:22:36.879: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5945.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5945.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 19:22:40.983: INFO: DNS probes using dns-5945/dns-test-72d713d1-83d9-11e9-a3c9-f265bfc55a07 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:22:40.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5945" for this suite.
May 31 19:22:47.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:22:47.083: INFO: namespace dns-5945 deletion completed in 6.080294599s

• [SLOW TEST:10.204 seconds]
[sig-network] DNS
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:22:47.085: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8323
I0531 19:22:47.131529      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8323, replica count: 1
I0531 19:22:48.181902      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 19:22:49.182126      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 19:22:50.182332      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 19:22:50.303: INFO: Created: latency-svc-x8bcp
May 31 19:22:50.306: INFO: Got endpoints: latency-svc-x8bcp [23.337178ms]
May 31 19:22:50.319: INFO: Created: latency-svc-qlpvl
May 31 19:22:50.332: INFO: Created: latency-svc-czwpb
May 31 19:22:50.342: INFO: Got endpoints: latency-svc-qlpvl [36.558392ms]
May 31 19:22:50.350: INFO: Created: latency-svc-8596b
May 31 19:22:50.353: INFO: Got endpoints: latency-svc-czwpb [46.863659ms]
May 31 19:22:50.370: INFO: Created: latency-svc-4bhls
May 31 19:22:50.376: INFO: Created: latency-svc-bzfq6
May 31 19:22:50.376: INFO: Created: latency-svc-fq79r
May 31 19:22:50.381: INFO: Got endpoints: latency-svc-8596b [75.265018ms]
May 31 19:22:50.395: INFO: Created: latency-svc-qqvdg
May 31 19:22:50.398: INFO: Created: latency-svc-svv2h
May 31 19:22:50.400: INFO: Got endpoints: latency-svc-4bhls [93.034406ms]
May 31 19:22:50.404: INFO: Created: latency-svc-f4ksd
May 31 19:22:50.418: INFO: Created: latency-svc-545cs
May 31 19:22:50.424: INFO: Got endpoints: latency-svc-f4ksd [117.011294ms]
May 31 19:22:50.424: INFO: Got endpoints: latency-svc-fq79r [117.507302ms]
May 31 19:22:50.424: INFO: Got endpoints: latency-svc-bzfq6 [117.972909ms]
May 31 19:22:50.425: INFO: Got endpoints: latency-svc-qqvdg [118.361116ms]
May 31 19:22:50.425: INFO: Got endpoints: latency-svc-svv2h [118.136512ms]
May 31 19:22:50.430: INFO: Got endpoints: latency-svc-545cs [121.387764ms]
May 31 19:22:50.443: INFO: Created: latency-svc-92zzs
May 31 19:22:50.455: INFO: Got endpoints: latency-svc-92zzs [147.04418ms]
May 31 19:22:50.460: INFO: Created: latency-svc-txfzg
May 31 19:22:50.466: INFO: Got endpoints: latency-svc-txfzg [159.319578ms]
May 31 19:22:50.479: INFO: Created: latency-svc-bt9sr
May 31 19:22:50.486: INFO: Got endpoints: latency-svc-bt9sr [177.515273ms]
May 31 19:22:50.490: INFO: Created: latency-svc-nbtk5
May 31 19:22:50.490: INFO: Got endpoints: latency-svc-nbtk5 [184.025678ms]
May 31 19:22:50.499: INFO: Created: latency-svc-6qrjs
May 31 19:22:50.503: INFO: Created: latency-svc-qrr5k
May 31 19:22:50.514: INFO: Got endpoints: latency-svc-6qrjs [205.476025ms]
May 31 19:22:50.521: INFO: Created: latency-svc-6rrll
May 31 19:22:50.522: INFO: Got endpoints: latency-svc-qrr5k [180.020913ms]
May 31 19:22:50.531: INFO: Created: latency-svc-7k5zd
May 31 19:22:50.531: INFO: Got endpoints: latency-svc-6rrll [177.379871ms]
May 31 19:22:50.539: INFO: Created: latency-svc-slqxp
May 31 19:22:50.544: INFO: Got endpoints: latency-svc-slqxp [144.813644ms]
May 31 19:22:50.545: INFO: Got endpoints: latency-svc-7k5zd [163.797251ms]
May 31 19:22:50.563: INFO: Created: latency-svc-wwgfv
May 31 19:22:50.571: INFO: Got endpoints: latency-svc-wwgfv [147.537287ms]
May 31 19:22:50.576: INFO: Created: latency-svc-7twb2
May 31 19:22:50.644: INFO: Created: latency-svc-bpcsx
May 31 19:22:50.647: INFO: Got endpoints: latency-svc-7twb2 [223.315614ms]
May 31 19:22:50.658: INFO: Created: latency-svc-6nq4k
May 31 19:22:50.658: INFO: Got endpoints: latency-svc-bpcsx [233.804584ms]
May 31 19:22:50.673: INFO: Got endpoints: latency-svc-6nq4k [248.224917ms]
May 31 19:22:50.678: INFO: Created: latency-svc-vgq49
May 31 19:22:50.679: INFO: Got endpoints: latency-svc-vgq49 [254.065812ms]
May 31 19:22:50.691: INFO: Created: latency-svc-4gcqh
May 31 19:22:50.695: INFO: Got endpoints: latency-svc-4gcqh [265.313094ms]
May 31 19:22:50.706: INFO: Created: latency-svc-lv7gp
May 31 19:22:50.717: INFO: Created: latency-svc-8fn52
May 31 19:22:50.721: INFO: Got endpoints: latency-svc-8fn52 [255.094529ms]
May 31 19:22:50.721: INFO: Got endpoints: latency-svc-lv7gp [265.994205ms]
May 31 19:22:50.729: INFO: Created: latency-svc-s9kgh
May 31 19:22:50.742: INFO: Got endpoints: latency-svc-s9kgh [255.894642ms]
May 31 19:22:50.747: INFO: Created: latency-svc-bmtqn
May 31 19:22:50.819: INFO: Got endpoints: latency-svc-bmtqn [328.977224ms]
May 31 19:22:50.828: INFO: Created: latency-svc-rvgvt
May 31 19:22:50.828: INFO: Created: latency-svc-529j8
May 31 19:22:50.833: INFO: Got endpoints: latency-svc-529j8 [319.270567ms]
May 31 19:22:50.837: INFO: Got endpoints: latency-svc-rvgvt [314.695693ms]
May 31 19:22:50.843: INFO: Created: latency-svc-g7t9q
May 31 19:22:50.849: INFO: Created: latency-svc-hfvlp
May 31 19:22:50.856: INFO: Got endpoints: latency-svc-g7t9q [325.203863ms]
May 31 19:22:50.866: INFO: Created: latency-svc-7wgtk
May 31 19:22:50.870: INFO: Created: latency-svc-kj5t2
May 31 19:22:50.879: INFO: Got endpoints: latency-svc-7wgtk [334.27031ms]
May 31 19:22:50.879: INFO: Got endpoints: latency-svc-hfvlp [333.610699ms]
May 31 19:22:50.883: INFO: Got endpoints: latency-svc-kj5t2 [310.880231ms]
May 31 19:22:50.887: INFO: Created: latency-svc-29298
May 31 19:22:50.907: INFO: Created: latency-svc-bkx9w
May 31 19:22:50.907: INFO: Created: latency-svc-66sml
May 31 19:22:50.908: INFO: Got endpoints: latency-svc-29298 [260.339513ms]
May 31 19:22:50.911: INFO: Got endpoints: latency-svc-66sml [252.685789ms]
May 31 19:22:50.922: INFO: Got endpoints: latency-svc-bkx9w [249.470938ms]
May 31 19:22:50.926: INFO: Created: latency-svc-hfd8d
May 31 19:22:50.932: INFO: Created: latency-svc-ntghb
May 31 19:22:50.932: INFO: Got endpoints: latency-svc-hfd8d [253.2906ms]
May 31 19:22:50.941: INFO: Created: latency-svc-w5s8k
May 31 19:22:50.944: INFO: Got endpoints: latency-svc-w5s8k [222.225997ms]
May 31 19:22:50.947: INFO: Got endpoints: latency-svc-ntghb [252.261183ms]
May 31 19:22:50.955: INFO: Created: latency-svc-l452g
May 31 19:22:50.964: INFO: Got endpoints: latency-svc-l452g [242.80713ms]
May 31 19:22:50.966: INFO: Created: latency-svc-wmhtm
May 31 19:22:50.970: INFO: Created: latency-svc-xlb7g
May 31 19:22:50.986: INFO: Created: latency-svc-7dd2v
May 31 19:22:50.994: INFO: Created: latency-svc-d7qx8
May 31 19:22:51.007: INFO: Got endpoints: latency-svc-wmhtm [265.417995ms]
May 31 19:22:51.023: INFO: Created: latency-svc-7dftj
May 31 19:22:51.023: INFO: Created: latency-svc-jssnh
May 31 19:22:51.023: INFO: Created: latency-svc-4w8pc
May 31 19:22:51.107: INFO: Created: latency-svc-gp96j
May 31 19:22:51.120: INFO: Created: latency-svc-sk552
May 31 19:22:51.121: INFO: Got endpoints: latency-svc-7dd2v [287.870959ms]
May 31 19:22:51.121: INFO: Got endpoints: latency-svc-xlb7g [301.170475ms]
May 31 19:22:51.128: INFO: Created: latency-svc-mnzwj
May 31 19:22:51.133: INFO: Created: latency-svc-98cch
May 31 19:22:51.145: INFO: Created: latency-svc-m5tl5
May 31 19:22:51.156: INFO: Got endpoints: latency-svc-d7qx8 [316.892129ms]
May 31 19:22:51.159: INFO: Created: latency-svc-xxrpc
May 31 19:22:51.162: INFO: Created: latency-svc-74r77
May 31 19:22:51.169: INFO: Created: latency-svc-gfbd7
May 31 19:22:51.177: INFO: Created: latency-svc-rwtf8
May 31 19:22:51.188: INFO: Created: latency-svc-rvtwn
May 31 19:22:51.209: INFO: Got endpoints: latency-svc-jssnh [353.269917ms]
May 31 19:22:51.211: INFO: Created: latency-svc-dpsx5
May 31 19:22:51.275: INFO: Got endpoints: latency-svc-7dftj [396.243113ms]
May 31 19:22:51.283: INFO: Created: latency-svc-fcffz
May 31 19:22:51.291: INFO: Created: latency-svc-pz6wm
May 31 19:22:51.300: INFO: Created: latency-svc-xxbhs
May 31 19:22:51.308: INFO: Got endpoints: latency-svc-4w8pc [428.689138ms]
May 31 19:22:51.337: INFO: Created: latency-svc-9dg5j
May 31 19:22:51.344: INFO: Got endpoints: latency-svc-gp96j [461.365367ms]
May 31 19:22:51.360: INFO: Created: latency-svc-7dhsp
May 31 19:22:51.397: INFO: Got endpoints: latency-svc-sk552 [489.503322ms]
May 31 19:22:51.424: INFO: Created: latency-svc-dt25r
May 31 19:22:51.447: INFO: Got endpoints: latency-svc-mnzwj [535.597268ms]
May 31 19:22:51.458: INFO: Created: latency-svc-fsnzw
May 31 19:22:51.496: INFO: Got endpoints: latency-svc-98cch [573.530382ms]
May 31 19:22:51.511: INFO: Created: latency-svc-xv6jf
May 31 19:22:51.544: INFO: Got endpoints: latency-svc-m5tl5 [612.181308ms]
May 31 19:22:51.557: INFO: Created: latency-svc-nz5rd
May 31 19:22:51.596: INFO: Got endpoints: latency-svc-xxrpc [651.995853ms]
May 31 19:22:51.606: INFO: Created: latency-svc-22ntt
May 31 19:22:51.646: INFO: Got endpoints: latency-svc-74r77 [698.350803ms]
May 31 19:22:51.658: INFO: Created: latency-svc-8gx7l
May 31 19:22:51.698: INFO: Got endpoints: latency-svc-gfbd7 [733.485871ms]
May 31 19:22:51.711: INFO: Created: latency-svc-lbkz7
May 31 19:22:51.746: INFO: Got endpoints: latency-svc-rwtf8 [738.425451ms]
May 31 19:22:51.755: INFO: Created: latency-svc-ml7lx
May 31 19:22:51.797: INFO: Got endpoints: latency-svc-rvtwn [675.95744ms]
May 31 19:22:51.817: INFO: Created: latency-svc-d5hxp
May 31 19:22:51.857: INFO: Got endpoints: latency-svc-dpsx5 [736.249616ms]
May 31 19:22:51.871: INFO: Created: latency-svc-glxrv
May 31 19:22:51.896: INFO: Got endpoints: latency-svc-fcffz [740.418284ms]
May 31 19:22:51.907: INFO: Created: latency-svc-fcqj5
May 31 19:22:51.944: INFO: Got endpoints: latency-svc-pz6wm [735.003496ms]
May 31 19:22:51.959: INFO: Created: latency-svc-x8mtz
May 31 19:22:51.996: INFO: Got endpoints: latency-svc-xxbhs [721.140471ms]
May 31 19:22:52.007: INFO: Created: latency-svc-xr24z
May 31 19:22:52.049: INFO: Got endpoints: latency-svc-9dg5j [741.152495ms]
May 31 19:22:52.076: INFO: Created: latency-svc-2bw9j
May 31 19:22:52.097: INFO: Got endpoints: latency-svc-7dhsp [752.733782ms]
May 31 19:22:52.111: INFO: Created: latency-svc-l7mcf
May 31 19:22:52.145: INFO: Got endpoints: latency-svc-dt25r [747.641601ms]
May 31 19:22:52.160: INFO: Created: latency-svc-jmvhh
May 31 19:22:52.195: INFO: Got endpoints: latency-svc-fsnzw [748.577916ms]
May 31 19:22:52.207: INFO: Created: latency-svc-7c4qj
May 31 19:22:52.246: INFO: Got endpoints: latency-svc-xv6jf [749.105624ms]
May 31 19:22:52.259: INFO: Created: latency-svc-6ztmf
May 31 19:22:52.306: INFO: Got endpoints: latency-svc-nz5rd [761.340622ms]
May 31 19:22:52.339: INFO: Created: latency-svc-phpkk
May 31 19:22:52.354: INFO: Got endpoints: latency-svc-22ntt [757.614962ms]
May 31 19:22:52.364: INFO: Created: latency-svc-tr52k
May 31 19:22:52.397: INFO: Got endpoints: latency-svc-8gx7l [750.525547ms]
May 31 19:22:52.420: INFO: Created: latency-svc-c7vnr
May 31 19:22:52.458: INFO: Got endpoints: latency-svc-lbkz7 [759.432491ms]
May 31 19:22:52.532: INFO: Got endpoints: latency-svc-ml7lx [786.326626ms]
May 31 19:22:52.549: INFO: Created: latency-svc-ng7tz
May 31 19:22:52.580: INFO: Created: latency-svc-q26bx
May 31 19:22:52.580: INFO: Got endpoints: latency-svc-d5hxp [783.006272ms]
May 31 19:22:52.609: INFO: Created: latency-svc-j5vcc
May 31 19:22:52.609: INFO: Got endpoints: latency-svc-glxrv [749.887336ms]
May 31 19:22:52.631: INFO: Created: latency-svc-crrxd
May 31 19:22:52.647: INFO: Got endpoints: latency-svc-fcqj5 [751.047456ms]
May 31 19:22:52.669: INFO: Created: latency-svc-w85zl
May 31 19:22:52.710: INFO: Got endpoints: latency-svc-x8mtz [765.322087ms]
May 31 19:22:52.720: INFO: Created: latency-svc-qgh5b
May 31 19:22:52.753: INFO: Got endpoints: latency-svc-xr24z [756.720147ms]
May 31 19:22:52.771: INFO: Created: latency-svc-bx4sh
May 31 19:22:52.821: INFO: Got endpoints: latency-svc-2bw9j [772.202498ms]
May 31 19:22:52.839: INFO: Created: latency-svc-5hpw5
May 31 19:22:52.846: INFO: Got endpoints: latency-svc-l7mcf [748.987022ms]
May 31 19:22:52.860: INFO: Created: latency-svc-sng8n
May 31 19:22:52.898: INFO: Got endpoints: latency-svc-jmvhh [752.235675ms]
May 31 19:22:52.923: INFO: Created: latency-svc-rlxcq
May 31 19:22:52.946: INFO: Got endpoints: latency-svc-7c4qj [750.868452ms]
May 31 19:22:52.960: INFO: Created: latency-svc-rhdh2
May 31 19:22:52.997: INFO: Got endpoints: latency-svc-6ztmf [751.260959ms]
May 31 19:22:53.009: INFO: Created: latency-svc-pb59j
May 31 19:22:53.051: INFO: Got endpoints: latency-svc-phpkk [745.434465ms]
May 31 19:22:53.066: INFO: Created: latency-svc-6zl8p
May 31 19:22:53.100: INFO: Got endpoints: latency-svc-tr52k [746.715385ms]
May 31 19:22:53.119: INFO: Created: latency-svc-dhhht
May 31 19:22:53.149: INFO: Got endpoints: latency-svc-c7vnr [752.050172ms]
May 31 19:22:53.163: INFO: Created: latency-svc-jqpk6
May 31 19:22:53.197: INFO: Got endpoints: latency-svc-ng7tz [739.737372ms]
May 31 19:22:53.212: INFO: Created: latency-svc-tw49c
May 31 19:22:53.246: INFO: Got endpoints: latency-svc-q26bx [712.264228ms]
May 31 19:22:53.261: INFO: Created: latency-svc-kfxb7
May 31 19:22:53.296: INFO: Got endpoints: latency-svc-j5vcc [715.652282ms]
May 31 19:22:53.311: INFO: Created: latency-svc-xqzqv
May 31 19:22:53.347: INFO: Got endpoints: latency-svc-crrxd [738.309049ms]
May 31 19:22:53.361: INFO: Created: latency-svc-w9z6n
May 31 19:22:53.401: INFO: Got endpoints: latency-svc-w85zl [753.645498ms]
May 31 19:22:53.423: INFO: Created: latency-svc-vdmxm
May 31 19:22:53.445: INFO: Got endpoints: latency-svc-qgh5b [735.497504ms]
May 31 19:22:53.459: INFO: Created: latency-svc-5v8lv
May 31 19:22:53.495: INFO: Got endpoints: latency-svc-bx4sh [741.899308ms]
May 31 19:22:53.508: INFO: Created: latency-svc-dd5bw
May 31 19:22:53.547: INFO: Got endpoints: latency-svc-5hpw5 [725.592043ms]
May 31 19:22:53.560: INFO: Created: latency-svc-g67hh
May 31 19:22:53.594: INFO: Got endpoints: latency-svc-sng8n [748.295411ms]
May 31 19:22:53.609: INFO: Created: latency-svc-p4vfv
May 31 19:22:53.647: INFO: Got endpoints: latency-svc-rlxcq [749.043823ms]
May 31 19:22:53.658: INFO: Created: latency-svc-x5bgf
May 31 19:22:53.698: INFO: Got endpoints: latency-svc-rhdh2 [751.29016ms]
May 31 19:22:53.713: INFO: Created: latency-svc-9vfld
May 31 19:22:53.749: INFO: Got endpoints: latency-svc-pb59j [751.605565ms]
May 31 19:22:53.762: INFO: Created: latency-svc-zkt6v
May 31 19:22:53.797: INFO: Got endpoints: latency-svc-6zl8p [745.523966ms]
May 31 19:22:53.817: INFO: Created: latency-svc-zjvv5
May 31 19:22:53.853: INFO: Got endpoints: latency-svc-dhhht [752.648882ms]
May 31 19:22:53.871: INFO: Created: latency-svc-g2kg9
May 31 19:22:53.905: INFO: Got endpoints: latency-svc-jqpk6 [756.676347ms]
May 31 19:22:53.932: INFO: Created: latency-svc-snrmg
May 31 19:22:53.954: INFO: Got endpoints: latency-svc-tw49c [755.087021ms]
May 31 19:22:53.968: INFO: Created: latency-svc-n92rk
May 31 19:22:53.997: INFO: Got endpoints: latency-svc-kfxb7 [750.458046ms]
May 31 19:22:54.008: INFO: Created: latency-svc-h27kx
May 31 19:22:54.048: INFO: Got endpoints: latency-svc-xqzqv [751.898669ms]
May 31 19:22:54.062: INFO: Created: latency-svc-tbzkm
May 31 19:22:54.101: INFO: Got endpoints: latency-svc-w9z6n [754.300008ms]
May 31 19:22:54.126: INFO: Created: latency-svc-88cpt
May 31 19:22:54.145: INFO: Got endpoints: latency-svc-vdmxm [744.250946ms]
May 31 19:22:54.157: INFO: Created: latency-svc-d7ptg
May 31 19:22:54.196: INFO: Got endpoints: latency-svc-5v8lv [751.156858ms]
May 31 19:22:54.209: INFO: Created: latency-svc-8fc2n
May 31 19:22:54.259: INFO: Got endpoints: latency-svc-dd5bw [763.577059ms]
May 31 19:22:54.274: INFO: Created: latency-svc-5ptqq
May 31 19:22:54.296: INFO: Got endpoints: latency-svc-g67hh [748.958822ms]
May 31 19:22:54.307: INFO: Created: latency-svc-ljjk9
May 31 19:22:54.346: INFO: Got endpoints: latency-svc-p4vfv [751.790368ms]
May 31 19:22:54.359: INFO: Created: latency-svc-59gnc
May 31 19:22:54.399: INFO: Got endpoints: latency-svc-x5bgf [752.55688ms]
May 31 19:22:54.411: INFO: Created: latency-svc-j4xql
May 31 19:22:54.446: INFO: Got endpoints: latency-svc-9vfld [747.918805ms]
May 31 19:22:54.463: INFO: Created: latency-svc-fjzkf
May 31 19:22:54.495: INFO: Got endpoints: latency-svc-zkt6v [746.460081ms]
May 31 19:22:54.510: INFO: Created: latency-svc-f9kpc
May 31 19:22:54.548: INFO: Got endpoints: latency-svc-zjvv5 [750.282543ms]
May 31 19:22:54.559: INFO: Created: latency-svc-j5hrx
May 31 19:22:54.598: INFO: Got endpoints: latency-svc-g2kg9 [744.415248ms]
May 31 19:22:54.610: INFO: Created: latency-svc-dk5kj
May 31 19:22:54.648: INFO: Got endpoints: latency-svc-snrmg [740.721388ms]
May 31 19:22:54.661: INFO: Created: latency-svc-pxzgp
May 31 19:22:54.696: INFO: Got endpoints: latency-svc-n92rk [741.227297ms]
May 31 19:22:54.710: INFO: Created: latency-svc-rbbfs
May 31 19:22:54.745: INFO: Got endpoints: latency-svc-h27kx [748.033607ms]
May 31 19:22:54.759: INFO: Created: latency-svc-x7jfm
May 31 19:22:54.805: INFO: Got endpoints: latency-svc-tbzkm [757.184255ms]
May 31 19:22:54.828: INFO: Created: latency-svc-mgldk
May 31 19:22:54.869: INFO: Got endpoints: latency-svc-88cpt [767.606623ms]
May 31 19:22:54.889: INFO: Created: latency-svc-dtdgr
May 31 19:22:54.906: INFO: Got endpoints: latency-svc-d7ptg [759.9536ms]
May 31 19:22:54.922: INFO: Created: latency-svc-8njhj
May 31 19:22:54.945: INFO: Got endpoints: latency-svc-8fc2n [748.678017ms]
May 31 19:22:54.957: INFO: Created: latency-svc-zfgcm
May 31 19:22:54.996: INFO: Got endpoints: latency-svc-5ptqq [737.287733ms]
May 31 19:22:55.008: INFO: Created: latency-svc-9tkp6
May 31 19:22:55.047: INFO: Got endpoints: latency-svc-ljjk9 [750.787952ms]
May 31 19:22:55.059: INFO: Created: latency-svc-tl9tp
May 31 19:22:55.098: INFO: Got endpoints: latency-svc-59gnc [751.061856ms]
May 31 19:22:55.134: INFO: Created: latency-svc-4fgt6
May 31 19:22:55.158: INFO: Got endpoints: latency-svc-j4xql [758.228672ms]
May 31 19:22:55.175: INFO: Created: latency-svc-zbswx
May 31 19:22:55.196: INFO: Got endpoints: latency-svc-fjzkf [750.276543ms]
May 31 19:22:55.207: INFO: Created: latency-svc-t7f9h
May 31 19:22:55.246: INFO: Got endpoints: latency-svc-f9kpc [750.790851ms]
May 31 19:22:55.260: INFO: Created: latency-svc-xl7k6
May 31 19:22:55.295: INFO: Got endpoints: latency-svc-j5hrx [747.530799ms]
May 31 19:22:55.306: INFO: Created: latency-svc-f7lx6
May 31 19:22:55.353: INFO: Got endpoints: latency-svc-dk5kj [755.074621ms]
May 31 19:22:55.367: INFO: Created: latency-svc-sbmkn
May 31 19:22:55.395: INFO: Got endpoints: latency-svc-pxzgp [747.495998ms]
May 31 19:22:55.407: INFO: Created: latency-svc-kbh56
May 31 19:22:55.445: INFO: Got endpoints: latency-svc-rbbfs [749.249326ms]
May 31 19:22:55.458: INFO: Created: latency-svc-4bcx2
May 31 19:22:55.495: INFO: Got endpoints: latency-svc-x7jfm [749.565032ms]
May 31 19:22:55.554: INFO: Created: latency-svc-j4r5s
May 31 19:22:55.555: INFO: Got endpoints: latency-svc-mgldk [749.44303ms]
May 31 19:22:55.569: INFO: Created: latency-svc-n4b5v
May 31 19:22:55.596: INFO: Got endpoints: latency-svc-dtdgr [726.840264ms]
May 31 19:22:55.639: INFO: Created: latency-svc-xv75r
May 31 19:22:55.652: INFO: Got endpoints: latency-svc-8njhj [745.203861ms]
May 31 19:22:55.664: INFO: Created: latency-svc-kz2xv
May 31 19:22:55.696: INFO: Got endpoints: latency-svc-zfgcm [750.763252ms]
May 31 19:22:55.710: INFO: Created: latency-svc-2f67w
May 31 19:22:55.760: INFO: Got endpoints: latency-svc-9tkp6 [763.253354ms]
May 31 19:22:55.786: INFO: Created: latency-svc-pll5r
May 31 19:22:55.806: INFO: Got endpoints: latency-svc-tl9tp [758.74158ms]
May 31 19:22:55.827: INFO: Created: latency-svc-qfcf4
May 31 19:22:55.938: INFO: Got endpoints: latency-svc-zbswx [779.483216ms]
May 31 19:22:55.938: INFO: Got endpoints: latency-svc-4fgt6 [840.226399ms]
May 31 19:22:55.958: INFO: Got endpoints: latency-svc-t7f9h [761.672928ms]
May 31 19:22:55.958: INFO: Created: latency-svc-d54kv
May 31 19:22:55.970: INFO: Created: latency-svc-l5fj2
May 31 19:22:55.983: INFO: Created: latency-svc-2m8n9
May 31 19:22:55.998: INFO: Got endpoints: latency-svc-xl7k6 [751.654066ms]
May 31 19:22:56.013: INFO: Created: latency-svc-np7nd
May 31 19:22:56.050: INFO: Got endpoints: latency-svc-f7lx6 [754.928019ms]
May 31 19:22:56.061: INFO: Created: latency-svc-n7g25
May 31 19:22:56.096: INFO: Got endpoints: latency-svc-sbmkn [743.044727ms]
May 31 19:22:56.110: INFO: Created: latency-svc-77j8w
May 31 19:22:56.145: INFO: Got endpoints: latency-svc-kbh56 [749.242227ms]
May 31 19:22:56.157: INFO: Created: latency-svc-pcn5t
May 31 19:22:56.196: INFO: Got endpoints: latency-svc-4bcx2 [750.452547ms]
May 31 19:22:56.208: INFO: Created: latency-svc-s87fj
May 31 19:22:56.250: INFO: Got endpoints: latency-svc-j4r5s [754.702515ms]
May 31 19:22:56.261: INFO: Created: latency-svc-fvxps
May 31 19:22:56.297: INFO: Got endpoints: latency-svc-n4b5v [741.867807ms]
May 31 19:22:56.310: INFO: Created: latency-svc-rbpxn
May 31 19:22:56.347: INFO: Got endpoints: latency-svc-xv75r [751.041455ms]
May 31 19:22:56.361: INFO: Created: latency-svc-zrtmg
May 31 19:22:56.396: INFO: Got endpoints: latency-svc-kz2xv [744.035643ms]
May 31 19:22:56.407: INFO: Created: latency-svc-vcklw
May 31 19:22:56.446: INFO: Got endpoints: latency-svc-2f67w [749.329628ms]
May 31 19:22:56.458: INFO: Created: latency-svc-ngnxl
May 31 19:22:56.497: INFO: Got endpoints: latency-svc-pll5r [737.454736ms]
May 31 19:22:56.510: INFO: Created: latency-svc-rzzj4
May 31 19:22:56.545: INFO: Got endpoints: latency-svc-qfcf4 [739.58187ms]
May 31 19:22:56.559: INFO: Created: latency-svc-sscvb
May 31 19:22:56.595: INFO: Got endpoints: latency-svc-d54kv [657.686445ms]
May 31 19:22:56.609: INFO: Created: latency-svc-d65lq
May 31 19:22:56.648: INFO: Got endpoints: latency-svc-l5fj2 [709.174578ms]
May 31 19:22:56.660: INFO: Created: latency-svc-lbxmp
May 31 19:22:56.706: INFO: Got endpoints: latency-svc-2m8n9 [747.911805ms]
May 31 19:22:56.723: INFO: Created: latency-svc-5tbs9
May 31 19:22:56.746: INFO: Got endpoints: latency-svc-np7nd [748.108308ms]
May 31 19:22:56.763: INFO: Created: latency-svc-5zf5v
May 31 19:22:56.796: INFO: Got endpoints: latency-svc-n7g25 [745.823671ms]
May 31 19:22:56.813: INFO: Created: latency-svc-f8czm
May 31 19:22:56.848: INFO: Got endpoints: latency-svc-77j8w [751.077557ms]
May 31 19:22:56.858: INFO: Created: latency-svc-clr2b
May 31 19:22:56.897: INFO: Got endpoints: latency-svc-pcn5t [752.590181ms]
May 31 19:22:56.911: INFO: Created: latency-svc-7mp87
May 31 19:22:56.953: INFO: Got endpoints: latency-svc-s87fj [757.091054ms]
May 31 19:22:56.985: INFO: Created: latency-svc-nh72w
May 31 19:22:56.999: INFO: Got endpoints: latency-svc-fvxps [749.476331ms]
May 31 19:22:57.014: INFO: Created: latency-svc-sntqh
May 31 19:22:57.045: INFO: Got endpoints: latency-svc-rbpxn [747.966506ms]
May 31 19:22:57.058: INFO: Created: latency-svc-t72dq
May 31 19:22:57.097: INFO: Got endpoints: latency-svc-zrtmg [749.034023ms]
May 31 19:22:57.125: INFO: Created: latency-svc-ccwqc
May 31 19:22:57.146: INFO: Got endpoints: latency-svc-vcklw [749.499631ms]
May 31 19:22:57.156: INFO: Created: latency-svc-dcdfl
May 31 19:22:57.197: INFO: Got endpoints: latency-svc-ngnxl [750.888053ms]
May 31 19:22:57.221: INFO: Created: latency-svc-cq595
May 31 19:22:57.245: INFO: Got endpoints: latency-svc-rzzj4 [747.889805ms]
May 31 19:22:57.260: INFO: Created: latency-svc-pn8qn
May 31 19:22:57.296: INFO: Got endpoints: latency-svc-sscvb [750.177942ms]
May 31 19:22:57.310: INFO: Created: latency-svc-9k5xq
May 31 19:22:57.351: INFO: Got endpoints: latency-svc-d65lq [755.63403ms]
May 31 19:22:57.380: INFO: Created: latency-svc-k4t9k
May 31 19:22:57.396: INFO: Got endpoints: latency-svc-lbxmp [748.236211ms]
May 31 19:22:57.410: INFO: Created: latency-svc-5z69h
May 31 19:22:57.446: INFO: Got endpoints: latency-svc-5tbs9 [739.666172ms]
May 31 19:22:57.464: INFO: Created: latency-svc-gz5sb
May 31 19:22:57.496: INFO: Got endpoints: latency-svc-5zf5v [749.851637ms]
May 31 19:22:57.521: INFO: Created: latency-svc-bd2tk
May 31 19:22:57.547: INFO: Got endpoints: latency-svc-f8czm [750.347045ms]
May 31 19:22:57.564: INFO: Created: latency-svc-qpwgd
May 31 19:22:57.597: INFO: Got endpoints: latency-svc-clr2b [749.162525ms]
May 31 19:22:57.610: INFO: Created: latency-svc-mljqx
May 31 19:22:57.646: INFO: Got endpoints: latency-svc-7mp87 [748.82292ms]
May 31 19:22:57.665: INFO: Created: latency-svc-f58g7
May 31 19:22:57.696: INFO: Got endpoints: latency-svc-nh72w [741.727106ms]
May 31 19:22:57.708: INFO: Created: latency-svc-tgklh
May 31 19:22:57.746: INFO: Got endpoints: latency-svc-sntqh [746.31478ms]
May 31 19:22:57.756: INFO: Created: latency-svc-29ps8
May 31 19:22:57.796: INFO: Got endpoints: latency-svc-t72dq [751.149658ms]
May 31 19:22:57.807: INFO: Created: latency-svc-5djzh
May 31 19:22:57.847: INFO: Got endpoints: latency-svc-ccwqc [750.738751ms]
May 31 19:22:57.860: INFO: Created: latency-svc-wvc85
May 31 19:22:57.895: INFO: Got endpoints: latency-svc-dcdfl [749.42993ms]
May 31 19:22:57.917: INFO: Created: latency-svc-cjc5g
May 31 19:22:57.947: INFO: Got endpoints: latency-svc-cq595 [750.234743ms]
May 31 19:22:57.959: INFO: Created: latency-svc-wwrdm
May 31 19:22:57.998: INFO: Got endpoints: latency-svc-pn8qn [752.727584ms]
May 31 19:22:58.010: INFO: Created: latency-svc-d895m
May 31 19:22:58.047: INFO: Got endpoints: latency-svc-9k5xq [751.346261ms]
May 31 19:22:58.077: INFO: Created: latency-svc-fbdfr
May 31 19:22:58.106: INFO: Got endpoints: latency-svc-k4t9k [754.38011ms]
May 31 19:22:58.123: INFO: Created: latency-svc-wdcfp
May 31 19:22:58.145: INFO: Got endpoints: latency-svc-5z69h [749.357528ms]
May 31 19:22:58.198: INFO: Got endpoints: latency-svc-gz5sb [751.806869ms]
May 31 19:22:58.246: INFO: Got endpoints: latency-svc-bd2tk [749.663034ms]
May 31 19:22:58.295: INFO: Got endpoints: latency-svc-qpwgd [747.58ms]
May 31 19:22:58.346: INFO: Got endpoints: latency-svc-mljqx [748.349213ms]
May 31 19:22:58.395: INFO: Got endpoints: latency-svc-f58g7 [748.83012ms]
May 31 19:22:58.446: INFO: Got endpoints: latency-svc-tgklh [749.487331ms]
May 31 19:22:58.496: INFO: Got endpoints: latency-svc-29ps8 [749.764535ms]
May 31 19:22:58.545: INFO: Got endpoints: latency-svc-5djzh [749.372229ms]
May 31 19:22:58.596: INFO: Got endpoints: latency-svc-wvc85 [747.894806ms]
May 31 19:22:58.647: INFO: Got endpoints: latency-svc-cjc5g [751.149358ms]
May 31 19:22:58.698: INFO: Got endpoints: latency-svc-wwrdm [750.729951ms]
May 31 19:22:58.749: INFO: Got endpoints: latency-svc-d895m [750.115042ms]
May 31 19:22:58.829: INFO: Got endpoints: latency-svc-fbdfr [781.846955ms]
May 31 19:22:58.846: INFO: Got endpoints: latency-svc-wdcfp [739.925176ms]
May 31 19:22:58.846: INFO: Latencies: [36.558392ms 46.863659ms 75.265018ms 93.034406ms 117.011294ms 117.507302ms 117.972909ms 118.136512ms 118.361116ms 121.387764ms 144.813644ms 147.04418ms 147.537287ms 159.319578ms 163.797251ms 177.379871ms 177.515273ms 180.020913ms 184.025678ms 205.476025ms 222.225997ms 223.315614ms 233.804584ms 242.80713ms 248.224917ms 249.470938ms 252.261183ms 252.685789ms 253.2906ms 254.065812ms 255.094529ms 255.894642ms 260.339513ms 265.313094ms 265.417995ms 265.994205ms 287.870959ms 301.170475ms 310.880231ms 314.695693ms 316.892129ms 319.270567ms 325.203863ms 328.977224ms 333.610699ms 334.27031ms 353.269917ms 396.243113ms 428.689138ms 461.365367ms 489.503322ms 535.597268ms 573.530382ms 612.181308ms 651.995853ms 657.686445ms 675.95744ms 698.350803ms 709.174578ms 712.264228ms 715.652282ms 721.140471ms 725.592043ms 726.840264ms 733.485871ms 735.003496ms 735.497504ms 736.249616ms 737.287733ms 737.454736ms 738.309049ms 738.425451ms 739.58187ms 739.666172ms 739.737372ms 739.925176ms 740.418284ms 740.721388ms 741.152495ms 741.227297ms 741.727106ms 741.867807ms 741.899308ms 743.044727ms 744.035643ms 744.250946ms 744.415248ms 745.203861ms 745.434465ms 745.523966ms 745.823671ms 746.31478ms 746.460081ms 746.715385ms 747.495998ms 747.530799ms 747.58ms 747.641601ms 747.889805ms 747.894806ms 747.911805ms 747.918805ms 747.966506ms 748.033607ms 748.108308ms 748.236211ms 748.295411ms 748.349213ms 748.577916ms 748.678017ms 748.82292ms 748.83012ms 748.958822ms 748.987022ms 749.034023ms 749.043823ms 749.105624ms 749.162525ms 749.242227ms 749.249326ms 749.329628ms 749.357528ms 749.372229ms 749.42993ms 749.44303ms 749.476331ms 749.487331ms 749.499631ms 749.565032ms 749.663034ms 749.764535ms 749.851637ms 749.887336ms 750.115042ms 750.177942ms 750.234743ms 750.276543ms 750.282543ms 750.347045ms 750.452547ms 750.458046ms 750.525547ms 750.729951ms 750.738751ms 750.763252ms 750.787952ms 750.790851ms 750.868452ms 750.888053ms 751.041455ms 751.047456ms 751.061856ms 751.077557ms 751.149358ms 751.149658ms 751.156858ms 751.260959ms 751.29016ms 751.346261ms 751.605565ms 751.654066ms 751.790368ms 751.806869ms 751.898669ms 752.050172ms 752.235675ms 752.55688ms 752.590181ms 752.648882ms 752.727584ms 752.733782ms 753.645498ms 754.300008ms 754.38011ms 754.702515ms 754.928019ms 755.074621ms 755.087021ms 755.63403ms 756.676347ms 756.720147ms 757.091054ms 757.184255ms 757.614962ms 758.228672ms 758.74158ms 759.432491ms 759.9536ms 761.340622ms 761.672928ms 763.253354ms 763.577059ms 765.322087ms 767.606623ms 772.202498ms 779.483216ms 781.846955ms 783.006272ms 786.326626ms 840.226399ms]
May 31 19:22:58.846: INFO: 50 %ile: 747.911805ms
May 31 19:22:58.846: INFO: 90 %ile: 756.720147ms
May 31 19:22:58.846: INFO: 99 %ile: 786.326626ms
May 31 19:22:58.846: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:22:58.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8323" for this suite.
May 31 19:23:24.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:23:24.950: INFO: namespace svc-latency-8323 deletion completed in 26.094850227s

• [SLOW TEST:37.865 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:23:24.951: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-8f7be377-83d9-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 19:23:25.002: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8f7c635d-83d9-11e9-a3c9-f265bfc55a07" in namespace "projected-545" to be "success or failure"
May 31 19:23:25.011: INFO: Pod "pod-projected-configmaps-8f7c635d-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 9.014645ms
May 31 19:23:27.014: INFO: Pod "pod-projected-configmaps-8f7c635d-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011812172s
May 31 19:23:29.017: INFO: Pod "pod-projected-configmaps-8f7c635d-83d9-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015061706s
STEP: Saw pod success
May 31 19:23:29.017: INFO: Pod "pod-projected-configmaps-8f7c635d-83d9-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:23:29.019: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-projected-configmaps-8f7c635d-83d9-11e9-a3c9-f265bfc55a07 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 19:23:29.046: INFO: Waiting for pod pod-projected-configmaps-8f7c635d-83d9-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:23:29.049: INFO: Pod pod-projected-configmaps-8f7c635d-83d9-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:23:29.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-545" for this suite.
May 31 19:23:35.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:23:35.152: INFO: namespace projected-545 deletion completed in 6.097469128s

• [SLOW TEST:10.201 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:23:35.154: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:23:35.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9598bdb0-83d9-11e9-a3c9-f265bfc55a07" in namespace "downward-api-3513" to be "success or failure"
May 31 19:23:35.259: INFO: Pod "downwardapi-volume-9598bdb0-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.726609ms
May 31 19:23:37.262: INFO: Pod "downwardapi-volume-9598bdb0-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010025347s
May 31 19:23:39.267: INFO: Pod "downwardapi-volume-9598bdb0-83d9-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014617507s
STEP: Saw pod success
May 31 19:23:39.267: INFO: Pod "downwardapi-volume-9598bdb0-83d9-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:23:39.273: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downwardapi-volume-9598bdb0-83d9-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:23:39.301: INFO: Waiting for pod downwardapi-volume-9598bdb0-83d9-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:23:39.313: INFO: Pod downwardapi-volume-9598bdb0-83d9-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:23:39.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3513" for this suite.
May 31 19:23:45.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:23:45.447: INFO: namespace downward-api-3513 deletion completed in 6.124679078s

• [SLOW TEST:10.293 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:23:45.447: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-9bb46d9d-83d9-11e9-a3c9-f265bfc55a07
STEP: Creating secret with name s-test-opt-upd-9bb46dda-83d9-11e9-a3c9-f265bfc55a07
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9bb46d9d-83d9-11e9-a3c9-f265bfc55a07
STEP: Updating secret s-test-opt-upd-9bb46dda-83d9-11e9-a3c9-f265bfc55a07
STEP: Creating secret with name s-test-opt-create-9bb46df0-83d9-11e9-a3c9-f265bfc55a07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:23:53.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1671" for this suite.
May 31 19:24:15.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:24:15.694: INFO: namespace projected-1671 deletion completed in 22.08212986s

• [SLOW TEST:30.247 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:24:15.694: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 31 19:24:15.747: INFO: Waiting up to 5m0s for pod "pod-adbaf1af-83d9-11e9-a3c9-f265bfc55a07" in namespace "emptydir-8516" to be "success or failure"
May 31 19:24:15.752: INFO: Pod "pod-adbaf1af-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.428588ms
May 31 19:24:17.758: INFO: Pod "pod-adbaf1af-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011155778s
May 31 19:24:19.761: INFO: Pod "pod-adbaf1af-83d9-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014586431s
STEP: Saw pod success
May 31 19:24:19.761: INFO: Pod "pod-adbaf1af-83d9-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:24:19.764: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-adbaf1af-83d9-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 19:24:19.779: INFO: Waiting for pod pod-adbaf1af-83d9-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:24:19.782: INFO: Pod pod-adbaf1af-83d9-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:24:19.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8516" for this suite.
May 31 19:24:25.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:24:25.870: INFO: namespace emptydir-8516 deletion completed in 6.084599168s

• [SLOW TEST:10.176 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:24:25.871: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-b3ce670a-83d9-11e9-a3c9-f265bfc55a07
May 31 19:24:25.943: INFO: Pod name my-hostname-basic-b3ce670a-83d9-11e9-a3c9-f265bfc55a07: Found 0 pods out of 1
May 31 19:24:30.946: INFO: Pod name my-hostname-basic-b3ce670a-83d9-11e9-a3c9-f265bfc55a07: Found 1 pods out of 1
May 31 19:24:30.946: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b3ce670a-83d9-11e9-a3c9-f265bfc55a07" are running
May 31 19:24:30.948: INFO: Pod "my-hostname-basic-b3ce670a-83d9-11e9-a3c9-f265bfc55a07-h6zdj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 19:24:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 19:24:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 19:24:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-31 19:24:25 +0000 UTC Reason: Message:}])
May 31 19:24:30.948: INFO: Trying to dial the pod
May 31 19:24:35.957: INFO: Controller my-hostname-basic-b3ce670a-83d9-11e9-a3c9-f265bfc55a07: Got expected result from replica 1 [my-hostname-basic-b3ce670a-83d9-11e9-a3c9-f265bfc55a07-h6zdj]: "my-hostname-basic-b3ce670a-83d9-11e9-a3c9-f265bfc55a07-h6zdj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:24:35.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5136" for this suite.
May 31 19:24:41.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:24:42.054: INFO: namespace replication-controller-5136 deletion completed in 6.093541928s

• [SLOW TEST:16.183 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:24:42.055: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-bd71d7c1-83d9-11e9-a3c9-f265bfc55a07
STEP: Creating secret with name s-test-opt-upd-bd71d7f8-83d9-11e9-a3c9-f265bfc55a07
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-bd71d7c1-83d9-11e9-a3c9-f265bfc55a07
STEP: Updating secret s-test-opt-upd-bd71d7f8-83d9-11e9-a3c9-f265bfc55a07
STEP: Creating secret with name s-test-opt-create-bd71d80e-83d9-11e9-a3c9-f265bfc55a07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:24:50.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5451" for this suite.
May 31 19:25:12.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:25:12.276: INFO: namespace secrets-5451 deletion completed in 22.08118573s

• [SLOW TEST:30.221 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:25:12.276: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-cf74ed01-83d9-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 19:25:12.334: INFO: Waiting up to 5m0s for pod "pod-secrets-cf75757a-83d9-11e9-a3c9-f265bfc55a07" in namespace "secrets-7857" to be "success or failure"
May 31 19:25:12.337: INFO: Pod "pod-secrets-cf75757a-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.121351ms
May 31 19:25:14.340: INFO: Pod "pod-secrets-cf75757a-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006374617s
May 31 19:25:16.344: INFO: Pod "pod-secrets-cf75757a-83d9-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009782987s
STEP: Saw pod success
May 31 19:25:16.344: INFO: Pod "pod-secrets-cf75757a-83d9-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:25:16.346: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-secrets-cf75757a-83d9-11e9-a3c9-f265bfc55a07 container secret-volume-test: <nil>
STEP: delete the pod
May 31 19:25:16.360: INFO: Waiting for pod pod-secrets-cf75757a-83d9-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:25:16.365: INFO: Pod pod-secrets-cf75757a-83d9-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:25:16.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7857" for this suite.
May 31 19:25:22.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:25:22.463: INFO: namespace secrets-7857 deletion completed in 6.095049787s

• [SLOW TEST:10.187 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:25:22.463: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:25:22.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3083" for this suite.
May 31 19:25:44.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:25:44.618: INFO: namespace pods-3083 deletion completed in 22.087704432s

• [SLOW TEST:22.155 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:25:44.621: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
May 31 19:25:44.665: INFO: Waiting up to 5m0s for pod "var-expansion-e2bbaf15-83d9-11e9-a3c9-f265bfc55a07" in namespace "var-expansion-582" to be "success or failure"
May 31 19:25:44.671: INFO: Pod "var-expansion-e2bbaf15-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.887396ms
May 31 19:25:46.674: INFO: Pod "var-expansion-e2bbaf15-83d9-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009276773s
May 31 19:25:48.678: INFO: Pod "var-expansion-e2bbaf15-83d9-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012821353s
STEP: Saw pod success
May 31 19:25:48.678: INFO: Pod "var-expansion-e2bbaf15-83d9-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:25:48.680: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod var-expansion-e2bbaf15-83d9-11e9-a3c9-f265bfc55a07 container dapi-container: <nil>
STEP: delete the pod
May 31 19:25:48.695: INFO: Waiting for pod var-expansion-e2bbaf15-83d9-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:25:48.697: INFO: Pod var-expansion-e2bbaf15-83d9-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:25:48.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-582" for this suite.
May 31 19:25:54.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:25:54.797: INFO: namespace var-expansion-582 deletion completed in 6.09676334s

• [SLOW TEST:10.176 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:25:54.797: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:25:54.842: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:25:58.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4714" for this suite.
May 31 19:26:36.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:26:37.139: INFO: namespace pods-4714 deletion completed in 38.152469549s

• [SLOW TEST:42.342 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:26:37.146: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:26:37.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-020d94ba-83da-11e9-a3c9-f265bfc55a07" in namespace "projected-6903" to be "success or failure"
May 31 19:26:37.297: INFO: Pod "downwardapi-volume-020d94ba-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.58149ms
May 31 19:26:39.302: INFO: Pod "downwardapi-volume-020d94ba-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010639508s
May 31 19:26:41.306: INFO: Pod "downwardapi-volume-020d94ba-83da-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014262302s
STEP: Saw pod success
May 31 19:26:41.306: INFO: Pod "downwardapi-volume-020d94ba-83da-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:26:41.309: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downwardapi-volume-020d94ba-83da-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:26:41.329: INFO: Waiting for pod downwardapi-volume-020d94ba-83da-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:26:41.333: INFO: Pod downwardapi-volume-020d94ba-83da-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:26:41.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6903" for this suite.
May 31 19:26:47.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:26:47.425: INFO: namespace projected-6903 deletion completed in 6.088983253s

• [SLOW TEST:10.280 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:26:47.426: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-082b82c0-83da-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 19:26:47.478: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-082c0a04-83da-11e9-a3c9-f265bfc55a07" in namespace "projected-6149" to be "success or failure"
May 31 19:26:47.484: INFO: Pod "pod-projected-configmaps-082c0a04-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.723893ms
May 31 19:26:49.541: INFO: Pod "pod-projected-configmaps-082c0a04-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063173462s
May 31 19:26:51.545: INFO: Pod "pod-projected-configmaps-082c0a04-83da-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066570155s
STEP: Saw pod success
May 31 19:26:51.545: INFO: Pod "pod-projected-configmaps-082c0a04-83da-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:26:51.547: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-projected-configmaps-082c0a04-83da-11e9-a3c9-f265bfc55a07 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 19:26:51.567: INFO: Waiting for pod pod-projected-configmaps-082c0a04-83da-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:26:51.569: INFO: Pod pod-projected-configmaps-082c0a04-83da-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:26:51.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6149" for this suite.
May 31 19:26:57.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:26:57.657: INFO: namespace projected-6149 deletion completed in 6.084144281s

• [SLOW TEST:10.231 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:26:57.657: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 19:26:57.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7596'
May 31 19:26:57.823: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 31 19:26:57.823: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
May 31 19:26:57.828: INFO: scanned /root for discovery docs: <nil>
May 31 19:26:57.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7596'
May 31 19:27:13.646: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 31 19:27:13.646: INFO: stdout: "Created e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59\nScaling up e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 31 19:27:13.646: INFO: stdout: "Created e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59\nScaling up e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 31 19:27:13.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7596'
May 31 19:27:13.733: INFO: stderr: ""
May 31 19:27:13.733: INFO: stdout: "e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59-mn9bd "
May 31 19:27:13.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59-mn9bd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7596'
May 31 19:27:13.810: INFO: stderr: ""
May 31 19:27:13.810: INFO: stdout: "true"
May 31 19:27:13.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59-mn9bd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7596'
May 31 19:27:13.888: INFO: stderr: ""
May 31 19:27:13.888: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May 31 19:27:13.888: INFO: e2e-test-nginx-rc-1471753eb8b06a11c5ded7282366fa59-mn9bd is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
May 31 19:27:13.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete rc e2e-test-nginx-rc --namespace=kubectl-7596'
May 31 19:27:13.973: INFO: stderr: ""
May 31 19:27:13.973: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:27:13.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7596" for this suite.
May 31 19:27:35.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:27:36.056: INFO: namespace kubectl-7596 deletion completed in 22.075585126s

• [SLOW TEST:38.399 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:27:36.057: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-2527460c-83da-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 19:27:36.102: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2527a9bc-83da-11e9-a3c9-f265bfc55a07" in namespace "projected-997" to be "success or failure"
May 31 19:27:36.106: INFO: Pod "pod-projected-configmaps-2527a9bc-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029949ms
May 31 19:27:38.109: INFO: Pod "pod-projected-configmaps-2527a9bc-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006511354s
May 31 19:27:40.112: INFO: Pod "pod-projected-configmaps-2527a9bc-83da-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00943895s
STEP: Saw pod success
May 31 19:27:40.112: INFO: Pod "pod-projected-configmaps-2527a9bc-83da-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:27:40.114: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-projected-configmaps-2527a9bc-83da-11e9-a3c9-f265bfc55a07 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 19:27:40.132: INFO: Waiting for pod pod-projected-configmaps-2527a9bc-83da-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:27:40.136: INFO: Pod pod-projected-configmaps-2527a9bc-83da-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:27:40.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-997" for this suite.
May 31 19:27:46.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:27:46.225: INFO: namespace projected-997 deletion completed in 6.086357349s

• [SLOW TEST:10.168 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:27:46.227: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:27:46.280: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b383a09-83da-11e9-a3c9-f265bfc55a07" in namespace "projected-2923" to be "success or failure"
May 31 19:27:46.283: INFO: Pod "downwardapi-volume-2b383a09-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.537341ms
May 31 19:27:48.286: INFO: Pod "downwardapi-volume-2b383a09-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005439739s
May 31 19:27:50.289: INFO: Pod "downwardapi-volume-2b383a09-83da-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008664241s
STEP: Saw pod success
May 31 19:27:50.289: INFO: Pod "downwardapi-volume-2b383a09-83da-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:27:50.291: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downwardapi-volume-2b383a09-83da-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:27:50.307: INFO: Waiting for pod downwardapi-volume-2b383a09-83da-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:27:50.309: INFO: Pod downwardapi-volume-2b383a09-83da-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:27:50.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2923" for this suite.
May 31 19:27:56.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:27:56.420: INFO: namespace projected-2923 deletion completed in 6.101044793s

• [SLOW TEST:10.193 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:27:56.420: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0531 19:28:06.535484      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 19:28:06.535: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:28:06.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9294" for this suite.
May 31 19:28:12.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:28:12.628: INFO: namespace gc-9294 deletion completed in 6.090791937s

• [SLOW TEST:16.208 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:28:12.629: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4189
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4189
STEP: Creating statefulset with conflicting port in namespace statefulset-4189
STEP: Waiting until pod test-pod will start running in namespace statefulset-4189
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4189
May 31 19:28:16.729: INFO: Observed stateful pod in namespace: statefulset-4189, name: ss-0, uid: 3d20cacc-83da-11e9-a607-001dd80c001e, status phase: Pending. Waiting for statefulset controller to delete.
May 31 19:28:17.310: INFO: Observed stateful pod in namespace: statefulset-4189, name: ss-0, uid: 3d20cacc-83da-11e9-a607-001dd80c001e, status phase: Failed. Waiting for statefulset controller to delete.
May 31 19:28:17.313: INFO: Observed stateful pod in namespace: statefulset-4189, name: ss-0, uid: 3d20cacc-83da-11e9-a607-001dd80c001e, status phase: Failed. Waiting for statefulset controller to delete.
May 31 19:28:17.316: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4189
STEP: Removing pod with conflicting port in namespace statefulset-4189
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4189 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 31 19:28:21.341: INFO: Deleting all statefulset in ns statefulset-4189
May 31 19:28:21.345: INFO: Scaling statefulset ss to 0
May 31 19:28:31.360: INFO: Waiting for statefulset status.replicas updated to 0
May 31 19:28:31.363: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:28:31.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4189" for this suite.
May 31 19:28:37.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:28:37.508: INFO: namespace statefulset-4189 deletion completed in 6.129100173s

• [SLOW TEST:24.880 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:28:37.511: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-hfrb
STEP: Creating a pod to test atomic-volume-subpath
May 31 19:28:37.595: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hfrb" in namespace "subpath-8982" to be "success or failure"
May 31 19:28:37.624: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.810135ms
May 31 19:28:39.627: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030117549s
May 31 19:28:41.630: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 4.033465464s
May 31 19:28:43.633: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 6.036209169s
May 31 19:28:45.636: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 8.039200778s
May 31 19:28:47.639: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 10.041915984s
May 31 19:28:49.643: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 12.045727108s
May 31 19:28:51.646: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 14.049169626s
May 31 19:28:53.649: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 16.052447141s
May 31 19:28:55.652: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 18.055183549s
May 31 19:28:57.655: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 20.057740553s
May 31 19:28:59.658: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Running", Reason="", readiness=true. Elapsed: 22.061240274s
May 31 19:29:01.661: INFO: Pod "pod-subpath-test-projected-hfrb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.06449079s
STEP: Saw pod success
May 31 19:29:01.661: INFO: Pod "pod-subpath-test-projected-hfrb" satisfied condition "success or failure"
May 31 19:29:01.664: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-subpath-test-projected-hfrb container test-container-subpath-projected-hfrb: <nil>
STEP: delete the pod
May 31 19:29:01.696: INFO: Waiting for pod pod-subpath-test-projected-hfrb to disappear
May 31 19:29:01.701: INFO: Pod pod-subpath-test-projected-hfrb no longer exists
STEP: Deleting pod pod-subpath-test-projected-hfrb
May 31 19:29:01.701: INFO: Deleting pod "pod-subpath-test-projected-hfrb" in namespace "subpath-8982"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:29:01.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8982" for this suite.
May 31 19:29:07.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:29:07.793: INFO: namespace subpath-8982 deletion completed in 6.087663017s

• [SLOW TEST:30.282 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:29:07.793: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
May 31 19:29:07.840: INFO: Waiting up to 5m0s for pod "var-expansion-5bd55d33-83da-11e9-a3c9-f265bfc55a07" in namespace "var-expansion-4072" to be "success or failure"
May 31 19:29:07.846: INFO: Pod "var-expansion-5bd55d33-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.216701ms
May 31 19:29:09.850: INFO: Pod "var-expansion-5bd55d33-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009533521s
May 31 19:29:11.853: INFO: Pod "var-expansion-5bd55d33-83da-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013153645s
STEP: Saw pod success
May 31 19:29:11.854: INFO: Pod "var-expansion-5bd55d33-83da-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:29:11.856: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod var-expansion-5bd55d33-83da-11e9-a3c9-f265bfc55a07 container dapi-container: <nil>
STEP: delete the pod
May 31 19:29:11.874: INFO: Waiting for pod var-expansion-5bd55d33-83da-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:29:11.876: INFO: Pod var-expansion-5bd55d33-83da-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:29:11.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4072" for this suite.
May 31 19:29:17.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:29:17.958: INFO: namespace var-expansion-4072 deletion completed in 6.079226185s

• [SLOW TEST:10.165 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:29:17.958: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-61e4f397-83da-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 19:29:18.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-61e5732d-83da-11e9-a3c9-f265bfc55a07" in namespace "configmap-1820" to be "success or failure"
May 31 19:29:18.019: INFO: Pod "pod-configmaps-61e5732d-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821027ms
May 31 19:29:20.023: INFO: Pod "pod-configmaps-61e5732d-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011483154s
May 31 19:29:22.027: INFO: Pod "pod-configmaps-61e5732d-83da-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01510008s
STEP: Saw pod success
May 31 19:29:22.027: INFO: Pod "pod-configmaps-61e5732d-83da-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:29:22.029: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-configmaps-61e5732d-83da-11e9-a3c9-f265bfc55a07 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 19:29:22.060: INFO: Waiting for pod pod-configmaps-61e5732d-83da-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:29:22.063: INFO: Pod pod-configmaps-61e5732d-83da-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:29:22.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1820" for this suite.
May 31 19:29:28.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:29:28.150: INFO: namespace configmap-1820 deletion completed in 6.083356958s

• [SLOW TEST:10.192 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:29:28.150: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-x29z
STEP: Creating a pod to test atomic-volume-subpath
May 31 19:29:28.204: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-x29z" in namespace "subpath-2363" to be "success or failure"
May 31 19:29:28.207: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Pending", Reason="", readiness=false. Elapsed: 3.609659ms
May 31 19:29:30.212: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007946098s
May 31 19:29:32.215: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 4.01121112s
May 31 19:29:34.218: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 6.01427594s
May 31 19:29:36.221: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 8.017277758s
May 31 19:29:38.225: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 10.021442496s
May 31 19:29:40.228: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 12.024487716s
May 31 19:29:42.231: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 14.027700239s
May 31 19:29:44.235: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 16.03141297s
May 31 19:29:46.238: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 18.034749196s
May 31 19:29:48.243: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 20.039124238s
May 31 19:29:50.246: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Running", Reason="", readiness=true. Elapsed: 22.042301462s
May 31 19:29:52.249: INFO: Pod "pod-subpath-test-secret-x29z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.045476786s
STEP: Saw pod success
May 31 19:29:52.249: INFO: Pod "pod-subpath-test-secret-x29z" satisfied condition "success or failure"
May 31 19:29:52.252: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-subpath-test-secret-x29z container test-container-subpath-secret-x29z: <nil>
STEP: delete the pod
May 31 19:29:52.271: INFO: Waiting for pod pod-subpath-test-secret-x29z to disappear
May 31 19:29:52.274: INFO: Pod pod-subpath-test-secret-x29z no longer exists
STEP: Deleting pod pod-subpath-test-secret-x29z
May 31 19:29:52.274: INFO: Deleting pod "pod-subpath-test-secret-x29z" in namespace "subpath-2363"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:29:52.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2363" for this suite.
May 31 19:29:58.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:29:58.421: INFO: namespace subpath-2363 deletion completed in 6.142531633s

• [SLOW TEST:30.271 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:29:58.423: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-7a02a0ed-83da-11e9-a3c9-f265bfc55a07
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:29:58.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-960" for this suite.
May 31 19:30:04.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:30:04.554: INFO: namespace configmap-960 deletion completed in 6.089018967s

• [SLOW TEST:6.132 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:30:04.556: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
May 31 19:30:09.127: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8492 pod-service-account-7df89976-83da-11e9-a3c9-f265bfc55a07 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 31 19:30:09.410: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8492 pod-service-account-7df89976-83da-11e9-a3c9-f265bfc55a07 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 31 19:30:09.607: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8492 pod-service-account-7df89976-83da-11e9-a3c9-f265bfc55a07 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:30:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8492" for this suite.
May 31 19:30:15.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:30:15.903: INFO: namespace svcaccounts-8492 deletion completed in 6.087306245s

• [SLOW TEST:11.347 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:30:15.903: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
May 31 19:30:15.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 create -f - --namespace=kubectl-8433'
May 31 19:30:16.210: INFO: stderr: ""
May 31 19:30:16.210: INFO: stdout: "pod/pause created\n"
May 31 19:30:16.210: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 31 19:30:16.210: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8433" to be "running and ready"
May 31 19:30:16.216: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209101ms
May 31 19:30:18.219: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008783719s
May 31 19:30:20.234: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.024189446s
May 31 19:30:20.234: INFO: Pod "pause" satisfied condition "running and ready"
May 31 19:30:20.234: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
May 31 19:30:20.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 label pods pause testing-label=testing-label-value --namespace=kubectl-8433'
May 31 19:30:20.320: INFO: stderr: ""
May 31 19:30:20.320: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 31 19:30:20.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pod pause -L testing-label --namespace=kubectl-8433'
May 31 19:30:20.405: INFO: stderr: ""
May 31 19:30:20.405: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 31 19:30:20.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 label pods pause testing-label- --namespace=kubectl-8433'
May 31 19:30:20.502: INFO: stderr: ""
May 31 19:30:20.502: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 31 19:30:20.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pod pause -L testing-label --namespace=kubectl-8433'
May 31 19:30:20.587: INFO: stderr: ""
May 31 19:30:20.587: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
May 31 19:30:20.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete --grace-period=0 --force -f - --namespace=kubectl-8433'
May 31 19:30:20.675: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 19:30:20.675: INFO: stdout: "pod \"pause\" force deleted\n"
May 31 19:30:20.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get rc,svc -l name=pause --no-headers --namespace=kubectl-8433'
May 31 19:30:20.774: INFO: stderr: "No resources found.\n"
May 31 19:30:20.774: INFO: stdout: ""
May 31 19:30:20.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 get pods -l name=pause --namespace=kubectl-8433 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 19:30:20.861: INFO: stderr: ""
May 31 19:30:20.861: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:30:20.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8433" for this suite.
May 31 19:30:26.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:30:26.956: INFO: namespace kubectl-8433 deletion completed in 6.091996026s

• [SLOW TEST:11.052 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:30:26.956: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:30:27.006: INFO: (0) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.904379ms)
May 31 19:30:27.010: INFO: (1) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.756961ms)
May 31 19:30:27.013: INFO: (2) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.422156ms)
May 31 19:30:27.017: INFO: (3) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.545658ms)
May 31 19:30:27.020: INFO: (4) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.148652ms)
May 31 19:30:27.023: INFO: (5) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.208552ms)
May 31 19:30:27.026: INFO: (6) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.376655ms)
May 31 19:30:27.030: INFO: (7) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.633559ms)
May 31 19:30:27.033: INFO: (8) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.022749ms)
May 31 19:30:27.037: INFO: (9) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.387055ms)
May 31 19:30:27.040: INFO: (10) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.280953ms)
May 31 19:30:27.043: INFO: (11) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.04565ms)
May 31 19:30:27.049: INFO: (12) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 5.992498ms)
May 31 19:30:27.052: INFO: (13) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.145151ms)
May 31 19:30:27.057: INFO: (14) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.419472ms)
May 31 19:30:27.061: INFO: (15) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 4.129867ms)
May 31 19:30:27.067: INFO: (16) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 6.1564ms)
May 31 19:30:27.078: INFO: (17) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 10.375768ms)
May 31 19:30:27.081: INFO: (18) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.723161ms)
May 31 19:30:27.085: INFO: (19) /api/v1/nodes/k8s-linuxpool-33506892-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="audit/">au... (200; 3.198952ms)
[AfterEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:30:27.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1435" for this suite.
May 31 19:30:33.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:30:33.201: INFO: namespace proxy-1435 deletion completed in 6.111292542s

• [SLOW TEST:6.245 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:30:33.202: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:30:33.249: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ebdbc5e-83da-11e9-a3c9-f265bfc55a07" in namespace "downward-api-4291" to be "success or failure"
May 31 19:30:33.255: INFO: Pod "downwardapi-volume-8ebdbc5e-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.320402ms
May 31 19:30:35.259: INFO: Pod "downwardapi-volume-8ebdbc5e-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009803438s
May 31 19:30:37.265: INFO: Pod "downwardapi-volume-8ebdbc5e-83da-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015763714s
STEP: Saw pod success
May 31 19:30:37.265: INFO: Pod "downwardapi-volume-8ebdbc5e-83da-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:30:37.269: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downwardapi-volume-8ebdbc5e-83da-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:30:37.287: INFO: Waiting for pod downwardapi-volume-8ebdbc5e-83da-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:30:37.295: INFO: Pod downwardapi-volume-8ebdbc5e-83da-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:30:37.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4291" for this suite.
May 31 19:30:43.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:30:43.405: INFO: namespace downward-api-4291 deletion completed in 6.100822177s

• [SLOW TEST:10.204 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:30:43.406: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
May 31 19:30:43.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 --namespace=kubectl-9654 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 31 19:30:46.459: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 31 19:30:46.459: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:30:48.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9654" for this suite.
May 31 19:30:54.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:30:54.549: INFO: namespace kubectl-9654 deletion completed in 6.081266464s

• [SLOW TEST:11.143 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:30:54.549: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0531 19:31:04.611754      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 19:31:04.611: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:31:04.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3595" for this suite.
May 31 19:31:10.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:31:10.696: INFO: namespace gc-3595 deletion completed in 6.082218286s

• [SLOW TEST:16.147 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:31:10.698: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-a5179bbd-83da-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 19:31:10.752: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a51829b9-83da-11e9-a3c9-f265bfc55a07" in namespace "projected-6461" to be "success or failure"
May 31 19:31:10.759: INFO: Pod "pod-projected-secrets-a51829b9-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 7.764027ms
May 31 19:31:12.763: INFO: Pod "pod-projected-secrets-a51829b9-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010995663s
May 31 19:31:14.766: INFO: Pod "pod-projected-secrets-a51829b9-83da-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014065298s
STEP: Saw pod success
May 31 19:31:14.766: INFO: Pod "pod-projected-secrets-a51829b9-83da-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:31:14.768: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-projected-secrets-a51829b9-83da-11e9-a3c9-f265bfc55a07 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 19:31:14.786: INFO: Waiting for pod pod-projected-secrets-a51829b9-83da-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:31:14.788: INFO: Pod pod-projected-secrets-a51829b9-83da-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:31:14.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6461" for this suite.
May 31 19:31:20.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:31:20.878: INFO: namespace projected-6461 deletion completed in 6.086142854s

• [SLOW TEST:10.180 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:31:20.879: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 31 19:31:28.979: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:28.982: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:30.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:30.985: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:32.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:32.985: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:34.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:34.985: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:36.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:36.986: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:38.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:38.986: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:40.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:40.986: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:42.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:42.986: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:44.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:44.985: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:46.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:46.985: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:48.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:48.985: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:50.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:50.985: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:52.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:52.985: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:55.014: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:55.022: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:56.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:56.985: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 19:31:58.982: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 19:31:58.985: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:31:58.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3533" for this suite.
May 31 19:32:21.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:32:21.091: INFO: namespace container-lifecycle-hook-3533 deletion completed in 22.102356872s

• [SLOW TEST:60.212 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:32:21.092: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-cf0ca0cb-83da-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 19:32:21.142: INFO: Waiting up to 5m0s for pod "pod-secrets-cf0d1b38-83da-11e9-a3c9-f265bfc55a07" in namespace "secrets-752" to be "success or failure"
May 31 19:32:21.147: INFO: Pod "pod-secrets-cf0d1b38-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.021581ms
May 31 19:32:23.152: INFO: Pod "pod-secrets-cf0d1b38-83da-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009731351s
May 31 19:32:25.155: INFO: Pod "pod-secrets-cf0d1b38-83da-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012758993s
STEP: Saw pod success
May 31 19:32:25.155: INFO: Pod "pod-secrets-cf0d1b38-83da-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:32:25.158: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-secrets-cf0d1b38-83da-11e9-a3c9-f265bfc55a07 container secret-volume-test: <nil>
STEP: delete the pod
May 31 19:32:25.178: INFO: Waiting for pod pod-secrets-cf0d1b38-83da-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:32:25.180: INFO: Pod pod-secrets-cf0d1b38-83da-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:32:25.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-752" for this suite.
May 31 19:32:31.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:32:31.333: INFO: namespace secrets-752 deletion completed in 6.147110472s

• [SLOW TEST:10.241 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:32:31.334: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 31 19:32:31.409: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-a,UID:d52c2946-83da-11e9-a607-001dd80c001e,ResourceVersion:19740,Generation:0,CreationTimestamp:2019-05-31 19:32:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 19:32:31.409: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-a,UID:d52c2946-83da-11e9-a607-001dd80c001e,ResourceVersion:19740,Generation:0,CreationTimestamp:2019-05-31 19:32:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 31 19:32:41.415: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-a,UID:d52c2946-83da-11e9-a607-001dd80c001e,ResourceVersion:19760,Generation:0,CreationTimestamp:2019-05-31 19:32:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 31 19:32:41.416: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-a,UID:d52c2946-83da-11e9-a607-001dd80c001e,ResourceVersion:19760,Generation:0,CreationTimestamp:2019-05-31 19:32:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 31 19:32:51.421: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-a,UID:d52c2946-83da-11e9-a607-001dd80c001e,ResourceVersion:19776,Generation:0,CreationTimestamp:2019-05-31 19:32:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 19:32:51.421: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-a,UID:d52c2946-83da-11e9-a607-001dd80c001e,ResourceVersion:19776,Generation:0,CreationTimestamp:2019-05-31 19:32:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 31 19:33:01.427: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-a,UID:d52c2946-83da-11e9-a607-001dd80c001e,ResourceVersion:19792,Generation:0,CreationTimestamp:2019-05-31 19:32:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 31 19:33:01.427: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-a,UID:d52c2946-83da-11e9-a607-001dd80c001e,ResourceVersion:19792,Generation:0,CreationTimestamp:2019-05-31 19:32:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 31 19:33:11.433: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-b,UID:ed072bed-83da-11e9-a607-001dd80c001e,ResourceVersion:19809,Generation:0,CreationTimestamp:2019-05-31 19:33:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 19:33:11.433: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-b,UID:ed072bed-83da-11e9-a607-001dd80c001e,ResourceVersion:19809,Generation:0,CreationTimestamp:2019-05-31 19:33:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 31 19:33:21.442: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-b,UID:ed072bed-83da-11e9-a607-001dd80c001e,ResourceVersion:19826,Generation:0,CreationTimestamp:2019-05-31 19:33:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 31 19:33:21.442: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9514,SelfLink:/api/v1/namespaces/watch-9514/configmaps/e2e-watch-test-configmap-b,UID:ed072bed-83da-11e9-a607-001dd80c001e,ResourceVersion:19826,Generation:0,CreationTimestamp:2019-05-31 19:33:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:33:31.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9514" for this suite.
May 31 19:33:37.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:33:37.573: INFO: namespace watch-9514 deletion completed in 6.125832448s

• [SLOW TEST:66.239 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:33:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 31 19:33:37.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1473'
May 31 19:33:38.657: INFO: stderr: ""
May 31 19:33:38.657: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
May 31 19:33:38.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-130611839 delete pods e2e-test-nginx-pod --namespace=kubectl-1473'
May 31 19:33:40.355: INFO: stderr: ""
May 31 19:33:40.355: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:33:40.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1473" for this suite.
May 31 19:33:46.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:33:46.478: INFO: namespace kubectl-1473 deletion completed in 6.11962205s

• [SLOW TEST:8.903 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:33:46.482: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3833
May 31 19:33:50.551: INFO: Started pod liveness-http in namespace container-probe-3833
STEP: checking the pod's current state and verifying that restartCount is present
May 31 19:33:50.554: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:37:51.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3833" for this suite.
May 31 19:37:57.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:37:57.247: INFO: namespace container-probe-3833 deletion completed in 6.092759774s

• [SLOW TEST:250.765 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:37:57.249: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
May 31 19:37:57.373: INFO: Waiting up to 5m0s for pod "client-containers-976a9b38-83db-11e9-a3c9-f265bfc55a07" in namespace "containers-5899" to be "success or failure"
May 31 19:37:57.376: INFO: Pod "client-containers-976a9b38-83db-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.444539ms
May 31 19:37:59.380: INFO: Pod "client-containers-976a9b38-83db-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006829633s
May 31 19:38:01.383: INFO: Pod "client-containers-976a9b38-83db-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010104809s
STEP: Saw pod success
May 31 19:38:01.384: INFO: Pod "client-containers-976a9b38-83db-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:38:01.386: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod client-containers-976a9b38-83db-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 19:38:01.407: INFO: Waiting for pod client-containers-976a9b38-83db-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:38:01.409: INFO: Pod client-containers-976a9b38-83db-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:38:01.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5899" for this suite.
May 31 19:38:07.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:38:07.540: INFO: namespace containers-5899 deletion completed in 6.127260637s

• [SLOW TEST:10.291 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:38:07.541: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:38:07.596: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 31 19:38:12.599: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 31 19:38:12.599: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 31 19:38:14.602: INFO: Creating deployment "test-rollover-deployment"
May 31 19:38:14.609: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 31 19:38:16.615: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 31 19:38:16.624: INFO: Ensure that both replica sets have 1 created replica
May 31 19:38:16.629: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 31 19:38:16.634: INFO: Updating deployment test-rollover-deployment
May 31 19:38:16.634: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 31 19:38:18.641: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 31 19:38:18.645: INFO: Make sure deployment "test-rollover-deployment" is complete
May 31 19:38:18.649: INFO: all replica sets need to contain the pod-template-hash label
May 31 19:38:18.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928296, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 19:38:20.656: INFO: all replica sets need to contain the pod-template-hash label
May 31 19:38:20.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928299, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 19:38:22.655: INFO: all replica sets need to contain the pod-template-hash label
May 31 19:38:22.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928299, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 19:38:24.655: INFO: all replica sets need to contain the pod-template-hash label
May 31 19:38:24.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928299, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 19:38:26.655: INFO: all replica sets need to contain the pod-template-hash label
May 31 19:38:26.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928299, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 19:38:28.656: INFO: all replica sets need to contain the pod-template-hash label
May 31 19:38:28.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928299, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694928294, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 19:38:30.656: INFO: 
May 31 19:38:30.656: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 19:38:30.662: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-3964,SelfLink:/apis/apps/v1/namespaces/deployment-3964/deployments/test-rollover-deployment,UID:a1bbdbf7-83db-11e9-a607-001dd80c001e,ResourceVersion:20478,Generation:2,CreationTimestamp:2019-05-31 19:38:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-31 19:38:14 +0000 UTC 2019-05-31 19:38:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-31 19:38:29 +0000 UTC 2019-05-31 19:38:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 31 19:38:30.665: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-3964,SelfLink:/apis/apps/v1/namespaces/deployment-3964/replicasets/test-rollover-deployment-766b4d6c9d,UID:a2f20387-83db-11e9-a607-001dd80c001e,ResourceVersion:20467,Generation:2,CreationTimestamp:2019-05-31 19:38:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a1bbdbf7-83db-11e9-a607-001dd80c001e 0xc002a10097 0xc002a10098}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 31 19:38:30.665: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 31 19:38:30.665: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-3964,SelfLink:/apis/apps/v1/namespaces/deployment-3964/replicasets/test-rollover-controller,UID:9d8dab99-83db-11e9-a607-001dd80c001e,ResourceVersion:20475,Generation:2,CreationTimestamp:2019-05-31 19:38:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a1bbdbf7-83db-11e9-a607-001dd80c001e 0xc0020e9e17 0xc0020e9e18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 19:38:30.666: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-3964,SelfLink:/apis/apps/v1/namespaces/deployment-3964/replicasets/test-rollover-deployment-6455657675,UID:a1bdd81b-83db-11e9-a607-001dd80c001e,ResourceVersion:20437,Generation:2,CreationTimestamp:2019-05-31 19:38:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a1bbdbf7-83db-11e9-a607-001dd80c001e 0xc0020e9f87 0xc0020e9f88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 19:38:30.668: INFO: Pod "test-rollover-deployment-766b4d6c9d-nz6zg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-nz6zg,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-3964,SelfLink:/api/v1/namespaces/deployment-3964/pods/test-rollover-deployment-766b4d6c9d-nz6zg,UID:a2f676e7-83db-11e9-a607-001dd80c001e,ResourceVersion:20449,Generation:0,CreationTimestamp:2019-05-31 19:38:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d a2f20387-83db-11e9-a607-001dd80c001e 0xc0022c34b7 0xc0022c34b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zznjq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zznjq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-zznjq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022c3520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022c3540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:38:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:38:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:38:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:38:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:10.244.1.131,StartTime:2019-05-31 19:38:16 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-31 19:38:18 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://3a3c86f95d96185883fcd5874d3477243a6bdb72eeacd64a24a540b242a23f9a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:38:30.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3964" for this suite.
May 31 19:38:36.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:38:36.782: INFO: namespace deployment-3964 deletion completed in 6.111699089s

• [SLOW TEST:29.242 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:38:36.788: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:38:40.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1448" for this suite.
May 31 19:39:18.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:39:19.018: INFO: namespace kubelet-test-1448 deletion completed in 38.092303392s

• [SLOW TEST:42.230 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:39:19.019: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 31 19:39:19.075: INFO: Waiting up to 5m0s for pod "pod-c8284e03-83db-11e9-a3c9-f265bfc55a07" in namespace "emptydir-6049" to be "success or failure"
May 31 19:39:19.080: INFO: Pod "pod-c8284e03-83db-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181568ms
May 31 19:39:21.083: INFO: Pod "pod-c8284e03-83db-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007288046s
May 31 19:39:23.086: INFO: Pod "pod-c8284e03-83db-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010662528s
STEP: Saw pod success
May 31 19:39:23.086: INFO: Pod "pod-c8284e03-83db-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:39:23.089: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod pod-c8284e03-83db-11e9-a3c9-f265bfc55a07 container test-container: <nil>
STEP: delete the pod
May 31 19:39:23.108: INFO: Waiting for pod pod-c8284e03-83db-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:39:23.110: INFO: Pod pod-c8284e03-83db-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:39:23.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6049" for this suite.
May 31 19:39:29.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:39:29.200: INFO: namespace emptydir-6049 deletion completed in 6.0871878s

• [SLOW TEST:10.181 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:39:29.200: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6085
May 31 19:39:33.260: INFO: Started pod liveness-http in namespace container-probe-6085
STEP: checking the pod's current state and verifying that restartCount is present
May 31 19:39:33.263: INFO: Initial restart count of pod liveness-http is 0
May 31 19:39:51.293: INFO: Restart count of pod container-probe-6085/liveness-http is now 1 (18.030175343s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:39:51.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6085" for this suite.
May 31 19:39:57.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:39:57.408: INFO: namespace container-probe-6085 deletion completed in 6.096596657s

• [SLOW TEST:28.208 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:39:57.409: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 31 19:39:57.453: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 19:39:57.459: INFO: Waiting for terminating namespaces to be deleted...
May 31 19:39:57.461: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-33506892-0 before test
May 31 19:39:57.467: INFO: kube-flannel-ds-5kgxc from kube-system started at 2019-05-31 18:09:36 +0000 UTC (2 container statuses recorded)
May 31 19:39:57.467: INFO: 	Container install-cni ready: true, restart count 0
May 31 19:39:57.467: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 19:39:57.467: INFO: azure-ip-masq-agent-hffrz from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.467: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 19:39:57.467: INFO: tiller-deploy-5d9ffb885b-sm2bb from kube-system started at 2019-05-31 18:10:05 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.467: INFO: 	Container tiller ready: true, restart count 0
May 31 19:39:57.467: INFO: sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-zhmp4 from heptio-sonobuoy started at 2019-05-31 18:11:14 +0000 UTC (2 container statuses recorded)
May 31 19:39:57.467: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 19:39:57.467: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 31 19:39:57.467: INFO: kube-proxy-5bk4d from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.467: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 19:39:57.467: INFO: metrics-server-6bf85bb69b-rc4nd from kube-system started at 2019-05-31 18:10:05 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.467: INFO: 	Container metrics-server ready: true, restart count 0
May 31 19:39:57.467: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-33506892-1 before test
May 31 19:39:57.481: INFO: sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-t5p7z from heptio-sonobuoy started at 2019-05-31 18:11:14 +0000 UTC (2 container statuses recorded)
May 31 19:39:57.481: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 19:39:57.481: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 31 19:39:57.481: INFO: azure-ip-masq-agent-8cg76 from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.481: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 19:39:57.481: INFO: kube-proxy-kkq24 from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.481: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 19:39:57.481: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-31 18:11:05 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.481: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 19:39:57.481: INFO: kube-flannel-ds-9s4jc from kube-system started at 2019-05-31 18:09:35 +0000 UTC (2 container statuses recorded)
May 31 19:39:57.481: INFO: 	Container install-cni ready: true, restart count 0
May 31 19:39:57.481: INFO: 	Container kube-flannel ready: true, restart count 1
May 31 19:39:57.481: INFO: 
Logging pods the kubelet thinks is on node k8s-linuxpool-33506892-2 before test
May 31 19:39:57.493: INFO: kube-flannel-ds-n8nxk from kube-system started at 2019-05-31 18:09:35 +0000 UTC (2 container statuses recorded)
May 31 19:39:57.494: INFO: 	Container install-cni ready: true, restart count 0
May 31 19:39:57.494: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 19:39:57.494: INFO: azure-ip-masq-agent-lnst2 from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.494: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 31 19:39:57.494: INFO: kube-proxy-49ngv from kube-system started at 2019-05-31 18:09:40 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.494: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 19:39:57.494: INFO: kubernetes-dashboard-54b795b69b-zsd4t from kube-system started at 2019-05-31 18:10:07 +0000 UTC (1 container statuses recorded)
May 31 19:39:57.495: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 31 19:39:57.495: INFO: sonobuoy-systemd-logs-daemon-set-a2aa36472a064b66-fnw89 from heptio-sonobuoy started at 2019-05-31 18:11:14 +0000 UTC (2 container statuses recorded)
May 31 19:39:57.495: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
May 31 19:39:57.495: INFO: 	Container sonobuoy-worker ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e177db0c-83db-11e9-a3c9-f265bfc55a07 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e177db0c-83db-11e9-a3c9-f265bfc55a07 off the node k8s-linuxpool-33506892-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e177db0c-83db-11e9-a3c9-f265bfc55a07
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:40:05.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4824" for this suite.
May 31 19:40:23.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:40:23.910: INFO: namespace sched-pred-4824 deletion completed in 18.337596657s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:26.501 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:40:23.918: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
May 31 19:40:24.008: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-130611839 proxy --unix-socket=/tmp/kubectl-proxy-unix050815270/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:40:24.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6514" for this suite.
May 31 19:40:30.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:40:30.341: INFO: namespace kubectl-6514 deletion completed in 6.103568475s

• [SLOW TEST:6.423 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:40:30.341: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 31 19:40:38.442: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 19:40:38.450: INFO: Pod pod-with-prestop-http-hook still exists
May 31 19:40:40.450: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 19:40:40.452: INFO: Pod pod-with-prestop-http-hook still exists
May 31 19:40:42.450: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 19:40:42.454: INFO: Pod pod-with-prestop-http-hook still exists
May 31 19:40:44.450: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 19:40:44.453: INFO: Pod pod-with-prestop-http-hook still exists
May 31 19:40:46.450: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 19:40:46.453: INFO: Pod pod-with-prestop-http-hook still exists
May 31 19:40:48.450: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 19:40:48.453: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:40:48.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6115" for this suite.
May 31 19:41:10.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:41:10.555: INFO: namespace container-lifecycle-hook-6115 deletion completed in 22.091797141s

• [SLOW TEST:40.214 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:41:10.558: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:41:10.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0aa3ad18-83dc-11e9-a3c9-f265bfc55a07" in namespace "downward-api-9579" to be "success or failure"
May 31 19:41:10.625: INFO: Pod "downwardapi-volume-0aa3ad18-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 12.106597ms
May 31 19:41:12.628: INFO: Pod "downwardapi-volume-0aa3ad18-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014980276s
May 31 19:41:14.630: INFO: Pod "downwardapi-volume-0aa3ad18-83dc-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017836755s
STEP: Saw pod success
May 31 19:41:14.630: INFO: Pod "downwardapi-volume-0aa3ad18-83dc-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:41:14.633: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downwardapi-volume-0aa3ad18-83dc-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:41:14.648: INFO: Waiting for pod downwardapi-volume-0aa3ad18-83dc-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:41:14.652: INFO: Pod downwardapi-volume-0aa3ad18-83dc-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:41:14.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9579" for this suite.
May 31 19:41:20.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:41:20.748: INFO: namespace downward-api-9579 deletion completed in 6.09305221s

• [SLOW TEST:10.190 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:41:20.748: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:41:20.815: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10b82ed3-83dc-11e9-a3c9-f265bfc55a07" in namespace "projected-7705" to be "success or failure"
May 31 19:41:20.822: INFO: Pod "downwardapi-volume-10b82ed3-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 7.274419ms
May 31 19:41:22.827: INFO: Pod "downwardapi-volume-10b82ed3-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011984228s
May 31 19:41:24.830: INFO: Pod "downwardapi-volume-10b82ed3-83dc-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015416316s
STEP: Saw pod success
May 31 19:41:24.830: INFO: Pod "downwardapi-volume-10b82ed3-83dc-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:41:24.833: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downwardapi-volume-10b82ed3-83dc-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:41:24.853: INFO: Waiting for pod downwardapi-volume-10b82ed3-83dc-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:41:24.855: INFO: Pod downwardapi-volume-10b82ed3-83dc-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:41:24.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7705" for this suite.
May 31 19:41:30.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:41:30.949: INFO: namespace projected-7705 deletion completed in 6.091113181s

• [SLOW TEST:10.201 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:41:30.949: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:41:55.025: INFO: Container started at 2019-05-31 19:41:32 +0000 UTC, pod became ready at 2019-05-31 19:41:54 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:41:55.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5459" for this suite.
May 31 19:42:17.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:42:17.112: INFO: namespace container-probe-5459 deletion completed in 22.084663455s

• [SLOW TEST:46.163 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:42:17.120: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 31 19:42:17.171: INFO: Waiting up to 5m0s for pod "downward-api-324ffac6-83dc-11e9-a3c9-f265bfc55a07" in namespace "downward-api-7561" to be "success or failure"
May 31 19:42:17.181: INFO: Pod "downward-api-324ffac6-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 10.236667ms
May 31 19:42:19.185: INFO: Pod "downward-api-324ffac6-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013637357s
May 31 19:42:21.188: INFO: Pod "downward-api-324ffac6-83dc-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01723315s
STEP: Saw pod success
May 31 19:42:21.189: INFO: Pod "downward-api-324ffac6-83dc-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:42:21.192: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downward-api-324ffac6-83dc-11e9-a3c9-f265bfc55a07 container dapi-container: <nil>
STEP: delete the pod
May 31 19:42:21.208: INFO: Waiting for pod downward-api-324ffac6-83dc-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:42:21.210: INFO: Pod downward-api-324ffac6-83dc-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:42:21.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7561" for this suite.
May 31 19:42:27.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:42:27.297: INFO: namespace downward-api-7561 deletion completed in 6.083811269s

• [SLOW TEST:10.177 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:42:27.297: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3644/configmap-test-38601714-83dc-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume configMaps
May 31 19:42:27.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-38609156-83dc-11e9-a3c9-f265bfc55a07" in namespace "configmap-3644" to be "success or failure"
May 31 19:42:27.351: INFO: Pod "pod-configmaps-38609156-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.788678ms
May 31 19:42:29.354: INFO: Pod "pod-configmaps-38609156-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008050766s
May 31 19:42:31.357: INFO: Pod "pod-configmaps-38609156-83dc-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011132251s
STEP: Saw pod success
May 31 19:42:31.357: INFO: Pod "pod-configmaps-38609156-83dc-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:42:31.359: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-configmaps-38609156-83dc-11e9-a3c9-f265bfc55a07 container env-test: <nil>
STEP: delete the pod
May 31 19:42:31.374: INFO: Waiting for pod pod-configmaps-38609156-83dc-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:42:31.377: INFO: Pod pod-configmaps-38609156-83dc-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:42:31.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3644" for this suite.
May 31 19:42:37.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:42:37.523: INFO: namespace configmap-3644 deletion completed in 6.143603642s

• [SLOW TEST:10.225 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:42:37.526: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:43:37.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1147" for this suite.
May 31 19:43:59.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:43:59.715: INFO: namespace container-probe-1147 deletion completed in 22.121808948s

• [SLOW TEST:82.190 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:43:59.716: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0531 19:44:39.793950      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 31 19:44:39.794: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:44:39.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7393" for this suite.
May 31 19:44:45.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:44:45.887: INFO: namespace gc-7393 deletion completed in 6.089912009s

• [SLOW TEST:46.171 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:44:45.888: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:44:45.946: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8afce879-83dc-11e9-a3c9-f265bfc55a07" in namespace "downward-api-161" to be "success or failure"
May 31 19:44:45.953: INFO: Pod "downwardapi-volume-8afce879-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.26109ms
May 31 19:44:47.956: INFO: Pod "downwardapi-volume-8afce879-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009418199s
May 31 19:44:49.959: INFO: Pod "downwardapi-volume-8afce879-83dc-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012851625s
STEP: Saw pod success
May 31 19:44:49.959: INFO: Pod "downwardapi-volume-8afce879-83dc-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:44:49.962: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod downwardapi-volume-8afce879-83dc-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:44:49.983: INFO: Waiting for pod downwardapi-volume-8afce879-83dc-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:44:49.985: INFO: Pod downwardapi-volume-8afce879-83dc-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:44:49.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-161" for this suite.
May 31 19:44:55.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:44:56.067: INFO: namespace downward-api-161 deletion completed in 6.079033547s

• [SLOW TEST:10.180 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:44:56.068: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:44:56.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6753" for this suite.
May 31 19:45:02.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:45:02.214: INFO: namespace kubelet-test-6753 deletion completed in 6.088146496s

• [SLOW TEST:6.146 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:45:02.214: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:45:27.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6552" for this suite.
May 31 19:45:33.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:45:33.572: INFO: namespace container-runtime-6552 deletion completed in 6.08769946s

• [SLOW TEST:31.357 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:45:33.572: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 31 19:45:33.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7670af9-83dc-11e9-a3c9-f265bfc55a07" in namespace "projected-4690" to be "success or failure"
May 31 19:45:33.621: INFO: Pod "downwardapi-volume-a7670af9-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.440865ms
May 31 19:45:35.624: INFO: Pod "downwardapi-volume-a7670af9-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007934467s
May 31 19:45:37.636: INFO: Pod "downwardapi-volume-a7670af9-83dc-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019222895s
STEP: Saw pod success
May 31 19:45:37.636: INFO: Pod "downwardapi-volume-a7670af9-83dc-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:45:37.641: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downwardapi-volume-a7670af9-83dc-11e9-a3c9-f265bfc55a07 container client-container: <nil>
STEP: delete the pod
May 31 19:45:37.699: INFO: Waiting for pod downwardapi-volume-a7670af9-83dc-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:45:37.707: INFO: Pod downwardapi-volume-a7670af9-83dc-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:45:37.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4690" for this suite.
May 31 19:45:43.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:45:43.813: INFO: namespace projected-4690 deletion completed in 6.098791001s

• [SLOW TEST:10.242 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:45:43.814: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
May 31 19:45:44.405: INFO: created pod pod-service-account-defaultsa
May 31 19:45:44.405: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 31 19:45:44.411: INFO: created pod pod-service-account-mountsa
May 31 19:45:44.411: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 31 19:45:44.420: INFO: created pod pod-service-account-nomountsa
May 31 19:45:44.420: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 31 19:45:44.431: INFO: created pod pod-service-account-defaultsa-mountspec
May 31 19:45:44.431: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 31 19:45:44.441: INFO: created pod pod-service-account-mountsa-mountspec
May 31 19:45:44.441: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 31 19:45:44.453: INFO: created pod pod-service-account-nomountsa-mountspec
May 31 19:45:44.453: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 31 19:45:44.470: INFO: created pod pod-service-account-defaultsa-nomountspec
May 31 19:45:44.470: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 31 19:45:44.540: INFO: created pod pod-service-account-mountsa-nomountspec
May 31 19:45:44.540: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 31 19:45:44.556: INFO: created pod pod-service-account-nomountsa-nomountspec
May 31 19:45:44.556: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:45:44.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1201" for this suite.
May 31 19:45:50.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:45:50.655: INFO: namespace svcaccounts-1201 deletion completed in 6.085921532s

• [SLOW TEST:6.842 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:45:50.657: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 31 19:45:50.702: INFO: Waiting up to 5m0s for pod "downward-api-b1963cc9-83dc-11e9-a3c9-f265bfc55a07" in namespace "downward-api-1100" to be "success or failure"
May 31 19:45:50.711: INFO: Pod "downward-api-b1963cc9-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 8.510625ms
May 31 19:45:52.714: INFO: Pod "downward-api-b1963cc9-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011437216s
May 31 19:45:54.717: INFO: Pod "downward-api-b1963cc9-83dc-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014751924s
STEP: Saw pod success
May 31 19:45:54.717: INFO: Pod "downward-api-b1963cc9-83dc-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:45:54.720: INFO: Trying to get logs from node k8s-linuxpool-33506892-0 pod downward-api-b1963cc9-83dc-11e9-a3c9-f265bfc55a07 container dapi-container: <nil>
STEP: delete the pod
May 31 19:45:54.752: INFO: Waiting for pod downward-api-b1963cc9-83dc-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:45:54.754: INFO: Pod downward-api-b1963cc9-83dc-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:45:54.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1100" for this suite.
May 31 19:46:00.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:46:00.856: INFO: namespace downward-api-1100 deletion completed in 6.09653856s

• [SLOW TEST:10.199 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:46:00.857: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-b7aba223-83dc-11e9-a3c9-f265bfc55a07
STEP: Creating a pod to test consume secrets
May 31 19:46:00.914: INFO: Waiting up to 5m0s for pod "pod-secrets-b7ac33a4-83dc-11e9-a3c9-f265bfc55a07" in namespace "secrets-8513" to be "success or failure"
May 31 19:46:00.921: INFO: Pod "pod-secrets-b7ac33a4-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.8182ms
May 31 19:46:02.925: INFO: Pod "pod-secrets-b7ac33a4-83dc-11e9-a3c9-f265bfc55a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010230654s
May 31 19:46:04.928: INFO: Pod "pod-secrets-b7ac33a4-83dc-11e9-a3c9-f265bfc55a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013356016s
STEP: Saw pod success
May 31 19:46:04.928: INFO: Pod "pod-secrets-b7ac33a4-83dc-11e9-a3c9-f265bfc55a07" satisfied condition "success or failure"
May 31 19:46:04.930: INFO: Trying to get logs from node k8s-linuxpool-33506892-1 pod pod-secrets-b7ac33a4-83dc-11e9-a3c9-f265bfc55a07 container secret-volume-test: <nil>
STEP: delete the pod
May 31 19:46:05.006: INFO: Waiting for pod pod-secrets-b7ac33a4-83dc-11e9-a3c9-f265bfc55a07 to disappear
May 31 19:46:05.020: INFO: Pod pod-secrets-b7ac33a4-83dc-11e9-a3c9-f265bfc55a07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:46:05.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8513" for this suite.
May 31 19:46:11.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:46:11.117: INFO: namespace secrets-8513 deletion completed in 6.093232482s

• [SLOW TEST:10.260 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 31 19:46:11.117: INFO: >>> kubeConfig: /tmp/kubeconfig-130611839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 31 19:46:11.160: INFO: Creating deployment "nginx-deployment"
May 31 19:46:11.165: INFO: Waiting for observed generation 1
May 31 19:46:13.170: INFO: Waiting for all required pods to come up
May 31 19:46:13.178: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 31 19:46:17.197: INFO: Waiting for deployment "nginx-deployment" to complete
May 31 19:46:17.201: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 31 19:46:17.208: INFO: Updating deployment nginx-deployment
May 31 19:46:17.208: INFO: Waiting for observed generation 2
May 31 19:46:19.245: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 31 19:46:19.247: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 31 19:46:19.249: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 31 19:46:19.255: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 31 19:46:19.255: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 31 19:46:19.257: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 31 19:46:19.260: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 31 19:46:19.260: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 31 19:46:19.266: INFO: Updating deployment nginx-deployment
May 31 19:46:19.266: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 31 19:46:19.274: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 31 19:46:19.281: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 31 19:46:19.301: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7203,SelfLink:/apis/apps/v1/namespaces/deployment-7203/deployments/nginx-deployment,UID:bdc8f269-83dc-11e9-a607-001dd80c001e,ResourceVersion:22275,Generation:3,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-05-31 19:46:17 +0000 UTC 2019-05-31 19:46:11 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-05-31 19:46:19 +0000 UTC 2019-05-31 19:46:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

May 31 19:46:19.329: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-7203,SelfLink:/apis/apps/v1/namespaces/deployment-7203/replicasets/nginx-deployment-5f9595f595,UID:c163c6e4-83dc-11e9-a607-001dd80c001e,ResourceVersion:22269,Generation:3,CreationTimestamp:2019-05-31 19:46:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment bdc8f269-83dc-11e9-a607-001dd80c001e 0xc003423637 0xc003423638}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 31 19:46:19.330: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 31 19:46:19.330: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-7203,SelfLink:/apis/apps/v1/namespaces/deployment-7203/replicasets/nginx-deployment-6f478d8d8,UID:bdc9dbcb-83dc-11e9-a607-001dd80c001e,ResourceVersion:22267,Generation:3,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment bdc8f269-83dc-11e9-a607-001dd80c001e 0xc003423707 0xc003423708}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 31 19:46:19.401: INFO: Pod "nginx-deployment-5f9595f595-22xdh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-22xdh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-22xdh,UID:c2a183e4-83dc-11e9-a607-001dd80c001e,ResourceVersion:22297,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc0031244e7 0xc0031244e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003124550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003124570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.403: INFO: Pod "nginx-deployment-5f9595f595-4sq68" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-4sq68,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-4sq68,UID:c1645e8b-83dc-11e9-a607-001dd80c001e,ResourceVersion:22229,Generation:0,CreationTimestamp:2019-05-31 19:46:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc0031245f0 0xc0031245f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003124660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003124680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2019-05-31 19:46:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.404: INFO: Pod "nginx-deployment-5f9595f595-4v2r4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-4v2r4,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-4v2r4,UID:c2a6059c-83dc-11e9-a607-001dd80c001e,ResourceVersion:22293,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc003124750 0xc003124751}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031247c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031247e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.405: INFO: Pod "nginx-deployment-5f9595f595-7zff6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7zff6,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-7zff6,UID:c2a5c89b-83dc-11e9-a607-001dd80c001e,ResourceVersion:22289,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc003124847 0xc003124848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031248b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031248d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.405: INFO: Pod "nginx-deployment-5f9595f595-9h9th" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9h9th,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-9h9th,UID:c1700456-83dc-11e9-a607-001dd80c001e,ResourceVersion:22252,Generation:0,CreationTimestamp:2019-05-31 19:46:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc003124937 0xc003124938}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031249a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031249c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2019-05-31 19:46:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.407: INFO: Pod "nginx-deployment-5f9595f595-bf5r9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-bf5r9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-bf5r9,UID:c29fb2b5-83dc-11e9-a607-001dd80c001e,ResourceVersion:22298,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc003124a90 0xc003124a91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003124b00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003124b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2019-05-31 19:46:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.409: INFO: Pod "nginx-deployment-5f9595f595-f95hw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-f95hw,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-f95hw,UID:c2a68a8b-83dc-11e9-a607-001dd80c001e,ResourceVersion:22295,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc003124bf0 0xc003124bf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003124c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003124c80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.410: INFO: Pod "nginx-deployment-5f9595f595-gl4gp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-gl4gp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-gl4gp,UID:c2a50cda-83dc-11e9-a607-001dd80c001e,ResourceVersion:22291,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc003124ce7 0xc003124ce8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003124d50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003124d70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.411: INFO: Pod "nginx-deployment-5f9595f595-hf2m4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-hf2m4,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-hf2m4,UID:c1728176-83dc-11e9-a607-001dd80c001e,ResourceVersion:22257,Generation:0,CreationTimestamp:2019-05-31 19:46:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc003124dd7 0xc003124dd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003124e40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003124e60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-31 19:46:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.414: INFO: Pod "nginx-deployment-5f9595f595-ldm9k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-ldm9k,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-ldm9k,UID:c1657408-83dc-11e9-a607-001dd80c001e,ResourceVersion:22230,Generation:0,CreationTimestamp:2019-05-31 19:46:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc003124f30 0xc003124f31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003124fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003124fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2019-05-31 19:46:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.415: INFO: Pod "nginx-deployment-5f9595f595-nd966" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-nd966,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-nd966,UID:c1657a24-83dc-11e9-a607-001dd80c001e,ResourceVersion:22235,Generation:0,CreationTimestamp:2019-05-31 19:46:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc003125090 0xc003125091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:17 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-31 19:46:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.417: INFO: Pod "nginx-deployment-5f9595f595-xxdw4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-xxdw4,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-5f9595f595-xxdw4,UID:c2a170e9-83dc-11e9-a607-001dd80c001e,ResourceVersion:22296,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 c163c6e4-83dc-11e9-a607-001dd80c001e 0xc0031251f0 0xc0031251f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.417: INFO: Pod "nginx-deployment-6f478d8d8-2vsjh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2vsjh,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-2vsjh,UID:bdcf8ea7-83dc-11e9-a607-001dd80c001e,ResourceVersion:22200,Generation:0,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125300 0xc003125301}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:10.244.1.151,StartTime:2019-05-31 19:46:11 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 19:46:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0405800821c4f46fdc4d2117bddee825909968e7f6520f8dacd08c70c0764aaa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.418: INFO: Pod "nginx-deployment-6f478d8d8-55c9l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-55c9l,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-55c9l,UID:c2a5f45f-83dc-11e9-a607-001dd80c001e,ResourceVersion:22290,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125450 0xc003125451}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031254b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031254d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.419: INFO: Pod "nginx-deployment-6f478d8d8-6fxnj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6fxnj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-6fxnj,UID:c29e9a79-83dc-11e9-a607-001dd80c001e,ResourceVersion:22286,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125537 0xc003125538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031255a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031255c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2019-05-31 19:46:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.419: INFO: Pod "nginx-deployment-6f478d8d8-7jf2v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7jf2v,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-7jf2v,UID:bdcfa905-83dc-11e9-a607-001dd80c001e,ResourceVersion:22170,Generation:0,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125680 0xc003125681}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031256e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.43,StartTime:2019-05-31 19:46:11 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 19:46:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://bc466cbccc63d7860e41a10c334637032b2f09f3b4109ca671e5c878ab536587}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.419: INFO: Pod "nginx-deployment-6f478d8d8-8hqrl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8hqrl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-8hqrl,UID:c2a5c5d2-83dc-11e9-a607-001dd80c001e,ResourceVersion:22303,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc0031257d0 0xc0031257d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.419: INFO: Pod "nginx-deployment-6f478d8d8-g7xvx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-g7xvx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-g7xvx,UID:bdd2e793-83dc-11e9-a607-001dd80c001e,ResourceVersion:22181,Generation:0,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc0031258d0 0xc0031258d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.143,StartTime:2019-05-31 19:46:11 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 19:46:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3aa5fe87eb380df92fda4e08e6ff69b171109dc694b0aa0dbc47bebc5cdc04c4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.419: INFO: Pod "nginx-deployment-6f478d8d8-h9qds" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h9qds,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-h9qds,UID:c2a45bbb-83dc-11e9-a607-001dd80c001e,ResourceVersion:22300,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125a20 0xc003125a21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.419: INFO: Pod "nginx-deployment-6f478d8d8-kzmpf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-kzmpf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-kzmpf,UID:bdcfb79e-83dc-11e9-a607-001dd80c001e,ResourceVersion:22184,Generation:0,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125b20 0xc003125b21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.142,StartTime:2019-05-31 19:46:11 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 19:46:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9324aa7f3a7430b62a17a0025f8d24b5284ff19247600d7cf311b327de2e2778}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.421: INFO: Pod "nginx-deployment-6f478d8d8-lzft8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lzft8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-lzft8,UID:c2a9ebe2-83dc-11e9-a607-001dd80c001e,ResourceVersion:22302,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125c70 0xc003125c71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.421: INFO: Pod "nginx-deployment-6f478d8d8-nhqgn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-nhqgn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-nhqgn,UID:bdd2ffce-83dc-11e9-a607-001dd80c001e,ResourceVersion:22196,Generation:0,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125d57 0xc003125d58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125de0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.4,PodIP:10.244.1.153,StartTime:2019-05-31 19:46:11 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 19:46:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://263c95e14511f76c9a90f246e5a5498238b0cf07e43c68e33a2265459ba4a8e1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.421: INFO: Pod "nginx-deployment-6f478d8d8-p54k6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-p54k6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-p54k6,UID:c2a60345-83dc-11e9-a607-001dd80c001e,ResourceVersion:22299,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125eb0 0xc003125eb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003125f10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003125f30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.424: INFO: Pod "nginx-deployment-6f478d8d8-rp2rp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rp2rp,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-rp2rp,UID:c2a9c4c4-83dc-11e9-a607-001dd80c001e,ResourceVersion:22301,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc003125fb0 0xc003125fb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031a2010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031a2030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.424: INFO: Pod "nginx-deployment-6f478d8d8-sf6p7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-sf6p7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-sf6p7,UID:c2a02cb1-83dc-11e9-a607-001dd80c001e,ResourceVersion:22282,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc0031a2097 0xc0031a2098}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031a2100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031a2120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.425: INFO: Pod "nginx-deployment-6f478d8d8-wgdrr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wgdrr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-wgdrr,UID:bdcf4066-83dc-11e9-a607-001dd80c001e,ResourceVersion:22167,Generation:0,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc0031a21a0 0xc0031a21a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031a2200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031a2220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.42,StartTime:2019-05-31 19:46:11 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 19:46:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6d02e726353bd905fac2f736c5055f467a3f701d2daecd613f80212931439bf7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.425: INFO: Pod "nginx-deployment-6f478d8d8-wsghv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wsghv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-wsghv,UID:bdcd558a-83dc-11e9-a607-001dd80c001e,ResourceVersion:22173,Generation:0,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc0031a22f0 0xc0031a22f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031a2350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031a2370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.44,StartTime:2019-05-31 19:46:11 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 19:46:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9d30eabf170f7f179695ee81c9331bc739e183025da00787f3d884f28a146981}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.425: INFO: Pod "nginx-deployment-6f478d8d8-z68c9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-z68c9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-z68c9,UID:bdcd8157-83dc-11e9-a607-001dd80c001e,ResourceVersion:22188,Generation:0,CreationTimestamp:2019-05-31 19:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc0031a2440 0xc0031a2441}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031a24a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031a24c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:11 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.3.141,StartTime:2019-05-31 19:46:11 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-31 19:46:14 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1905136677c3bdef65ded392685eb1047fd8a051c0008f3fee50bb7873592e69}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 31 19:46:19.427: INFO: Pod "nginx-deployment-6f478d8d8-zcjh5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zcjh5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7203,SelfLink:/api/v1/namespaces/deployment-7203/pods/nginx-deployment-6f478d8d8-zcjh5,UID:c2a0855e-83dc-11e9-a607-001dd80c001e,ResourceVersion:22283,Generation:0,CreationTimestamp:2019-05-31 19:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 bdc9dbcb-83dc-11e9-a607-001dd80c001e 0xc0031a2590 0xc0031a2591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tq98z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tq98z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tq98z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-33506892-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031a25f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031a2610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-31 19:46:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 31 19:46:19.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7203" for this suite.
May 31 19:46:25.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 31 19:46:25.608: INFO: namespace deployment-7203 deletion completed in 6.164186359s

• [SLOW TEST:14.492 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSMay 31 19:46:25.609: INFO: Running AfterSuite actions on all nodes
May 31 19:46:25.609: INFO: Running AfterSuite actions on node 1
May 31 19:46:25.609: INFO: Skipping dumping logs from cluster

Ran 204 of 3585 Specs in 5671.859 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3381 Skipped PASS

Ginkgo ran 1 suite in 1h34m33.44160711s
Test Suite Passed
