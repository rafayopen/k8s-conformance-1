I0529 17:50:48.556265      16 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-947231839
I0529 17:50:48.556500      16 e2e.go:240] Starting e2e run "4a28b949-823a-11e9-b32a-4ae2266202a5" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1559152247 - Will randomize all specs
Will run 204 of 3585 specs

May 29 17:50:48.713: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 17:50:48.715: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 29 17:50:48.744: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 29 17:50:48.776: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 29 17:50:48.776: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
May 29 17:50:48.776: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 29 17:50:48.783: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'azure-cni-networkmonitor' (0 seconds elapsed)
May 29 17:50:48.783: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
May 29 17:50:48.783: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'blobfuse-flexvol-installer' (0 seconds elapsed)
May 29 17:50:48.783: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'keyvault-flexvolume' (0 seconds elapsed)
May 29 17:50:48.783: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 29 17:50:48.783: INFO: e2e test version: v1.14.2
May 29 17:50:48.784: INFO: kube-apiserver version: v1.14.2
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:50:48.784: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
May 29 17:50:48.836: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-4aeed0a4-823a-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 17:50:48.846: INFO: Waiting up to 5m0s for pod "pod-secrets-4aef332a-823a-11e9-b32a-4ae2266202a5" in namespace "secrets-6228" to be "success or failure"
May 29 17:50:48.848: INFO: Pod "pod-secrets-4aef332a-823a-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556014ms
May 29 17:50:50.852: INFO: Pod "pod-secrets-4aef332a-823a-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006029118s
May 29 17:50:52.855: INFO: Pod "pod-secrets-4aef332a-823a-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009395035s
STEP: Saw pod success
May 29 17:50:52.855: INFO: Pod "pod-secrets-4aef332a-823a-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:50:52.858: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-secrets-4aef332a-823a-11e9-b32a-4ae2266202a5 container secret-volume-test: <nil>
STEP: delete the pod
May 29 17:50:52.901: INFO: Waiting for pod pod-secrets-4aef332a-823a-11e9-b32a-4ae2266202a5 to disappear
May 29 17:50:52.903: INFO: Pod pod-secrets-4aef332a-823a-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:50:52.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6228" for this suite.
May 29 17:50:58.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:50:59.001: INFO: namespace secrets-6228 deletion completed in 6.094608357s

• [SLOW TEST:10.217 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:50:59.001: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-510bf819-823a-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 17:50:59.152: INFO: Waiting up to 5m0s for pod "pod-secrets-5113cb89-823a-11e9-b32a-4ae2266202a5" in namespace "secrets-5940" to be "success or failure"
May 29 17:50:59.157: INFO: Pod "pod-secrets-5113cb89-823a-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.070318ms
May 29 17:51:01.161: INFO: Pod "pod-secrets-5113cb89-823a-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008552723s
May 29 17:51:03.165: INFO: Pod "pod-secrets-5113cb89-823a-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012339513s
STEP: Saw pod success
May 29 17:51:03.165: INFO: Pod "pod-secrets-5113cb89-823a-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:51:03.167: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-secrets-5113cb89-823a-11e9-b32a-4ae2266202a5 container secret-volume-test: <nil>
STEP: delete the pod
May 29 17:51:03.196: INFO: Waiting for pod pod-secrets-5113cb89-823a-11e9-b32a-4ae2266202a5 to disappear
May 29 17:51:03.198: INFO: Pod pod-secrets-5113cb89-823a-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:51:03.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5940" for this suite.
May 29 17:51:09.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:51:09.286: INFO: namespace secrets-5940 deletion completed in 6.086029952s
STEP: Destroying namespace "secret-namespace-6772" for this suite.
May 29 17:51:15.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:51:15.371: INFO: namespace secret-namespace-6772 deletion completed in 6.084553047s

• [SLOW TEST:16.370 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:51:15.371: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 29 17:51:15.419: INFO: Waiting up to 5m0s for pod "pod-5ac5c2cf-823a-11e9-b32a-4ae2266202a5" in namespace "emptydir-6206" to be "success or failure"
May 29 17:51:15.429: INFO: Pod "pod-5ac5c2cf-823a-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.382621ms
May 29 17:51:17.433: INFO: Pod "pod-5ac5c2cf-823a-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014129903s
May 29 17:51:19.437: INFO: Pod "pod-5ac5c2cf-823a-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017906033s
STEP: Saw pod success
May 29 17:51:19.437: INFO: Pod "pod-5ac5c2cf-823a-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:51:19.440: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-5ac5c2cf-823a-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 17:51:19.462: INFO: Waiting for pod pod-5ac5c2cf-823a-11e9-b32a-4ae2266202a5 to disappear
May 29 17:51:19.465: INFO: Pod pod-5ac5c2cf-823a-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:51:19.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6206" for this suite.
May 29 17:51:25.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:51:25.548: INFO: namespace emptydir-6206 deletion completed in 6.079831913s

• [SLOW TEST:10.177 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:51:25.549: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 17:51:25.589: INFO: Creating deployment "nginx-deployment"
May 29 17:51:25.592: INFO: Waiting for observed generation 1
May 29 17:51:27.598: INFO: Waiting for all required pods to come up
May 29 17:51:27.602: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 29 17:51:43.613: INFO: Waiting for deployment "nginx-deployment" to complete
May 29 17:51:43.617: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 29 17:51:43.623: INFO: Updating deployment nginx-deployment
May 29 17:51:43.623: INFO: Waiting for observed generation 2
May 29 17:51:45.630: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 29 17:51:45.632: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 29 17:51:45.634: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 29 17:51:45.641: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 29 17:51:45.641: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 29 17:51:45.643: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 29 17:51:45.659: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 29 17:51:45.659: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 29 17:51:45.667: INFO: Updating deployment nginx-deployment
May 29 17:51:45.667: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 29 17:51:45.691: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 29 17:51:47.710: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 17:51:47.717: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-8411,SelfLink:/apis/apps/v1/namespaces/deployment-8411/deployments/nginx-deployment,UID:60d74584-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1621,Generation:3,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-05-29 17:51:45 +0000 UTC 2019-05-29 17:51:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-29 17:51:45 +0000 UTC 2019-05-29 17:51:25 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

May 29 17:51:47.719: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-8411,SelfLink:/apis/apps/v1/namespaces/deployment-8411/replicasets/nginx-deployment-5f9595f595,UID:6b970fc9-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1613,Generation:3,CreationTimestamp:2019-05-29 17:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 60d74584-823a-11e9-9319-000d3afdb8b2 0xc00233a567 0xc00233a568}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 17:51:47.719: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 29 17:51:47.719: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-8411,SelfLink:/apis/apps/v1/namespaces/deployment-8411/replicasets/nginx-deployment-6f478d8d8,UID:60d7dc39-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1618,Generation:3,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 60d74584-823a-11e9-9319-000d3afdb8b2 0xc00233a637 0xc00233a638}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 29 17:51:47.726: INFO: Pod "nginx-deployment-5f9595f595-59kn9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-59kn9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-59kn9,UID:6cd5d41d-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1601,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002136a40 0xc002136a41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002136ab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002136ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.726: INFO: Pod "nginx-deployment-5f9595f595-fbvjh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-fbvjh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-fbvjh,UID:6ba17dba-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1558,Generation:0,CreationTimestamp:2019-05-29 17:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002136b60 0xc002136b61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002136bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002136bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.726: INFO: Pod "nginx-deployment-5f9595f595-g929s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-g929s,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-g929s,UID:6cd61142-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1614,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002136cc0 0xc002136cc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002136d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002136d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.727: INFO: Pod "nginx-deployment-5f9595f595-gw26j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-gw26j,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-gw26j,UID:6b99e5fb-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1549,Generation:0,CreationTimestamp:2019-05-29 17:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002136de0 0xc002136de1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002136e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002136e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.727: INFO: Pod "nginx-deployment-5f9595f595-j47vr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-j47vr,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-j47vr,UID:6b981bd4-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1688,Generation:0,CreationTimestamp:2019-05-29 17:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002136f40 0xc002136f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002136fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002136fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.55,StartTime:2019-05-29 17:51:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found: manifest unknown: manifest unknown,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.727: INFO: Pod "nginx-deployment-5f9595f595-jv2wm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jv2wm,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-jv2wm,UID:6cd29f0e-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1631,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc0021370c0 0xc0021370c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.727: INFO: Pod "nginx-deployment-5f9595f595-kc8ph" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kc8ph,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-kc8ph,UID:6cd015b0-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1623,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002137220 0xc002137221}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021372b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.727: INFO: Pod "nginx-deployment-5f9595f595-l9bt4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-l9bt4,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-l9bt4,UID:6cd58a66-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1685,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002137380 0xc002137381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021373f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.727: INFO: Pod "nginx-deployment-5f9595f595-pmb2b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-pmb2b,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-pmb2b,UID:6cd8d80a-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1645,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc0021374e0 0xc0021374e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.727: INFO: Pod "nginx-deployment-5f9595f595-trjk5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-trjk5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-trjk5,UID:6ba402bf-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1560,Generation:0,CreationTimestamp:2019-05-29 17:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002137600 0xc002137601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.727: INFO: Pod "nginx-deployment-5f9595f595-w56sd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-w56sd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-w56sd,UID:6cd2f895-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1635,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002137760 0xc002137761}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021377d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021377f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.727: INFO: Pod "nginx-deployment-5f9595f595-wwzhk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wwzhk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-wwzhk,UID:6cd555b5-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1683,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc0021378c0 0xc0021378c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-5f9595f595-zqmww" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-zqmww,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-5f9595f595-zqmww,UID:6b9a046c-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1556,Generation:0,CreationTimestamp:2019-05-29 17:51:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 6b970fc9-823a-11e9-9319-000d3afdb8b2 0xc002137a20 0xc002137a21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:43 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:43 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-6f478d8d8-2n47x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2n47x,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-2n47x,UID:6cd6225c-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1630,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc002137b80 0xc002137b81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-6f478d8d8-46lgs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-46lgs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-46lgs,UID:60df84f7-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1473,Generation:0,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc002137c90 0xc002137c91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.45,StartTime:2019-05-29 17:51:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 17:51:37 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://200ab52105cd2373cbe39ba7d809426b18142239f93c7c6647c393eb6c6174ba}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-6f478d8d8-4fqxv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4fqxv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-4fqxv,UID:6cd962a1-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1657,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc002137de0 0xc002137de1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137e40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137e60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:46 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-6f478d8d8-6lr4t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6lr4t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-6lr4t,UID:6cd92f80-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1647,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc002137ef0 0xc002137ef1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002137f50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002137f70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-6f478d8d8-89bk2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-89bk2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-89bk2,UID:60da20a0-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1444,Generation:0,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0000 0xc0025e0001}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e0060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.61,StartTime:2019-05-29 17:51:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 17:51:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c67eee858f4601764590d84e514f79806f84072f8b921aa9378bb2d2d07de47f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-6f478d8d8-8kgfg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8kgfg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-8kgfg,UID:6cd921b2-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1653,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0150 0xc0025e0151}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e01b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e01d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-6f478d8d8-c48k7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-c48k7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-c48k7,UID:6cd35256-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1641,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0260 0xc0025e0261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e02c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e02e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-6f478d8d8-cl4lj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-cl4lj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-cl4lj,UID:60e15da1-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1460,Generation:0,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e03a0 0xc0025e03a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e0400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.43,StartTime:2019-05-29 17:51:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 17:51:34 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8701de80f3514ed29bccf884c25c6f5e12c998800d7fe74ed5303094a68491f2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.728: INFO: Pod "nginx-deployment-6f478d8d8-d8nl7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-d8nl7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-d8nl7,UID:6cd94787-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1655,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e04f0 0xc0025e04f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e0550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-dzbwt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dzbwt,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-dzbwt,UID:6ccfac39-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1615,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0600 0xc0025e0601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e0660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-ghz9g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ghz9g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-ghz9g,UID:60df923d-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1492,Generation:0,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0740 0xc0025e0741}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e07a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e07c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.59,StartTime:2019-05-29 17:51:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 17:51:39 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4defa93b0701ab6aaec238421200cd605e666341a932fe5b690ebbf413781276}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-gkwkj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-gkwkj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-gkwkj,UID:6cd9178f-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1651,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0890 0xc0025e0891}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e08f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-h4gtb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h4gtb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-h4gtb,UID:60db371e-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1456,Generation:0,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e09a0 0xc0025e09a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e0a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.64,StartTime:2019-05-29 17:51:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 17:51:33 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b2fa8a168dead7725d4ccbfd828156e9483b26cf8f964390223549a51adfefb3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-lxm2t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lxm2t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-lxm2t,UID:60df123f-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1467,Generation:0,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0af0 0xc0025e0af1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e0b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.49,StartTime:2019-05-29 17:51:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 17:51:35 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6d0a0ce9d0393ffba70c1d6d916fabbd318078bd0a6f4af49bd76793e7428a8d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-m94hw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-m94hw,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-m94hw,UID:6cd6582e-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1640,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0c40 0xc0025e0c41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e0ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0cc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-rtbkf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rtbkf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-rtbkf,UID:6cd66988-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1626,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0d50 0xc0025e0d51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e0db0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0dd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-sspjr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-sspjr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-sspjr,UID:6cd3629b-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1675,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0e60 0xc0025e0e61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e0ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e0ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 17:51:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-wqgtg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wqgtg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-wqgtg,UID:6cd63f04-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1634,Generation:0,CreationTimestamp:2019-05-29 17:51:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e0fa0 0xc0025e0fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e1000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e1020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:45 +0000 UTC Unschedulable 0/2 nodes are available: 1 Insufficient pods, 1 node(s) had taints that the pod didn't tolerate.}],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.729: INFO: Pod "nginx-deployment-6f478d8d8-xmj9k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xmj9k,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-xmj9k,UID:60e1846d-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1481,Generation:0,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e10b0 0xc0025e10b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e1110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e1130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.47,StartTime:2019-05-29 17:51:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 17:51:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8d7eb2ce844fcd6e558f2c7853c22f1f800b6d500fcb153c3f719d7ef6d68081}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 17:51:47.730: INFO: Pod "nginx-deployment-6f478d8d8-xwp5w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xwp5w,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8411,SelfLink:/api/v1/namespaces/deployment-8411/pods/nginx-deployment-6f478d8d8-xwp5w,UID:60dad97b-823a-11e9-9319-000d3afdb8b2,ResourceVersion:1447,Generation:0,CreationTimestamp:2019-05-29 17:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 60d7dc39-823a-11e9-9319-000d3afdb8b2 0xc0025e1200 0xc0025e1201}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5v2nx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5v2nx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5v2nx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025e1260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025e1280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 17:51:25 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.39,StartTime:2019-05-29 17:51:25 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 17:51:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://21a6b5b22c7b81f0d181a0fb908f59a58a8dd7cf257a92b6b2a6c9967cd35dc1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:51:47.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8411" for this suite.
May 29 17:51:55.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:51:55.814: INFO: namespace deployment-8411 deletion completed in 8.081587456s

• [SLOW TEST:30.264 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:51:55.814: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-zkb8
STEP: Creating a pod to test atomic-volume-subpath
May 29 17:51:55.870: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-zkb8" in namespace "subpath-7766" to be "success or failure"
May 29 17:51:55.873: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.10221ms
May 29 17:51:57.876: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006362319s
May 29 17:51:59.879: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 4.009293713s
May 29 17:52:01.882: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 6.012809131s
May 29 17:52:03.885: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 8.015863441s
May 29 17:52:05.889: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 10.019662393s
May 29 17:52:07.893: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 12.023454968s
May 29 17:52:09.899: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 14.029034229s
May 29 17:52:11.903: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 16.032982969s
May 29 17:52:13.906: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 18.036398428s
May 29 17:52:15.909: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 20.039782527s
May 29 17:52:17.913: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 22.04320518s
May 29 17:52:19.921: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Running", Reason="", readiness=true. Elapsed: 24.051110438s
May 29 17:52:21.925: INFO: Pod "pod-subpath-test-secret-zkb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.055044124s
STEP: Saw pod success
May 29 17:52:21.925: INFO: Pod "pod-subpath-test-secret-zkb8" satisfied condition "success or failure"
May 29 17:52:21.927: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-subpath-test-secret-zkb8 container test-container-subpath-secret-zkb8: <nil>
STEP: delete the pod
May 29 17:52:21.951: INFO: Waiting for pod pod-subpath-test-secret-zkb8 to disappear
May 29 17:52:21.968: INFO: Pod pod-subpath-test-secret-zkb8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-zkb8
May 29 17:52:21.968: INFO: Deleting pod "pod-subpath-test-secret-zkb8" in namespace "subpath-7766"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:52:21.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7766" for this suite.
May 29 17:52:27.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:52:28.060: INFO: namespace subpath-7766 deletion completed in 6.085965276s

• [SLOW TEST:32.246 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:52:28.060: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 29 17:52:28.103: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 29 17:52:28.117: INFO: Waiting for terminating namespaces to be deleted...
May 29 17:52:28.120: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-11779686-vmss000000 before test
May 29 17:52:28.129: INFO: azure-ip-masq-agent-lgffz from kube-system started at 2019-05-29 17:43:50 +0000 UTC (1 container statuses recorded)
May 29 17:52:28.129: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 17:52:28.129: INFO: keyvault-flexvolume-k242s from kube-system started at 2019-05-29 17:43:47 +0000 UTC (1 container statuses recorded)
May 29 17:52:28.129: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 17:52:28.129: INFO: kube-proxy-k8hqh from kube-system started at 2019-05-29 17:43:50 +0000 UTC (1 container statuses recorded)
May 29 17:52:28.129: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 17:52:28.130: INFO: azure-cni-networkmonitor-jtwxq from kube-system started at 2019-05-29 17:43:46 +0000 UTC (1 container statuses recorded)
May 29 17:52:28.130: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 17:52:28.130: INFO: sonobuoy-systemd-logs-daemon-set-270e25e81e454394-4gl5v from heptio-sonobuoy started at 2019-05-29 17:50:08 +0000 UTC (2 container statuses recorded)
May 29 17:52:28.130: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 29 17:52:28.130: INFO: 	Container systemd-logs ready: true, restart count 0
May 29 17:52:28.130: INFO: blobfuse-flexvol-installer-kmkt7 from kube-system started at 2019-05-29 17:43:49 +0000 UTC (1 container statuses recorded)
May 29 17:52:28.130: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 17:52:28.130: INFO: metrics-server-6bf85bb69b-88c8p from kube-system started at 2019-05-29 17:43:50 +0000 UTC (1 container statuses recorded)
May 29 17:52:28.130: INFO: 	Container metrics-server ready: true, restart count 1
May 29 17:52:28.130: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-29 17:49:58 +0000 UTC (1 container statuses recorded)
May 29 17:52:28.130: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 29 17:52:28.130: INFO: sonobuoy-e2e-job-ab3403e7f77e4843 from heptio-sonobuoy started at 2019-05-29 17:50:08 +0000 UTC (2 container statuses recorded)
May 29 17:52:28.131: INFO: 	Container e2e ready: true, restart count 0
May 29 17:52:28.131: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node k8s-pool1-11779686-vmss000000
May 29 17:52:28.174: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-pool1-11779686-vmss000000
May 29 17:52:28.174: INFO: Pod sonobuoy-e2e-job-ab3403e7f77e4843 requesting resource cpu=0m on Node k8s-pool1-11779686-vmss000000
May 29 17:52:28.174: INFO: Pod sonobuoy-systemd-logs-daemon-set-270e25e81e454394-4gl5v requesting resource cpu=0m on Node k8s-pool1-11779686-vmss000000
May 29 17:52:28.174: INFO: Pod azure-cni-networkmonitor-jtwxq requesting resource cpu=0m on Node k8s-pool1-11779686-vmss000000
May 29 17:52:28.174: INFO: Pod azure-ip-masq-agent-lgffz requesting resource cpu=50m on Node k8s-pool1-11779686-vmss000000
May 29 17:52:28.174: INFO: Pod blobfuse-flexvol-installer-kmkt7 requesting resource cpu=50m on Node k8s-pool1-11779686-vmss000000
May 29 17:52:28.174: INFO: Pod keyvault-flexvolume-k242s requesting resource cpu=50m on Node k8s-pool1-11779686-vmss000000
May 29 17:52:28.174: INFO: Pod kube-proxy-k8hqh requesting resource cpu=100m on Node k8s-pool1-11779686-vmss000000
May 29 17:52:28.174: INFO: Pod metrics-server-6bf85bb69b-88c8p requesting resource cpu=0m on Node k8s-pool1-11779686-vmss000000
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-862427f0-823a-11e9-b32a-4ae2266202a5.15a338ccc8ae6ea7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-301/filler-pod-862427f0-823a-11e9-b32a-4ae2266202a5 to k8s-pool1-11779686-vmss000000]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-862427f0-823a-11e9-b32a-4ae2266202a5.15a338cd11e63eff], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-862427f0-823a-11e9-b32a-4ae2266202a5.15a338cd2ffbbdca], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-862427f0-823a-11e9-b32a-4ae2266202a5.15a338cd41bac4af], Reason = [Created], Message = [Created container filler-pod-862427f0-823a-11e9-b32a-4ae2266202a5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-862427f0-823a-11e9-b32a-4ae2266202a5.15a338cd502475f2], Reason = [Started], Message = [Started container filler-pod-862427f0-823a-11e9-b32a-4ae2266202a5]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15a338cdb7b4da69], Reason = [FailedScheduling], Message = [0/2 nodes are available: 1 Insufficient cpu, 1 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node k8s-pool1-11779686-vmss000000
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:52:33.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-301" for this suite.
May 29 17:52:39.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:52:39.318: INFO: namespace sched-pred-301 deletion completed in 6.096749107s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.257 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:52:39.318: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-8646
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8646
STEP: Deleting pre-stop pod
May 29 17:52:56.414: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:52:56.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8646" for this suite.
May 29 17:53:34.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:53:34.530: INFO: namespace prestop-8646 deletion completed in 38.102987856s

• [SLOW TEST:55.212 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:53:34.530: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:53:38.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8619" for this suite.
May 29 17:54:16.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:54:16.688: INFO: namespace kubelet-test-8619 deletion completed in 38.088939009s

• [SLOW TEST:42.158 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:54:16.688: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:54:16.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4754" for this suite.
May 29 17:54:38.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:54:38.834: INFO: namespace pods-4754 deletion completed in 22.084086703s

• [SLOW TEST:22.146 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:54:38.835: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
May 29 17:54:38.878: INFO: Waiting up to 5m0s for pod "pod-d40b76a6-823a-11e9-b32a-4ae2266202a5" in namespace "emptydir-8402" to be "success or failure"
May 29 17:54:38.881: INFO: Pod "pod-d40b76a6-823a-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.901676ms
May 29 17:54:40.885: INFO: Pod "pod-d40b76a6-823a-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00641623s
May 29 17:54:42.888: INFO: Pod "pod-d40b76a6-823a-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01022018s
STEP: Saw pod success
May 29 17:54:42.888: INFO: Pod "pod-d40b76a6-823a-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:54:42.890: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-d40b76a6-823a-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 17:54:42.906: INFO: Waiting for pod pod-d40b76a6-823a-11e9-b32a-4ae2266202a5 to disappear
May 29 17:54:42.908: INFO: Pod pod-d40b76a6-823a-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:54:42.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8402" for this suite.
May 29 17:54:48.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:54:49.005: INFO: namespace emptydir-8402 deletion completed in 6.093827736s

• [SLOW TEST:10.170 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:54:49.006: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 29 17:54:49.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-6196'
May 29 17:54:50.250: INFO: stderr: ""
May 29 17:54:50.250: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 17:54:50.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6196'
May 29 17:54:50.344: INFO: stderr: ""
May 29 17:54:50.344: INFO: stdout: "update-demo-nautilus-72ljt update-demo-nautilus-rfvh7 "
May 29 17:54:50.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-72ljt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6196'
May 29 17:54:50.430: INFO: stderr: ""
May 29 17:54:50.430: INFO: stdout: ""
May 29 17:54:50.430: INFO: update-demo-nautilus-72ljt is created but not running
May 29 17:54:55.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6196'
May 29 17:54:55.517: INFO: stderr: ""
May 29 17:54:55.517: INFO: stdout: "update-demo-nautilus-72ljt update-demo-nautilus-rfvh7 "
May 29 17:54:55.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-72ljt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6196'
May 29 17:54:55.589: INFO: stderr: ""
May 29 17:54:55.589: INFO: stdout: "true"
May 29 17:54:55.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-72ljt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6196'
May 29 17:54:55.667: INFO: stderr: ""
May 29 17:54:55.667: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 17:54:55.667: INFO: validating pod update-demo-nautilus-72ljt
May 29 17:54:55.674: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 17:54:55.674: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 17:54:55.674: INFO: update-demo-nautilus-72ljt is verified up and running
May 29 17:54:55.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-rfvh7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6196'
May 29 17:54:55.749: INFO: stderr: ""
May 29 17:54:55.749: INFO: stdout: "true"
May 29 17:54:55.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-rfvh7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6196'
May 29 17:54:55.824: INFO: stderr: ""
May 29 17:54:55.824: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 17:54:55.824: INFO: validating pod update-demo-nautilus-rfvh7
May 29 17:54:55.830: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 17:54:55.830: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 17:54:55.830: INFO: update-demo-nautilus-rfvh7 is verified up and running
STEP: using delete to clean up resources
May 29 17:54:55.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-6196'
May 29 17:54:55.905: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 17:54:55.905: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 29 17:54:55.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6196'
May 29 17:54:55.981: INFO: stderr: "No resources found.\n"
May 29 17:54:55.981: INFO: stdout: ""
May 29 17:54:55.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -l name=update-demo --namespace=kubectl-6196 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 17:54:56.057: INFO: stderr: ""
May 29 17:54:56.057: INFO: stdout: "update-demo-nautilus-72ljt\nupdate-demo-nautilus-rfvh7\n"
May 29 17:54:56.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6196'
May 29 17:54:56.645: INFO: stderr: "No resources found.\n"
May 29 17:54:56.645: INFO: stdout: ""
May 29 17:54:56.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -l name=update-demo --namespace=kubectl-6196 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 17:54:56.720: INFO: stderr: ""
May 29 17:54:56.720: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:54:56.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6196" for this suite.
May 29 17:55:18.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:55:18.809: INFO: namespace kubectl-6196 deletion completed in 22.08684765s

• [SLOW TEST:29.804 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:55:18.810: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-730
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-730
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-730
May 29 17:55:18.882: INFO: Found 0 stateful pods, waiting for 1
May 29 17:55:28.887: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 29 17:55:28.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-730 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 17:55:29.134: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 17:55:29.134: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 17:55:29.134: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 17:55:29.137: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 29 17:55:39.141: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 29 17:55:39.141: INFO: Waiting for statefulset status.replicas updated to 0
May 29 17:55:39.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999997s
May 29 17:55:40.156: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996884286s
May 29 17:55:41.160: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993611986s
May 29 17:55:42.163: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989911593s
May 29 17:55:43.167: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.986337921s
May 29 17:55:44.170: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982287454s
May 29 17:55:45.173: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.979206928s
May 29 17:55:46.177: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.976201322s
May 29 17:55:47.180: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.972852923s
May 29 17:55:48.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 969.510342ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-730
May 29 17:55:49.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-730 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 17:55:49.392: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 29 17:55:49.392: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 17:55:49.392: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 17:55:49.395: INFO: Found 1 stateful pods, waiting for 3
May 29 17:55:59.399: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 17:55:59.399: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 17:55:59.399: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 29 17:55:59.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-730 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 17:55:59.612: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 17:55:59.612: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 17:55:59.612: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 17:55:59.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-730 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 17:55:59.818: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 17:55:59.818: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 17:55:59.818: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 17:55:59.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-730 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 17:56:00.057: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 17:56:00.057: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 17:56:00.057: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 17:56:00.057: INFO: Waiting for statefulset status.replicas updated to 0
May 29 17:56:00.061: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 29 17:56:10.068: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 29 17:56:10.068: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 29 17:56:10.068: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 29 17:56:10.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999998s
May 29 17:56:11.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996058353s
May 29 17:56:12.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992014216s
May 29 17:56:13.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988568309s
May 29 17:56:14.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985128215s
May 29 17:56:15.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98195554s
May 29 17:56:16.098: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97877348s
May 29 17:56:17.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974612908s
May 29 17:56:18.105: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970794258s
May 29 17:56:19.109: INFO: Verifying statefulset ss doesn't scale past 3 for another 967.29803ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-730
May 29 17:56:20.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-730 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 17:56:20.334: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 29 17:56:20.334: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 17:56:20.334: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 17:56:20.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-730 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 17:56:20.552: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 29 17:56:20.552: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 17:56:20.552: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 17:56:20.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-730 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 17:56:20.759: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 29 17:56:20.759: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 17:56:20.759: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 17:56:20.759: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 17:56:50.770: INFO: Deleting all statefulset in ns statefulset-730
May 29 17:56:50.772: INFO: Scaling statefulset ss to 0
May 29 17:56:50.779: INFO: Waiting for statefulset status.replicas updated to 0
May 29 17:56:50.782: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:56:50.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-730" for this suite.
May 29 17:56:56.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:56:56.884: INFO: namespace statefulset-730 deletion completed in 6.091748158s

• [SLOW TEST:98.074 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:56:56.885: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
May 29 17:56:56.981: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-947231839 proxy --unix-socket=/tmp/kubectl-proxy-unix214954034/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:56:57.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8335" for this suite.
May 29 17:57:03.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:57:03.152: INFO: namespace kubectl-8335 deletion completed in 6.105638822s

• [SLOW TEST:6.267 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:57:03.153: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
May 29 17:57:03.198: INFO: Waiting up to 5m0s for pod "var-expansion-2a10d306-823b-11e9-b32a-4ae2266202a5" in namespace "var-expansion-5516" to be "success or failure"
May 29 17:57:03.207: INFO: Pod "var-expansion-2a10d306-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.075617ms
May 29 17:57:05.210: INFO: Pod "var-expansion-2a10d306-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012022377s
May 29 17:57:07.214: INFO: Pod "var-expansion-2a10d306-823b-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015432213s
STEP: Saw pod success
May 29 17:57:07.214: INFO: Pod "var-expansion-2a10d306-823b-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:57:07.216: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod var-expansion-2a10d306-823b-11e9-b32a-4ae2266202a5 container dapi-container: <nil>
STEP: delete the pod
May 29 17:57:07.239: INFO: Waiting for pod var-expansion-2a10d306-823b-11e9-b32a-4ae2266202a5 to disappear
May 29 17:57:07.242: INFO: Pod var-expansion-2a10d306-823b-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:57:07.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5516" for this suite.
May 29 17:57:13.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:57:13.324: INFO: namespace var-expansion-5516 deletion completed in 6.079486839s

• [SLOW TEST:10.171 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:57:13.324: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-30211e25-823b-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 17:57:13.377: INFO: Waiting up to 5m0s for pod "pod-configmaps-3021edb4-823b-11e9-b32a-4ae2266202a5" in namespace "configmap-2767" to be "success or failure"
May 29 17:57:13.389: INFO: Pod "pod-configmaps-3021edb4-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.857581ms
May 29 17:57:15.392: INFO: Pod "pod-configmaps-3021edb4-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015101771s
May 29 17:57:17.396: INFO: Pod "pod-configmaps-3021edb4-823b-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018758338s
STEP: Saw pod success
May 29 17:57:17.396: INFO: Pod "pod-configmaps-3021edb4-823b-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:57:17.398: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-3021edb4-823b-11e9-b32a-4ae2266202a5 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 17:57:17.413: INFO: Waiting for pod pod-configmaps-3021edb4-823b-11e9-b32a-4ae2266202a5 to disappear
May 29 17:57:17.415: INFO: Pod pod-configmaps-3021edb4-823b-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:57:17.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2767" for this suite.
May 29 17:57:23.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:57:23.510: INFO: namespace configmap-2767 deletion completed in 6.09155331s

• [SLOW TEST:10.185 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:57:23.510: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-363362f2-823b-11e9-b32a-4ae2266202a5
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-363362f2-823b-11e9-b32a-4ae2266202a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:57:29.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8931" for this suite.
May 29 17:57:51.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:57:51.692: INFO: namespace configmap-8931 deletion completed in 22.091336718s

• [SLOW TEST:28.182 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:57:51.693: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-47007911-823b-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 17:57:51.758: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4700f51b-823b-11e9-b32a-4ae2266202a5" in namespace "projected-8636" to be "success or failure"
May 29 17:57:51.773: INFO: Pod "pod-projected-configmaps-4700f51b-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.077087ms
May 29 17:57:53.776: INFO: Pod "pod-projected-configmaps-4700f51b-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017283285s
May 29 17:57:55.779: INFO: Pod "pod-projected-configmaps-4700f51b-823b-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020660452s
STEP: Saw pod success
May 29 17:57:55.779: INFO: Pod "pod-projected-configmaps-4700f51b-823b-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:57:55.781: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-configmaps-4700f51b-823b-11e9-b32a-4ae2266202a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 17:57:55.801: INFO: Waiting for pod pod-projected-configmaps-4700f51b-823b-11e9-b32a-4ae2266202a5 to disappear
May 29 17:57:55.803: INFO: Pod pod-projected-configmaps-4700f51b-823b-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:57:55.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8636" for this suite.
May 29 17:58:01.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:58:01.902: INFO: namespace projected-8636 deletion completed in 6.097234227s

• [SLOW TEST:10.209 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:58:01.903: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-4d154ed4-823b-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 17:58:01.956: INFO: Waiting up to 5m0s for pod "pod-secrets-4d159ed3-823b-11e9-b32a-4ae2266202a5" in namespace "secrets-1547" to be "success or failure"
May 29 17:58:01.964: INFO: Pod "pod-secrets-4d159ed3-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.526935ms
May 29 17:58:03.967: INFO: Pod "pod-secrets-4d159ed3-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011493416s
May 29 17:58:05.973: INFO: Pod "pod-secrets-4d159ed3-823b-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016923298s
STEP: Saw pod success
May 29 17:58:05.973: INFO: Pod "pod-secrets-4d159ed3-823b-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:58:05.976: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-secrets-4d159ed3-823b-11e9-b32a-4ae2266202a5 container secret-volume-test: <nil>
STEP: delete the pod
May 29 17:58:05.991: INFO: Waiting for pod pod-secrets-4d159ed3-823b-11e9-b32a-4ae2266202a5 to disappear
May 29 17:58:05.994: INFO: Pod pod-secrets-4d159ed3-823b-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:58:05.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1547" for this suite.
May 29 17:58:12.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:58:12.088: INFO: namespace secrets-1547 deletion completed in 6.090740463s

• [SLOW TEST:10.186 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:58:12.088: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:58:18.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3068" for this suite.
May 29 17:58:24.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:58:24.342: INFO: namespace namespaces-3068 deletion completed in 6.092099532s
STEP: Destroying namespace "nsdeletetest-971" for this suite.
May 29 17:58:24.344: INFO: Namespace nsdeletetest-971 was already deleted
STEP: Destroying namespace "nsdeletetest-1437" for this suite.
May 29 17:58:30.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:58:30.428: INFO: namespace nsdeletetest-1437 deletion completed in 6.083993159s

• [SLOW TEST:18.339 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:58:30.428: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 29 17:58:35.015: INFO: Successfully updated pod "labelsupdate5e1777e0-823b-11e9-b32a-4ae2266202a5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:58:37.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-317" for this suite.
May 29 17:59:05.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:59:05.125: INFO: namespace projected-317 deletion completed in 28.087472095s

• [SLOW TEST:34.696 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:59:05.125: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 17:59:05.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72c3ff94-823b-11e9-b32a-4ae2266202a5" in namespace "projected-1579" to be "success or failure"
May 29 17:59:05.178: INFO: Pod "downwardapi-volume-72c3ff94-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.499433ms
May 29 17:59:07.181: INFO: Pod "downwardapi-volume-72c3ff94-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011661694s
May 29 17:59:09.185: INFO: Pod "downwardapi-volume-72c3ff94-823b-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016239252s
STEP: Saw pod success
May 29 17:59:09.185: INFO: Pod "downwardapi-volume-72c3ff94-823b-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:59:09.188: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-72c3ff94-823b-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 17:59:09.209: INFO: Waiting for pod downwardapi-volume-72c3ff94-823b-11e9-b32a-4ae2266202a5 to disappear
May 29 17:59:09.213: INFO: Pod downwardapi-volume-72c3ff94-823b-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:59:09.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1579" for this suite.
May 29 17:59:15.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:59:15.301: INFO: namespace projected-1579 deletion completed in 6.081817913s

• [SLOW TEST:10.176 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:59:15.302: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 29 17:59:15.351: INFO: Waiting up to 5m0s for pod "pod-78d58035-823b-11e9-b32a-4ae2266202a5" in namespace "emptydir-9043" to be "success or failure"
May 29 17:59:15.362: INFO: Pod "pod-78d58035-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.492025ms
May 29 17:59:17.365: INFO: Pod "pod-78d58035-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014270827s
May 29 17:59:19.369: INFO: Pod "pod-78d58035-823b-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018060768s
STEP: Saw pod success
May 29 17:59:19.369: INFO: Pod "pod-78d58035-823b-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 17:59:19.372: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-78d58035-823b-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 17:59:19.386: INFO: Waiting for pod pod-78d58035-823b-11e9-b32a-4ae2266202a5 to disappear
May 29 17:59:19.389: INFO: Pod pod-78d58035-823b-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:59:19.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9043" for this suite.
May 29 17:59:25.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:59:25.472: INFO: namespace emptydir-9043 deletion completed in 6.080545859s

• [SLOW TEST:10.170 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:59:25.473: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
May 29 17:59:25.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 --namespace=kubectl-1730 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 29 17:59:28.260: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 29 17:59:28.260: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 17:59:30.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1730" for this suite.
May 29 17:59:36.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 17:59:36.345: INFO: namespace kubectl-1730 deletion completed in 6.078512622s

• [SLOW TEST:10.873 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 17:59:36.347: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:00:36.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2257" for this suite.
May 29 18:00:58.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:00:58.488: INFO: namespace container-probe-2257 deletion completed in 22.086331166s

• [SLOW TEST:82.141 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:00:58.489: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:00:58.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6992" for this suite.
May 29 18:01:58.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:01:58.640: INFO: namespace kubelet-test-6992 deletion completed in 1m0.081732202s

• [SLOW TEST:60.151 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:01:58.641: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-da313610-823b-11e9-b32a-4ae2266202a5
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:02:04.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-517" for this suite.
May 29 18:02:26.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:02:26.819: INFO: namespace configmap-517 deletion completed in 22.094450808s

• [SLOW TEST:28.178 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:02:26.820: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 18:02:26.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eafd203c-823b-11e9-b32a-4ae2266202a5" in namespace "projected-8075" to be "success or failure"
May 29 18:02:26.873: INFO: Pod "downwardapi-volume-eafd203c-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520233ms
May 29 18:02:28.876: INFO: Pod "downwardapi-volume-eafd203c-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006283684s
May 29 18:02:30.879: INFO: Pod "downwardapi-volume-eafd203c-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009837394s
May 29 18:02:32.883: INFO: Pod "downwardapi-volume-eafd203c-823b-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013669744s
STEP: Saw pod success
May 29 18:02:32.883: INFO: Pod "downwardapi-volume-eafd203c-823b-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:02:32.885: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-eafd203c-823b-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 18:02:32.906: INFO: Waiting for pod downwardapi-volume-eafd203c-823b-11e9-b32a-4ae2266202a5 to disappear
May 29 18:02:32.908: INFO: Pod downwardapi-volume-eafd203c-823b-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:02:32.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8075" for this suite.
May 29 18:02:38.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:02:39.001: INFO: namespace projected-8075 deletion completed in 6.090139964s

• [SLOW TEST:12.181 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:02:39.001: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-9773/configmap-test-f23f0b5b-823b-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 18:02:39.053: INFO: Waiting up to 5m0s for pod "pod-configmaps-f23f6d5a-823b-11e9-b32a-4ae2266202a5" in namespace "configmap-9773" to be "success or failure"
May 29 18:02:39.058: INFO: Pod "pod-configmaps-f23f6d5a-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.701712ms
May 29 18:02:41.062: INFO: Pod "pod-configmaps-f23f6d5a-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009709987s
May 29 18:02:43.066: INFO: Pod "pod-configmaps-f23f6d5a-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013241081s
May 29 18:02:45.069: INFO: Pod "pod-configmaps-f23f6d5a-823b-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016416099s
STEP: Saw pod success
May 29 18:02:45.069: INFO: Pod "pod-configmaps-f23f6d5a-823b-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:02:45.071: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-f23f6d5a-823b-11e9-b32a-4ae2266202a5 container env-test: <nil>
STEP: delete the pod
May 29 18:02:45.087: INFO: Waiting for pod pod-configmaps-f23f6d5a-823b-11e9-b32a-4ae2266202a5 to disappear
May 29 18:02:45.092: INFO: Pod pod-configmaps-f23f6d5a-823b-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:02:45.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9773" for this suite.
May 29 18:02:51.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:02:51.175: INFO: namespace configmap-9773 deletion completed in 6.079671264s

• [SLOW TEST:12.174 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:02:51.176: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 18:02:51.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9109'
May 29 18:02:51.320: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 18:02:51.320: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
May 29 18:02:51.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9109'
May 29 18:02:51.427: INFO: stderr: ""
May 29 18:02:51.427: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:02:51.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9109" for this suite.
May 29 18:02:57.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:02:57.533: INFO: namespace kubectl-9109 deletion completed in 6.091497685s

• [SLOW TEST:6.357 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:02:57.533: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 29 18:02:57.584: INFO: Waiting up to 5m0s for pod "downward-api-fd4acdb3-823b-11e9-b32a-4ae2266202a5" in namespace "downward-api-7060" to be "success or failure"
May 29 18:02:57.588: INFO: Pod "downward-api-fd4acdb3-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.202716ms
May 29 18:02:59.591: INFO: Pod "downward-api-fd4acdb3-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006692009s
May 29 18:03:01.595: INFO: Pod "downward-api-fd4acdb3-823b-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010647365s
May 29 18:03:03.599: INFO: Pod "downward-api-fd4acdb3-823b-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014515567s
STEP: Saw pod success
May 29 18:03:03.599: INFO: Pod "downward-api-fd4acdb3-823b-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:03:03.601: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downward-api-fd4acdb3-823b-11e9-b32a-4ae2266202a5 container dapi-container: <nil>
STEP: delete the pod
May 29 18:03:03.624: INFO: Waiting for pod downward-api-fd4acdb3-823b-11e9-b32a-4ae2266202a5 to disappear
May 29 18:03:03.626: INFO: Pod downward-api-fd4acdb3-823b-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:03:03.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7060" for this suite.
May 29 18:03:09.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:03:09.715: INFO: namespace downward-api-7060 deletion completed in 6.086415403s

• [SLOW TEST:12.182 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:03:09.716: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 29 18:03:09.765: INFO: Waiting up to 5m0s for pod "pod-048e7999-823c-11e9-b32a-4ae2266202a5" in namespace "emptydir-6162" to be "success or failure"
May 29 18:03:09.775: INFO: Pod "pod-048e7999-823c-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.891857ms
May 29 18:03:11.778: INFO: Pod "pod-048e7999-823c-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012962126s
May 29 18:03:13.782: INFO: Pod "pod-048e7999-823c-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016659173s
May 29 18:03:15.785: INFO: Pod "pod-048e7999-823c-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020134369s
STEP: Saw pod success
May 29 18:03:15.785: INFO: Pod "pod-048e7999-823c-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:03:15.788: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-048e7999-823c-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 18:03:15.807: INFO: Waiting for pod pod-048e7999-823c-11e9-b32a-4ae2266202a5 to disappear
May 29 18:03:15.809: INFO: Pod pod-048e7999-823c-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:03:15.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6162" for this suite.
May 29 18:03:21.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:03:21.896: INFO: namespace emptydir-6162 deletion completed in 6.084571969s

• [SLOW TEST:12.181 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:03:21.897: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6828
May 29 18:03:27.955: INFO: Started pod liveness-http in namespace container-probe-6828
STEP: checking the pod's current state and verifying that restartCount is present
May 29 18:03:27.958: INFO: Initial restart count of pod liveness-http is 0
May 29 18:03:45.999: INFO: Restart count of pod container-probe-6828/liveness-http is now 1 (18.041412858s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:03:46.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6828" for this suite.
May 29 18:03:52.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:03:52.118: INFO: namespace container-probe-6828 deletion completed in 6.0940291s

• [SLOW TEST:30.222 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:03:52.118: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-1dd3c96f-823c-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 18:03:52.166: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1dd41ec2-823c-11e9-b32a-4ae2266202a5" in namespace "projected-5855" to be "success or failure"
May 29 18:03:52.175: INFO: Pod "pod-projected-secrets-1dd41ec2-823c-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.391925ms
May 29 18:03:54.181: INFO: Pod "pod-projected-secrets-1dd41ec2-823c-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015376656s
May 29 18:03:56.185: INFO: Pod "pod-projected-secrets-1dd41ec2-823c-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019121188s
May 29 18:03:58.189: INFO: Pod "pod-projected-secrets-1dd41ec2-823c-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022550987s
STEP: Saw pod success
May 29 18:03:58.189: INFO: Pod "pod-projected-secrets-1dd41ec2-823c-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:03:58.191: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-secrets-1dd41ec2-823c-11e9-b32a-4ae2266202a5 container secret-volume-test: <nil>
STEP: delete the pod
May 29 18:03:58.206: INFO: Waiting for pod pod-projected-secrets-1dd41ec2-823c-11e9-b32a-4ae2266202a5 to disappear
May 29 18:03:58.208: INFO: Pod pod-projected-secrets-1dd41ec2-823c-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:03:58.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5855" for this suite.
May 29 18:04:04.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:04:04.297: INFO: namespace projected-5855 deletion completed in 6.086113165s

• [SLOW TEST:12.178 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:04:04.298: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4488
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-4488
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4488
May 29 18:04:04.358: INFO: Found 0 stateful pods, waiting for 1
May 29 18:04:14.362: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 29 18:04:14.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 18:04:14.594: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 18:04:14.594: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 18:04:14.594: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 18:04:14.597: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 29 18:04:24.601: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 29 18:04:24.601: INFO: Waiting for statefulset status.replicas updated to 0
May 29 18:04:24.611: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:24.611: INFO: ss-0  k8s-pool1-11779686-vmss000000  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:24.611: INFO: 
May 29 18:04:24.611: INFO: StatefulSet ss has not reached scale 3, at 1
May 29 18:04:25.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997581479s
May 29 18:04:26.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993966344s
May 29 18:04:27.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990248334s
May 29 18:04:28.628: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983979565s
May 29 18:04:29.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980637621s
May 29 18:04:30.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976860189s
May 29 18:04:31.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973174287s
May 29 18:04:32.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969656717s
May 29 18:04:33.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.864964ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4488
May 29 18:04:34.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:04:34.837: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 29 18:04:34.837: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 18:04:34.837: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 18:04:34.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:04:35.033: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 29 18:04:35.033: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 18:04:35.033: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 18:04:35.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:04:35.235: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 29 18:04:35.235: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 18:04:35.235: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 18:04:35.238: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:04:35.238: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:04:35.238: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 29 18:04:35.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 18:04:35.445: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 18:04:35.445: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 18:04:35.445: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 18:04:35.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 18:04:35.689: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 18:04:35.689: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 18:04:35.689: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 18:04:35.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 18:04:35.914: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 18:04:35.914: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 18:04:35.914: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 18:04:35.914: INFO: Waiting for statefulset status.replicas updated to 0
May 29 18:04:35.916: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 29 18:04:45.924: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 29 18:04:45.924: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 29 18:04:45.924: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 29 18:04:45.932: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:45.932: INFO: ss-0  k8s-pool1-11779686-vmss000000  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:45.932: INFO: ss-1  k8s-pool1-11779686-vmss000000  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:45.932: INFO: ss-2  k8s-pool1-11779686-vmss000000  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:45.932: INFO: 
May 29 18:04:45.932: INFO: StatefulSet ss has not reached scale 0, at 3
May 29 18:04:46.936: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:46.936: INFO: ss-0  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:46.936: INFO: ss-1  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:46.936: INFO: ss-2  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:46.936: INFO: 
May 29 18:04:46.936: INFO: StatefulSet ss has not reached scale 0, at 3
May 29 18:04:47.940: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:47.940: INFO: ss-0  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:47.940: INFO: ss-1  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:47.940: INFO: ss-2  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:47.940: INFO: 
May 29 18:04:47.940: INFO: StatefulSet ss has not reached scale 0, at 3
May 29 18:04:48.947: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:48.947: INFO: ss-0  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:48.947: INFO: ss-1  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:48.947: INFO: ss-2  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:48.947: INFO: 
May 29 18:04:48.947: INFO: StatefulSet ss has not reached scale 0, at 3
May 29 18:04:49.951: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:49.951: INFO: ss-0  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:49.951: INFO: ss-1  k8s-pool1-11779686-vmss000000  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:49.951: INFO: ss-2  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:49.951: INFO: 
May 29 18:04:49.951: INFO: StatefulSet ss has not reached scale 0, at 3
May 29 18:04:50.954: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:50.954: INFO: ss-0  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:50.954: INFO: ss-1  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:50.954: INFO: 
May 29 18:04:50.954: INFO: StatefulSet ss has not reached scale 0, at 2
May 29 18:04:51.958: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:51.958: INFO: ss-0  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:51.958: INFO: ss-1  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:51.958: INFO: 
May 29 18:04:51.958: INFO: StatefulSet ss has not reached scale 0, at 2
May 29 18:04:52.962: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:52.962: INFO: ss-0  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:52.962: INFO: ss-1  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:52.962: INFO: 
May 29 18:04:52.962: INFO: StatefulSet ss has not reached scale 0, at 2
May 29 18:04:53.965: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:53.965: INFO: ss-0  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:53.965: INFO: ss-1  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:53.965: INFO: 
May 29 18:04:53.965: INFO: StatefulSet ss has not reached scale 0, at 2
May 29 18:04:54.969: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 29 18:04:54.969: INFO: ss-0  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:04 +0000 UTC  }]
May 29 18:04:54.969: INFO: ss-1  k8s-pool1-11779686-vmss000000  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:04:24 +0000 UTC  }]
May 29 18:04:54.969: INFO: 
May 29 18:04:54.969: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4488
May 29 18:04:55.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:04:56.101: INFO: rc: 1
May 29 18:04:56.101: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002a83bc0 exit status 1 <nil> <nil> true [0xc001e42218 0xc001e42268 0xc001e42298] [0xc001e42218 0xc001e42268 0xc001e42298] [0xc001e42248 0xc001e42288] [0x9c00a0 0x9c00a0] 0xc001f84f00 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 29 18:05:06.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:05:06.178: INFO: rc: 1
May 29 18:05:06.178: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002785d10 exit status 1 <nil> <nil> true [0xc002ba6168 0xc002ba6180 0xc002ba6198] [0xc002ba6168 0xc002ba6180 0xc002ba6198] [0xc002ba6178 0xc002ba6190] [0x9c00a0 0x9c00a0] 0xc0022a1380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:05:16.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:05:16.252: INFO: rc: 1
May 29 18:05:16.252: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a180c0 exit status 1 <nil> <nil> true [0xc002ba61a0 0xc002ba61b8 0xc002ba61d0] [0xc002ba61a0 0xc002ba61b8 0xc002ba61d0] [0xc002ba61b0 0xc002ba61c8] [0x9c00a0 0x9c00a0] 0xc0022a16e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:05:26.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:05:26.325: INFO: rc: 1
May 29 18:05:26.325: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a18420 exit status 1 <nil> <nil> true [0xc002ba61d8 0xc002ba61f0 0xc002ba6208] [0xc002ba61d8 0xc002ba61f0 0xc002ba6208] [0xc002ba61e8 0xc002ba6200] [0x9c00a0 0x9c00a0] 0xc0022a1a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:05:36.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:05:36.395: INFO: rc: 1
May 29 18:05:36.396: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a18780 exit status 1 <nil> <nil> true [0xc002ba6210 0xc002ba6228 0xc002ba6240] [0xc002ba6210 0xc002ba6228 0xc002ba6240] [0xc002ba6220 0xc002ba6238] [0x9c00a0 0x9c00a0] 0xc0022a1da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:05:46.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:05:46.476: INFO: rc: 1
May 29 18:05:46.477: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a18b10 exit status 1 <nil> <nil> true [0xc002ba6248 0xc002ba6260 0xc002ba6278] [0xc002ba6248 0xc002ba6260 0xc002ba6278] [0xc002ba6258 0xc002ba6270] [0x9c00a0 0x9c00a0] 0xc001fbc120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:05:56.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:05:56.555: INFO: rc: 1
May 29 18:05:56.555: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a83fb0 exit status 1 <nil> <nil> true [0xc001e422a8 0xc001e422c8 0xc001e422f8] [0xc001e422a8 0xc001e422c8 0xc001e422f8] [0xc001e422c0 0xc001e422e8] [0x9c00a0 0x9c00a0] 0xc001f85260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:06:06.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:06:06.632: INFO: rc: 1
May 29 18:06:06.632: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a18ff0 exit status 1 <nil> <nil> true [0xc002ba6280 0xc002ba6298 0xc002ba62b0] [0xc002ba6280 0xc002ba6298 0xc002ba62b0] [0xc002ba6290 0xc002ba62a8] [0x9c00a0 0x9c00a0] 0xc001fbc480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:06:16.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:06:16.723: INFO: rc: 1
May 29 18:06:16.723: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a19380 exit status 1 <nil> <nil> true [0xc002ba62b8 0xc002ba62d0 0xc002ba62e8] [0xc002ba62b8 0xc002ba62d0 0xc002ba62e8] [0xc002ba62c8 0xc002ba62e0] [0x9c00a0 0x9c00a0] 0xc001fbc840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:06:26.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:06:26.813: INFO: rc: 1
May 29 18:06:26.813: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a19710 exit status 1 <nil> <nil> true [0xc002ba62f0 0xc002ba6308 0xc002ba6320] [0xc002ba62f0 0xc002ba6308 0xc002ba6320] [0xc002ba6300 0xc002ba6318] [0x9c00a0 0x9c00a0] 0xc001fbcf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:06:36.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:06:36.887: INFO: rc: 1
May 29 18:06:36.888: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000a19aa0 exit status 1 <nil> <nil> true [0xc002ba6328 0xc002ba6340 0xc002ba6358] [0xc002ba6328 0xc002ba6340 0xc002ba6358] [0xc002ba6338 0xc002ba6350] [0x9c00a0 0x9c00a0] 0xc001fbd5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:06:46.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:06:46.957: INFO: rc: 1
May 29 18:06:46.957: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002784360 exit status 1 <nil> <nil> true [0xc002ba6008 0xc002ba6020 0xc002ba6038] [0xc002ba6008 0xc002ba6020 0xc002ba6038] [0xc002ba6018 0xc002ba6030] [0x9c00a0 0x9c00a0] 0xc0022a05a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:06:56.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:06:57.035: INFO: rc: 1
May 29 18:06:57.036: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a82390 exit status 1 <nil> <nil> true [0xc001e42000 0xc001e42020 0xc001e42068] [0xc001e42000 0xc001e42020 0xc001e42068] [0xc001e42010 0xc001e42058] [0x9c00a0 0x9c00a0] 0xc001fbc2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:07:07.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:07:07.111: INFO: rc: 1
May 29 18:07:07.111: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002784840 exit status 1 <nil> <nil> true [0xc002ba6040 0xc002ba6058 0xc002ba6070] [0xc002ba6040 0xc002ba6058 0xc002ba6070] [0xc002ba6050 0xc002ba6068] [0x9c00a0 0x9c00a0] 0xc0022a0c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:07:17.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:07:17.191: INFO: rc: 1
May 29 18:07:17.191: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002784c00 exit status 1 <nil> <nil> true [0xc002ba6078 0xc002ba6090 0xc002ba60a8] [0xc002ba6078 0xc002ba6090 0xc002ba60a8] [0xc002ba6088 0xc002ba60a0] [0x9c00a0 0x9c00a0] 0xc0022a0fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:07:27.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:07:27.275: INFO: rc: 1
May 29 18:07:27.275: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a82720 exit status 1 <nil> <nil> true [0xc001e42070 0xc001e42098 0xc001e420d8] [0xc001e42070 0xc001e42098 0xc001e420d8] [0xc001e42080 0xc001e420d0] [0x9c00a0 0x9c00a0] 0xc001fbc600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:07:37.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:07:37.352: INFO: rc: 1
May 29 18:07:37.352: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002784f60 exit status 1 <nil> <nil> true [0xc002ba60b0 0xc002ba60c8 0xc002ba60e0] [0xc002ba60b0 0xc002ba60c8 0xc002ba60e0] [0xc002ba60c0 0xc002ba60d8] [0x9c00a0 0x9c00a0] 0xc0022a1320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:07:47.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:07:47.429: INFO: rc: 1
May 29 18:07:47.429: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a82ab0 exit status 1 <nil> <nil> true [0xc001e420e0 0xc001e42120 0xc001e42150] [0xc001e420e0 0xc001e42120 0xc001e42150] [0xc001e42108 0xc001e42140] [0x9c00a0 0x9c00a0] 0xc001fbcb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:07:57.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:07:57.502: INFO: rc: 1
May 29 18:07:57.502: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002785350 exit status 1 <nil> <nil> true [0xc002ba60e8 0xc002ba6100 0xc002ba6118] [0xc002ba60e8 0xc002ba6100 0xc002ba6118] [0xc002ba60f8 0xc002ba6110] [0x9c00a0 0x9c00a0] 0xc0022a1680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:08:07.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:08:07.576: INFO: rc: 1
May 29 18:08:07.576: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002785800 exit status 1 <nil> <nil> true [0xc002ba6120 0xc002ba6138 0xc002ba6150] [0xc002ba6120 0xc002ba6138 0xc002ba6150] [0xc002ba6130 0xc002ba6148] [0x9c00a0 0x9c00a0] 0xc0022a19e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:08:17.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:08:17.650: INFO: rc: 1
May 29 18:08:17.650: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002785b90 exit status 1 <nil> <nil> true [0xc002ba6158 0xc002ba6170 0xc002ba6188] [0xc002ba6158 0xc002ba6170 0xc002ba6188] [0xc002ba6168 0xc002ba6180] [0x9c00a0 0x9c00a0] 0xc0022a1d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:08:27.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:08:27.731: INFO: rc: 1
May 29 18:08:27.731: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002785f50 exit status 1 <nil> <nil> true [0xc002ba6190 0xc002ba61a8 0xc002ba61c0] [0xc002ba6190 0xc002ba61a8 0xc002ba61c0] [0xc002ba61a0 0xc002ba61b8] [0x9c00a0 0x9c00a0] 0xc001f840c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:08:37.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:08:37.811: INFO: rc: 1
May 29 18:08:37.811: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a82ea0 exit status 1 <nil> <nil> true [0xc001e42160 0xc001e421a8 0xc001e421c0] [0xc001e42160 0xc001e421a8 0xc001e421c0] [0xc001e42188 0xc001e421b8] [0x9c00a0 0x9c00a0] 0xc001fbd200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:08:47.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:08:47.905: INFO: rc: 1
May 29 18:08:47.905: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a82360 exit status 1 <nil> <nil> true [0xc001e42008 0xc001e42040 0xc001e42070] [0xc001e42008 0xc001e42040 0xc001e42070] [0xc001e42020 0xc001e42068] [0x9c00a0 0x9c00a0] 0xc0022a05a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:08:57.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:08:57.978: INFO: rc: 1
May 29 18:08:57.978: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a82750 exit status 1 <nil> <nil> true [0xc001e42078 0xc001e420b8 0xc001e420e0] [0xc001e42078 0xc001e420b8 0xc001e420e0] [0xc001e42098 0xc001e420d8] [0x9c00a0 0x9c00a0] 0xc0022a0c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:09:07.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:09:08.052: INFO: rc: 1
May 29 18:09:08.052: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a82ae0 exit status 1 <nil> <nil> true [0xc001e420e8 0xc001e42130 0xc001e42160] [0xc001e420e8 0xc001e42130 0xc001e42160] [0xc001e42120 0xc001e42150] [0x9c00a0 0x9c00a0] 0xc0022a0fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:09:18.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:09:18.132: INFO: rc: 1
May 29 18:09:18.132: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027843c0 exit status 1 <nil> <nil> true [0xc002ba6000 0xc002ba6018 0xc002ba6030] [0xc002ba6000 0xc002ba6018 0xc002ba6030] [0xc002ba6010 0xc002ba6028] [0x9c00a0 0x9c00a0] 0xc001fbc2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:09:28.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:09:28.209: INFO: rc: 1
May 29 18:09:28.209: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027848d0 exit status 1 <nil> <nil> true [0xc002ba6038 0xc002ba6050 0xc002ba6068] [0xc002ba6038 0xc002ba6050 0xc002ba6068] [0xc002ba6048 0xc002ba6060] [0x9c00a0 0x9c00a0] 0xc001fbc600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:09:38.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:09:38.285: INFO: rc: 1
May 29 18:09:38.285: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a82f00 exit status 1 <nil> <nil> true [0xc001e42170 0xc001e421b0 0xc001e421e0] [0xc001e42170 0xc001e421b0 0xc001e421e0] [0xc001e421a8 0xc001e421c0] [0x9c00a0 0x9c00a0] 0xc0022a1320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:09:48.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:09:48.358: INFO: rc: 1
May 29 18:09:48.358: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a83260 exit status 1 <nil> <nil> true [0xc001e421f8 0xc001e42238 0xc001e42280] [0xc001e421f8 0xc001e42238 0xc001e42280] [0xc001e42218 0xc001e42268] [0x9c00a0 0x9c00a0] 0xc0022a1680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 29 18:09:58.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-4488 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:09:58.431: INFO: rc: 1
May 29 18:09:58.431: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
May 29 18:09:58.431: INFO: Scaling statefulset ss to 0
May 29 18:09:58.437: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 18:09:58.440: INFO: Deleting all statefulset in ns statefulset-4488
May 29 18:09:58.442: INFO: Scaling statefulset ss to 0
May 29 18:09:58.449: INFO: Waiting for statefulset status.replicas updated to 0
May 29 18:09:58.451: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:09:58.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4488" for this suite.
May 29 18:10:04.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:10:04.565: INFO: namespace statefulset-4488 deletion completed in 6.098965444s

• [SLOW TEST:360.268 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:10:04.566: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6498.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6498.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 130.24.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.24.130_udp@PTR;check="$$(dig +tcp +noall +answer +search 130.24.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.24.130_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6498.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6498.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6498.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6498.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6498.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 130.24.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.24.130_udp@PTR;check="$$(dig +tcp +noall +answer +search 130.24.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.24.130_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 29 18:10:32.673: INFO: Unable to read wheezy_udp@dns-test-service.dns-6498.svc.cluster.local from pod dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5: the server could not find the requested resource (get pods dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5)
May 29 18:10:32.677: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6498.svc.cluster.local from pod dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5: the server could not find the requested resource (get pods dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5)
May 29 18:10:32.680: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local from pod dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5: the server could not find the requested resource (get pods dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5)
May 29 18:10:32.682: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local from pod dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5: the server could not find the requested resource (get pods dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5)
May 29 18:10:32.702: INFO: Unable to read jessie_udp@dns-test-service.dns-6498.svc.cluster.local from pod dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5: the server could not find the requested resource (get pods dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5)
May 29 18:10:32.707: INFO: Unable to read jessie_tcp@dns-test-service.dns-6498.svc.cluster.local from pod dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5: the server could not find the requested resource (get pods dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5)
May 29 18:10:32.713: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local from pod dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5: the server could not find the requested resource (get pods dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5)
May 29 18:10:32.716: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local from pod dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5: the server could not find the requested resource (get pods dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5)
May 29 18:10:32.740: INFO: Lookups using dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5 failed for: [wheezy_udp@dns-test-service.dns-6498.svc.cluster.local wheezy_tcp@dns-test-service.dns-6498.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local jessie_udp@dns-test-service.dns-6498.svc.cluster.local jessie_tcp@dns-test-service.dns-6498.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6498.svc.cluster.local]

May 29 18:10:37.797: INFO: DNS probes using dns-6498/dns-test-fbd71ece-823c-11e9-b32a-4ae2266202a5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:10:37.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6498" for this suite.
May 29 18:10:43.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:10:43.961: INFO: namespace dns-6498 deletion completed in 6.084962129s

• [SLOW TEST:39.396 seconds]
[sig-network] DNS
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:10:43.962: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 29 18:10:44.010: INFO: Waiting up to 5m0s for pod "pod-134e879d-823d-11e9-b32a-4ae2266202a5" in namespace "emptydir-559" to be "success or failure"
May 29 18:10:44.017: INFO: Pod "pod-134e879d-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.196575ms
May 29 18:10:46.020: INFO: Pod "pod-134e879d-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009329821s
May 29 18:10:48.026: INFO: Pod "pod-134e879d-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015349224s
May 29 18:10:50.036: INFO: Pod "pod-134e879d-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025741926s
May 29 18:10:52.040: INFO: Pod "pod-134e879d-823d-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029314811s
STEP: Saw pod success
May 29 18:10:52.040: INFO: Pod "pod-134e879d-823d-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:10:52.042: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-134e879d-823d-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 18:10:52.058: INFO: Waiting for pod pod-134e879d-823d-11e9-b32a-4ae2266202a5 to disappear
May 29 18:10:52.060: INFO: Pod pod-134e879d-823d-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:10:52.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-559" for this suite.
May 29 18:10:58.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:10:58.147: INFO: namespace emptydir-559 deletion completed in 6.085000507s

• [SLOW TEST:14.185 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:10:58.148: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-7601
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7601 to expose endpoints map[]
May 29 18:10:58.253: INFO: successfully validated that service endpoint-test2 in namespace services-7601 exposes endpoints map[] (5.486555ms elapsed)
STEP: Creating pod pod1 in namespace services-7601
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7601 to expose endpoints map[pod1:[80]]
May 29 18:11:02.303: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.042649553s elapsed, will retry)
May 29 18:11:03.308: INFO: successfully validated that service endpoint-test2 in namespace services-7601 exposes endpoints map[pod1:[80]] (5.04740916s elapsed)
STEP: Creating pod pod2 in namespace services-7601
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7601 to expose endpoints map[pod1:[80] pod2:[80]]
May 29 18:11:07.358: INFO: Unexpected endpoints: found map[1bcea623-823d-11e9-9319-000d3afdb8b2:[80]], expected map[pod1:[80] pod2:[80]] (4.047674478s elapsed, will retry)
May 29 18:11:08.366: INFO: successfully validated that service endpoint-test2 in namespace services-7601 exposes endpoints map[pod1:[80] pod2:[80]] (5.05576045s elapsed)
STEP: Deleting pod pod1 in namespace services-7601
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7601 to expose endpoints map[pod2:[80]]
May 29 18:11:09.390: INFO: successfully validated that service endpoint-test2 in namespace services-7601 exposes endpoints map[pod2:[80]] (1.018808969s elapsed)
STEP: Deleting pod pod2 in namespace services-7601
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7601 to expose endpoints map[]
May 29 18:11:10.399: INFO: successfully validated that service endpoint-test2 in namespace services-7601 exposes endpoints map[] (1.004980773s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:11:10.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7601" for this suite.
May 29 18:11:32.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:11:32.511: INFO: namespace services-7601 deletion completed in 22.087517615s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:34.364 seconds]
[sig-network] Services
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:11:32.512: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 29 18:11:32.573: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5699,SelfLink:/api/v1/namespaces/watch-5699/configmaps/e2e-watch-test-resource-version,UID:30400655-823d-11e9-9319-000d3afdb8b2,ResourceVersion:4922,Generation:0,CreationTimestamp:2019-05-29 18:11:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 18:11:32.573: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5699,SelfLink:/api/v1/namespaces/watch-5699/configmaps/e2e-watch-test-resource-version,UID:30400655-823d-11e9-9319-000d3afdb8b2,ResourceVersion:4923,Generation:0,CreationTimestamp:2019-05-29 18:11:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:11:32.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5699" for this suite.
May 29 18:11:38.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:11:38.657: INFO: namespace watch-5699 deletion completed in 6.081404563s

• [SLOW TEST:6.145 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:11:38.657: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 18:11:38.709: INFO: Conformance test suite needs a cluster with at least 2 nodes.
May 29 18:11:38.709: INFO: Create a RollingUpdate DaemonSet
May 29 18:11:38.712: INFO: Check that daemon pods launch on every node of the cluster
May 29 18:11:38.715: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:38.718: INFO: Number of nodes with available pods: 0
May 29 18:11:38.718: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:11:39.721: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:39.723: INFO: Number of nodes with available pods: 0
May 29 18:11:39.723: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:11:40.722: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:40.724: INFO: Number of nodes with available pods: 0
May 29 18:11:40.724: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:11:41.722: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:41.725: INFO: Number of nodes with available pods: 0
May 29 18:11:41.725: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:11:42.721: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:42.724: INFO: Number of nodes with available pods: 0
May 29 18:11:42.724: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:11:43.722: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:43.724: INFO: Number of nodes with available pods: 1
May 29 18:11:43.724: INFO: Number of running nodes: 1, number of available pods: 1
May 29 18:11:43.725: INFO: Update the DaemonSet to trigger a rollout
May 29 18:11:43.731: INFO: Updating DaemonSet daemon-set
May 29 18:11:50.744: INFO: Roll back the DaemonSet before rollout is complete
May 29 18:11:50.749: INFO: Updating DaemonSet daemon-set
May 29 18:11:50.749: INFO: Make sure DaemonSet rollback is complete
May 29 18:11:50.758: INFO: Wrong image for pod: daemon-set-f62f6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 29 18:11:50.758: INFO: Pod daemon-set-f62f6 is not available
May 29 18:11:50.760: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:51.764: INFO: Wrong image for pod: daemon-set-f62f6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 29 18:11:51.764: INFO: Pod daemon-set-f62f6 is not available
May 29 18:11:51.767: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:52.764: INFO: Wrong image for pod: daemon-set-f62f6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 29 18:11:52.764: INFO: Pod daemon-set-f62f6 is not available
May 29 18:11:52.767: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:53.764: INFO: Wrong image for pod: daemon-set-f62f6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 29 18:11:53.764: INFO: Pod daemon-set-f62f6 is not available
May 29 18:11:53.768: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:54.764: INFO: Wrong image for pod: daemon-set-f62f6. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 29 18:11:54.764: INFO: Pod daemon-set-f62f6 is not available
May 29 18:11:54.767: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:11:55.764: INFO: Pod daemon-set-xzmpc is not available
May 29 18:11:55.767: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2523, will wait for the garbage collector to delete the pods
May 29 18:11:55.832: INFO: Deleting DaemonSet.extensions daemon-set took: 5.266647ms
May 29 18:11:56.132: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.211476ms
May 29 18:11:58.835: INFO: Number of nodes with available pods: 0
May 29 18:11:58.835: INFO: Number of running nodes: 0, number of available pods: 0
May 29 18:11:58.838: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2523/daemonsets","resourceVersion":"5027"},"items":null}

May 29 18:11:58.841: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2523/pods","resourceVersion":"5027"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:11:58.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2523" for this suite.
May 29 18:12:04.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:12:04.953: INFO: namespace daemonsets-2523 deletion completed in 6.091180671s

• [SLOW TEST:26.296 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:12:04.954: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-4394ff9d-823d-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 18:12:05.003: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43956a98-823d-11e9-b32a-4ae2266202a5" in namespace "projected-5701" to be "success or failure"
May 29 18:12:05.005: INFO: Pod "pod-projected-configmaps-43956a98-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049557ms
May 29 18:12:07.010: INFO: Pod "pod-projected-configmaps-43956a98-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006510687s
May 29 18:12:09.013: INFO: Pod "pod-projected-configmaps-43956a98-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009561361s
May 29 18:12:11.016: INFO: Pod "pod-projected-configmaps-43956a98-823d-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013050128s
STEP: Saw pod success
May 29 18:12:11.016: INFO: Pod "pod-projected-configmaps-43956a98-823d-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:12:11.020: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-configmaps-43956a98-823d-11e9-b32a-4ae2266202a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 18:12:11.035: INFO: Waiting for pod pod-projected-configmaps-43956a98-823d-11e9-b32a-4ae2266202a5 to disappear
May 29 18:12:11.041: INFO: Pod pod-projected-configmaps-43956a98-823d-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:12:11.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5701" for this suite.
May 29 18:12:17.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:12:17.133: INFO: namespace projected-5701 deletion completed in 6.089171186s

• [SLOW TEST:12.180 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:12:17.134: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:12:17.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8985" for this suite.
May 29 18:12:23.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:12:23.267: INFO: namespace services-8985 deletion completed in 6.086282947s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.133 seconds]
[sig-network] Services
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:12:23.267: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
May 29 18:12:23.325: INFO: Waiting up to 5m0s for pod "client-containers-4e80b39c-823d-11e9-b32a-4ae2266202a5" in namespace "containers-4362" to be "success or failure"
May 29 18:12:23.334: INFO: Pod "client-containers-4e80b39c-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.971449ms
May 29 18:12:25.337: INFO: Pod "client-containers-4e80b39c-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012532496s
May 29 18:12:27.341: INFO: Pod "client-containers-4e80b39c-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016343334s
May 29 18:12:29.344: INFO: Pod "client-containers-4e80b39c-823d-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019779744s
STEP: Saw pod success
May 29 18:12:29.344: INFO: Pod "client-containers-4e80b39c-823d-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:12:29.347: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod client-containers-4e80b39c-823d-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 18:12:29.363: INFO: Waiting for pod client-containers-4e80b39c-823d-11e9-b32a-4ae2266202a5 to disappear
May 29 18:12:29.369: INFO: Pod client-containers-4e80b39c-823d-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:12:29.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4362" for this suite.
May 29 18:12:35.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:12:35.465: INFO: namespace containers-4362 deletion completed in 6.09311753s

• [SLOW TEST:12.198 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:12:35.466: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 29 18:12:51.547: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:12:51.554: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:12:53.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:12:53.558: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:12:55.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:12:55.558: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:12:57.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:12:57.558: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:12:59.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:12:59.558: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:13:01.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:13:01.558: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:13:03.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:13:03.558: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:13:05.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:13:05.558: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:13:07.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:13:07.557: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:13:09.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:13:09.558: INFO: Pod pod-with-prestop-exec-hook still exists
May 29 18:13:11.554: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 29 18:13:11.560: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:13:11.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5109" for this suite.
May 29 18:13:33.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:13:33.658: INFO: namespace container-lifecycle-hook-5109 deletion completed in 22.088151003s

• [SLOW TEST:58.192 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:13:33.658: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-ccjh
STEP: Creating a pod to test atomic-volume-subpath
May 29 18:13:33.712: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ccjh" in namespace "subpath-9930" to be "success or failure"
May 29 18:13:33.720: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.34913ms
May 29 18:13:35.724: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012296476s
May 29 18:13:37.728: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015678093s
May 29 18:13:39.731: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 6.019214902s
May 29 18:13:41.737: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 8.025221167s
May 29 18:13:43.741: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 10.02908096s
May 29 18:13:45.744: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 12.032609532s
May 29 18:13:47.749: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 14.036905913s
May 29 18:13:49.752: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 16.040247656s
May 29 18:13:51.756: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 18.0440618s
May 29 18:13:53.760: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 20.048104038s
May 29 18:13:55.763: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 22.051348842s
May 29 18:13:57.767: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Running", Reason="", readiness=true. Elapsed: 24.055229553s
May 29 18:13:59.771: INFO: Pod "pod-subpath-test-configmap-ccjh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.058628538s
STEP: Saw pod success
May 29 18:13:59.771: INFO: Pod "pod-subpath-test-configmap-ccjh" satisfied condition "success or failure"
May 29 18:13:59.773: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-subpath-test-configmap-ccjh container test-container-subpath-configmap-ccjh: <nil>
STEP: delete the pod
May 29 18:13:59.795: INFO: Waiting for pod pod-subpath-test-configmap-ccjh to disappear
May 29 18:13:59.797: INFO: Pod pod-subpath-test-configmap-ccjh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ccjh
May 29 18:13:59.797: INFO: Deleting pod "pod-subpath-test-configmap-ccjh" in namespace "subpath-9930"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:13:59.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9930" for this suite.
May 29 18:14:05.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:14:05.894: INFO: namespace subpath-9930 deletion completed in 6.091397114s

• [SLOW TEST:32.236 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:14:05.894: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 18:14:05.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bab21e2-823d-11e9-b32a-4ae2266202a5" in namespace "downward-api-4762" to be "success or failure"
May 29 18:14:05.951: INFO: Pod "downwardapi-volume-8bab21e2-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034811ms
May 29 18:14:07.954: INFO: Pod "downwardapi-volume-8bab21e2-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007589254s
May 29 18:14:09.958: INFO: Pod "downwardapi-volume-8bab21e2-823d-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01091758s
May 29 18:14:11.962: INFO: Pod "downwardapi-volume-8bab21e2-823d-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015372926s
STEP: Saw pod success
May 29 18:14:11.962: INFO: Pod "downwardapi-volume-8bab21e2-823d-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:14:11.965: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-8bab21e2-823d-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 18:14:11.983: INFO: Waiting for pod downwardapi-volume-8bab21e2-823d-11e9-b32a-4ae2266202a5 to disappear
May 29 18:14:11.985: INFO: Pod downwardapi-volume-8bab21e2-823d-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:14:11.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4762" for this suite.
May 29 18:14:18.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:14:18.080: INFO: namespace downward-api-4762 deletion completed in 6.08813892s

• [SLOW TEST:12.186 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:14:18.081: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5
May 29 18:14:18.128: INFO: Pod name my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5: Found 0 pods out of 1
May 29 18:14:23.131: INFO: Pod name my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5: Found 1 pods out of 1
May 29 18:14:23.131: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5" are running
May 29 18:14:25.138: INFO: Pod "my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5-gdx2g" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 18:14:18 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 18:14:18 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 18:14:18 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 18:14:18 +0000 UTC Reason: Message:}])
May 29 18:14:25.138: INFO: Trying to dial the pod
May 29 18:14:30.150: INFO: Controller my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5: Got expected result from replica 1 [my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5-gdx2g]: "my-hostname-basic-92ee5063-823d-11e9-b32a-4ae2266202a5-gdx2g", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:14:30.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9085" for this suite.
May 29 18:14:36.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:14:36.242: INFO: namespace replication-controller-9085 deletion completed in 6.088509743s

• [SLOW TEST:18.161 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:14:36.242: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0529 18:14:42.309681      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 18:14:42.309: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:14:42.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9512" for this suite.
May 29 18:14:48.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:14:48.402: INFO: namespace gc-9512 deletion completed in 6.090126006s

• [SLOW TEST:12.160 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:14:48.402: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-a5021249-823d-11e9-b32a-4ae2266202a5
STEP: Creating configMap with name cm-test-opt-upd-a5021284-823d-11e9-b32a-4ae2266202a5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a5021249-823d-11e9-b32a-4ae2266202a5
STEP: Updating configmap cm-test-opt-upd-a5021284-823d-11e9-b32a-4ae2266202a5
STEP: Creating configMap with name cm-test-opt-create-a5021297-823d-11e9-b32a-4ae2266202a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:16:06.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6752" for this suite.
May 29 18:16:28.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:16:28.896: INFO: namespace configmap-6752 deletion completed in 22.095327014s

• [SLOW TEST:100.494 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:16:28.896: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 29 18:16:28.943: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-a,UID:e0e96744-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5779,Generation:0,CreationTimestamp:2019-05-29 18:16:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 18:16:28.943: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-a,UID:e0e96744-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5779,Generation:0,CreationTimestamp:2019-05-29 18:16:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 29 18:16:38.951: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-a,UID:e0e96744-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5793,Generation:0,CreationTimestamp:2019-05-29 18:16:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 29 18:16:38.951: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-a,UID:e0e96744-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5793,Generation:0,CreationTimestamp:2019-05-29 18:16:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 29 18:16:48.957: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-a,UID:e0e96744-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5811,Generation:0,CreationTimestamp:2019-05-29 18:16:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 18:16:48.957: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-a,UID:e0e96744-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5811,Generation:0,CreationTimestamp:2019-05-29 18:16:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 29 18:16:58.962: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-a,UID:e0e96744-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5825,Generation:0,CreationTimestamp:2019-05-29 18:16:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 18:16:58.962: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-a,UID:e0e96744-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5825,Generation:0,CreationTimestamp:2019-05-29 18:16:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 29 18:17:08.967: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-b,UID:f8c4445d-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5838,Generation:0,CreationTimestamp:2019-05-29 18:17:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 18:17:08.967: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-b,UID:f8c4445d-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5838,Generation:0,CreationTimestamp:2019-05-29 18:17:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 29 18:17:18.974: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-b,UID:f8c4445d-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5851,Generation:0,CreationTimestamp:2019-05-29 18:17:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 18:17:18.974: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4451,SelfLink:/api/v1/namespaces/watch-4451/configmaps/e2e-watch-test-configmap-b,UID:f8c4445d-823d-11e9-9319-000d3afdb8b2,ResourceVersion:5851,Generation:0,CreationTimestamp:2019-05-29 18:17:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:17:28.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4451" for this suite.
May 29 18:17:34.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:17:35.061: INFO: namespace watch-4451 deletion completed in 6.083391047s

• [SLOW TEST:66.165 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:17:35.061: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 29 18:17:35.110: INFO: Pod name pod-release: Found 0 pods out of 1
May 29 18:17:40.114: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:17:41.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3388" for this suite.
May 29 18:17:47.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:17:47.229: INFO: namespace replication-controller-3388 deletion completed in 6.094654231s

• [SLOW TEST:12.168 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:17:47.229: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 29 18:17:53.805: INFO: Successfully updated pod "annotationupdate0f9736ae-823e-11e9-b32a-4ae2266202a5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:17:55.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9103" for this suite.
May 29 18:18:17.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:18:17.915: INFO: namespace downward-api-9103 deletion completed in 22.088634819s

• [SLOW TEST:30.686 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:18:17.916: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 18:18:17.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2010'
May 29 18:18:20.047: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 18:18:20.047: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
May 29 18:18:20.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete jobs e2e-test-nginx-job --namespace=kubectl-2010'
May 29 18:18:20.147: INFO: stderr: ""
May 29 18:18:20.147: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:18:20.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2010" for this suite.
May 29 18:18:42.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:18:42.239: INFO: namespace kubectl-2010 deletion completed in 22.089198086s

• [SLOW TEST:24.323 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:18:42.239: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 18:18:42.292: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 29 18:18:42.298: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:42.301: INFO: Number of nodes with available pods: 0
May 29 18:18:42.301: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:43.305: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:43.307: INFO: Number of nodes with available pods: 0
May 29 18:18:43.308: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:44.305: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:44.308: INFO: Number of nodes with available pods: 0
May 29 18:18:44.308: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:45.305: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:45.308: INFO: Number of nodes with available pods: 0
May 29 18:18:45.308: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:46.306: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:46.308: INFO: Number of nodes with available pods: 0
May 29 18:18:46.308: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:47.307: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:47.309: INFO: Number of nodes with available pods: 1
May 29 18:18:47.309: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 29 18:18:47.333: INFO: Wrong image for pod: daemon-set-sf6fv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 29 18:18:47.341: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:48.345: INFO: Wrong image for pod: daemon-set-sf6fv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 29 18:18:48.348: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:49.345: INFO: Wrong image for pod: daemon-set-sf6fv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 29 18:18:49.349: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:50.345: INFO: Wrong image for pod: daemon-set-sf6fv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 29 18:18:50.348: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:51.345: INFO: Wrong image for pod: daemon-set-sf6fv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 29 18:18:51.349: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:52.345: INFO: Wrong image for pod: daemon-set-sf6fv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 29 18:18:52.345: INFO: Pod daemon-set-sf6fv is not available
May 29 18:18:52.348: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:53.346: INFO: Pod daemon-set-t7vmf is not available
May 29 18:18:53.350: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 29 18:18:53.358: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:53.361: INFO: Number of nodes with available pods: 0
May 29 18:18:53.361: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:54.365: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:54.367: INFO: Number of nodes with available pods: 0
May 29 18:18:54.367: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:55.365: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:55.367: INFO: Number of nodes with available pods: 0
May 29 18:18:55.367: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:56.365: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:56.367: INFO: Number of nodes with available pods: 0
May 29 18:18:56.368: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:57.370: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:57.373: INFO: Number of nodes with available pods: 0
May 29 18:18:57.373: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:18:58.365: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:18:58.367: INFO: Number of nodes with available pods: 1
May 29 18:18:58.367: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9340, will wait for the garbage collector to delete the pods
May 29 18:18:58.437: INFO: Deleting DaemonSet.extensions daemon-set took: 6.065771ms
May 29 18:18:58.737: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.205638ms
May 29 18:19:09.940: INFO: Number of nodes with available pods: 0
May 29 18:19:09.940: INFO: Number of running nodes: 0, number of available pods: 0
May 29 18:19:09.942: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9340/daemonsets","resourceVersion":"6161"},"items":null}

May 29 18:19:09.944: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9340/pods","resourceVersion":"6161"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:19:09.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9340" for this suite.
May 29 18:19:15.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:19:16.040: INFO: namespace daemonsets-9340 deletion completed in 6.086470584s

• [SLOW TEST:33.801 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:19:16.041: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-448ea650-823e-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 18:19:16.136: INFO: Waiting up to 5m0s for pod "pod-secrets-448f0496-823e-11e9-b32a-4ae2266202a5" in namespace "secrets-2448" to be "success or failure"
May 29 18:19:16.140: INFO: Pod "pod-secrets-448f0496-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28519ms
May 29 18:19:18.144: INFO: Pod "pod-secrets-448f0496-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008368117s
May 29 18:19:20.148: INFO: Pod "pod-secrets-448f0496-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01239513s
May 29 18:19:22.152: INFO: Pod "pod-secrets-448f0496-823e-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016198718s
STEP: Saw pod success
May 29 18:19:22.152: INFO: Pod "pod-secrets-448f0496-823e-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:19:22.154: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-secrets-448f0496-823e-11e9-b32a-4ae2266202a5 container secret-env-test: <nil>
STEP: delete the pod
May 29 18:19:22.175: INFO: Waiting for pod pod-secrets-448f0496-823e-11e9-b32a-4ae2266202a5 to disappear
May 29 18:19:22.177: INFO: Pod pod-secrets-448f0496-823e-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:19:22.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2448" for this suite.
May 29 18:19:28.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:19:28.263: INFO: namespace secrets-2448 deletion completed in 6.083363648s

• [SLOW TEST:12.222 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:19:28.263: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-4bd06284-823e-11e9-b32a-4ae2266202a5
STEP: Creating configMap with name cm-test-opt-upd-4bd062bd-823e-11e9-b32a-4ae2266202a5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4bd06284-823e-11e9-b32a-4ae2266202a5
STEP: Updating configmap cm-test-opt-upd-4bd062bd-823e-11e9-b32a-4ae2266202a5
STEP: Creating configMap with name cm-test-opt-create-4bd062cf-823e-11e9-b32a-4ae2266202a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:19:40.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7239" for this suite.
May 29 18:20:02.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:20:02.482: INFO: namespace projected-7239 deletion completed in 22.086934826s

• [SLOW TEST:34.219 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:20:02.482: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
May 29 18:20:02.529: INFO: Waiting up to 5m0s for pod "pod-6035e17b-823e-11e9-b32a-4ae2266202a5" in namespace "emptydir-1453" to be "success or failure"
May 29 18:20:02.531: INFO: Pod "pod-6035e17b-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.721817ms
May 29 18:20:04.536: INFO: Pod "pod-6035e17b-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007604161s
May 29 18:20:06.540: INFO: Pod "pod-6035e17b-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011058739s
May 29 18:20:08.543: INFO: Pod "pod-6035e17b-823e-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01465322s
STEP: Saw pod success
May 29 18:20:08.543: INFO: Pod "pod-6035e17b-823e-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:20:08.546: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-6035e17b-823e-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 18:20:08.560: INFO: Waiting for pod pod-6035e17b-823e-11e9-b32a-4ae2266202a5 to disappear
May 29 18:20:08.563: INFO: Pod pod-6035e17b-823e-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:20:08.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1453" for this suite.
May 29 18:20:14.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:20:14.649: INFO: namespace emptydir-1453 deletion completed in 6.082968006s

• [SLOW TEST:12.167 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:20:14.649: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 18:20:14.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6071'
May 29 18:20:14.793: INFO: stderr: ""
May 29 18:20:14.793: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
May 29 18:20:14.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete pods e2e-test-nginx-pod --namespace=kubectl-6071'
May 29 18:20:21.071: INFO: stderr: ""
May 29 18:20:21.071: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:20:21.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6071" for this suite.
May 29 18:20:27.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:20:27.161: INFO: namespace kubectl-6071 deletion completed in 6.087138266s

• [SLOW TEST:12.512 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:20:27.162: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 18:20:27.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6eebd0c7-823e-11e9-b32a-4ae2266202a5" in namespace "downward-api-8136" to be "success or failure"
May 29 18:20:27.212: INFO: Pod "downwardapi-volume-6eebd0c7-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111731ms
May 29 18:20:29.215: INFO: Pod "downwardapi-volume-6eebd0c7-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006517962s
May 29 18:20:31.218: INFO: Pod "downwardapi-volume-6eebd0c7-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010008798s
May 29 18:20:33.222: INFO: Pod "downwardapi-volume-6eebd0c7-823e-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014039557s
STEP: Saw pod success
May 29 18:20:33.223: INFO: Pod "downwardapi-volume-6eebd0c7-823e-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:20:33.225: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-6eebd0c7-823e-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 18:20:33.243: INFO: Waiting for pod downwardapi-volume-6eebd0c7-823e-11e9-b32a-4ae2266202a5 to disappear
May 29 18:20:33.245: INFO: Pod downwardapi-volume-6eebd0c7-823e-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:20:33.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8136" for this suite.
May 29 18:20:39.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:20:39.335: INFO: namespace downward-api-8136 deletion completed in 6.087204149s

• [SLOW TEST:12.173 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:20:39.335: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 18:20:39.377: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 29 18:20:39.388: INFO: Pod name sample-pod: Found 0 pods out of 1
May 29 18:20:44.391: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 29 18:20:46.399: INFO: Creating deployment "test-rolling-update-deployment"
May 29 18:20:46.403: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 29 18:20:46.411: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 29 18:20:48.420: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 29 18:20:48.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694750846, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694750846, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694750846, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694750846, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 18:20:50.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694750846, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694750846, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694750846, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694750846, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 18:20:52.425: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 18:20:52.433: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2048,SelfLink:/apis/apps/v1/namespaces/deployment-2048/deployments/test-rolling-update-deployment,UID:7a5e4dcf-823e-11e9-9319-000d3afdb8b2,ResourceVersion:6508,Generation:1,CreationTimestamp:2019-05-29 18:20:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-29 18:20:46 +0000 UTC 2019-05-29 18:20:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-29 18:20:50 +0000 UTC 2019-05-29 18:20:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 29 18:20:52.436: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-2048,SelfLink:/apis/apps/v1/namespaces/deployment-2048/replicasets/test-rolling-update-deployment-67599b4d9,UID:7a610215-823e-11e9-9319-000d3afdb8b2,ResourceVersion:6497,Generation:1,CreationTimestamp:2019-05-29 18:20:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 7a5e4dcf-823e-11e9-9319-000d3afdb8b2 0xc0027e1d20 0xc0027e1d21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 29 18:20:52.436: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 29 18:20:52.437: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2048,SelfLink:/apis/apps/v1/namespaces/deployment-2048/replicasets/test-rolling-update-controller,UID:762ec438-823e-11e9-9319-000d3afdb8b2,ResourceVersion:6507,Generation:2,CreationTimestamp:2019-05-29 18:20:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 7a5e4dcf-823e-11e9-9319-000d3afdb8b2 0xc0027e1c3f 0xc0027e1c60}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 18:20:52.440: INFO: Pod "test-rolling-update-deployment-67599b4d9-8bdws" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-8bdws,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-2048,SelfLink:/api/v1/namespaces/deployment-2048/pods/test-rolling-update-deployment-67599b4d9-8bdws,UID:7a61dc10-823e-11e9-9319-000d3afdb8b2,ResourceVersion:6496,Generation:0,CreationTimestamp:2019-05-29 18:20:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 7a610215-823e-11e9-9319-000d3afdb8b2 0xc002d4a5b0 0xc002d4a5b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r2x8g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r2x8g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-r2x8g true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d4a610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d4a630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:20:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:20:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:20:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:20:46 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.42,StartTime:2019-05-29 18:20:46 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-29 18:20:49 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://0003db2eb14aa8c78a9996f497abb2ddf0164c4dc06386c241f389cb440a7e2c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:20:52.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2048" for this suite.
May 29 18:20:58.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:20:58.532: INFO: namespace deployment-2048 deletion completed in 6.088380914s

• [SLOW TEST:19.196 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:20:58.532: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5706
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 29 18:20:58.623: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 29 18:21:22.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.50:8080/dial?request=hostName&protocol=udp&host=10.240.0.41&port=8081&tries=1'] Namespace:pod-network-test-5706 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 18:21:22.677: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 18:21:22.806: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:21:22.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5706" for this suite.
May 29 18:21:44.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:21:44.897: INFO: namespace pod-network-test-5706 deletion completed in 22.087567825s

• [SLOW TEST:46.365 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:21:44.898: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-9d423b27-823e-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 18:21:44.953: INFO: Waiting up to 5m0s for pod "pod-secrets-9d429ecf-823e-11e9-b32a-4ae2266202a5" in namespace "secrets-9711" to be "success or failure"
May 29 18:21:44.959: INFO: Pod "pod-secrets-9d429ecf-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.243254ms
May 29 18:21:46.963: INFO: Pod "pod-secrets-9d429ecf-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009930332s
May 29 18:21:48.967: INFO: Pod "pod-secrets-9d429ecf-823e-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013349815s
May 29 18:21:50.970: INFO: Pod "pod-secrets-9d429ecf-823e-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0164302s
STEP: Saw pod success
May 29 18:21:50.970: INFO: Pod "pod-secrets-9d429ecf-823e-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:21:50.972: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-secrets-9d429ecf-823e-11e9-b32a-4ae2266202a5 container secret-volume-test: <nil>
STEP: delete the pod
May 29 18:21:50.995: INFO: Waiting for pod pod-secrets-9d429ecf-823e-11e9-b32a-4ae2266202a5 to disappear
May 29 18:21:50.997: INFO: Pod pod-secrets-9d429ecf-823e-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:21:50.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9711" for this suite.
May 29 18:21:57.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:21:57.089: INFO: namespace secrets-9711 deletion completed in 6.089540384s

• [SLOW TEST:12.191 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:21:57.090: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-1310
May 29 18:22:03.143: INFO: Started pod liveness-http in namespace container-probe-1310
STEP: checking the pod's current state and verifying that restartCount is present
May 29 18:22:03.145: INFO: Initial restart count of pod liveness-http is 0
May 29 18:22:19.179: INFO: Restart count of pod container-probe-1310/liveness-http is now 1 (16.034541686s elapsed)
May 29 18:22:37.214: INFO: Restart count of pod container-probe-1310/liveness-http is now 2 (34.068755656s elapsed)
May 29 18:22:57.259: INFO: Restart count of pod container-probe-1310/liveness-http is now 3 (54.114362484s elapsed)
May 29 18:23:17.299: INFO: Restart count of pod container-probe-1310/liveness-http is now 4 (1m14.154060379s elapsed)
May 29 18:24:29.438: INFO: Restart count of pod container-probe-1310/liveness-http is now 5 (2m26.293117415s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:24:29.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1310" for this suite.
May 29 18:24:35.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:24:35.536: INFO: namespace container-probe-1310 deletion completed in 6.081128466s

• [SLOW TEST:158.446 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:24:35.536: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-3578
May 29 18:24:41.588: INFO: Started pod liveness-exec in namespace container-probe-3578
STEP: checking the pod's current state and verifying that restartCount is present
May 29 18:24:41.591: INFO: Initial restart count of pod liveness-exec is 0
May 29 18:25:31.706: INFO: Restart count of pod container-probe-3578/liveness-exec is now 1 (50.114633489s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:25:31.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3578" for this suite.
May 29 18:25:37.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:25:37.798: INFO: namespace container-probe-3578 deletion completed in 6.077696188s

• [SLOW TEST:62.261 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:25:37.798: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 29 18:25:43.858: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-2812b300-823f-11e9-b32a-4ae2266202a5,GenerateName:,Namespace:events-1203,SelfLink:/api/v1/namespaces/events-1203/pods/send-events-2812b300-823f-11e9-b32a-4ae2266202a5,UID:28146a57-823f-11e9-9319-000d3afdb8b2,ResourceVersion:7114,Generation:0,CreationTimestamp:2019-05-29 18:25:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 837545594,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nf77n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nf77n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-nf77n true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000970bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000970bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:25:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:25:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:25:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 18:25:37 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.63,StartTime:2019-05-29 18:25:37 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-29 18:25:41 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://e3e214b1abdd899a212352f3518e8e40a8d3b020757956e3a6e59b2a4666fdc5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 29 18:25:45.864: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 29 18:25:47.868: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:25:47.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1203" for this suite.
May 29 18:26:31.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:26:31.972: INFO: namespace events-1203 deletion completed in 44.090532697s

• [SLOW TEST:54.174 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:26:31.972: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 29 18:26:38.614: INFO: Successfully updated pod "labelsupdate48646718-823f-11e9-b32a-4ae2266202a5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:26:40.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1255" for this suite.
May 29 18:27:02.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:27:02.735: INFO: namespace downward-api-1255 deletion completed in 22.102603533s

• [SLOW TEST:30.764 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:27:02.736: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0529 18:27:03.824941      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 18:27:03.824: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:27:03.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9719" for this suite.
May 29 18:27:09.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:27:09.909: INFO: namespace gc-9719 deletion completed in 6.081956399s

• [SLOW TEST:7.173 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:27:09.909: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 29 18:27:09.955: INFO: Waiting up to 5m0s for pod "pod-5ef9f5fe-823f-11e9-b32a-4ae2266202a5" in namespace "emptydir-2336" to be "success or failure"
May 29 18:27:09.963: INFO: Pod "pod-5ef9f5fe-823f-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.551903ms
May 29 18:27:11.967: INFO: Pod "pod-5ef9f5fe-823f-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01193036s
May 29 18:27:13.970: INFO: Pod "pod-5ef9f5fe-823f-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015565481s
May 29 18:27:15.974: INFO: Pod "pod-5ef9f5fe-823f-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019296761s
STEP: Saw pod success
May 29 18:27:15.974: INFO: Pod "pod-5ef9f5fe-823f-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:27:15.976: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-5ef9f5fe-823f-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 18:27:15.992: INFO: Waiting for pod pod-5ef9f5fe-823f-11e9-b32a-4ae2266202a5 to disappear
May 29 18:27:15.994: INFO: Pod pod-5ef9f5fe-823f-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:27:15.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2336" for this suite.
May 29 18:27:22.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:27:22.091: INFO: namespace emptydir-2336 deletion completed in 6.094594418s

• [SLOW TEST:12.183 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:27:22.092: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 29 18:27:22.161: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:22.163: INFO: Number of nodes with available pods: 0
May 29 18:27:22.163: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:23.168: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:23.170: INFO: Number of nodes with available pods: 0
May 29 18:27:23.170: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:24.168: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:24.170: INFO: Number of nodes with available pods: 0
May 29 18:27:24.170: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:25.168: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:25.171: INFO: Number of nodes with available pods: 0
May 29 18:27:25.171: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:26.169: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:26.171: INFO: Number of nodes with available pods: 0
May 29 18:27:26.171: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:27.168: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:27.173: INFO: Number of nodes with available pods: 1
May 29 18:27:27.173: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 29 18:27:27.187: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:27.197: INFO: Number of nodes with available pods: 0
May 29 18:27:27.197: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:28.200: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:28.202: INFO: Number of nodes with available pods: 0
May 29 18:27:28.202: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:29.201: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:29.204: INFO: Number of nodes with available pods: 0
May 29 18:27:29.204: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:30.200: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:30.202: INFO: Number of nodes with available pods: 0
May 29 18:27:30.203: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:31.204: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:31.206: INFO: Number of nodes with available pods: 0
May 29 18:27:31.206: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:27:32.200: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:27:32.203: INFO: Number of nodes with available pods: 1
May 29 18:27:32.203: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7786, will wait for the garbage collector to delete the pods
May 29 18:27:32.270: INFO: Deleting DaemonSet.extensions daemon-set took: 5.616898ms
May 29 18:27:32.570: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.257478ms
May 29 18:27:39.874: INFO: Number of nodes with available pods: 0
May 29 18:27:39.874: INFO: Number of running nodes: 0, number of available pods: 0
May 29 18:27:39.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7786/daemonsets","resourceVersion":"7421"},"items":null}

May 29 18:27:39.878: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7786/pods","resourceVersion":"7421"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:27:39.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7786" for this suite.
May 29 18:27:45.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:27:45.971: INFO: namespace daemonsets-7786 deletion completed in 6.085736337s

• [SLOW TEST:23.879 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:27:45.971: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-6f6h
STEP: Creating a pod to test atomic-volume-subpath
May 29 18:27:46.022: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6f6h" in namespace "subpath-9584" to be "success or failure"
May 29 18:27:46.024: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052872ms
May 29 18:27:48.028: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005751757s
May 29 18:27:50.032: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009313496s
May 29 18:27:52.035: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 6.012715688s
May 29 18:27:54.039: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 8.016629255s
May 29 18:27:56.042: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 10.019800456s
May 29 18:27:58.046: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 12.023186022s
May 29 18:28:00.050: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 14.027300873s
May 29 18:28:02.053: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 16.030566454s
May 29 18:28:04.057: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 18.034267309s
May 29 18:28:06.060: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 20.037712214s
May 29 18:28:08.063: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 22.041069776s
May 29 18:28:10.068: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Running", Reason="", readiness=true. Elapsed: 24.045536736s
May 29 18:28:12.072: INFO: Pod "pod-subpath-test-downwardapi-6f6h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.049995056s
STEP: Saw pod success
May 29 18:28:12.072: INFO: Pod "pod-subpath-test-downwardapi-6f6h" satisfied condition "success or failure"
May 29 18:28:12.075: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-subpath-test-downwardapi-6f6h container test-container-subpath-downwardapi-6f6h: <nil>
STEP: delete the pod
May 29 18:28:12.096: INFO: Waiting for pod pod-subpath-test-downwardapi-6f6h to disappear
May 29 18:28:12.099: INFO: Pod pod-subpath-test-downwardapi-6f6h no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-6f6h
May 29 18:28:12.099: INFO: Deleting pod "pod-subpath-test-downwardapi-6f6h" in namespace "subpath-9584"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:28:12.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9584" for this suite.
May 29 18:28:18.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:28:18.204: INFO: namespace subpath-9584 deletion completed in 6.100497446s

• [SLOW TEST:32.233 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:28:18.204: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-87b68f97-823f-11e9-b32a-4ae2266202a5
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:28:18.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6859" for this suite.
May 29 18:28:24.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:28:24.385: INFO: namespace configmap-6859 deletion completed in 6.086279681s

• [SLOW TEST:6.181 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:28:24.386: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 18:28:30.462: INFO: Waiting up to 5m0s for pod "client-envvars-8ef68350-823f-11e9-b32a-4ae2266202a5" in namespace "pods-6437" to be "success or failure"
May 29 18:28:30.473: INFO: Pod "client-envvars-8ef68350-823f-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.621268ms
May 29 18:28:32.478: INFO: Pod "client-envvars-8ef68350-823f-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015017685s
May 29 18:28:34.481: INFO: Pod "client-envvars-8ef68350-823f-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018496333s
May 29 18:28:36.485: INFO: Pod "client-envvars-8ef68350-823f-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022465459s
STEP: Saw pod success
May 29 18:28:36.485: INFO: Pod "client-envvars-8ef68350-823f-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:28:36.488: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod client-envvars-8ef68350-823f-11e9-b32a-4ae2266202a5 container env3cont: <nil>
STEP: delete the pod
May 29 18:28:36.508: INFO: Waiting for pod client-envvars-8ef68350-823f-11e9-b32a-4ae2266202a5 to disappear
May 29 18:28:36.510: INFO: Pod client-envvars-8ef68350-823f-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:28:36.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6437" for this suite.
May 29 18:29:20.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:29:20.601: INFO: namespace pods-6437 deletion completed in 44.087145368s

• [SLOW TEST:56.215 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:29:20.601: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 18:29:20.648: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ace060b1-823f-11e9-b32a-4ae2266202a5" in namespace "projected-6297" to be "success or failure"
May 29 18:29:20.654: INFO: Pod "downwardapi-volume-ace060b1-823f-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.973804ms
May 29 18:29:22.658: INFO: Pod "downwardapi-volume-ace060b1-823f-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009545575s
May 29 18:29:24.662: INFO: Pod "downwardapi-volume-ace060b1-823f-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01311271s
May 29 18:29:26.666: INFO: Pod "downwardapi-volume-ace060b1-823f-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017164928s
STEP: Saw pod success
May 29 18:29:26.666: INFO: Pod "downwardapi-volume-ace060b1-823f-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:29:26.668: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-ace060b1-823f-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 18:29:26.684: INFO: Waiting for pod downwardapi-volume-ace060b1-823f-11e9-b32a-4ae2266202a5 to disappear
May 29 18:29:26.686: INFO: Pod downwardapi-volume-ace060b1-823f-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:29:26.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6297" for this suite.
May 29 18:29:32.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:29:32.776: INFO: namespace projected-6297 deletion completed in 6.086736486s

• [SLOW TEST:12.174 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:29:32.776: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 29 18:29:32.837: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4389,SelfLink:/api/v1/namespaces/watch-4389/configmaps/e2e-watch-test-label-changed,UID:b423ed87-823f-11e9-9319-000d3afdb8b2,ResourceVersion:7720,Generation:0,CreationTimestamp:2019-05-29 18:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 18:29:32.837: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4389,SelfLink:/api/v1/namespaces/watch-4389/configmaps/e2e-watch-test-label-changed,UID:b423ed87-823f-11e9-9319-000d3afdb8b2,ResourceVersion:7721,Generation:0,CreationTimestamp:2019-05-29 18:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 29 18:29:32.837: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4389,SelfLink:/api/v1/namespaces/watch-4389/configmaps/e2e-watch-test-label-changed,UID:b423ed87-823f-11e9-9319-000d3afdb8b2,ResourceVersion:7722,Generation:0,CreationTimestamp:2019-05-29 18:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 29 18:29:42.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4389,SelfLink:/api/v1/namespaces/watch-4389/configmaps/e2e-watch-test-label-changed,UID:b423ed87-823f-11e9-9319-000d3afdb8b2,ResourceVersion:7737,Generation:0,CreationTimestamp:2019-05-29 18:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 18:29:42.857: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4389,SelfLink:/api/v1/namespaces/watch-4389/configmaps/e2e-watch-test-label-changed,UID:b423ed87-823f-11e9-9319-000d3afdb8b2,ResourceVersion:7738,Generation:0,CreationTimestamp:2019-05-29 18:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 29 18:29:42.857: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4389,SelfLink:/api/v1/namespaces/watch-4389/configmaps/e2e-watch-test-label-changed,UID:b423ed87-823f-11e9-9319-000d3afdb8b2,ResourceVersion:7739,Generation:0,CreationTimestamp:2019-05-29 18:29:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:29:42.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4389" for this suite.
May 29 18:29:48.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:29:48.954: INFO: namespace watch-4389 deletion completed in 6.093433285s

• [SLOW TEST:16.178 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:29:48.954: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
May 29 18:29:48.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-421'
May 29 18:29:51.570: INFO: stderr: ""
May 29 18:29:51.570: INFO: stdout: "pod/pause created\n"
May 29 18:29:51.570: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 29 18:29:51.570: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-421" to be "running and ready"
May 29 18:29:51.575: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206742ms
May 29 18:29:53.578: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00747608s
May 29 18:29:55.581: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010741285s
May 29 18:29:57.585: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.01496529s
May 29 18:29:57.585: INFO: Pod "pause" satisfied condition "running and ready"
May 29 18:29:57.585: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
May 29 18:29:57.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 label pods pause testing-label=testing-label-value --namespace=kubectl-421'
May 29 18:29:57.678: INFO: stderr: ""
May 29 18:29:57.678: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 29 18:29:57.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pod pause -L testing-label --namespace=kubectl-421'
May 29 18:29:57.765: INFO: stderr: ""
May 29 18:29:57.765: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 29 18:29:57.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 label pods pause testing-label- --namespace=kubectl-421'
May 29 18:29:57.863: INFO: stderr: ""
May 29 18:29:57.863: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 29 18:29:57.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pod pause -L testing-label --namespace=kubectl-421'
May 29 18:29:57.945: INFO: stderr: ""
May 29 18:29:57.945: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
May 29 18:29:57.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-421'
May 29 18:29:58.042: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 18:29:58.042: INFO: stdout: "pod \"pause\" force deleted\n"
May 29 18:29:58.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get rc,svc -l name=pause --no-headers --namespace=kubectl-421'
May 29 18:29:58.176: INFO: stderr: "No resources found.\n"
May 29 18:29:58.176: INFO: stdout: ""
May 29 18:29:58.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -l name=pause --namespace=kubectl-421 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 18:29:58.267: INFO: stderr: ""
May 29 18:29:58.267: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:29:58.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-421" for this suite.
May 29 18:30:04.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:30:04.357: INFO: namespace kubectl-421 deletion completed in 6.086533485s

• [SLOW TEST:15.403 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:30:04.358: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 29 18:30:16.439: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 29 18:30:16.446: INFO: Pod pod-with-prestop-http-hook still exists
May 29 18:30:18.447: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 29 18:30:18.450: INFO: Pod pod-with-prestop-http-hook still exists
May 29 18:30:20.447: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 29 18:30:20.450: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:30:20.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2859" for this suite.
May 29 18:30:42.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:30:42.542: INFO: namespace container-lifecycle-hook-2859 deletion completed in 22.081201587s

• [SLOW TEST:38.184 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:30:42.542: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ddb740ca-823f-11e9-b32a-4ae2266202a5
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ddb740ca-823f-11e9-b32a-4ae2266202a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:32:19.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7968" for this suite.
May 29 18:32:41.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:32:41.128: INFO: namespace projected-7968 deletion completed in 22.082977023s

• [SLOW TEST:118.586 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:32:41.129: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-jr6j8 in namespace proxy-4452
I0529 18:32:41.187778      16 runners.go:184] Created replication controller with name: proxy-service-jr6j8, namespace: proxy-4452, replica count: 1
I0529 18:32:42.238272      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 18:32:43.238564      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 18:32:44.238874      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 18:32:45.239098      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 18:32:46.239349      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 18:32:47.239559      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 18:32:48.239854      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 18:32:49.240075      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 18:32:50.240305      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 18:32:51.240705      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 18:32:52.241016      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 18:32:53.241379      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 18:32:54.241716      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 18:32:55.241987      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0529 18:32:56.242290      16 runners.go:184] proxy-service-jr6j8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 29 18:32:56.244: INFO: setup took 15.074934468s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 29 18:32:56.252: INFO: (0) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 8.27607ms)
May 29 18:32:56.253: INFO: (0) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 8.964592ms)
May 29 18:32:56.261: INFO: (0) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 16.564841ms)
May 29 18:32:56.261: INFO: (0) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 16.661644ms)
May 29 18:32:56.261: INFO: (0) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 16.84465ms)
May 29 18:32:56.262: INFO: (0) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 17.44257ms)
May 29 18:32:56.266: INFO: (0) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 21.845913ms)
May 29 18:32:56.267: INFO: (0) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 22.572937ms)
May 29 18:32:56.267: INFO: (0) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 23.006451ms)
May 29 18:32:56.275: INFO: (0) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 30.846407ms)
May 29 18:32:56.275: INFO: (0) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 30.750404ms)
May 29 18:32:56.276: INFO: (0) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 31.233419ms)
May 29 18:32:56.276: INFO: (0) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 31.426026ms)
May 29 18:32:56.276: INFO: (0) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 31.54713ms)
May 29 18:32:56.277: INFO: (0) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 31.940642ms)
May 29 18:32:56.279: INFO: (0) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 34.878339ms)
May 29 18:32:56.284: INFO: (1) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 4.223738ms)
May 29 18:32:56.288: INFO: (1) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 8.148266ms)
May 29 18:32:56.289: INFO: (1) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 8.58508ms)
May 29 18:32:56.290: INFO: (1) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 9.825721ms)
May 29 18:32:56.291: INFO: (1) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 10.233534ms)
May 29 18:32:56.292: INFO: (1) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 11.902489ms)
May 29 18:32:56.292: INFO: (1) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 12.063194ms)
May 29 18:32:56.292: INFO: (1) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 12.387204ms)
May 29 18:32:56.292: INFO: (1) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 11.485875ms)
May 29 18:32:56.293: INFO: (1) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 11.889988ms)
May 29 18:32:56.293: INFO: (1) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 12.391605ms)
May 29 18:32:56.293: INFO: (1) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 12.014192ms)
May 29 18:32:56.294: INFO: (1) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 13.690447ms)
May 29 18:32:56.294: INFO: (1) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 13.870853ms)
May 29 18:32:56.295: INFO: (1) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 13.695047ms)
May 29 18:32:56.295: INFO: (1) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 13.953256ms)
May 29 18:32:56.301: INFO: (2) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 5.741188ms)
May 29 18:32:56.301: INFO: (2) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 4.837758ms)
May 29 18:32:56.303: INFO: (2) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 7.189635ms)
May 29 18:32:56.303: INFO: (2) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 8.093065ms)
May 29 18:32:56.304: INFO: (2) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 7.803254ms)
May 29 18:32:56.304: INFO: (2) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 9.385207ms)
May 29 18:32:56.306: INFO: (2) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 9.80062ms)
May 29 18:32:56.306: INFO: (2) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 10.850854ms)
May 29 18:32:56.306: INFO: (2) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 10.072028ms)
May 29 18:32:56.307: INFO: (2) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 10.648548ms)
May 29 18:32:56.306: INFO: (2) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 11.199266ms)
May 29 18:32:56.307: INFO: (2) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 11.142964ms)
May 29 18:32:56.307: INFO: (2) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 11.961791ms)
May 29 18:32:56.308: INFO: (2) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 13.520342ms)
May 29 18:32:56.309: INFO: (2) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 12.773817ms)
May 29 18:32:56.309: INFO: (2) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 14.149062ms)
May 29 18:32:56.315: INFO: (3) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 5.674886ms)
May 29 18:32:56.315: INFO: (3) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 5.588383ms)
May 29 18:32:56.318: INFO: (3) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 8.945292ms)
May 29 18:32:56.324: INFO: (3) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 14.592377ms)
May 29 18:32:56.324: INFO: (3) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 14.449872ms)
May 29 18:32:56.325: INFO: (3) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 15.836217ms)
May 29 18:32:56.325: INFO: (3) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 15.444304ms)
May 29 18:32:56.325: INFO: (3) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 16.23403ms)
May 29 18:32:56.326: INFO: (3) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 15.815416ms)
May 29 18:32:56.326: INFO: (3) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 16.907752ms)
May 29 18:32:56.326: INFO: (3) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 17.069357ms)
May 29 18:32:56.326: INFO: (3) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 16.447837ms)
May 29 18:32:56.326: INFO: (3) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 16.624943ms)
May 29 18:32:56.326: INFO: (3) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 16.787248ms)
May 29 18:32:56.326: INFO: (3) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 17.074557ms)
May 29 18:32:56.329: INFO: (3) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 19.452435ms)
May 29 18:32:56.334: INFO: (4) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 4.195637ms)
May 29 18:32:56.340: INFO: (4) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 10.360138ms)
May 29 18:32:56.340: INFO: (4) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 10.355538ms)
May 29 18:32:56.340: INFO: (4) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 11.251968ms)
May 29 18:32:56.341: INFO: (4) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 11.380572ms)
May 29 18:32:56.341: INFO: (4) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 11.584278ms)
May 29 18:32:56.341: INFO: (4) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 11.927289ms)
May 29 18:32:56.341: INFO: (4) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 12.317602ms)
May 29 18:32:56.341: INFO: (4) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 11.95909ms)
May 29 18:32:56.342: INFO: (4) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 12.58051ms)
May 29 18:32:56.342: INFO: (4) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 12.615312ms)
May 29 18:32:56.342: INFO: (4) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 12.86942ms)
May 29 18:32:56.343: INFO: (4) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 13.015725ms)
May 29 18:32:56.343: INFO: (4) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 13.196031ms)
May 29 18:32:56.343: INFO: (4) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 13.511041ms)
May 29 18:32:56.343: INFO: (4) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 13.946556ms)
May 29 18:32:56.351: INFO: (5) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 7.052931ms)
May 29 18:32:56.351: INFO: (5) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 6.848423ms)
May 29 18:32:56.351: INFO: (5) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 7.199335ms)
May 29 18:32:56.351: INFO: (5) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 7.04013ms)
May 29 18:32:56.355: INFO: (5) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 11.239367ms)
May 29 18:32:56.356: INFO: (5) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 11.168165ms)
May 29 18:32:56.356: INFO: (5) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 12.132696ms)
May 29 18:32:56.356: INFO: (5) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 11.884588ms)
May 29 18:32:56.356: INFO: (5) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 11.92849ms)
May 29 18:32:56.357: INFO: (5) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 12.220199ms)
May 29 18:32:56.357: INFO: (5) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 12.462006ms)
May 29 18:32:56.357: INFO: (5) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 12.353903ms)
May 29 18:32:56.357: INFO: (5) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 13.406638ms)
May 29 18:32:56.358: INFO: (5) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 13.669546ms)
May 29 18:32:56.358: INFO: (5) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 13.438038ms)
May 29 18:32:56.358: INFO: (5) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 13.429139ms)
May 29 18:32:56.364: INFO: (6) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 5.81039ms)
May 29 18:32:56.364: INFO: (6) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 5.972095ms)
May 29 18:32:56.365: INFO: (6) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 6.294405ms)
May 29 18:32:56.365: INFO: (6) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 6.533613ms)
May 29 18:32:56.365: INFO: (6) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 6.716519ms)
May 29 18:32:56.366: INFO: (6) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 7.33424ms)
May 29 18:32:56.366: INFO: (6) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 7.03033ms)
May 29 18:32:56.370: INFO: (6) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 11.225567ms)
May 29 18:32:56.370: INFO: (6) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 11.293369ms)
May 29 18:32:56.371: INFO: (6) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 12.340103ms)
May 29 18:32:56.372: INFO: (6) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 12.436406ms)
May 29 18:32:56.372: INFO: (6) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 13.051726ms)
May 29 18:32:56.374: INFO: (6) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 15.232697ms)
May 29 18:32:56.374: INFO: (6) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 14.730881ms)
May 29 18:32:56.374: INFO: (6) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 14.72378ms)
May 29 18:32:56.374: INFO: (6) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 14.69278ms)
May 29 18:32:56.377: INFO: (7) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 3.033099ms)
May 29 18:32:56.380: INFO: (7) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 5.599083ms)
May 29 18:32:56.381: INFO: (7) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 6.950727ms)
May 29 18:32:56.382: INFO: (7) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 7.64685ms)
May 29 18:32:56.383: INFO: (7) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 8.514478ms)
May 29 18:32:56.384: INFO: (7) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 9.148099ms)
May 29 18:32:56.385: INFO: (7) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 9.726117ms)
May 29 18:32:56.386: INFO: (7) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 11.095762ms)
May 29 18:32:56.386: INFO: (7) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 12.418505ms)
May 29 18:32:56.388: INFO: (7) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 12.787218ms)
May 29 18:32:56.388: INFO: (7) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 13.546243ms)
May 29 18:32:56.388: INFO: (7) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 12.979324ms)
May 29 18:32:56.388: INFO: (7) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 13.382837ms)
May 29 18:32:56.388: INFO: (7) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 13.330335ms)
May 29 18:32:56.389: INFO: (7) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 13.947156ms)
May 29 18:32:56.389: INFO: (7) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 13.824251ms)
May 29 18:32:56.400: INFO: (8) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 11.31967ms)
May 29 18:32:56.403: INFO: (8) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 13.48514ms)
May 29 18:32:56.403: INFO: (8) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 13.725248ms)
May 29 18:32:56.404: INFO: (8) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 15.409403ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 15.61161ms)
May 29 18:32:56.404: INFO: (8) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 15.633911ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 15.691513ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 15.895119ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 15.822516ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 15.680712ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 16.087626ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 15.903819ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 16.093926ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 16.396736ms)
May 29 18:32:56.405: INFO: (8) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 16.717946ms)
May 29 18:32:56.406: INFO: (8) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 16.798348ms)
May 29 18:32:56.414: INFO: (9) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 7.941959ms)
May 29 18:32:56.416: INFO: (9) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 9.535212ms)
May 29 18:32:56.416: INFO: (9) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 9.436708ms)
May 29 18:32:56.416: INFO: (9) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 9.225902ms)
May 29 18:32:56.416: INFO: (9) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 9.781119ms)
May 29 18:32:56.416: INFO: (9) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 9.452809ms)
May 29 18:32:56.416: INFO: (9) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 9.402607ms)
May 29 18:32:56.416: INFO: (9) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 9.616114ms)
May 29 18:32:56.417: INFO: (9) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 10.533244ms)
May 29 18:32:56.418: INFO: (9) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 11.756384ms)
May 29 18:32:56.418: INFO: (9) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 12.301602ms)
May 29 18:32:56.419: INFO: (9) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 13.048426ms)
May 29 18:32:56.419: INFO: (9) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 13.009325ms)
May 29 18:32:56.419: INFO: (9) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 12.404005ms)
May 29 18:32:56.419: INFO: (9) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 12.649413ms)
May 29 18:32:56.419: INFO: (9) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 12.977424ms)
May 29 18:32:56.426: INFO: (10) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 5.916793ms)
May 29 18:32:56.426: INFO: (10) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 6.503613ms)
May 29 18:32:56.427: INFO: (10) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 7.109632ms)
May 29 18:32:56.427: INFO: (10) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 7.469543ms)
May 29 18:32:56.428: INFO: (10) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 8.459277ms)
May 29 18:32:56.430: INFO: (10) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 9.823221ms)
May 29 18:32:56.431: INFO: (10) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 10.779251ms)
May 29 18:32:56.432: INFO: (10) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 12.140597ms)
May 29 18:32:56.432: INFO: (10) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 11.887388ms)
May 29 18:32:56.432: INFO: (10) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 12.538009ms)
May 29 18:32:56.432: INFO: (10) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 12.56621ms)
May 29 18:32:56.433: INFO: (10) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 13.008025ms)
May 29 18:32:56.433: INFO: (10) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 13.468039ms)
May 29 18:32:56.433: INFO: (10) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 13.563242ms)
May 29 18:32:56.434: INFO: (10) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 13.651746ms)
May 29 18:32:56.434: INFO: (10) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 13.758549ms)
May 29 18:32:56.441: INFO: (11) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 7.098532ms)
May 29 18:32:56.442: INFO: (11) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 7.689451ms)
May 29 18:32:56.442: INFO: (11) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 8.346673ms)
May 29 18:32:56.443: INFO: (11) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 8.135765ms)
May 29 18:32:56.443: INFO: (11) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 8.855589ms)
May 29 18:32:56.444: INFO: (11) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 9.978226ms)
May 29 18:32:56.444: INFO: (11) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 9.418407ms)
May 29 18:32:56.445: INFO: (11) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 9.985726ms)
May 29 18:32:56.445: INFO: (11) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 10.42984ms)
May 29 18:32:56.445: INFO: (11) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 10.564445ms)
May 29 18:32:56.445: INFO: (11) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 11.047361ms)
May 29 18:32:56.446: INFO: (11) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 11.408572ms)
May 29 18:32:56.447: INFO: (11) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 12.375104ms)
May 29 18:32:56.447: INFO: (11) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 12.830419ms)
May 29 18:32:56.448: INFO: (11) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 13.437139ms)
May 29 18:32:56.449: INFO: (11) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 14.310167ms)
May 29 18:32:56.460: INFO: (12) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 11.160864ms)
May 29 18:32:56.460: INFO: (12) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 11.554377ms)
May 29 18:32:56.461: INFO: (12) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 12.215499ms)
May 29 18:32:56.463: INFO: (12) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 13.77365ms)
May 29 18:32:56.464: INFO: (12) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 14.422071ms)
May 29 18:32:56.464: INFO: (12) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 15.359801ms)
May 29 18:32:56.465: INFO: (12) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 15.672611ms)
May 29 18:32:56.465: INFO: (12) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 15.732213ms)
May 29 18:32:56.466: INFO: (12) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 16.739046ms)
May 29 18:32:56.466: INFO: (12) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 16.919953ms)
May 29 18:32:56.467: INFO: (12) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 17.834182ms)
May 29 18:32:56.467: INFO: (12) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 18.187894ms)
May 29 18:32:56.468: INFO: (12) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 18.3885ms)
May 29 18:32:56.468: INFO: (12) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 18.909617ms)
May 29 18:32:56.478: INFO: (12) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 28.849342ms)
May 29 18:32:56.486: INFO: (12) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 37.398921ms)
May 29 18:32:56.503: INFO: (13) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 15.563108ms)
May 29 18:32:56.503: INFO: (13) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 16.157327ms)
May 29 18:32:56.503: INFO: (13) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 15.819116ms)
May 29 18:32:56.504: INFO: (13) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 16.659144ms)
May 29 18:32:56.504: INFO: (13) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 16.85335ms)
May 29 18:32:56.504: INFO: (13) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 17.13786ms)
May 29 18:32:56.505: INFO: (13) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 17.633276ms)
May 29 18:32:56.504: INFO: (13) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 17.958786ms)
May 29 18:32:56.504: INFO: (13) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 17.690078ms)
May 29 18:32:56.505: INFO: (13) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 17.792581ms)
May 29 18:32:56.505: INFO: (13) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 17.641976ms)
May 29 18:32:56.505: INFO: (13) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 17.784481ms)
May 29 18:32:56.505: INFO: (13) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 17.957586ms)
May 29 18:32:56.505: INFO: (13) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 18.257897ms)
May 29 18:32:56.508: INFO: (13) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 20.664475ms)
May 29 18:32:56.508: INFO: (13) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 21.423199ms)
May 29 18:32:56.514: INFO: (14) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 5.742288ms)
May 29 18:32:56.515: INFO: (14) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 6.549914ms)
May 29 18:32:56.516: INFO: (14) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 7.444143ms)
May 29 18:32:56.517: INFO: (14) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 7.983261ms)
May 29 18:32:56.517: INFO: (14) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 8.311471ms)
May 29 18:32:56.517: INFO: (14) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 8.546679ms)
May 29 18:32:56.518: INFO: (14) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 8.697984ms)
May 29 18:32:56.518: INFO: (14) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 8.930091ms)
May 29 18:32:56.518: INFO: (14) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 9.217301ms)
May 29 18:32:56.518: INFO: (14) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 9.512011ms)
May 29 18:32:56.518: INFO: (14) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 9.687716ms)
May 29 18:32:56.519: INFO: (14) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 9.80092ms)
May 29 18:32:56.519: INFO: (14) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 9.948524ms)
May 29 18:32:56.520: INFO: (14) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 11.668581ms)
May 29 18:32:56.521: INFO: (14) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 12.578911ms)
May 29 18:32:56.522: INFO: (14) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 12.701415ms)
May 29 18:32:56.524: INFO: (15) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 2.599185ms)
May 29 18:32:56.545: INFO: (15) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 23.56777ms)
May 29 18:32:56.547: INFO: (15) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 24.207191ms)
May 29 18:32:56.548: INFO: (15) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 25.742041ms)
May 29 18:32:56.548: INFO: (15) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 25.623636ms)
May 29 18:32:56.548: INFO: (15) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 25.489432ms)
May 29 18:32:56.550: INFO: (15) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 27.527799ms)
May 29 18:32:56.550: INFO: (15) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 27.938113ms)
May 29 18:32:56.550: INFO: (15) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 27.925312ms)
May 29 18:32:56.550: INFO: (15) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 27.692004ms)
May 29 18:32:56.551: INFO: (15) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 28.310525ms)
May 29 18:32:56.552: INFO: (15) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 28.551432ms)
May 29 18:32:56.552: INFO: (15) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 29.079949ms)
May 29 18:32:56.552: INFO: (15) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 29.006947ms)
May 29 18:32:56.552: INFO: (15) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 29.355359ms)
May 29 18:32:56.552: INFO: (15) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 30.026981ms)
May 29 18:32:56.557: INFO: (16) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 4.448145ms)
May 29 18:32:56.558: INFO: (16) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 5.365975ms)
May 29 18:32:56.559: INFO: (16) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 6.206903ms)
May 29 18:32:56.560: INFO: (16) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 6.674117ms)
May 29 18:32:56.560: INFO: (16) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 7.388642ms)
May 29 18:32:56.560: INFO: (16) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 7.673051ms)
May 29 18:32:56.561: INFO: (16) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 7.858356ms)
May 29 18:32:56.561: INFO: (16) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 7.97116ms)
May 29 18:32:56.561: INFO: (16) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 8.166367ms)
May 29 18:32:56.565: INFO: (16) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 12.379804ms)
May 29 18:32:56.566: INFO: (16) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 12.548009ms)
May 29 18:32:56.566: INFO: (16) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 13.219432ms)
May 29 18:32:56.567: INFO: (16) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 14.139161ms)
May 29 18:32:56.567: INFO: (16) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 14.311368ms)
May 29 18:32:56.568: INFO: (16) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 14.408471ms)
May 29 18:32:56.568: INFO: (16) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 14.775882ms)
May 29 18:32:56.573: INFO: (17) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 5.51478ms)
May 29 18:32:56.574: INFO: (17) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 5.82939ms)
May 29 18:32:56.574: INFO: (17) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 6.334707ms)
May 29 18:32:56.575: INFO: (17) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 6.75252ms)
May 29 18:32:56.575: INFO: (17) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 6.880224ms)
May 29 18:32:56.576: INFO: (17) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 8.080564ms)
May 29 18:32:56.577: INFO: (17) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 8.811688ms)
May 29 18:32:56.577: INFO: (17) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 8.759186ms)
May 29 18:32:56.580: INFO: (17) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 11.411573ms)
May 29 18:32:56.580: INFO: (17) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 11.31217ms)
May 29 18:32:56.581: INFO: (17) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 12.056093ms)
May 29 18:32:56.581: INFO: (17) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 12.601511ms)
May 29 18:32:56.581: INFO: (17) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 12.734916ms)
May 29 18:32:56.582: INFO: (17) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 13.15793ms)
May 29 18:32:56.582: INFO: (17) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 13.800651ms)
May 29 18:32:56.582: INFO: (17) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 14.09796ms)
May 29 18:32:56.592: INFO: (18) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 9.025094ms)
May 29 18:32:56.593: INFO: (18) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 10.189132ms)
May 29 18:32:56.593: INFO: (18) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 10.577345ms)
May 29 18:32:56.593: INFO: (18) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 10.616346ms)
May 29 18:32:56.593: INFO: (18) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 10.882255ms)
May 29 18:32:56.593: INFO: (18) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 10.630447ms)
May 29 18:32:56.594: INFO: (18) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 11.506176ms)
May 29 18:32:56.594: INFO: (18) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 11.988191ms)
May 29 18:32:56.596: INFO: (18) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 12.731316ms)
May 29 18:32:56.596: INFO: (18) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 12.86342ms)
May 29 18:32:56.596: INFO: (18) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 13.090028ms)
May 29 18:32:56.601: INFO: (18) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 18.06929ms)
May 29 18:32:56.601: INFO: (18) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 18.486104ms)
May 29 18:32:56.602: INFO: (18) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 18.951319ms)
May 29 18:32:56.602: INFO: (18) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 19.100423ms)
May 29 18:32:56.602: INFO: (18) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 19.413034ms)
May 29 18:32:56.610: INFO: (19) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk/proxy/rewriteme">test</a> (200; 6.382508ms)
May 29 18:32:56.611: INFO: (19) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 8.137465ms)
May 29 18:32:56.611: INFO: (19) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">test<... (200; 7.808055ms)
May 29 18:32:56.611: INFO: (19) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:462/proxy/: tls qux (200; 7.374641ms)
May 29 18:32:56.611: INFO: (19) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:1080/proxy/rewriteme">... (200; 8.88419ms)
May 29 18:32:56.615: INFO: (19) /api/v1/namespaces/proxy-4452/pods/http:proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 11.747083ms)
May 29 18:32:56.616: INFO: (19) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:160/proxy/: foo (200; 12.575311ms)
May 29 18:32:56.616: INFO: (19) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:460/proxy/: tls baz (200; 14.022758ms)
May 29 18:32:56.617: INFO: (19) /api/v1/namespaces/proxy-4452/pods/proxy-service-jr6j8-sr5kk:162/proxy/: bar (200; 13.177031ms)
May 29 18:32:56.617: INFO: (19) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname1/proxy/: foo (200; 14.564276ms)
May 29 18:32:56.617: INFO: (19) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname1/proxy/: tls baz (200; 13.576543ms)
May 29 18:32:56.617: INFO: (19) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname2/proxy/: bar (200; 14.268866ms)
May 29 18:32:56.617: INFO: (19) /api/v1/namespaces/proxy-4452/services/https:proxy-service-jr6j8:tlsportname2/proxy/: tls qux (200; 14.141261ms)
May 29 18:32:56.617: INFO: (19) /api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/: <a href="/api/v1/namespaces/proxy-4452/pods/https:proxy-service-jr6j8-sr5kk:443/proxy/tlsrewritem... (200; 13.595644ms)
May 29 18:32:56.617: INFO: (19) /api/v1/namespaces/proxy-4452/services/proxy-service-jr6j8:portname1/proxy/: foo (200; 14.432171ms)
May 29 18:32:56.617: INFO: (19) /api/v1/namespaces/proxy-4452/services/http:proxy-service-jr6j8:portname2/proxy/: bar (200; 14.804383ms)
STEP: deleting ReplicationController proxy-service-jr6j8 in namespace proxy-4452, will wait for the garbage collector to delete the pods
May 29 18:32:56.675: INFO: Deleting ReplicationController proxy-service-jr6j8 took: 5.21717ms
May 29 18:32:56.975: INFO: Terminating ReplicationController proxy-service-jr6j8 pods took: 300.194602ms
[AfterEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:33:09.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4452" for this suite.
May 29 18:33:15.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:33:15.958: INFO: namespace proxy-4452 deletion completed in 6.078674206s

• [SLOW TEST:34.829 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:33:15.958: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 29 18:33:16.016: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:16.019: INFO: Number of nodes with available pods: 0
May 29 18:33:16.019: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:17.023: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:17.025: INFO: Number of nodes with available pods: 0
May 29 18:33:17.026: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:18.022: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:18.025: INFO: Number of nodes with available pods: 0
May 29 18:33:18.025: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:19.022: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:19.025: INFO: Number of nodes with available pods: 0
May 29 18:33:19.025: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:20.022: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:20.025: INFO: Number of nodes with available pods: 0
May 29 18:33:20.025: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:21.023: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:21.026: INFO: Number of nodes with available pods: 1
May 29 18:33:21.026: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 29 18:33:21.037: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:21.040: INFO: Number of nodes with available pods: 0
May 29 18:33:21.040: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:22.043: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:22.046: INFO: Number of nodes with available pods: 0
May 29 18:33:22.046: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:23.045: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:23.048: INFO: Number of nodes with available pods: 0
May 29 18:33:23.048: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:24.043: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:24.046: INFO: Number of nodes with available pods: 0
May 29 18:33:24.046: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:25.045: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:25.047: INFO: Number of nodes with available pods: 0
May 29 18:33:25.047: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:26.043: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:26.046: INFO: Number of nodes with available pods: 0
May 29 18:33:26.046: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:27.044: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:27.047: INFO: Number of nodes with available pods: 0
May 29 18:33:27.047: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:28.046: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:28.050: INFO: Number of nodes with available pods: 0
May 29 18:33:28.050: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:29.045: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:29.049: INFO: Number of nodes with available pods: 0
May 29 18:33:29.049: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:30.044: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:30.047: INFO: Number of nodes with available pods: 0
May 29 18:33:30.047: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:33:31.044: INFO: DaemonSet pods can't tolerate node k8s-master-11779686-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 29 18:33:31.047: INFO: Number of nodes with available pods: 1
May 29 18:33:31.047: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3367, will wait for the garbage collector to delete the pods
May 29 18:33:31.106: INFO: Deleting DaemonSet.extensions daemon-set took: 4.500046ms
May 29 18:33:31.406: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.184543ms
May 29 18:33:39.908: INFO: Number of nodes with available pods: 0
May 29 18:33:39.908: INFO: Number of running nodes: 0, number of available pods: 0
May 29 18:33:39.911: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3367/daemonsets","resourceVersion":"8271"},"items":null}

May 29 18:33:39.913: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3367/pods","resourceVersion":"8271"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:33:39.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3367" for this suite.
May 29 18:33:45.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:33:46.032: INFO: namespace daemonsets-3367 deletion completed in 6.111525588s

• [SLOW TEST:30.074 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:33:46.033: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 18:33:46.080: INFO: Creating ReplicaSet my-hostname-basic-4b16bcc1-8240-11e9-b32a-4ae2266202a5
May 29 18:33:46.087: INFO: Pod name my-hostname-basic-4b16bcc1-8240-11e9-b32a-4ae2266202a5: Found 0 pods out of 1
May 29 18:33:51.091: INFO: Pod name my-hostname-basic-4b16bcc1-8240-11e9-b32a-4ae2266202a5: Found 1 pods out of 1
May 29 18:33:51.091: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4b16bcc1-8240-11e9-b32a-4ae2266202a5" is running
May 29 18:33:51.093: INFO: Pod "my-hostname-basic-4b16bcc1-8240-11e9-b32a-4ae2266202a5-r5tqc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 18:33:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 18:33:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 18:33:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-29 18:33:46 +0000 UTC Reason: Message:}])
May 29 18:33:51.093: INFO: Trying to dial the pod
May 29 18:33:56.105: INFO: Controller my-hostname-basic-4b16bcc1-8240-11e9-b32a-4ae2266202a5: Got expected result from replica 1 [my-hostname-basic-4b16bcc1-8240-11e9-b32a-4ae2266202a5-r5tqc]: "my-hostname-basic-4b16bcc1-8240-11e9-b32a-4ae2266202a5-r5tqc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:33:56.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9914" for this suite.
May 29 18:34:02.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:34:02.191: INFO: namespace replicaset-9914 deletion completed in 6.082656848s

• [SLOW TEST:16.159 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:34:02.192: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 18:34:02.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8942'
May 29 18:34:02.320: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 18:34:02.320: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May 29 18:34:02.334: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 29 18:34:02.339: INFO: scanned /root for discovery docs: <nil>
May 29 18:34:02.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8942'
May 29 18:34:21.122: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 29 18:34:21.122: INFO: stdout: "Created e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3\nScaling up e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 29 18:34:21.122: INFO: stdout: "Created e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3\nScaling up e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 29 18:34:21.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8942'
May 29 18:34:21.208: INFO: stderr: ""
May 29 18:34:21.208: INFO: stdout: "e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3-x47lb "
May 29 18:34:21.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3-x47lb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8942'
May 29 18:34:21.300: INFO: stderr: ""
May 29 18:34:21.300: INFO: stdout: "true"
May 29 18:34:21.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3-x47lb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8942'
May 29 18:34:21.378: INFO: stderr: ""
May 29 18:34:21.378: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May 29 18:34:21.378: INFO: e2e-test-nginx-rc-d9387da647c65ecb87473e3d7d2f11c3-x47lb is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
May 29 18:34:21.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete rc e2e-test-nginx-rc --namespace=kubectl-8942'
May 29 18:34:21.463: INFO: stderr: ""
May 29 18:34:21.463: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:34:21.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8942" for this suite.
May 29 18:34:43.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:34:43.579: INFO: namespace kubectl-8942 deletion completed in 22.110008031s

• [SLOW TEST:41.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:34:43.579: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 29 18:34:55.660: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 29 18:34:55.663: INFO: Pod pod-with-poststart-http-hook still exists
May 29 18:34:57.663: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 29 18:34:57.667: INFO: Pod pod-with-poststart-http-hook still exists
May 29 18:34:59.663: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 29 18:34:59.667: INFO: Pod pod-with-poststart-http-hook still exists
May 29 18:35:01.663: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 29 18:35:01.666: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:35:01.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4000" for this suite.
May 29 18:35:23.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:35:23.754: INFO: namespace container-lifecycle-hook-4000 deletion completed in 22.085024988s

• [SLOW TEST:40.175 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:35:23.755: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 18:35:23.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 version --client'
May 29 18:35:23.852: INFO: stderr: ""
May 29 18:35:23.852: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.2\", GitCommit:\"66049e3b21efe110454d67df4fa62b08ea79a19b\", GitTreeState:\"clean\", BuildDate:\"2019-05-16T16:23:09Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 29 18:35:23.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-149'
May 29 18:35:24.297: INFO: stderr: ""
May 29 18:35:24.297: INFO: stdout: "replicationcontroller/redis-master created\n"
May 29 18:35:24.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-149'
May 29 18:35:24.649: INFO: stderr: ""
May 29 18:35:24.649: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 29 18:35:25.652: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:35:25.652: INFO: Found 0 / 1
May 29 18:35:26.652: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:35:26.652: INFO: Found 0 / 1
May 29 18:35:27.651: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:35:27.651: INFO: Found 0 / 1
May 29 18:35:28.652: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:35:28.652: INFO: Found 1 / 1
May 29 18:35:28.652: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 29 18:35:28.654: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:35:28.654: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 29 18:35:28.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 describe pod redis-master-f694k --namespace=kubectl-149'
May 29 18:35:28.755: INFO: stderr: ""
May 29 18:35:28.755: INFO: stdout: "Name:               redis-master-f694k\nNamespace:          kubectl-149\nPriority:           0\nPriorityClassName:  <none>\nNode:               k8s-pool1-11779686-vmss000000/10.240.0.34\nStart Time:         Wed, 29 May 2019 18:35:24 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.240.0.55\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://0b41e87d9e6301c29c2be94fdd7d543b583c0e02baccb284e6c9525e662cc575\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 29 May 2019 18:35:27 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7swdq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-7swdq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-7swdq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                    Message\n  ----    ------     ----  ----                                    -------\n  Normal  Scheduled  4s    default-scheduler                       Successfully assigned kubectl-149/redis-master-f694k to k8s-pool1-11779686-vmss000000\n  Normal  Pulled     2s    kubelet, k8s-pool1-11779686-vmss000000  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, k8s-pool1-11779686-vmss000000  Created container redis-master\n  Normal  Started    1s    kubelet, k8s-pool1-11779686-vmss000000  Started container redis-master\n"
May 29 18:35:28.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 describe rc redis-master --namespace=kubectl-149'
May 29 18:35:28.850: INFO: stderr: ""
May 29 18:35:28.850: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-149\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-f694k\n"
May 29 18:35:28.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 describe service redis-master --namespace=kubectl-149'
May 29 18:35:28.941: INFO: stderr: ""
May 29 18:35:28.941: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-149\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.240.200\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.240.0.55:6379\nSession Affinity:  None\nEvents:\n  Type     Reason                     Age   From                  Message\n  ----     ------                     ----  ----                  -------\n  Warning  ListLoadBalancers          4s    azure-cloud-provider  network.LoadBalancersClient#List: Failure responding to request: StatusCode=403 -- Original Error: autorest/azure: Service returned an error. Status=403 Code=\"AuthorizationFailed\" Message=\"The client 'f09a010f-7103-4653-9b1a-aeb475315576' with object id 'f09a010f-7103-4653-9b1a-aeb475315576' does not have authorization to perform action 'Microsoft.Network/loadBalancers/read' over scope '/subscriptions/01db32b1-e169-43b0-a791-de0e1ca5d8cd/resourceGroups/levo-1-14-5ceec3f3/providers/Microsoft.Network'.\"\n  Warning  CleanupLoadBalancerFailed  4s    service-controller    Error cleaning up load balancer (will retry): error getting LB for service kubectl-149/redis-master: network.LoadBalancersClient#List: Failure responding to request: StatusCode=403 -- Original Error: autorest/azure: Service returned an error. Status=403 Code=\"AuthorizationFailed\" Message=\"The client 'f09a010f-7103-4653-9b1a-aeb475315576' with object id 'f09a010f-7103-4653-9b1a-aeb475315576' does not have authorization to perform action 'Microsoft.Network/loadBalancers/read' over scope '/subscriptions/01db32b1-e169-43b0-a791-de0e1ca5d8cd/resourceGroups/levo-1-14-5ceec3f3/providers/Microsoft.Network'.\"\n"
May 29 18:35:28.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 describe node k8s-master-11779686-0'
May 29 18:35:29.046: INFO: stderr: ""
May 29 18:35:29.046: INFO: stdout: "Name:               k8s-master-11779686-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_DS2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=westus2\n                    failure-domain.beta.kubernetes.io/zone=0\n                    kubernetes.azure.com/cluster=levo-1-14-5ceec3f3\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-master-11779686-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=master\n                    node-role.kubernetes.io/master=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 29 May 2019 17:43:31 +0000\nTaints:             node-role.kubernetes.io/master=true:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 29 May 2019 18:34:52 +0000   Wed, 29 May 2019 17:43:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 29 May 2019 18:34:52 +0000   Wed, 29 May 2019 17:43:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 29 May 2019 18:34:52 +0000   Wed, 29 May 2019 17:43:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 29 May 2019 18:34:52 +0000   Wed, 29 May 2019 17:43:25 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    k8s-master-11779686-0\n  InternalIP:  10.255.255.5\nCapacity:\n attachable-volumes-azure-disk:  16\n cpu:                            2\n ephemeral-storage:              30428648Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7113160Ki\n pods:                           30\nAllocatable:\n attachable-volumes-azure-disk:  16\n cpu:                            2\n ephemeral-storage:              28043041951\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         6345160Ki\n pods:                           30\nSystem Info:\n Machine ID:                 dda2888388d94615a7da0161ad4387f4\n System UUID:                34130FCB-A0C0-6644-9513-D195D8CC3870\n Boot ID:                    ad9f911b-5326-4545-9658-1b73efa281fc\n Kernel Version:             4.15.0-1045-azure\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://3.0.5\n Kubelet Version:            v1.14.2\n Kube-Proxy Version:         v1.14.2\nProviderID:                  azure:///subscriptions/01db32b1-e169-43b0-a791-de0e1ca5d8cd/resourceGroups/levo-1-14-5ceec3f3/providers/Microsoft.Compute/virtualMachines/k8s-master-11779686-0\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-270e25e81e454394-2klts    0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                azure-cni-networkmonitor-rzrvq                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                azure-ip-masq-agent-jbn2g                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (4%)     51m\n  kube-system                coredns-7bc69b7975-jfqs2                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (2%)     51m\n  kube-system                kube-addon-manager-k8s-master-11779686-0                   5m (0%)       0 (0%)      50Mi (0%)        0 (0%)         50m\n  kube-system                kube-apiserver-k8s-master-11779686-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                kube-controller-manager-k8s-master-11779686-0              0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                kube-proxy-d9gwt                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                kube-scheduler-k8s-master-11779686-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            255m (12%)  50m (2%)\n  memory                         170Mi (2%)  420Mi (6%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:\n  Type    Reason                   Age                From                               Message\n  ----    ------                   ----               ----                               -------\n  Normal  NodeHasSufficientMemory  52m (x8 over 52m)  kubelet, k8s-master-11779686-0     Node k8s-master-11779686-0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    52m (x8 over 52m)  kubelet, k8s-master-11779686-0     Node k8s-master-11779686-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     52m (x7 over 52m)  kubelet, k8s-master-11779686-0     Node k8s-master-11779686-0 status is now: NodeHasSufficientPID\n  Normal  Starting                 51m                kube-proxy, k8s-master-11779686-0  Starting kube-proxy.\n"
May 29 18:35:29.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 describe namespace kubectl-149'
May 29 18:35:29.129: INFO: stderr: ""
May 29 18:35:29.129: INFO: stdout: "Name:         kubectl-149\nLabels:       e2e-framework=kubectl\n              e2e-run=4a28b949-823a-11e9-b32a-4ae2266202a5\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:35:29.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-149" for this suite.
May 29 18:35:51.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:35:51.227: INFO: namespace kubectl-149 deletion completed in 22.096211759s

• [SLOW TEST:27.473 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:35:51.227: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0529 18:36:21.796854      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 18:36:21.796: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:36:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1102" for this suite.
May 29 18:36:27.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:36:27.882: INFO: namespace gc-1102 deletion completed in 6.083407675s

• [SLOW TEST:36.655 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:36:27.882: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 29 18:36:27.922: INFO: namespace kubectl-4269
May 29 18:36:27.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-4269'
May 29 18:36:28.344: INFO: stderr: ""
May 29 18:36:28.344: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 29 18:36:29.348: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:36:29.348: INFO: Found 0 / 1
May 29 18:36:30.348: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:36:30.348: INFO: Found 0 / 1
May 29 18:36:31.348: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:36:31.348: INFO: Found 0 / 1
May 29 18:36:32.348: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:36:32.348: INFO: Found 0 / 1
May 29 18:36:33.348: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:36:33.348: INFO: Found 0 / 1
May 29 18:36:34.348: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:36:34.349: INFO: Found 1 / 1
May 29 18:36:34.349: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 29 18:36:34.351: INFO: Selector matched 1 pods for map[app:redis]
May 29 18:36:34.351: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 29 18:36:34.351: INFO: wait on redis-master startup in kubectl-4269 
May 29 18:36:34.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 logs redis-master-fs6xj redis-master --namespace=kubectl-4269'
May 29 18:36:34.458: INFO: stderr: ""
May 29 18:36:34.458: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 May 18:36:32.848 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 May 18:36:32.848 # Server started, Redis version 3.2.12\n1:M 29 May 18:36:32.848 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 May 18:36:32.848 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 29 18:36:34.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4269'
May 29 18:36:34.563: INFO: stderr: ""
May 29 18:36:34.563: INFO: stdout: "service/rm2 exposed\n"
May 29 18:36:34.574: INFO: Service rm2 in namespace kubectl-4269 found.
STEP: exposing service
May 29 18:36:36.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4269'
May 29 18:36:36.688: INFO: stderr: ""
May 29 18:36:36.688: INFO: stdout: "service/rm3 exposed\n"
May 29 18:36:36.692: INFO: Service rm3 in namespace kubectl-4269 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:36:38.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4269" for this suite.
May 29 18:37:00.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:37:00.800: INFO: namespace kubectl-4269 deletion completed in 22.099982418s

• [SLOW TEST:32.918 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:37:00.800: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 29 18:37:00.838: INFO: PodSpec: initContainers in spec.initContainers
May 29 18:37:45.464: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bf2c6ac9-8240-11e9-b32a-4ae2266202a5", GenerateName:"", Namespace:"init-container-2754", SelfLink:"/api/v1/namespaces/init-container-2754/pods/pod-init-bf2c6ac9-8240-11e9-b32a-4ae2266202a5", UID:"bf2eb2bf-8240-11e9-9319-000d3afdb8b2", ResourceVersion:"9002", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63694751820, loc:(*time.Location)(0x8a140e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"838782500"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-vchk9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0022aa2c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vchk9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vchk9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vchk9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002fb9b88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-pool1-11779686-vmss000000", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001b5e360), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002fb9c10)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002fb9c30)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002fb9c38), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002fb9c3c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694751820, loc:(*time.Location)(0x8a140e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694751820, loc:(*time.Location)(0x8a140e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694751820, loc:(*time.Location)(0x8a140e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694751820, loc:(*time.Location)(0x8a140e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.34", PodIP:"10.240.0.45", StartTime:(*v1.Time)(0xc0027e8d40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00205bb90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00205bc00)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://43747be02bd739e4a53c3de854d083da5478a5420112ad7efa9948a0694e6bf5"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0027e8d80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0027e8d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:37:45.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2754" for this suite.
May 29 18:38:07.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:38:07.550: INFO: namespace init-container-2754 deletion completed in 22.081666037s

• [SLOW TEST:66.749 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:38:07.550: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-e6f5ead2-8240-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 18:38:07.600: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e6f6b9af-8240-11e9-b32a-4ae2266202a5" in namespace "projected-2800" to be "success or failure"
May 29 18:38:07.603: INFO: Pod "pod-projected-configmaps-e6f6b9af-8240-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51621ms
May 29 18:38:09.607: INFO: Pod "pod-projected-configmaps-e6f6b9af-8240-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007230657s
May 29 18:38:11.610: INFO: Pod "pod-projected-configmaps-e6f6b9af-8240-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010540179s
May 29 18:38:13.614: INFO: Pod "pod-projected-configmaps-e6f6b9af-8240-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01452911s
STEP: Saw pod success
May 29 18:38:13.615: INFO: Pod "pod-projected-configmaps-e6f6b9af-8240-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:38:13.617: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-configmaps-e6f6b9af-8240-11e9-b32a-4ae2266202a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 18:38:13.639: INFO: Waiting for pod pod-projected-configmaps-e6f6b9af-8240-11e9-b32a-4ae2266202a5 to disappear
May 29 18:38:13.642: INFO: Pod pod-projected-configmaps-e6f6b9af-8240-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:38:13.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2800" for this suite.
May 29 18:38:19.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:38:19.728: INFO: namespace projected-2800 deletion completed in 6.082890537s

• [SLOW TEST:12.178 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:38:19.729: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 29 18:38:19.772: INFO: Waiting up to 5m0s for pod "pod-ee37fea2-8240-11e9-b32a-4ae2266202a5" in namespace "emptydir-3453" to be "success or failure"
May 29 18:38:19.782: INFO: Pod "pod-ee37fea2-8240-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.517198ms
May 29 18:38:21.785: INFO: Pod "pod-ee37fea2-8240-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013124967s
May 29 18:38:23.789: INFO: Pod "pod-ee37fea2-8240-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01692833s
May 29 18:38:25.793: INFO: Pod "pod-ee37fea2-8240-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020472372s
STEP: Saw pod success
May 29 18:38:25.793: INFO: Pod "pod-ee37fea2-8240-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:38:25.795: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-ee37fea2-8240-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 18:38:25.816: INFO: Waiting for pod pod-ee37fea2-8240-11e9-b32a-4ae2266202a5 to disappear
May 29 18:38:25.818: INFO: Pod pod-ee37fea2-8240-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:38:25.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3453" for this suite.
May 29 18:38:31.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:38:31.912: INFO: namespace emptydir-3453 deletion completed in 6.091178172s

• [SLOW TEST:12.183 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:38:31.913: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
May 29 18:38:31.955: INFO: Waiting up to 5m0s for pod "client-containers-f57b1739-8240-11e9-b32a-4ae2266202a5" in namespace "containers-9763" to be "success or failure"
May 29 18:38:31.959: INFO: Pod "client-containers-f57b1739-8240-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076428ms
May 29 18:38:33.963: INFO: Pod "client-containers-f57b1739-8240-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007324812s
May 29 18:38:35.966: INFO: Pod "client-containers-f57b1739-8240-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010902395s
May 29 18:38:37.970: INFO: Pod "client-containers-f57b1739-8240-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014865479s
STEP: Saw pod success
May 29 18:38:37.970: INFO: Pod "client-containers-f57b1739-8240-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:38:37.973: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod client-containers-f57b1739-8240-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 18:38:37.992: INFO: Waiting for pod client-containers-f57b1739-8240-11e9-b32a-4ae2266202a5 to disappear
May 29 18:38:37.994: INFO: Pod client-containers-f57b1739-8240-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:38:37.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9763" for this suite.
May 29 18:38:44.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:38:44.082: INFO: namespace containers-9763 deletion completed in 6.085594079s

• [SLOW TEST:12.170 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:38:44.083: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-387
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 29 18:38:44.121: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 29 18:39:14.176: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.40 8081 | grep -v '^\s*$'] Namespace:pod-network-test-387 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 18:39:14.176: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 18:39:15.308: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:39:15.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-387" for this suite.
May 29 18:39:37.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:39:37.406: INFO: namespace pod-network-test-387 deletion completed in 22.094299948s

• [SLOW TEST:53.323 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:39:37.406: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 18:39:37.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c857098-8241-11e9-b32a-4ae2266202a5" in namespace "downward-api-8007" to be "success or failure"
May 29 18:39:37.460: INFO: Pod "downwardapi-volume-1c857098-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.050157ms
May 29 18:39:39.464: INFO: Pod "downwardapi-volume-1c857098-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009313908s
May 29 18:39:41.468: INFO: Pod "downwardapi-volume-1c857098-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012872028s
May 29 18:39:43.472: INFO: Pod "downwardapi-volume-1c857098-8241-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017032156s
STEP: Saw pod success
May 29 18:39:43.472: INFO: Pod "downwardapi-volume-1c857098-8241-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:39:43.474: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-1c857098-8241-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 18:39:43.490: INFO: Waiting for pod downwardapi-volume-1c857098-8241-11e9-b32a-4ae2266202a5 to disappear
May 29 18:39:43.492: INFO: Pod downwardapi-volume-1c857098-8241-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:39:43.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8007" for this suite.
May 29 18:39:49.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:39:49.588: INFO: namespace downward-api-8007 deletion completed in 6.092267797s

• [SLOW TEST:12.182 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:39:49.588: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 29 18:39:56.157: INFO: Successfully updated pod "pod-update-23c78e58-8241-11e9-b32a-4ae2266202a5"
STEP: verifying the updated pod is in kubernetes
May 29 18:39:56.167: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:39:56.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7484" for this suite.
May 29 18:40:18.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:40:18.256: INFO: namespace pods-7484 deletion completed in 22.085955378s

• [SLOW TEST:28.668 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:40:18.257: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 29 18:40:18.305: INFO: Waiting up to 5m0s for pod "pod-34de19ac-8241-11e9-b32a-4ae2266202a5" in namespace "emptydir-7520" to be "success or failure"
May 29 18:40:18.309: INFO: Pod "pod-34de19ac-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282532ms
May 29 18:40:20.313: INFO: Pod "pod-34de19ac-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008206269s
May 29 18:40:22.317: INFO: Pod "pod-34de19ac-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011901589s
May 29 18:40:24.321: INFO: Pod "pod-34de19ac-8241-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015808605s
STEP: Saw pod success
May 29 18:40:24.321: INFO: Pod "pod-34de19ac-8241-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:40:24.323: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-34de19ac-8241-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 18:40:24.338: INFO: Waiting for pod pod-34de19ac-8241-11e9-b32a-4ae2266202a5 to disappear
May 29 18:40:24.340: INFO: Pod pod-34de19ac-8241-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:40:24.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7520" for this suite.
May 29 18:40:30.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:40:30.432: INFO: namespace emptydir-7520 deletion completed in 6.087808446s

• [SLOW TEST:12.176 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:40:30.432: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 29 18:40:30.475: INFO: Waiting up to 5m0s for pod "downward-api-3c1fba0b-8241-11e9-b32a-4ae2266202a5" in namespace "downward-api-9349" to be "success or failure"
May 29 18:40:30.477: INFO: Pod "downward-api-3c1fba0b-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.413875ms
May 29 18:40:32.481: INFO: Pod "downward-api-3c1fba0b-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006187549s
May 29 18:40:34.484: INFO: Pod "downward-api-3c1fba0b-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009659304s
May 29 18:40:36.488: INFO: Pod "downward-api-3c1fba0b-8241-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013742169s
STEP: Saw pod success
May 29 18:40:36.489: INFO: Pod "downward-api-3c1fba0b-8241-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:40:36.491: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downward-api-3c1fba0b-8241-11e9-b32a-4ae2266202a5 container dapi-container: <nil>
STEP: delete the pod
May 29 18:40:36.520: INFO: Waiting for pod downward-api-3c1fba0b-8241-11e9-b32a-4ae2266202a5 to disappear
May 29 18:40:36.521: INFO: Pod downward-api-3c1fba0b-8241-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:40:36.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9349" for this suite.
May 29 18:40:42.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:40:42.610: INFO: namespace downward-api-9349 deletion completed in 6.086124121s

• [SLOW TEST:12.178 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:40:42.611: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 18:40:42.664: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 29 18:40:42.669: INFO: Number of nodes with available pods: 0
May 29 18:40:42.669: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 29 18:40:42.694: INFO: Number of nodes with available pods: 0
May 29 18:40:42.694: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:43.697: INFO: Number of nodes with available pods: 0
May 29 18:40:43.697: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:44.698: INFO: Number of nodes with available pods: 0
May 29 18:40:44.698: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:45.698: INFO: Number of nodes with available pods: 0
May 29 18:40:45.698: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:46.698: INFO: Number of nodes with available pods: 0
May 29 18:40:46.698: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:47.698: INFO: Number of nodes with available pods: 1
May 29 18:40:47.698: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 29 18:40:47.714: INFO: Number of nodes with available pods: 1
May 29 18:40:47.714: INFO: Number of running nodes: 0, number of available pods: 1
May 29 18:40:48.718: INFO: Number of nodes with available pods: 0
May 29 18:40:48.718: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 29 18:40:48.728: INFO: Number of nodes with available pods: 0
May 29 18:40:48.728: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:49.731: INFO: Number of nodes with available pods: 0
May 29 18:40:49.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:50.731: INFO: Number of nodes with available pods: 0
May 29 18:40:50.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:51.731: INFO: Number of nodes with available pods: 0
May 29 18:40:51.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:52.731: INFO: Number of nodes with available pods: 0
May 29 18:40:52.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:53.732: INFO: Number of nodes with available pods: 0
May 29 18:40:53.732: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:54.731: INFO: Number of nodes with available pods: 0
May 29 18:40:54.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:55.732: INFO: Number of nodes with available pods: 0
May 29 18:40:55.732: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:56.731: INFO: Number of nodes with available pods: 0
May 29 18:40:56.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:57.732: INFO: Number of nodes with available pods: 0
May 29 18:40:57.732: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:58.732: INFO: Number of nodes with available pods: 0
May 29 18:40:58.732: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:40:59.731: INFO: Number of nodes with available pods: 0
May 29 18:40:59.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:41:00.731: INFO: Number of nodes with available pods: 0
May 29 18:41:00.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:41:01.731: INFO: Number of nodes with available pods: 0
May 29 18:41:01.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:41:02.732: INFO: Number of nodes with available pods: 0
May 29 18:41:02.732: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:41:03.731: INFO: Number of nodes with available pods: 0
May 29 18:41:03.731: INFO: Node k8s-pool1-11779686-vmss000000 is running more than one daemon pod
May 29 18:41:04.732: INFO: Number of nodes with available pods: 1
May 29 18:41:04.732: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5562, will wait for the garbage collector to delete the pods
May 29 18:41:04.793: INFO: Deleting DaemonSet.extensions daemon-set took: 4.782247ms
May 29 18:41:05.093: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.544073ms
May 29 18:41:19.897: INFO: Number of nodes with available pods: 0
May 29 18:41:19.897: INFO: Number of running nodes: 0, number of available pods: 0
May 29 18:41:19.899: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5562/daemonsets","resourceVersion":"9606"},"items":null}

May 29 18:41:19.906: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5562/pods","resourceVersion":"9606"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:41:19.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5562" for this suite.
May 29 18:41:25.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:41:26.074: INFO: namespace daemonsets-5562 deletion completed in 6.15051642s

• [SLOW TEST:43.463 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:41:26.074: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
May 29 18:41:32.133: INFO: Pod pod-hostip-5d4ab9d4-8241-11e9-b32a-4ae2266202a5 has hostIP: 10.240.0.34
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:41:32.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2354" for this suite.
May 29 18:41:54.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:41:54.235: INFO: namespace pods-2354 deletion completed in 22.096249739s

• [SLOW TEST:28.160 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:41:54.235: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:42:22.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1844" for this suite.
May 29 18:42:28.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:42:28.520: INFO: namespace namespaces-1844 deletion completed in 6.088286543s
STEP: Destroying namespace "nsdeletetest-795" for this suite.
May 29 18:42:28.522: INFO: Namespace nsdeletetest-795 was already deleted
STEP: Destroying namespace "nsdeletetest-5340" for this suite.
May 29 18:42:34.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:42:34.618: INFO: namespace nsdeletetest-5340 deletion completed in 6.095578097s

• [SLOW TEST:40.383 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:42:34.618: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-86260ad5-8241-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 18:42:34.670: INFO: Waiting up to 5m0s for pod "pod-configmaps-86265e91-8241-11e9-b32a-4ae2266202a5" in namespace "configmap-2031" to be "success or failure"
May 29 18:42:34.684: INFO: Pod "pod-configmaps-86265e91-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.783323ms
May 29 18:42:36.687: INFO: Pod "pod-configmaps-86265e91-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017099665s
May 29 18:42:38.691: INFO: Pod "pod-configmaps-86265e91-8241-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020315696s
May 29 18:42:40.694: INFO: Pod "pod-configmaps-86265e91-8241-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023648123s
STEP: Saw pod success
May 29 18:42:40.694: INFO: Pod "pod-configmaps-86265e91-8241-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:42:40.697: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-86265e91-8241-11e9-b32a-4ae2266202a5 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 18:42:40.715: INFO: Waiting for pod pod-configmaps-86265e91-8241-11e9-b32a-4ae2266202a5 to disappear
May 29 18:42:40.717: INFO: Pod pod-configmaps-86265e91-8241-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:42:40.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2031" for this suite.
May 29 18:42:46.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:42:46.799: INFO: namespace configmap-2031 deletion completed in 6.07923976s

• [SLOW TEST:12.181 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:42:46.799: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 18:43:08.854: INFO: Container started at 2019-05-29 18:42:50 +0000 UTC, pod became ready at 2019-05-29 18:43:08 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:43:08.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8223" for this suite.
May 29 18:43:30.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:43:30.944: INFO: namespace container-probe-8223 deletion completed in 22.087331795s

• [SLOW TEST:44.145 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:43:30.944: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 18:43:31.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 version'
May 29 18:43:31.185: INFO: stderr: ""
May 29 18:43:31.185: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.2\", GitCommit:\"66049e3b21efe110454d67df4fa62b08ea79a19b\", GitTreeState:\"clean\", BuildDate:\"2019-05-16T16:23:09Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.2\", GitCommit:\"66049e3b21efe110454d67df4fa62b08ea79a19b\", GitTreeState:\"clean\", BuildDate:\"2019-05-16T16:14:56Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:43:31.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1229" for this suite.
May 29 18:43:37.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:43:37.276: INFO: namespace kubectl-1229 deletion completed in 6.087860494s

• [SLOW TEST:6.332 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:43:37.276: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 29 18:43:49.367: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:43:49.369: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:43:51.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:43:51.373: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:43:53.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:43:53.373: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:43:55.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:43:55.373: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:43:57.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:43:57.373: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:43:59.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:43:59.373: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:44:01.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:44:01.374: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:44:03.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:44:03.373: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:44:05.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:44:05.373: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:44:07.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:44:07.373: INFO: Pod pod-with-poststart-exec-hook still exists
May 29 18:44:09.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 29 18:44:09.373: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:44:09.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5517" for this suite.
May 29 18:44:31.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:44:31.472: INFO: namespace container-lifecycle-hook-5517 deletion completed in 22.095361211s

• [SLOW TEST:54.196 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:44:31.472: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 29 18:44:31.742: INFO: Pod name wrapped-volume-race-cbe5487d-8241-11e9-b32a-4ae2266202a5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cbe5487d-8241-11e9-b32a-4ae2266202a5 in namespace emptydir-wrapper-9617, will wait for the garbage collector to delete the pods
May 29 18:44:55.868: INFO: Deleting ReplicationController wrapped-volume-race-cbe5487d-8241-11e9-b32a-4ae2266202a5 took: 4.853648ms
May 29 18:44:56.268: INFO: Terminating ReplicationController wrapped-volume-race-cbe5487d-8241-11e9-b32a-4ae2266202a5 pods took: 400.257985ms
STEP: Creating RC which spawns configmap-volume pods
May 29 18:45:39.985: INFO: Pod name wrapped-volume-race-f4996266-8241-11e9-b32a-4ae2266202a5: Found 0 pods out of 5
May 29 18:45:44.992: INFO: Pod name wrapped-volume-race-f4996266-8241-11e9-b32a-4ae2266202a5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f4996266-8241-11e9-b32a-4ae2266202a5 in namespace emptydir-wrapper-9617, will wait for the garbage collector to delete the pods
May 29 18:46:05.086: INFO: Deleting ReplicationController wrapped-volume-race-f4996266-8241-11e9-b32a-4ae2266202a5 took: 5.730374ms
May 29 18:46:05.386: INFO: Terminating ReplicationController wrapped-volume-race-f4996266-8241-11e9-b32a-4ae2266202a5 pods took: 300.152311ms
STEP: Creating RC which spawns configmap-volume pods
May 29 18:46:50.005: INFO: Pod name wrapped-volume-race-1e554315-8242-11e9-b32a-4ae2266202a5: Found 0 pods out of 5
May 29 18:46:55.011: INFO: Pod name wrapped-volume-race-1e554315-8242-11e9-b32a-4ae2266202a5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1e554315-8242-11e9-b32a-4ae2266202a5 in namespace emptydir-wrapper-9617, will wait for the garbage collector to delete the pods
May 29 18:47:15.098: INFO: Deleting ReplicationController wrapped-volume-race-1e554315-8242-11e9-b32a-4ae2266202a5 took: 4.587339ms
May 29 18:47:15.398: INFO: Terminating ReplicationController wrapped-volume-race-1e554315-8242-11e9-b32a-4ae2266202a5 pods took: 300.190387ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:48:00.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9617" for this suite.
May 29 18:48:08.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:48:08.650: INFO: namespace emptydir-wrapper-9617 deletion completed in 8.084139029s

• [SLOW TEST:217.177 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:48:08.650: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 29 18:48:08.719: INFO: Waiting up to 5m0s for pod "downward-api-4d404ed0-8242-11e9-b32a-4ae2266202a5" in namespace "downward-api-2575" to be "success or failure"
May 29 18:48:08.728: INFO: Pod "downward-api-4d404ed0-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.396984ms
May 29 18:48:10.732: INFO: Pod "downward-api-4d404ed0-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012776123s
May 29 18:48:12.736: INFO: Pod "downward-api-4d404ed0-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017227191s
May 29 18:48:14.752: INFO: Pod "downward-api-4d404ed0-8242-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032664386s
STEP: Saw pod success
May 29 18:48:14.752: INFO: Pod "downward-api-4d404ed0-8242-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:48:14.762: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downward-api-4d404ed0-8242-11e9-b32a-4ae2266202a5 container dapi-container: <nil>
STEP: delete the pod
May 29 18:48:14.783: INFO: Waiting for pod downward-api-4d404ed0-8242-11e9-b32a-4ae2266202a5 to disappear
May 29 18:48:14.786: INFO: Pod downward-api-4d404ed0-8242-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:48:14.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2575" for this suite.
May 29 18:48:20.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:48:20.877: INFO: namespace downward-api-2575 deletion completed in 6.087994923s

• [SLOW TEST:12.227 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:48:20.878: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
May 29 18:48:20.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 cluster-info'
May 29 18:48:22.996: INFO: stderr: ""
May 29 18:48:22.996: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:48:22.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5049" for this suite.
May 29 18:48:29.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:48:29.088: INFO: namespace kubectl-5049 deletion completed in 6.089413819s

• [SLOW TEST:8.211 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:48:29.088: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:48:37.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2082" for this suite.
May 29 18:48:43.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:48:43.229: INFO: namespace kubelet-test-2082 deletion completed in 6.080950584s

• [SLOW TEST:14.141 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:48:43.229: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 18:48:43.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61db1062-8242-11e9-b32a-4ae2266202a5" in namespace "downward-api-8886" to be "success or failure"
May 29 18:48:43.283: INFO: Pod "downwardapi-volume-61db1062-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.061313ms
May 29 18:48:45.286: INFO: Pod "downwardapi-volume-61db1062-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010324685s
May 29 18:48:47.290: INFO: Pod "downwardapi-volume-61db1062-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013594452s
May 29 18:48:49.293: INFO: Pod "downwardapi-volume-61db1062-8242-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017091624s
STEP: Saw pod success
May 29 18:48:49.293: INFO: Pod "downwardapi-volume-61db1062-8242-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:48:49.296: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-61db1062-8242-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 18:48:49.312: INFO: Waiting for pod downwardapi-volume-61db1062-8242-11e9-b32a-4ae2266202a5 to disappear
May 29 18:48:49.314: INFO: Pod downwardapi-volume-61db1062-8242-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:48:49.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8886" for this suite.
May 29 18:48:55.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:48:55.410: INFO: namespace downward-api-8886 deletion completed in 6.093297391s

• [SLOW TEST:12.181 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:48:55.411: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-692604d1-8242-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 18:48:55.513: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-69266265-8242-11e9-b32a-4ae2266202a5" in namespace "projected-6046" to be "success or failure"
May 29 18:48:55.525: INFO: Pod "pod-projected-secrets-69266265-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.547379ms
May 29 18:48:57.529: INFO: Pod "pod-projected-secrets-69266265-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016652754s
May 29 18:48:59.533: INFO: Pod "pod-projected-secrets-69266265-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019953001s
May 29 18:49:01.536: INFO: Pod "pod-projected-secrets-69266265-8242-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023632156s
STEP: Saw pod success
May 29 18:49:01.536: INFO: Pod "pod-projected-secrets-69266265-8242-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:49:01.539: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-secrets-69266265-8242-11e9-b32a-4ae2266202a5 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 18:49:01.563: INFO: Waiting for pod pod-projected-secrets-69266265-8242-11e9-b32a-4ae2266202a5 to disappear
May 29 18:49:01.569: INFO: Pod pod-projected-secrets-69266265-8242-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:49:01.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6046" for this suite.
May 29 18:49:07.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:49:07.667: INFO: namespace projected-6046 deletion completed in 6.094609166s

• [SLOW TEST:12.256 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:49:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 18:49:07.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8527'
May 29 18:49:07.791: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 18:49:07.791: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
May 29 18:49:11.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8527'
May 29 18:49:11.905: INFO: stderr: ""
May 29 18:49:11.905: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:49:11.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8527" for this suite.
May 29 18:49:33.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:49:34.001: INFO: namespace kubectl-8527 deletion completed in 22.088325531s

• [SLOW TEST:26.334 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:49:34.001: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-qq6d
STEP: Creating a pod to test atomic-volume-subpath
May 29 18:49:34.056: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qq6d" in namespace "subpath-5941" to be "success or failure"
May 29 18:49:34.064: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.511926ms
May 29 18:49:36.068: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011509508s
May 29 18:49:38.073: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016953733s
May 29 18:49:40.077: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 6.020730963s
May 29 18:49:42.081: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 8.02444851s
May 29 18:49:44.090: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 10.03347916s
May 29 18:49:46.094: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 12.037437973s
May 29 18:49:48.097: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 14.040785371s
May 29 18:49:50.100: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 16.043902281s
May 29 18:49:52.104: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 18.047167429s
May 29 18:49:54.107: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 20.050879141s
May 29 18:49:56.111: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 22.054643888s
May 29 18:49:58.114: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Running", Reason="", readiness=true. Elapsed: 24.05791603s
May 29 18:50:00.118: INFO: Pod "pod-subpath-test-projected-qq6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.061433223s
STEP: Saw pod success
May 29 18:50:00.118: INFO: Pod "pod-subpath-test-projected-qq6d" satisfied condition "success or failure"
May 29 18:50:00.121: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-subpath-test-projected-qq6d container test-container-subpath-projected-qq6d: <nil>
STEP: delete the pod
May 29 18:50:00.140: INFO: Waiting for pod pod-subpath-test-projected-qq6d to disappear
May 29 18:50:00.142: INFO: Pod pod-subpath-test-projected-qq6d no longer exists
STEP: Deleting pod pod-subpath-test-projected-qq6d
May 29 18:50:00.142: INFO: Deleting pod "pod-subpath-test-projected-qq6d" in namespace "subpath-5941"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:50:00.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5941" for this suite.
May 29 18:50:06.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:50:06.230: INFO: namespace subpath-5941 deletion completed in 6.08377779s

• [SLOW TEST:32.228 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:50:06.230: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 29 18:50:06.272: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:50:14.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-507" for this suite.
May 29 18:50:20.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:50:20.688: INFO: namespace init-container-507 deletion completed in 6.082670907s

• [SLOW TEST:14.457 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:50:20.688: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 18:50:20.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9bf19616-8242-11e9-b32a-4ae2266202a5" in namespace "projected-3308" to be "success or failure"
May 29 18:50:20.741: INFO: Pod "downwardapi-volume-9bf19616-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.051186ms
May 29 18:50:22.744: INFO: Pod "downwardapi-volume-9bf19616-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010366424s
May 29 18:50:24.747: INFO: Pod "downwardapi-volume-9bf19616-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013645105s
May 29 18:50:26.751: INFO: Pod "downwardapi-volume-9bf19616-8242-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017141847s
STEP: Saw pod success
May 29 18:50:26.751: INFO: Pod "downwardapi-volume-9bf19616-8242-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:50:26.753: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-9bf19616-8242-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 18:50:26.767: INFO: Waiting for pod downwardapi-volume-9bf19616-8242-11e9-b32a-4ae2266202a5 to disappear
May 29 18:50:26.769: INFO: Pod downwardapi-volume-9bf19616-8242-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:50:26.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3308" for this suite.
May 29 18:50:32.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:50:32.874: INFO: namespace projected-3308 deletion completed in 6.102152857s

• [SLOW TEST:12.186 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:50:32.874: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-986/configmap-test-a3359710-8242-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 18:50:32.926: INFO: Waiting up to 5m0s for pod "pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5" in namespace "configmap-986" to be "success or failure"
May 29 18:50:32.935: INFO: Pod "pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.278332ms
May 29 18:50:34.939: INFO: Pod "pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01262684s
May 29 18:50:36.942: INFO: Pod "pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016019302s
May 29 18:50:38.946: INFO: Pod "pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019669435s
May 29 18:50:40.949: INFO: Pod "pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023308921s
STEP: Saw pod success
May 29 18:50:40.949: INFO: Pod "pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:50:40.951: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5 container env-test: <nil>
STEP: delete the pod
May 29 18:50:40.971: INFO: Waiting for pod pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5 to disappear
May 29 18:50:40.973: INFO: Pod pod-configmaps-a336082a-8242-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:50:40.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-986" for this suite.
May 29 18:50:46.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:50:47.072: INFO: namespace configmap-986 deletion completed in 6.09525425s

• [SLOW TEST:14.197 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:50:47.072: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-ababefc8-8242-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 18:50:47.126: INFO: Waiting up to 5m0s for pod "pod-configmaps-abad3e4a-8242-11e9-b32a-4ae2266202a5" in namespace "configmap-396" to be "success or failure"
May 29 18:50:47.135: INFO: Pod "pod-configmaps-abad3e4a-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.767889ms
May 29 18:50:49.139: INFO: Pod "pod-configmaps-abad3e4a-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0127338s
May 29 18:50:51.142: INFO: Pod "pod-configmaps-abad3e4a-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015850012s
May 29 18:50:53.146: INFO: Pod "pod-configmaps-abad3e4a-8242-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019546022s
STEP: Saw pod success
May 29 18:50:53.146: INFO: Pod "pod-configmaps-abad3e4a-8242-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:50:53.148: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-abad3e4a-8242-11e9-b32a-4ae2266202a5 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 18:50:53.169: INFO: Waiting for pod pod-configmaps-abad3e4a-8242-11e9-b32a-4ae2266202a5 to disappear
May 29 18:50:53.171: INFO: Pod pod-configmaps-abad3e4a-8242-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:50:53.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-396" for this suite.
May 29 18:50:59.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:50:59.251: INFO: namespace configmap-396 deletion completed in 6.076700851s

• [SLOW TEST:12.179 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:50:59.251: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
May 29 18:50:59.292: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-947231839 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:50:59.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4710" for this suite.
May 29 18:51:05.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:51:05.447: INFO: namespace kubectl-4710 deletion completed in 6.085126092s

• [SLOW TEST:6.196 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:51:05.449: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 18:51:05.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6a1bbfa-8242-11e9-b32a-4ae2266202a5" in namespace "projected-1414" to be "success or failure"
May 29 18:51:05.511: INFO: Pod "downwardapi-volume-b6a1bbfa-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.563169ms
May 29 18:51:07.514: INFO: Pod "downwardapi-volume-b6a1bbfa-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005848255s
May 29 18:51:09.518: INFO: Pod "downwardapi-volume-b6a1bbfa-8242-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009933461s
May 29 18:51:11.522: INFO: Pod "downwardapi-volume-b6a1bbfa-8242-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013906927s
STEP: Saw pod success
May 29 18:51:11.522: INFO: Pod "downwardapi-volume-b6a1bbfa-8242-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:51:11.525: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-b6a1bbfa-8242-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 18:51:11.542: INFO: Waiting for pod downwardapi-volume-b6a1bbfa-8242-11e9-b32a-4ae2266202a5 to disappear
May 29 18:51:11.544: INFO: Pod downwardapi-volume-b6a1bbfa-8242-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:51:11.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1414" for this suite.
May 29 18:51:17.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:51:17.657: INFO: namespace projected-1414 deletion completed in 6.109793912s

• [SLOW TEST:12.208 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:51:17.658: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
May 29 18:51:17.706: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1519" to be "success or failure"
May 29 18:51:17.716: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 9.365413ms
May 29 18:51:19.719: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012275171s
May 29 18:51:21.723: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016169064s
May 29 18:51:23.726: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019039765s
May 29 18:51:25.729: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022407972s
STEP: Saw pod success
May 29 18:51:25.729: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 29 18:51:25.731: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 29 18:51:25.748: INFO: Waiting for pod pod-host-path-test to disappear
May 29 18:51:25.750: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:51:25.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1519" for this suite.
May 29 18:51:31.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:51:31.832: INFO: namespace hostpath-1519 deletion completed in 6.079369049s

• [SLOW TEST:14.174 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:51:31.833: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9033
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 29 18:51:31.895: INFO: Found 0 stateful pods, waiting for 3
May 29 18:51:41.900: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:51:41.900: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:51:41.900: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 29 18:51:51.899: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:51:51.899: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:51:51.899: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 29 18:51:51.922: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 29 18:52:01.959: INFO: Updating stateful set ss2
May 29 18:52:01.975: INFO: Waiting for Pod statefulset-9033/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 29 18:52:12.047: INFO: Found 2 stateful pods, waiting for 3
May 29 18:52:22.051: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:52:22.051: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:52:22.051: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 29 18:52:32.054: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:52:32.054: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:52:32.054: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 29 18:52:32.076: INFO: Updating stateful set ss2
May 29 18:52:32.090: INFO: Waiting for Pod statefulset-9033/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 18:52:42.115: INFO: Updating stateful set ss2
May 29 18:52:42.122: INFO: Waiting for StatefulSet statefulset-9033/ss2 to complete update
May 29 18:52:42.122: INFO: Waiting for Pod statefulset-9033/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 18:52:52.129: INFO: Waiting for StatefulSet statefulset-9033/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 18:53:02.128: INFO: Deleting all statefulset in ns statefulset-9033
May 29 18:53:02.130: INFO: Scaling statefulset ss2 to 0
May 29 18:53:32.162: INFO: Waiting for statefulset status.replicas updated to 0
May 29 18:53:32.165: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:53:32.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9033" for this suite.
May 29 18:53:38.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:53:38.270: INFO: namespace statefulset-9033 deletion completed in 6.086837274s

• [SLOW TEST:126.438 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:53:38.271: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 29 18:53:38.323: INFO: Waiting up to 5m0s for pod "pod-11b75fe8-8243-11e9-b32a-4ae2266202a5" in namespace "emptydir-893" to be "success or failure"
May 29 18:53:38.329: INFO: Pod "pod-11b75fe8-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.833741ms
May 29 18:53:40.332: INFO: Pod "pod-11b75fe8-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00946246s
May 29 18:53:42.336: INFO: Pod "pod-11b75fe8-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013255216s
May 29 18:53:44.339: INFO: Pod "pod-11b75fe8-8243-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016574572s
STEP: Saw pod success
May 29 18:53:44.340: INFO: Pod "pod-11b75fe8-8243-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:53:44.342: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-11b75fe8-8243-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 18:53:44.362: INFO: Waiting for pod pod-11b75fe8-8243-11e9-b32a-4ae2266202a5 to disappear
May 29 18:53:44.366: INFO: Pod pod-11b75fe8-8243-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:53:44.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-893" for this suite.
May 29 18:53:50.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:53:50.466: INFO: namespace emptydir-893 deletion completed in 6.09775672s

• [SLOW TEST:12.196 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:53:50.467: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1657
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 29 18:53:50.559: INFO: Found 0 stateful pods, waiting for 3
May 29 18:54:00.563: INFO: Found 2 stateful pods, waiting for 3
May 29 18:54:10.563: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:54:10.563: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:54:10.563: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 29 18:54:10.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-1657 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 18:54:10.806: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 18:54:10.806: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 18:54:10.806: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 29 18:54:20.837: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 29 18:54:30.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-1657 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:54:31.067: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 29 18:54:31.067: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 18:54:31.067: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 18:54:41.084: INFO: Waiting for StatefulSet statefulset-1657/ss2 to complete update
May 29 18:54:41.084: INFO: Waiting for Pod statefulset-1657/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 18:54:41.084: INFO: Waiting for Pod statefulset-1657/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 18:54:51.092: INFO: Waiting for StatefulSet statefulset-1657/ss2 to complete update
May 29 18:54:51.092: INFO: Waiting for Pod statefulset-1657/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 29 18:55:01.090: INFO: Waiting for StatefulSet statefulset-1657/ss2 to complete update
STEP: Rolling back to a previous revision
May 29 18:55:11.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-1657 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 29 18:55:11.318: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 29 18:55:11.318: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 29 18:55:11.318: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 29 18:55:21.344: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 29 18:55:31.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 exec --namespace=statefulset-1657 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 29 18:55:31.576: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 29 18:55:31.576: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 29 18:55:31.576: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 29 18:55:51.593: INFO: Waiting for StatefulSet statefulset-1657/ss2 to complete update
May 29 18:55:51.593: INFO: Waiting for Pod statefulset-1657/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 29 18:56:01.599: INFO: Waiting for StatefulSet statefulset-1657/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 18:56:11.600: INFO: Deleting all statefulset in ns statefulset-1657
May 29 18:56:11.602: INFO: Scaling statefulset ss2 to 0
May 29 18:56:41.614: INFO: Waiting for statefulset status.replicas updated to 0
May 29 18:56:41.618: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:56:41.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1657" for this suite.
May 29 18:56:47.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:56:47.719: INFO: namespace statefulset-1657 deletion completed in 6.086636371s

• [SLOW TEST:177.252 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:56:47.720: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
May 29 18:56:54.290: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3210 pod-service-account-82f0b69d-8243-11e9-b32a-4ae2266202a5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 29 18:56:54.503: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3210 pod-service-account-82f0b69d-8243-11e9-b32a-4ae2266202a5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 29 18:56:54.712: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3210 pod-service-account-82f0b69d-8243-11e9-b32a-4ae2266202a5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:56:54.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3210" for this suite.
May 29 18:57:00.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:57:01.005: INFO: namespace svcaccounts-3210 deletion completed in 6.085534619s

• [SLOW TEST:13.285 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:57:01.007: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-8a8f3399-8243-11e9-b32a-4ae2266202a5
STEP: Creating secret with name s-test-opt-upd-8a8f33dc-8243-11e9-b32a-4ae2266202a5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8a8f3399-8243-11e9-b32a-4ae2266202a5
STEP: Updating secret s-test-opt-upd-8a8f33dc-8243-11e9-b32a-4ae2266202a5
STEP: Creating secret with name s-test-opt-create-8a8f33f3-8243-11e9-b32a-4ae2266202a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:57:11.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-626" for this suite.
May 29 18:57:33.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:57:33.228: INFO: namespace projected-626 deletion completed in 22.078844297s

• [SLOW TEST:32.221 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:57:33.229: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
May 29 18:57:33.274: INFO: Waiting up to 5m0s for pod "var-expansion-9dc26a3b-8243-11e9-b32a-4ae2266202a5" in namespace "var-expansion-8077" to be "success or failure"
May 29 18:57:33.279: INFO: Pod "var-expansion-9dc26a3b-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.571781ms
May 29 18:57:35.283: INFO: Pod "var-expansion-9dc26a3b-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009048814s
May 29 18:57:37.287: INFO: Pod "var-expansion-9dc26a3b-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013375782s
May 29 18:57:39.292: INFO: Pod "var-expansion-9dc26a3b-8243-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01790795s
STEP: Saw pod success
May 29 18:57:39.292: INFO: Pod "var-expansion-9dc26a3b-8243-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:57:39.294: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod var-expansion-9dc26a3b-8243-11e9-b32a-4ae2266202a5 container dapi-container: <nil>
STEP: delete the pod
May 29 18:57:39.314: INFO: Waiting for pod var-expansion-9dc26a3b-8243-11e9-b32a-4ae2266202a5 to disappear
May 29 18:57:39.316: INFO: Pod var-expansion-9dc26a3b-8243-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:57:39.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8077" for this suite.
May 29 18:57:45.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:57:45.407: INFO: namespace var-expansion-8077 deletion completed in 6.088686306s

• [SLOW TEST:12.179 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:57:45.408: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-a505b2a2-8243-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 18:57:45.463: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5065458-8243-11e9-b32a-4ae2266202a5" in namespace "projected-74" to be "success or failure"
May 29 18:57:45.465: INFO: Pod "pod-projected-configmaps-a5065458-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324416ms
May 29 18:57:47.469: INFO: Pod "pod-projected-configmaps-a5065458-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005587482s
May 29 18:57:49.472: INFO: Pod "pod-projected-configmaps-a5065458-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009327165s
May 29 18:57:51.476: INFO: Pod "pod-projected-configmaps-a5065458-8243-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01264612s
STEP: Saw pod success
May 29 18:57:51.476: INFO: Pod "pod-projected-configmaps-a5065458-8243-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:57:51.478: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-configmaps-a5065458-8243-11e9-b32a-4ae2266202a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 18:57:51.501: INFO: Waiting for pod pod-projected-configmaps-a5065458-8243-11e9-b32a-4ae2266202a5 to disappear
May 29 18:57:51.503: INFO: Pod pod-projected-configmaps-a5065458-8243-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:57:51.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-74" for this suite.
May 29 18:57:57.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:57:57.587: INFO: namespace projected-74 deletion completed in 6.080662942s

• [SLOW TEST:12.179 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:57:57.587: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-4196/secret-test-ac473745-8243-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 18:57:57.637: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac47b0f8-8243-11e9-b32a-4ae2266202a5" in namespace "secrets-4196" to be "success or failure"
May 29 18:57:57.646: INFO: Pod "pod-configmaps-ac47b0f8-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.742734ms
May 29 18:57:59.649: INFO: Pod "pod-configmaps-ac47b0f8-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01183875s
May 29 18:58:01.652: INFO: Pod "pod-configmaps-ac47b0f8-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015520191s
May 29 18:58:03.656: INFO: Pod "pod-configmaps-ac47b0f8-8243-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019169526s
STEP: Saw pod success
May 29 18:58:03.656: INFO: Pod "pod-configmaps-ac47b0f8-8243-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:58:03.658: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-ac47b0f8-8243-11e9-b32a-4ae2266202a5 container env-test: <nil>
STEP: delete the pod
May 29 18:58:03.681: INFO: Waiting for pod pod-configmaps-ac47b0f8-8243-11e9-b32a-4ae2266202a5 to disappear
May 29 18:58:03.683: INFO: Pod pod-configmaps-ac47b0f8-8243-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:58:03.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4196" for this suite.
May 29 18:58:09.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:58:09.772: INFO: namespace secrets-4196 deletion completed in 6.079750879s

• [SLOW TEST:12.185 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:58:09.773: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-b38a4fd8-8243-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 18:58:09.818: INFO: Waiting up to 5m0s for pod "pod-secrets-b38ab0ab-8243-11e9-b32a-4ae2266202a5" in namespace "secrets-8534" to be "success or failure"
May 29 18:58:09.826: INFO: Pod "pod-secrets-b38ab0ab-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.767883ms
May 29 18:58:11.829: INFO: Pod "pod-secrets-b38ab0ab-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011236191s
May 29 18:58:13.833: INFO: Pod "pod-secrets-b38ab0ab-8243-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014709598s
May 29 18:58:15.836: INFO: Pod "pod-secrets-b38ab0ab-8243-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018264007s
STEP: Saw pod success
May 29 18:58:15.836: INFO: Pod "pod-secrets-b38ab0ab-8243-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 18:58:15.839: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-secrets-b38ab0ab-8243-11e9-b32a-4ae2266202a5 container secret-volume-test: <nil>
STEP: delete the pod
May 29 18:58:15.857: INFO: Waiting for pod pod-secrets-b38ab0ab-8243-11e9-b32a-4ae2266202a5 to disappear
May 29 18:58:15.861: INFO: Pod pod-secrets-b38ab0ab-8243-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:58:15.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8534" for this suite.
May 29 18:58:21.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:58:21.952: INFO: namespace secrets-8534 deletion completed in 6.087421686s

• [SLOW TEST:12.179 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:58:21.952: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:58:28.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1774" for this suite.
May 29 18:59:12.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:59:12.109: INFO: namespace kubelet-test-1774 deletion completed in 44.088129885s

• [SLOW TEST:50.157 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:59:12.110: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-d8b31dac-8243-11e9-b32a-4ae2266202a5
STEP: Creating secret with name s-test-opt-upd-d8b31df1-8243-11e9-b32a-4ae2266202a5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d8b31dac-8243-11e9-b32a-4ae2266202a5
STEP: Updating secret s-test-opt-upd-d8b31df1-8243-11e9-b32a-4ae2266202a5
STEP: Creating secret with name s-test-opt-create-d8b31e09-8243-11e9-b32a-4ae2266202a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:59:24.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7143" for this suite.
May 29 18:59:46.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 18:59:46.342: INFO: namespace secrets-7143 deletion completed in 22.085976144s

• [SLOW TEST:34.232 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 18:59:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 29 18:59:46.394: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 18:59:54.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6687" for this suite.
May 29 19:00:00.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:00:00.195: INFO: namespace init-container-6687 deletion completed in 6.090285733s

• [SLOW TEST:13.853 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:00:00.196: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 19:00:00.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6978'
May 29 19:00:02.276: INFO: stderr: ""
May 29 19:00:02.276: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 29 19:00:07.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pod e2e-test-nginx-pod --namespace=kubectl-6978 -o json'
May 29 19:00:07.413: INFO: stderr: ""
May 29 19:00:07.413: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-05-29T19:00:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6978\",\n        \"resourceVersion\": \"13608\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6978/pods/e2e-test-nginx-pod\",\n        \"uid\": \"f692e079-8243-11e9-9319-000d3afdb8b2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-pz9pn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-pool1-11779686-vmss000000\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-pz9pn\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-pz9pn\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-29T19:00:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-29T19:00:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-29T19:00:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-29T19:00:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://db227cccd581420a9436cea83e8cdd0f99d6c3e882f001c83ae90f2e56015a1a\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-29T19:00:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.34\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.240.0.38\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-29T19:00:02Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 29 19:00:07.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 replace -f - --namespace=kubectl-6978'
May 29 19:00:07.758: INFO: stderr: ""
May 29 19:00:07.758: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
May 29 19:00:07.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete pods e2e-test-nginx-pod --namespace=kubectl-6978'
May 29 19:00:13.751: INFO: stderr: ""
May 29 19:00:13.751: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:00:13.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6978" for this suite.
May 29 19:00:19.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:00:19.845: INFO: namespace kubectl-6978 deletion completed in 6.090326017s

• [SLOW TEST:19.649 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:00:19.849: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0529 19:00:29.909314      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 19:00:29.909: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:00:29.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2886" for this suite.
May 29 19:00:35.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:00:35.995: INFO: namespace gc-2886 deletion completed in 6.083163366s

• [SLOW TEST:16.145 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:00:35.995: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:01:14.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1432" for this suite.
May 29 19:01:20.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:01:20.399: INFO: namespace container-runtime-1432 deletion completed in 6.086942367s

• [SLOW TEST:44.404 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:01:20.400: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 19:01:20.442: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:01:21.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2539" for this suite.
May 29 19:01:27.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:01:27.611: INFO: namespace custom-resource-definition-2539 deletion completed in 6.096964352s

• [SLOW TEST:7.212 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:01:27.612: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 29 19:01:27.706: INFO: Waiting up to 5m0s for pod "downward-api-297da914-8244-11e9-b32a-4ae2266202a5" in namespace "downward-api-8528" to be "success or failure"
May 29 19:01:27.710: INFO: Pod "downward-api-297da914-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.399454ms
May 29 19:01:29.713: INFO: Pod "downward-api-297da914-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007144435s
May 29 19:01:31.720: INFO: Pod "downward-api-297da914-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013462363s
May 29 19:01:33.724: INFO: Pod "downward-api-297da914-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017437016s
STEP: Saw pod success
May 29 19:01:33.724: INFO: Pod "downward-api-297da914-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:01:33.726: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downward-api-297da914-8244-11e9-b32a-4ae2266202a5 container dapi-container: <nil>
STEP: delete the pod
May 29 19:01:33.740: INFO: Waiting for pod downward-api-297da914-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:01:33.746: INFO: Pod downward-api-297da914-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:01:33.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8528" for this suite.
May 29 19:01:39.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:01:39.844: INFO: namespace downward-api-8528 deletion completed in 6.095045787s

• [SLOW TEST:12.232 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:01:39.844: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-30c100af-8244-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 19:01:39.893: INFO: Waiting up to 5m0s for pod "pod-secrets-30c1611d-8244-11e9-b32a-4ae2266202a5" in namespace "secrets-106" to be "success or failure"
May 29 19:01:39.900: INFO: Pod "pod-secrets-30c1611d-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.863508ms
May 29 19:01:41.909: INFO: Pod "pod-secrets-30c1611d-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015611999s
May 29 19:01:43.912: INFO: Pod "pod-secrets-30c1611d-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019182788s
May 29 19:01:45.917: INFO: Pod "pod-secrets-30c1611d-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023879662s
STEP: Saw pod success
May 29 19:01:45.917: INFO: Pod "pod-secrets-30c1611d-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:01:45.919: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-secrets-30c1611d-8244-11e9-b32a-4ae2266202a5 container secret-volume-test: <nil>
STEP: delete the pod
May 29 19:01:45.943: INFO: Waiting for pod pod-secrets-30c1611d-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:01:45.945: INFO: Pod pod-secrets-30c1611d-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:01:45.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-106" for this suite.
May 29 19:01:51.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:01:52.039: INFO: namespace secrets-106 deletion completed in 6.091511782s

• [SLOW TEST:12.195 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:01:52.039: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0529 19:02:02.187728      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 19:02:02.187: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:02:02.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9216" for this suite.
May 29 19:02:08.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:02:08.275: INFO: namespace gc-9216 deletion completed in 6.084380252s

• [SLOW TEST:16.236 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:02:08.275: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 19:02:08.366: INFO: Creating deployment "test-recreate-deployment"
May 29 19:02:08.369: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 29 19:02:08.384: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 29 19:02:10.389: INFO: Waiting deployment "test-recreate-deployment" to complete
May 29 19:02:10.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:02:12.397: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:02:14.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:02:16.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:02:18.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753328, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:02:20.396: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 29 19:02:20.402: INFO: Updating deployment test-recreate-deployment
May 29 19:02:20.402: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 19:02:20.482: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3245,SelfLink:/apis/apps/v1/namespaces/deployment-3245/deployments/test-recreate-deployment,UID:41bc00de-8244-11e9-9319-000d3afdb8b2,ResourceVersion:14181,Generation:2,CreationTimestamp:2019-05-29 19:02:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-29 19:02:20 +0000 UTC 2019-05-29 19:02:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-29 19:02:20 +0000 UTC 2019-05-29 19:02:08 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 29 19:02:20.485: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-3245,SelfLink:/apis/apps/v1/namespaces/deployment-3245/replicasets/test-recreate-deployment-c9cbd8684,UID:48ee8050-8244-11e9-9319-000d3afdb8b2,ResourceVersion:14180,Generation:1,CreationTimestamp:2019-05-29 19:02:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 41bc00de-8244-11e9-9319-000d3afdb8b2 0xc0024c5620 0xc0024c5621}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 19:02:20.485: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 29 19:02:20.485: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-3245,SelfLink:/apis/apps/v1/namespaces/deployment-3245/replicasets/test-recreate-deployment-7d57d5ff7c,UID:41bc9c53-8244-11e9-9319-000d3afdb8b2,ResourceVersion:14170,Generation:2,CreationTimestamp:2019-05-29 19:02:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 41bc00de-8244-11e9-9319-000d3afdb8b2 0xc0024c5557 0xc0024c5558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 19:02:20.488: INFO: Pod "test-recreate-deployment-c9cbd8684-tjffk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-tjffk,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-3245,SelfLink:/api/v1/namespaces/deployment-3245/pods/test-recreate-deployment-c9cbd8684-tjffk,UID:48eeef41-8244-11e9-9319-000d3afdb8b2,ResourceVersion:14182,Generation:0,CreationTimestamp:2019-05-29 19:02:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 48ee8050-8244-11e9-9319-000d3afdb8b2 0xc0024c5e70 0xc0024c5e71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tqn75 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tqn75,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tqn75 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024c5ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024c5ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:02:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:02:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:02:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:02:20 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:,StartTime:2019-05-29 19:02:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:02:20.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3245" for this suite.
May 29 19:02:26.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:02:26.577: INFO: namespace deployment-3245 deletion completed in 6.086331084s

• [SLOW TEST:18.302 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:02:26.577: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 29 19:02:26.621: INFO: Waiting up to 5m0s for pod "pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5" in namespace "emptydir-7564" to be "success or failure"
May 29 19:02:26.634: INFO: Pod "pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.960173ms
May 29 19:02:28.638: INFO: Pod "pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016740334s
May 29 19:02:30.643: INFO: Pod "pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021317169s
May 29 19:02:32.646: INFO: Pod "pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024298773s
May 29 19:02:34.648: INFO: Pod "pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026775993s
STEP: Saw pod success
May 29 19:02:34.648: INFO: Pod "pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:02:34.651: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 19:02:34.670: INFO: Waiting for pod pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:02:34.673: INFO: Pod pod-4c9bae5b-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:02:34.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7564" for this suite.
May 29 19:02:40.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:02:40.760: INFO: namespace emptydir-7564 deletion completed in 6.084560888s

• [SLOW TEST:14.183 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:02:40.760: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
May 29 19:02:40.802: INFO: Waiting up to 5m0s for pod "client-containers-550f7e10-8244-11e9-b32a-4ae2266202a5" in namespace "containers-1138" to be "success or failure"
May 29 19:02:40.810: INFO: Pod "client-containers-550f7e10-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.924149ms
May 29 19:02:42.813: INFO: Pod "client-containers-550f7e10-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011302062s
May 29 19:02:44.816: INFO: Pod "client-containers-550f7e10-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014578611s
May 29 19:02:46.820: INFO: Pod "client-containers-550f7e10-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017774598s
STEP: Saw pod success
May 29 19:02:46.820: INFO: Pod "client-containers-550f7e10-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:02:46.822: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod client-containers-550f7e10-8244-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 19:02:46.841: INFO: Waiting for pod client-containers-550f7e10-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:02:46.843: INFO: Pod client-containers-550f7e10-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:02:46.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1138" for this suite.
May 29 19:02:52.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:02:52.939: INFO: namespace containers-1138 deletion completed in 6.092311227s

• [SLOW TEST:12.179 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:02:52.939: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-5c5adad9-8244-11e9-b32a-4ae2266202a5
STEP: Creating secret with name secret-projected-all-test-volume-5c5adacc-8244-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test Check all projections for projected volume plugin
May 29 19:02:53.046: INFO: Waiting up to 5m0s for pod "projected-volume-5c5adaa2-8244-11e9-b32a-4ae2266202a5" in namespace "projected-9303" to be "success or failure"
May 29 19:02:53.055: INFO: Pod "projected-volume-5c5adaa2-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.339909ms
May 29 19:02:55.059: INFO: Pod "projected-volume-5c5adaa2-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01293847s
May 29 19:02:57.062: INFO: Pod "projected-volume-5c5adaa2-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01577464s
May 29 19:02:59.066: INFO: Pod "projected-volume-5c5adaa2-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019803803s
STEP: Saw pod success
May 29 19:02:59.066: INFO: Pod "projected-volume-5c5adaa2-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:02:59.068: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod projected-volume-5c5adaa2-8244-11e9-b32a-4ae2266202a5 container projected-all-volume-test: <nil>
STEP: delete the pod
May 29 19:02:59.090: INFO: Waiting for pod projected-volume-5c5adaa2-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:02:59.092: INFO: Pod projected-volume-5c5adaa2-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:02:59.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9303" for this suite.
May 29 19:03:05.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:03:05.179: INFO: namespace projected-9303 deletion completed in 6.084189387s

• [SLOW TEST:12.240 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:03:05.180: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
May 29 19:03:05.220: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 29 19:03:05.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-5184'
May 29 19:03:05.624: INFO: stderr: ""
May 29 19:03:05.624: INFO: stdout: "service/redis-slave created\n"
May 29 19:03:05.624: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 29 19:03:05.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-5184'
May 29 19:03:06.016: INFO: stderr: ""
May 29 19:03:06.016: INFO: stdout: "service/redis-master created\n"
May 29 19:03:06.018: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 29 19:03:06.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-5184'
May 29 19:03:06.386: INFO: stderr: ""
May 29 19:03:06.386: INFO: stdout: "service/frontend created\n"
May 29 19:03:06.387: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 29 19:03:06.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-5184'
May 29 19:03:06.712: INFO: stderr: ""
May 29 19:03:06.712: INFO: stdout: "deployment.apps/frontend created\n"
May 29 19:03:06.713: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 29 19:03:06.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-5184'
May 29 19:03:07.079: INFO: stderr: ""
May 29 19:03:07.079: INFO: stdout: "deployment.apps/redis-master created\n"
May 29 19:03:07.079: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 29 19:03:07.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-5184'
May 29 19:03:08.662: INFO: stderr: ""
May 29 19:03:08.662: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 29 19:03:08.662: INFO: Waiting for all frontend pods to be Running.
May 29 19:03:58.714: INFO: Waiting for frontend to serve content.
May 29 19:03:58.730: INFO: Trying to add a new entry to the guestbook.
May 29 19:03:58.739: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 29 19:03:58.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-5184'
May 29 19:03:58.867: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 19:03:58.867: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 29 19:03:58.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-5184'
May 29 19:03:59.000: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 19:03:59.002: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 29 19:03:59.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-5184'
May 29 19:03:59.132: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 19:03:59.132: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 29 19:03:59.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-5184'
May 29 19:03:59.235: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 19:03:59.235: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 29 19:03:59.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-5184'
May 29 19:03:59.323: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 19:03:59.323: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 29 19:03:59.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-5184'
May 29 19:03:59.408: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 19:03:59.408: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:03:59.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5184" for this suite.
May 29 19:04:41.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:04:41.495: INFO: namespace kubectl-5184 deletion completed in 42.08369764s

• [SLOW TEST:96.316 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:04:41.496: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-9d078d97-8244-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 19:04:41.548: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d07f472-8244-11e9-b32a-4ae2266202a5" in namespace "configmap-4226" to be "success or failure"
May 29 19:04:41.555: INFO: Pod "pod-configmaps-9d07f472-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681084ms
May 29 19:04:43.559: INFO: Pod "pod-configmaps-9d07f472-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010861513s
May 29 19:04:45.562: INFO: Pod "pod-configmaps-9d07f472-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014026053s
May 29 19:04:47.565: INFO: Pod "pod-configmaps-9d07f472-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017070141s
STEP: Saw pod success
May 29 19:04:47.565: INFO: Pod "pod-configmaps-9d07f472-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:04:47.568: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-9d07f472-8244-11e9-b32a-4ae2266202a5 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 19:04:47.582: INFO: Waiting for pod pod-configmaps-9d07f472-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:04:47.584: INFO: Pod pod-configmaps-9d07f472-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:04:47.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4226" for this suite.
May 29 19:04:53.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:04:53.676: INFO: namespace configmap-4226 deletion completed in 6.087113976s

• [SLOW TEST:12.180 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:04:53.677: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-a44909e3-8244-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 19:04:53.726: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a44a1cdb-8244-11e9-b32a-4ae2266202a5" in namespace "projected-9851" to be "success or failure"
May 29 19:04:53.728: INFO: Pod "pod-projected-secrets-a44a1cdb-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.290197ms
May 29 19:04:55.732: INFO: Pod "pod-projected-secrets-a44a1cdb-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005984122s
May 29 19:04:57.735: INFO: Pod "pod-projected-secrets-a44a1cdb-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009508994s
May 29 19:04:59.739: INFO: Pod "pod-projected-secrets-a44a1cdb-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013397735s
STEP: Saw pod success
May 29 19:04:59.739: INFO: Pod "pod-projected-secrets-a44a1cdb-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:04:59.742: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-secrets-a44a1cdb-8244-11e9-b32a-4ae2266202a5 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 19:04:59.765: INFO: Waiting for pod pod-projected-secrets-a44a1cdb-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:04:59.766: INFO: Pod pod-projected-secrets-a44a1cdb-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:04:59.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9851" for this suite.
May 29 19:05:05.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:05:05.862: INFO: namespace projected-9851 deletion completed in 6.09197703s

• [SLOW TEST:12.185 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:05:05.862: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 29 19:05:12.441: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ab8e365f-8244-11e9-b32a-4ae2266202a5"
May 29 19:05:12.441: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ab8e365f-8244-11e9-b32a-4ae2266202a5" in namespace "pods-5129" to be "terminated due to deadline exceeded"
May 29 19:05:12.444: INFO: Pod "pod-update-activedeadlineseconds-ab8e365f-8244-11e9-b32a-4ae2266202a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.793217ms
May 29 19:05:14.448: INFO: Pod "pod-update-activedeadlineseconds-ab8e365f-8244-11e9-b32a-4ae2266202a5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.006646225s
May 29 19:05:14.448: INFO: Pod "pod-update-activedeadlineseconds-ab8e365f-8244-11e9-b32a-4ae2266202a5" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:05:14.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5129" for this suite.
May 29 19:05:20.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:05:20.545: INFO: namespace pods-5129 deletion completed in 6.093825207s

• [SLOW TEST:14.683 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:05:20.545: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-b44cebd9-8244-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 19:05:20.591: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b44d49e2-8244-11e9-b32a-4ae2266202a5" in namespace "projected-8332" to be "success or failure"
May 29 19:05:20.593: INFO: Pod "pod-projected-secrets-b44d49e2-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.196992ms
May 29 19:05:22.596: INFO: Pod "pod-projected-secrets-b44d49e2-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005419993s
May 29 19:05:24.600: INFO: Pod "pod-projected-secrets-b44d49e2-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008712053s
May 29 19:05:26.603: INFO: Pod "pod-projected-secrets-b44d49e2-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012288182s
STEP: Saw pod success
May 29 19:05:26.603: INFO: Pod "pod-projected-secrets-b44d49e2-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:05:26.605: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-secrets-b44d49e2-8244-11e9-b32a-4ae2266202a5 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 19:05:26.622: INFO: Waiting for pod pod-projected-secrets-b44d49e2-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:05:26.625: INFO: Pod pod-projected-secrets-b44d49e2-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:05:26.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8332" for this suite.
May 29 19:05:32.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:05:32.717: INFO: namespace projected-8332 deletion completed in 6.089325811s

• [SLOW TEST:12.172 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:05:32.717: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 19:05:32.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb95fc71-8244-11e9-b32a-4ae2266202a5" in namespace "projected-3394" to be "success or failure"
May 29 19:05:32.817: INFO: Pod "downwardapi-volume-bb95fc71-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.961266ms
May 29 19:05:34.821: INFO: Pod "downwardapi-volume-bb95fc71-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007847231s
May 29 19:05:36.824: INFO: Pod "downwardapi-volume-bb95fc71-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011657949s
May 29 19:05:38.828: INFO: Pod "downwardapi-volume-bb95fc71-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015163513s
STEP: Saw pod success
May 29 19:05:38.828: INFO: Pod "downwardapi-volume-bb95fc71-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:05:38.830: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-bb95fc71-8244-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 19:05:38.856: INFO: Waiting for pod downwardapi-volume-bb95fc71-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:05:38.858: INFO: Pod downwardapi-volume-bb95fc71-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:05:38.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3394" for this suite.
May 29 19:05:44.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:05:44.952: INFO: namespace projected-3394 deletion completed in 6.091526612s

• [SLOW TEST:12.235 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:05:44.954: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 19:05:45.005: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 29 19:05:50.009: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 29 19:05:50.009: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 19:05:50.031: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-4191,SelfLink:/apis/apps/v1/namespaces/deployment-4191/deployments/test-cleanup-deployment,UID:c5d873ec-8244-11e9-9319-000d3afdb8b2,ResourceVersion:14966,Generation:1,CreationTimestamp:2019-05-29 19:05:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 29 19:05:50.036: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-4191,SelfLink:/apis/apps/v1/namespaces/deployment-4191/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:c5da4525-8244-11e9-9319-000d3afdb8b2,ResourceVersion:14968,Generation:1,CreationTimestamp:2019-05-29 19:05:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment c5d873ec-8244-11e9-9319-000d3afdb8b2 0xc002431727 0xc002431728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 19:05:50.036: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 29 19:05:50.036: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-4191,SelfLink:/apis/apps/v1/namespaces/deployment-4191/replicasets/test-cleanup-controller,UID:c2dae26e-8244-11e9-9319-000d3afdb8b2,ResourceVersion:14967,Generation:1,CreationTimestamp:2019-05-29 19:05:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment c5d873ec-8244-11e9-9319-000d3afdb8b2 0xc002431617 0xc002431618}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 29 19:05:50.043: INFO: Pod "test-cleanup-controller-j8vlz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-j8vlz,GenerateName:test-cleanup-controller-,Namespace:deployment-4191,SelfLink:/api/v1/namespaces/deployment-4191/pods/test-cleanup-controller-j8vlz,UID:c2dc3c30-8244-11e9-9319-000d3afdb8b2,ResourceVersion:14964,Generation:0,CreationTimestamp:2019-05-29 19:05:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller c2dae26e-8244-11e9-9319-000d3afdb8b2 0xc0029880e7 0xc0029880e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cxch2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cxch2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cxch2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029881b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002988230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:05:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:05:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:05:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:05:45 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.40,StartTime:2019-05-29 19:05:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-29 19:05:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d94f26ad4446c3181206b0f5b575899d8d0e83b854fb6aed0ba41429df8b93b8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 29 19:05:50.043: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-tjd7n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-tjd7n,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-4191,SelfLink:/api/v1/namespaces/deployment-4191/pods/test-cleanup-deployment-55cbfbc8f5-tjd7n,UID:c5db72dd-8244-11e9-9319-000d3afdb8b2,ResourceVersion:14969,Generation:0,CreationTimestamp:2019-05-29 19:05:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 c5da4525-8244-11e9-9319-000d3afdb8b2 0xc002988307 0xc002988308}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cxch2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cxch2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-cxch2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002988370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002988390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:05:50.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4191" for this suite.
May 29 19:05:56.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:05:56.149: INFO: namespace deployment-4191 deletion completed in 6.096974429s

• [SLOW TEST:11.195 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:05:56.149: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3407
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 29 19:05:56.193: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 29 19:06:20.246: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.61:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3407 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:06:20.246: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:06:20.387: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:06:20.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3407" for this suite.
May 29 19:06:42.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:06:42.479: INFO: namespace pod-network-test-3407 deletion completed in 22.088438089s

• [SLOW TEST:46.330 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:06:42.481: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
May 29 19:06:43.066: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 29 19:06:45.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:06:47.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:06:49.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:06:51.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:06:53.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:06:55.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:06:57.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:06:59.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:07:01.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:07:03.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:07:05.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:07:07.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:07:09.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753603, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:07:11.946: INFO: Waited 820.630025ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:07:12.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1437" for this suite.
May 29 19:07:18.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:07:18.622: INFO: namespace aggregator-1437 deletion completed in 6.17486534s

• [SLOW TEST:36.141 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:07:18.622: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 19:07:18.675: INFO: Waiting up to 5m0s for pod "downwardapi-volume-faaf7513-8244-11e9-b32a-4ae2266202a5" in namespace "projected-2777" to be "success or failure"
May 29 19:07:18.680: INFO: Pod "downwardapi-volume-faaf7513-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.198212ms
May 29 19:07:20.683: INFO: Pod "downwardapi-volume-faaf7513-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007900984s
May 29 19:07:22.686: INFO: Pod "downwardapi-volume-faaf7513-8244-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010919034s
May 29 19:07:24.693: INFO: Pod "downwardapi-volume-faaf7513-8244-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018214723s
STEP: Saw pod success
May 29 19:07:24.693: INFO: Pod "downwardapi-volume-faaf7513-8244-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:07:24.696: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-faaf7513-8244-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 19:07:24.714: INFO: Waiting for pod downwardapi-volume-faaf7513-8244-11e9-b32a-4ae2266202a5 to disappear
May 29 19:07:24.716: INFO: Pod downwardapi-volume-faaf7513-8244-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:07:24.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2777" for this suite.
May 29 19:07:30.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:07:30.828: INFO: namespace projected-2777 deletion completed in 6.107966063s

• [SLOW TEST:12.206 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:07:30.830: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-01f73ef8-8245-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 19:07:30.891: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-01f7a1f8-8245-11e9-b32a-4ae2266202a5" in namespace "projected-5702" to be "success or failure"
May 29 19:07:30.899: INFO: Pod "pod-projected-configmaps-01f7a1f8-8245-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.596809ms
May 29 19:07:32.902: INFO: Pod "pod-projected-configmaps-01f7a1f8-8245-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010976398s
May 29 19:07:34.906: INFO: Pod "pod-projected-configmaps-01f7a1f8-8245-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014645866s
May 29 19:07:36.909: INFO: Pod "pod-projected-configmaps-01f7a1f8-8245-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018105391s
STEP: Saw pod success
May 29 19:07:36.909: INFO: Pod "pod-projected-configmaps-01f7a1f8-8245-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:07:36.911: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-configmaps-01f7a1f8-8245-11e9-b32a-4ae2266202a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 19:07:36.938: INFO: Waiting for pod pod-projected-configmaps-01f7a1f8-8245-11e9-b32a-4ae2266202a5 to disappear
May 29 19:07:36.940: INFO: Pod pod-projected-configmaps-01f7a1f8-8245-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:07:36.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5702" for this suite.
May 29 19:07:42.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:07:43.021: INFO: namespace projected-5702 deletion completed in 6.078114422s

• [SLOW TEST:12.192 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:07:43.021: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-621
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 29 19:07:43.060: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 29 19:08:07.111: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.41:8080/dial?request=hostName&protocol=http&host=10.240.0.43&port=8080&tries=1'] Namespace:pod-network-test-621 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:08:07.111: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:08:07.247: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:08:07.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-621" for this suite.
May 29 19:08:29.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:08:29.340: INFO: namespace pod-network-test-621 deletion completed in 22.090038747s

• [SLOW TEST:46.319 seconds]
[sig-network] Networking
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:08:29.341: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 29 19:08:29.384: INFO: Waiting up to 5m0s for pod "pod-24d4d6e7-8245-11e9-b32a-4ae2266202a5" in namespace "emptydir-9748" to be "success or failure"
May 29 19:08:29.387: INFO: Pod "pod-24d4d6e7-8245-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314993ms
May 29 19:08:31.391: INFO: Pod "pod-24d4d6e7-8245-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006420474s
May 29 19:08:33.394: INFO: Pod "pod-24d4d6e7-8245-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00964899s
May 29 19:08:35.397: INFO: Pod "pod-24d4d6e7-8245-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01301338s
STEP: Saw pod success
May 29 19:08:35.398: INFO: Pod "pod-24d4d6e7-8245-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:08:35.400: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-24d4d6e7-8245-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 19:08:35.413: INFO: Waiting for pod pod-24d4d6e7-8245-11e9-b32a-4ae2266202a5 to disappear
May 29 19:08:35.416: INFO: Pod pod-24d4d6e7-8245-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:08:35.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9748" for this suite.
May 29 19:08:41.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:08:41.505: INFO: namespace emptydir-9748 deletion completed in 6.086197946s

• [SLOW TEST:12.164 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:08:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 19:08:41.556: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 29 19:08:46.559: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 29 19:08:46.559: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 29 19:08:48.562: INFO: Creating deployment "test-rollover-deployment"
May 29 19:08:48.574: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 29 19:08:50.583: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 29 19:08:50.588: INFO: Ensure that both replica sets have 1 created replica
May 29 19:08:50.592: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 29 19:08:50.599: INFO: Updating deployment test-rollover-deployment
May 29 19:08:50.599: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 29 19:08:52.608: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 29 19:08:52.612: INFO: Make sure deployment "test-rollover-deployment" is complete
May 29 19:08:52.617: INFO: all replica sets need to contain the pod-template-hash label
May 29 19:08:52.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753730, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:08:54.625: INFO: all replica sets need to contain the pod-template-hash label
May 29 19:08:54.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753730, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:08:56.623: INFO: all replica sets need to contain the pod-template-hash label
May 29 19:08:56.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753735, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:08:58.623: INFO: all replica sets need to contain the pod-template-hash label
May 29 19:08:58.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753735, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:09:00.623: INFO: all replica sets need to contain the pod-template-hash label
May 29 19:09:00.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753735, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:09:02.624: INFO: all replica sets need to contain the pod-template-hash label
May 29 19:09:02.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753735, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:09:04.623: INFO: all replica sets need to contain the pod-template-hash label
May 29 19:09:04.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753735, loc:(*time.Location)(0x8a140e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694753728, loc:(*time.Location)(0x8a140e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 29 19:09:06.623: INFO: 
May 29 19:09:06.623: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 29 19:09:06.629: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5027,SelfLink:/apis/apps/v1/namespaces/deployment-5027/deployments/test-rollover-deployment,UID:3044db01-8245-11e9-9319-000d3afdb8b2,ResourceVersion:15631,Generation:2,CreationTimestamp:2019-05-29 19:08:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-29 19:08:48 +0000 UTC 2019-05-29 19:08:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-29 19:09:06 +0000 UTC 2019-05-29 19:08:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 29 19:09:06.632: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-5027,SelfLink:/apis/apps/v1/namespaces/deployment-5027/replicasets/test-rollover-deployment-766b4d6c9d,UID:317b6be6-8245-11e9-9319-000d3afdb8b2,ResourceVersion:15620,Generation:2,CreationTimestamp:2019-05-29 19:08:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 3044db01-8245-11e9-9319-000d3afdb8b2 0xc000bf2bc7 0xc000bf2bc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 29 19:09:06.632: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 29 19:09:06.632: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5027,SelfLink:/apis/apps/v1/namespaces/deployment-5027/replicasets/test-rollover-controller,UID:2c16b19a-8245-11e9-9319-000d3afdb8b2,ResourceVersion:15630,Generation:2,CreationTimestamp:2019-05-29 19:08:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 3044db01-8245-11e9-9319-000d3afdb8b2 0xc000bf2a17 0xc000bf2a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 19:09:06.633: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-5027,SelfLink:/apis/apps/v1/namespaces/deployment-5027/replicasets/test-rollover-deployment-6455657675,UID:30479149-8245-11e9-9319-000d3afdb8b2,ResourceVersion:15578,Generation:2,CreationTimestamp:2019-05-29 19:08:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 3044db01-8245-11e9-9319-000d3afdb8b2 0xc000bf2ae7 0xc000bf2ae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 29 19:09:06.635: INFO: Pod "test-rollover-deployment-766b4d6c9d-cpzr7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-cpzr7,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-5027,SelfLink:/api/v1/namespaces/deployment-5027/pods/test-rollover-deployment-766b4d6c9d-cpzr7,UID:317eeaad-8245-11e9-9319-000d3afdb8b2,ResourceVersion:15596,Generation:0,CreationTimestamp:2019-05-29 19:08:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 317b6be6-8245-11e9-9319-000d3afdb8b2 0xc000bf3727 0xc000bf3728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-4c4fw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4c4fw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-4c4fw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-pool1-11779686-vmss000000,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000bf3790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000bf37b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:08:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:08:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:08:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-29 19:08:50 +0000 UTC  }],Message:,Reason:,HostIP:10.240.0.34,PodIP:10.240.0.38,StartTime:2019-05-29 19:08:50 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-29 19:08:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://eced001516931911b09151861d6cd992b7ea4e92766fdf96b1983511020c52e4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:09:06.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5027" for this suite.
May 29 19:09:12.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:09:12.726: INFO: namespace deployment-5027 deletion completed in 6.087704635s

• [SLOW TEST:31.221 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:09:12.726: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-2339
May 29 19:09:18.784: INFO: Started pod liveness-exec in namespace container-probe-2339
STEP: checking the pod's current state and verifying that restartCount is present
May 29 19:09:18.787: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:13:19.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2339" for this suite.
May 29 19:13:25.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:13:25.371: INFO: namespace container-probe-2339 deletion completed in 6.098102045s

• [SLOW TEST:252.645 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:13:25.371: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 29 19:13:25.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-2455'
May 29 19:13:27.800: INFO: stderr: ""
May 29 19:13:27.800: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 19:13:27.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2455'
May 29 19:13:27.914: INFO: stderr: ""
May 29 19:13:27.914: INFO: stdout: "update-demo-nautilus-gsbvk update-demo-nautilus-q92vb "
May 29 19:13:27.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-gsbvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:27.990: INFO: stderr: ""
May 29 19:13:27.990: INFO: stdout: ""
May 29 19:13:27.990: INFO: update-demo-nautilus-gsbvk is created but not running
May 29 19:13:32.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2455'
May 29 19:13:33.087: INFO: stderr: ""
May 29 19:13:33.087: INFO: stdout: "update-demo-nautilus-gsbvk update-demo-nautilus-q92vb "
May 29 19:13:33.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-gsbvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:33.166: INFO: stderr: ""
May 29 19:13:33.166: INFO: stdout: ""
May 29 19:13:33.166: INFO: update-demo-nautilus-gsbvk is created but not running
May 29 19:13:38.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2455'
May 29 19:13:38.251: INFO: stderr: ""
May 29 19:13:38.251: INFO: stdout: "update-demo-nautilus-gsbvk update-demo-nautilus-q92vb "
May 29 19:13:38.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-gsbvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:38.332: INFO: stderr: ""
May 29 19:13:38.332: INFO: stdout: "true"
May 29 19:13:38.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-gsbvk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:38.402: INFO: stderr: ""
May 29 19:13:38.402: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 19:13:38.402: INFO: validating pod update-demo-nautilus-gsbvk
May 29 19:13:38.408: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 19:13:38.408: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 19:13:38.408: INFO: update-demo-nautilus-gsbvk is verified up and running
May 29 19:13:38.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-q92vb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:38.487: INFO: stderr: ""
May 29 19:13:38.487: INFO: stdout: "true"
May 29 19:13:38.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-q92vb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:38.561: INFO: stderr: ""
May 29 19:13:38.561: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 19:13:38.561: INFO: validating pod update-demo-nautilus-q92vb
May 29 19:13:38.566: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 19:13:38.566: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 19:13:38.566: INFO: update-demo-nautilus-q92vb is verified up and running
STEP: scaling down the replication controller
May 29 19:13:38.568: INFO: scanned /root for discovery docs: <nil>
May 29 19:13:38.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2455'
May 29 19:13:39.672: INFO: stderr: ""
May 29 19:13:39.672: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 19:13:39.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2455'
May 29 19:13:39.774: INFO: stderr: ""
May 29 19:13:39.774: INFO: stdout: "update-demo-nautilus-gsbvk update-demo-nautilus-q92vb "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 29 19:13:44.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2455'
May 29 19:13:44.851: INFO: stderr: ""
May 29 19:13:44.851: INFO: stdout: "update-demo-nautilus-q92vb "
May 29 19:13:44.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-q92vb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:44.932: INFO: stderr: ""
May 29 19:13:44.932: INFO: stdout: "true"
May 29 19:13:44.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-q92vb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:45.012: INFO: stderr: ""
May 29 19:13:45.012: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 19:13:45.012: INFO: validating pod update-demo-nautilus-q92vb
May 29 19:13:45.015: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 19:13:45.015: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 19:13:45.015: INFO: update-demo-nautilus-q92vb is verified up and running
STEP: scaling up the replication controller
May 29 19:13:45.017: INFO: scanned /root for discovery docs: <nil>
May 29 19:13:45.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2455'
May 29 19:13:46.121: INFO: stderr: ""
May 29 19:13:46.121: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 19:13:46.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2455'
May 29 19:13:46.209: INFO: stderr: ""
May 29 19:13:46.209: INFO: stdout: "update-demo-nautilus-7wq5m update-demo-nautilus-q92vb "
May 29 19:13:46.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-7wq5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:46.290: INFO: stderr: ""
May 29 19:13:46.290: INFO: stdout: ""
May 29 19:13:46.290: INFO: update-demo-nautilus-7wq5m is created but not running
May 29 19:13:51.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2455'
May 29 19:13:51.368: INFO: stderr: ""
May 29 19:13:51.368: INFO: stdout: "update-demo-nautilus-7wq5m update-demo-nautilus-q92vb "
May 29 19:13:51.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-7wq5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:51.443: INFO: stderr: ""
May 29 19:13:51.443: INFO: stdout: "true"
May 29 19:13:51.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-7wq5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:51.518: INFO: stderr: ""
May 29 19:13:51.518: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 19:13:51.518: INFO: validating pod update-demo-nautilus-7wq5m
May 29 19:13:51.523: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 19:13:51.523: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 19:13:51.523: INFO: update-demo-nautilus-7wq5m is verified up and running
May 29 19:13:51.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-q92vb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:51.594: INFO: stderr: ""
May 29 19:13:51.594: INFO: stdout: "true"
May 29 19:13:51.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-q92vb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2455'
May 29 19:13:51.666: INFO: stderr: ""
May 29 19:13:51.666: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 19:13:51.666: INFO: validating pod update-demo-nautilus-q92vb
May 29 19:13:51.670: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 19:13:51.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 19:13:51.670: INFO: update-demo-nautilus-q92vb is verified up and running
STEP: using delete to clean up resources
May 29 19:13:51.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-2455'
May 29 19:13:51.748: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 19:13:51.748: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 29 19:13:51.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2455'
May 29 19:13:51.825: INFO: stderr: "No resources found.\n"
May 29 19:13:51.825: INFO: stdout: ""
May 29 19:13:51.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -l name=update-demo --namespace=kubectl-2455 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 19:13:51.914: INFO: stderr: ""
May 29 19:13:51.914: INFO: stdout: "update-demo-nautilus-7wq5m\nupdate-demo-nautilus-q92vb\n"
May 29 19:13:52.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2455'
May 29 19:13:52.495: INFO: stderr: "No resources found.\n"
May 29 19:13:52.495: INFO: stdout: ""
May 29 19:13:52.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -l name=update-demo --namespace=kubectl-2455 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 19:13:52.569: INFO: stderr: ""
May 29 19:13:52.569: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:13:52.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2455" for this suite.
May 29 19:14:14.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:14:14.670: INFO: namespace kubectl-2455 deletion completed in 22.098570964s

• [SLOW TEST:49.299 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:14:14.671: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 19:14:14.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2aa6855-8245-11e9-b32a-4ae2266202a5" in namespace "downward-api-6304" to be "success or failure"
May 29 19:14:14.720: INFO: Pod "downwardapi-volume-f2aa6855-8245-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439832ms
May 29 19:14:16.724: INFO: Pod "downwardapi-volume-f2aa6855-8245-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007087894s
May 29 19:14:18.728: INFO: Pod "downwardapi-volume-f2aa6855-8245-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011064553s
May 29 19:14:20.732: INFO: Pod "downwardapi-volume-f2aa6855-8245-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015404611s
STEP: Saw pod success
May 29 19:14:20.732: INFO: Pod "downwardapi-volume-f2aa6855-8245-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:14:20.734: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-f2aa6855-8245-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 19:14:20.757: INFO: Waiting for pod downwardapi-volume-f2aa6855-8245-11e9-b32a-4ae2266202a5 to disappear
May 29 19:14:20.759: INFO: Pod downwardapi-volume-f2aa6855-8245-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:14:20.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6304" for this suite.
May 29 19:14:26.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:14:26.845: INFO: namespace downward-api-6304 deletion completed in 6.082929757s

• [SLOW TEST:12.174 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:14:26.845: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8110
May 29 19:14:32.905: INFO: Started pod liveness-http in namespace container-probe-8110
STEP: checking the pod's current state and verifying that restartCount is present
May 29 19:14:32.907: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:18:33.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8110" for this suite.
May 29 19:18:39.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:18:39.495: INFO: namespace container-probe-8110 deletion completed in 6.088856888s

• [SLOW TEST:252.650 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:18:39.496: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
May 29 19:18:40.064: INFO: created pod pod-service-account-defaultsa
May 29 19:18:40.064: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 29 19:18:40.076: INFO: created pod pod-service-account-mountsa
May 29 19:18:40.076: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 29 19:18:40.081: INFO: created pod pod-service-account-nomountsa
May 29 19:18:40.081: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 29 19:18:40.085: INFO: created pod pod-service-account-defaultsa-mountspec
May 29 19:18:40.085: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 29 19:18:40.092: INFO: created pod pod-service-account-mountsa-mountspec
May 29 19:18:40.092: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 29 19:18:40.097: INFO: created pod pod-service-account-nomountsa-mountspec
May 29 19:18:40.097: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 29 19:18:40.106: INFO: created pod pod-service-account-defaultsa-nomountspec
May 29 19:18:40.106: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 29 19:18:40.110: INFO: created pod pod-service-account-mountsa-nomountspec
May 29 19:18:40.110: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 29 19:18:40.121: INFO: created pod pod-service-account-nomountsa-nomountspec
May 29 19:18:40.121: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:18:40.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-344" for this suite.
May 29 19:19:02.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:19:02.230: INFO: namespace svcaccounts-344 deletion completed in 22.091767865s

• [SLOW TEST:22.734 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:19:02.230: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 29 19:19:02.269: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:19:12.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4476" for this suite.
May 29 19:19:34.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:19:34.918: INFO: namespace init-container-4476 deletion completed in 22.095869949s

• [SLOW TEST:32.688 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:19:34.919: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4267
I0529 19:19:34.974091      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4267, replica count: 1
I0529 19:19:36.024548      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 19:19:37.024799      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 19:19:38.025022      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 19:19:39.025191      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0529 19:19:40.025426      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 29 19:19:40.141: INFO: Created: latency-svc-wkpvx
May 29 19:19:40.145: INFO: Got endpoints: latency-svc-wkpvx [19.78334ms]
May 29 19:19:40.161: INFO: Created: latency-svc-nz6lg
May 29 19:19:40.174: INFO: Got endpoints: latency-svc-nz6lg [28.809677ms]
May 29 19:19:40.176: INFO: Created: latency-svc-6xb22
May 29 19:19:40.190: INFO: Got endpoints: latency-svc-6xb22 [44.13925ms]
May 29 19:19:40.191: INFO: Created: latency-svc-dsgct
May 29 19:19:40.203: INFO: Got endpoints: latency-svc-dsgct [55.343469ms]
May 29 19:19:40.207: INFO: Created: latency-svc-w7cfp
May 29 19:19:40.220: INFO: Got endpoints: latency-svc-w7cfp [72.46921ms]
May 29 19:19:40.224: INFO: Created: latency-svc-mw8fq
May 29 19:19:40.229: INFO: Got endpoints: latency-svc-mw8fq [80.670016ms]
May 29 19:19:40.240: INFO: Created: latency-svc-j7rmc
May 29 19:19:40.240: INFO: Got endpoints: latency-svc-j7rmc [89.966664ms]
May 29 19:19:40.252: INFO: Created: latency-svc-gn7f9
May 29 19:19:40.257: INFO: Got endpoints: latency-svc-gn7f9 [109.017376ms]
May 29 19:19:40.266: INFO: Created: latency-svc-j4g5l
May 29 19:19:40.275: INFO: Created: latency-svc-r7x2b
May 29 19:19:40.279: INFO: Got endpoints: latency-svc-j4g5l [130.866592ms]
May 29 19:19:40.289: INFO: Got endpoints: latency-svc-r7x2b [140.018235ms]
May 29 19:19:40.297: INFO: Created: latency-svc-5wzqz
May 29 19:19:40.302: INFO: Got endpoints: latency-svc-5wzqz [153.444737ms]
May 29 19:19:40.318: INFO: Created: latency-svc-f8brt
May 29 19:19:40.329: INFO: Got endpoints: latency-svc-f8brt [180.487148ms]
May 29 19:19:40.336: INFO: Created: latency-svc-xggkr
May 29 19:19:40.353: INFO: Got endpoints: latency-svc-xggkr [203.703116ms]
May 29 19:19:40.353: INFO: Created: latency-svc-9mtc6
May 29 19:19:40.368: INFO: Created: latency-svc-w6l9z
May 29 19:19:40.371: INFO: Got endpoints: latency-svc-9mtc6 [222.302611ms]
May 29 19:19:40.377: INFO: Got endpoints: latency-svc-w6l9z [228.095727ms]
May 29 19:19:40.378: INFO: Created: latency-svc-kq7kj
May 29 19:19:40.380: INFO: Got endpoints: latency-svc-kq7kj [230.481216ms]
May 29 19:19:40.393: INFO: Created: latency-svc-nkxsm
May 29 19:19:40.403: INFO: Got endpoints: latency-svc-nkxsm [228.666949ms]
May 29 19:19:40.406: INFO: Created: latency-svc-pz2mc
May 29 19:19:40.414: INFO: Got endpoints: latency-svc-pz2mc [223.660862ms]
May 29 19:19:40.419: INFO: Created: latency-svc-xvskk
May 29 19:19:40.427: INFO: Created: latency-svc-tr9fk
May 29 19:19:40.430: INFO: Got endpoints: latency-svc-xvskk [226.844681ms]
May 29 19:19:40.437: INFO: Got endpoints: latency-svc-tr9fk [216.319387ms]
May 29 19:19:40.448: INFO: Created: latency-svc-s6lm7
May 29 19:19:40.456: INFO: Created: latency-svc-jmhll
May 29 19:19:40.459: INFO: Got endpoints: latency-svc-s6lm7 [230.788129ms]
May 29 19:19:40.476: INFO: Got endpoints: latency-svc-jmhll [235.997423ms]
May 29 19:19:40.480: INFO: Created: latency-svc-dqk4m
May 29 19:19:40.488: INFO: Created: latency-svc-z5lvx
May 29 19:19:40.490: INFO: Got endpoints: latency-svc-dqk4m [232.968509ms]
May 29 19:19:40.507: INFO: Got endpoints: latency-svc-z5lvx [227.64191ms]
May 29 19:19:40.513: INFO: Created: latency-svc-5t7x4
May 29 19:19:40.513: INFO: Got endpoints: latency-svc-5t7x4 [224.760303ms]
May 29 19:19:40.527: INFO: Created: latency-svc-lcs6n
May 29 19:19:40.527: INFO: Got endpoints: latency-svc-lcs6n [225.024413ms]
May 29 19:19:40.537: INFO: Created: latency-svc-4f9kg
May 29 19:19:40.552: INFO: Got endpoints: latency-svc-4f9kg [222.583222ms]
May 29 19:19:40.553: INFO: Created: latency-svc-dk7ck
May 29 19:19:40.565: INFO: Created: latency-svc-cz5nt
May 29 19:19:40.566: INFO: Got endpoints: latency-svc-dk7ck [213.614286ms]
May 29 19:19:40.572: INFO: Got endpoints: latency-svc-cz5nt [201.018515ms]
May 29 19:19:40.580: INFO: Created: latency-svc-l7vvz
May 29 19:19:40.597: INFO: Created: latency-svc-jpcq2
May 29 19:19:40.611: INFO: Created: latency-svc-mjwkh
May 29 19:19:40.614: INFO: Got endpoints: latency-svc-l7vvz [236.501141ms]
May 29 19:19:40.614: INFO: Got endpoints: latency-svc-jpcq2 [234.049051ms]
May 29 19:19:40.617: INFO: Got endpoints: latency-svc-mjwkh [213.654787ms]
May 29 19:19:40.628: INFO: Created: latency-svc-tjddq
May 29 19:19:40.638: INFO: Got endpoints: latency-svc-tjddq [224.168381ms]
May 29 19:19:40.642: INFO: Created: latency-svc-lj5tl
May 29 19:19:40.646: INFO: Created: latency-svc-69m2v
May 29 19:19:40.651: INFO: Got endpoints: latency-svc-lj5tl [221.293673ms]
May 29 19:19:40.657: INFO: Got endpoints: latency-svc-69m2v [220.510644ms]
May 29 19:19:40.667: INFO: Created: latency-svc-s5fzg
May 29 19:19:40.671: INFO: Got endpoints: latency-svc-s5fzg [211.194696ms]
May 29 19:19:40.695: INFO: Created: latency-svc-896kn
May 29 19:19:40.700: INFO: Got endpoints: latency-svc-896kn [224.225283ms]
May 29 19:19:40.714: INFO: Created: latency-svc-kttxb
May 29 19:19:40.727: INFO: Got endpoints: latency-svc-kttxb [236.075426ms]
May 29 19:19:40.733: INFO: Created: latency-svc-gqjfs
May 29 19:19:40.748: INFO: Got endpoints: latency-svc-gqjfs [240.805402ms]
May 29 19:19:40.754: INFO: Created: latency-svc-fgbtc
May 29 19:19:40.757: INFO: Got endpoints: latency-svc-fgbtc [243.544406ms]
May 29 19:19:40.765: INFO: Created: latency-svc-tzln5
May 29 19:19:40.782: INFO: Got endpoints: latency-svc-tzln5 [254.153202ms]
May 29 19:19:40.792: INFO: Created: latency-svc-25gt2
May 29 19:19:40.796: INFO: Created: latency-svc-zrkwp
May 29 19:19:40.799: INFO: Got endpoints: latency-svc-25gt2 [246.395012ms]
May 29 19:19:40.815: INFO: Created: latency-svc-jx9gw
May 29 19:19:40.816: INFO: Got endpoints: latency-svc-zrkwp [58.384383ms]
May 29 19:19:40.828: INFO: Created: latency-svc-rgptn
May 29 19:19:40.841: INFO: Created: latency-svc-hf2tc
May 29 19:19:40.851: INFO: Got endpoints: latency-svc-jx9gw [284.610441ms]
May 29 19:19:40.853: INFO: Created: latency-svc-xcqgn
May 29 19:19:40.860: INFO: Created: latency-svc-d4b7l
May 29 19:19:40.879: INFO: Created: latency-svc-r8mq2
May 29 19:19:40.890: INFO: Created: latency-svc-qz4xl
May 29 19:19:40.897: INFO: Created: latency-svc-cs8px
May 29 19:19:40.898: INFO: Got endpoints: latency-svc-rgptn [325.900484ms]
May 29 19:19:40.917: INFO: Created: latency-svc-bvhc7
May 29 19:19:40.932: INFO: Created: latency-svc-w9g4h
May 29 19:19:40.945: INFO: Created: latency-svc-vjkx5
May 29 19:19:40.951: INFO: Got endpoints: latency-svc-hf2tc [336.707989ms]
May 29 19:19:40.960: INFO: Created: latency-svc-tlscm
May 29 19:19:40.975: INFO: Created: latency-svc-82wkg
May 29 19:19:40.984: INFO: Created: latency-svc-2xl5f
May 29 19:19:40.993: INFO: Created: latency-svc-txbsz
May 29 19:19:41.000: INFO: Got endpoints: latency-svc-xcqgn [384.314068ms]
May 29 19:19:41.003: INFO: Created: latency-svc-njgln
May 29 19:19:41.021: INFO: Created: latency-svc-xkzfd
May 29 19:19:41.028: INFO: Created: latency-svc-hkh8j
May 29 19:19:41.040: INFO: Created: latency-svc-h2c2k
May 29 19:19:41.044: INFO: Got endpoints: latency-svc-d4b7l [427.357377ms]
May 29 19:19:41.061: INFO: Created: latency-svc-p6cpg
May 29 19:19:41.098: INFO: Got endpoints: latency-svc-r8mq2 [459.953796ms]
May 29 19:19:41.112: INFO: Created: latency-svc-ltcgq
May 29 19:19:41.144: INFO: Got endpoints: latency-svc-qz4xl [492.174201ms]
May 29 19:19:41.157: INFO: Created: latency-svc-ph2nt
May 29 19:19:41.191: INFO: Got endpoints: latency-svc-cs8px [533.434643ms]
May 29 19:19:41.204: INFO: Created: latency-svc-jjbzj
May 29 19:19:41.242: INFO: Got endpoints: latency-svc-bvhc7 [570.751838ms]
May 29 19:19:41.264: INFO: Created: latency-svc-7ldvl
May 29 19:19:41.291: INFO: Got endpoints: latency-svc-w9g4h [590.966894ms]
May 29 19:19:41.316: INFO: Created: latency-svc-4bcts
May 29 19:19:41.343: INFO: Got endpoints: latency-svc-vjkx5 [616.807759ms]
May 29 19:19:41.357: INFO: Created: latency-svc-qv2qv
May 29 19:19:41.394: INFO: Got endpoints: latency-svc-tlscm [646.26136ms]
May 29 19:19:41.406: INFO: Created: latency-svc-t2jv7
May 29 19:19:41.444: INFO: Got endpoints: latency-svc-82wkg [662.363963ms]
May 29 19:19:41.456: INFO: Created: latency-svc-dx94g
May 29 19:19:41.492: INFO: Got endpoints: latency-svc-2xl5f [693.05661ms]
May 29 19:19:41.508: INFO: Created: latency-svc-sjw6s
May 29 19:19:41.544: INFO: Got endpoints: latency-svc-txbsz [728.63584ms]
May 29 19:19:41.558: INFO: Created: latency-svc-2jqgh
May 29 19:19:41.592: INFO: Got endpoints: latency-svc-njgln [740.615788ms]
May 29 19:19:41.607: INFO: Created: latency-svc-btzdc
May 29 19:19:41.642: INFO: Got endpoints: latency-svc-xkzfd [743.862409ms]
May 29 19:19:41.664: INFO: Created: latency-svc-h6w47
May 29 19:19:41.692: INFO: Got endpoints: latency-svc-hkh8j [741.061504ms]
May 29 19:19:41.709: INFO: Created: latency-svc-nchqz
May 29 19:19:41.742: INFO: Got endpoints: latency-svc-h2c2k [742.334952ms]
May 29 19:19:41.777: INFO: Created: latency-svc-mp8vd
May 29 19:19:41.793: INFO: Got endpoints: latency-svc-p6cpg [748.213571ms]
May 29 19:19:41.824: INFO: Created: latency-svc-dqsvj
May 29 19:19:41.849: INFO: Got endpoints: latency-svc-ltcgq [751.177182ms]
May 29 19:19:41.868: INFO: Created: latency-svc-bqddr
May 29 19:19:41.892: INFO: Got endpoints: latency-svc-ph2nt [748.601486ms]
May 29 19:19:41.922: INFO: Created: latency-svc-rszfb
May 29 19:19:41.956: INFO: Got endpoints: latency-svc-jjbzj [765.252008ms]
May 29 19:19:42.001: INFO: Created: latency-svc-76kck
May 29 19:19:42.006: INFO: Got endpoints: latency-svc-7ldvl [763.132329ms]
May 29 19:19:42.024: INFO: Created: latency-svc-qjf8k
May 29 19:19:42.043: INFO: Got endpoints: latency-svc-4bcts [751.099179ms]
May 29 19:19:42.057: INFO: Created: latency-svc-gckhx
May 29 19:19:42.093: INFO: Got endpoints: latency-svc-qv2qv [749.044202ms]
May 29 19:19:42.108: INFO: Created: latency-svc-9dxm5
May 29 19:19:42.142: INFO: Got endpoints: latency-svc-t2jv7 [747.423542ms]
May 29 19:19:42.163: INFO: Created: latency-svc-cqwpq
May 29 19:19:42.192: INFO: Got endpoints: latency-svc-dx94g [747.571447ms]
May 29 19:19:42.207: INFO: Created: latency-svc-r697f
May 29 19:19:42.245: INFO: Got endpoints: latency-svc-sjw6s [753.328862ms]
May 29 19:19:42.260: INFO: Created: latency-svc-8qrbb
May 29 19:19:42.293: INFO: Got endpoints: latency-svc-2jqgh [748.641687ms]
May 29 19:19:42.311: INFO: Created: latency-svc-6xqrh
May 29 19:19:42.341: INFO: Got endpoints: latency-svc-btzdc [748.878996ms]
May 29 19:19:42.357: INFO: Created: latency-svc-mllbm
May 29 19:19:42.392: INFO: Got endpoints: latency-svc-h6w47 [749.944736ms]
May 29 19:19:42.406: INFO: Created: latency-svc-jtmdn
May 29 19:19:42.442: INFO: Got endpoints: latency-svc-nchqz [749.401715ms]
May 29 19:19:42.460: INFO: Created: latency-svc-7jp2k
May 29 19:19:42.493: INFO: Got endpoints: latency-svc-mp8vd [751.352688ms]
May 29 19:19:42.507: INFO: Created: latency-svc-6x6mf
May 29 19:19:42.543: INFO: Got endpoints: latency-svc-dqsvj [750.191645ms]
May 29 19:19:42.561: INFO: Created: latency-svc-t6p9k
May 29 19:19:42.592: INFO: Got endpoints: latency-svc-bqddr [742.925473ms]
May 29 19:19:42.610: INFO: Created: latency-svc-p9hcp
May 29 19:19:42.643: INFO: Got endpoints: latency-svc-rszfb [750.523357ms]
May 29 19:19:42.657: INFO: Created: latency-svc-7zpcm
May 29 19:19:42.692: INFO: Got endpoints: latency-svc-76kck [736.070317ms]
May 29 19:19:42.710: INFO: Created: latency-svc-qfrt8
May 29 19:19:42.743: INFO: Got endpoints: latency-svc-qjf8k [737.426767ms]
May 29 19:19:42.757: INFO: Created: latency-svc-5k4qd
May 29 19:19:42.796: INFO: Got endpoints: latency-svc-gckhx [753.594971ms]
May 29 19:19:42.811: INFO: Created: latency-svc-5ncjs
May 29 19:19:42.842: INFO: Got endpoints: latency-svc-9dxm5 [749.759728ms]
May 29 19:19:42.860: INFO: Created: latency-svc-v4wcz
May 29 19:19:42.891: INFO: Got endpoints: latency-svc-cqwpq [748.800092ms]
May 29 19:19:42.906: INFO: Created: latency-svc-js8p7
May 29 19:19:42.942: INFO: Got endpoints: latency-svc-r697f [749.365613ms]
May 29 19:19:42.958: INFO: Created: latency-svc-hhzg4
May 29 19:19:42.992: INFO: Got endpoints: latency-svc-8qrbb [747.13603ms]
May 29 19:19:43.010: INFO: Created: latency-svc-7jfs5
May 29 19:19:43.042: INFO: Got endpoints: latency-svc-6xqrh [748.664886ms]
May 29 19:19:43.054: INFO: Created: latency-svc-2ntjs
May 29 19:19:43.090: INFO: Got endpoints: latency-svc-mllbm [749.402015ms]
May 29 19:19:43.112: INFO: Created: latency-svc-qfr8q
May 29 19:19:43.142: INFO: Got endpoints: latency-svc-jtmdn [749.620522ms]
May 29 19:19:43.159: INFO: Created: latency-svc-8rlmd
May 29 19:19:43.192: INFO: Got endpoints: latency-svc-7jp2k [750.065139ms]
May 29 19:19:43.206: INFO: Created: latency-svc-2q787
May 29 19:19:43.260: INFO: Got endpoints: latency-svc-6x6mf [766.769963ms]
May 29 19:19:43.280: INFO: Created: latency-svc-2m84m
May 29 19:19:43.312: INFO: Got endpoints: latency-svc-t6p9k [769.282357ms]
May 29 19:19:43.355: INFO: Created: latency-svc-fw692
May 29 19:19:43.357: INFO: Got endpoints: latency-svc-p9hcp [764.261169ms]
May 29 19:19:43.407: INFO: Created: latency-svc-84hjp
May 29 19:19:43.413: INFO: Got endpoints: latency-svc-7zpcm [770.278694ms]
May 29 19:19:43.444: INFO: Created: latency-svc-vlfc9
May 29 19:19:43.476: INFO: Got endpoints: latency-svc-qfrt8 [783.484688ms]
May 29 19:19:43.511: INFO: Got endpoints: latency-svc-5k4qd [767.996209ms]
May 29 19:19:43.521: INFO: Created: latency-svc-bnjhb
May 29 19:19:43.542: INFO: Created: latency-svc-cwnpj
May 29 19:19:43.550: INFO: Got endpoints: latency-svc-5ncjs [753.87458ms]
May 29 19:19:43.568: INFO: Created: latency-svc-k74mg
May 29 19:19:43.591: INFO: Got endpoints: latency-svc-v4wcz [748.377975ms]
May 29 19:19:43.606: INFO: Created: latency-svc-8hdv9
May 29 19:19:43.650: INFO: Got endpoints: latency-svc-js8p7 [759.283783ms]
May 29 19:19:43.672: INFO: Created: latency-svc-mprth
May 29 19:19:43.693: INFO: Got endpoints: latency-svc-hhzg4 [750.856567ms]
May 29 19:19:43.708: INFO: Created: latency-svc-ft597
May 29 19:19:43.788: INFO: Got endpoints: latency-svc-7jfs5 [795.019818ms]
May 29 19:19:43.792: INFO: Got endpoints: latency-svc-2ntjs [750.546356ms]
May 29 19:19:43.804: INFO: Created: latency-svc-2rpqb
May 29 19:19:43.820: INFO: Created: latency-svc-6b5jn
May 29 19:19:43.846: INFO: Got endpoints: latency-svc-qfr8q [755.446339ms]
May 29 19:19:43.865: INFO: Created: latency-svc-glnp7
May 29 19:19:43.891: INFO: Got endpoints: latency-svc-8rlmd [749.080601ms]
May 29 19:19:43.907: INFO: Created: latency-svc-8rb4w
May 29 19:19:43.948: INFO: Got endpoints: latency-svc-2q787 [756.129364ms]
May 29 19:19:43.961: INFO: Created: latency-svc-727rv
May 29 19:19:43.991: INFO: Got endpoints: latency-svc-2m84m [731.003925ms]
May 29 19:19:44.012: INFO: Created: latency-svc-zb45s
May 29 19:19:44.042: INFO: Got endpoints: latency-svc-fw692 [729.741978ms]
May 29 19:19:44.054: INFO: Created: latency-svc-tnpbv
May 29 19:19:44.093: INFO: Got endpoints: latency-svc-84hjp [736.559033ms]
May 29 19:19:44.107: INFO: Created: latency-svc-vnscc
May 29 19:19:44.153: INFO: Got endpoints: latency-svc-vlfc9 [739.398339ms]
May 29 19:19:44.171: INFO: Created: latency-svc-6j4mb
May 29 19:19:44.194: INFO: Got endpoints: latency-svc-bnjhb [718.450555ms]
May 29 19:19:44.212: INFO: Created: latency-svc-q4wfx
May 29 19:19:44.242: INFO: Got endpoints: latency-svc-cwnpj [730.420502ms]
May 29 19:19:44.260: INFO: Created: latency-svc-hx8tt
May 29 19:19:44.293: INFO: Got endpoints: latency-svc-k74mg [742.942971ms]
May 29 19:19:44.306: INFO: Created: latency-svc-vncxk
May 29 19:19:44.350: INFO: Got endpoints: latency-svc-8hdv9 [758.403248ms]
May 29 19:19:44.370: INFO: Created: latency-svc-6jb5g
May 29 19:19:44.396: INFO: Got endpoints: latency-svc-mprth [746.017285ms]
May 29 19:19:44.408: INFO: Created: latency-svc-pb9gv
May 29 19:19:44.445: INFO: Got endpoints: latency-svc-ft597 [751.952807ms]
May 29 19:19:44.461: INFO: Created: latency-svc-52nfb
May 29 19:19:44.508: INFO: Got endpoints: latency-svc-2rpqb [720.592035ms]
May 29 19:19:44.551: INFO: Created: latency-svc-s4987
May 29 19:19:44.552: INFO: Got endpoints: latency-svc-6b5jn [759.999408ms]
May 29 19:19:44.566: INFO: Created: latency-svc-8brnv
May 29 19:19:44.595: INFO: Got endpoints: latency-svc-glnp7 [749.233405ms]
May 29 19:19:44.613: INFO: Created: latency-svc-zq89n
May 29 19:19:44.642: INFO: Got endpoints: latency-svc-8rb4w [749.822727ms]
May 29 19:19:44.662: INFO: Created: latency-svc-6xn4d
May 29 19:19:44.693: INFO: Got endpoints: latency-svc-727rv [744.069513ms]
May 29 19:19:44.709: INFO: Created: latency-svc-8nkh4
May 29 19:19:44.745: INFO: Got endpoints: latency-svc-zb45s [753.36486ms]
May 29 19:19:44.758: INFO: Created: latency-svc-9s9wz
May 29 19:19:44.793: INFO: Got endpoints: latency-svc-tnpbv [751.532191ms]
May 29 19:19:44.809: INFO: Created: latency-svc-z5lhv
May 29 19:19:44.844: INFO: Got endpoints: latency-svc-vnscc [750.673359ms]
May 29 19:19:44.861: INFO: Created: latency-svc-knj6m
May 29 19:19:44.899: INFO: Got endpoints: latency-svc-6j4mb [746.008685ms]
May 29 19:19:44.911: INFO: Created: latency-svc-8lpvj
May 29 19:19:44.941: INFO: Got endpoints: latency-svc-q4wfx [746.557505ms]
May 29 19:19:44.958: INFO: Created: latency-svc-dw7cf
May 29 19:19:44.993: INFO: Got endpoints: latency-svc-hx8tt [750.95977ms]
May 29 19:19:45.006: INFO: Created: latency-svc-zgdlx
May 29 19:19:45.042: INFO: Got endpoints: latency-svc-vncxk [748.165565ms]
May 29 19:19:45.057: INFO: Created: latency-svc-q9wds
May 29 19:19:45.092: INFO: Got endpoints: latency-svc-6jb5g [742.746363ms]
May 29 19:19:45.105: INFO: Created: latency-svc-f52kb
May 29 19:19:45.149: INFO: Got endpoints: latency-svc-pb9gv [752.499027ms]
May 29 19:19:45.175: INFO: Created: latency-svc-p22hj
May 29 19:19:45.196: INFO: Got endpoints: latency-svc-52nfb [750.44435ms]
May 29 19:19:45.211: INFO: Created: latency-svc-2vjhv
May 29 19:19:45.247: INFO: Got endpoints: latency-svc-s4987 [739.092526ms]
May 29 19:19:45.260: INFO: Created: latency-svc-xrzls
May 29 19:19:45.296: INFO: Got endpoints: latency-svc-8brnv [742.98397ms]
May 29 19:19:45.315: INFO: Created: latency-svc-chfdk
May 29 19:19:45.347: INFO: Got endpoints: latency-svc-zq89n [752.165514ms]
May 29 19:19:45.359: INFO: Created: latency-svc-px5xq
May 29 19:19:45.393: INFO: Got endpoints: latency-svc-6xn4d [751.589092ms]
May 29 19:19:45.412: INFO: Created: latency-svc-2hdxm
May 29 19:19:45.444: INFO: Got endpoints: latency-svc-8nkh4 [751.373584ms]
May 29 19:19:45.463: INFO: Created: latency-svc-zxmrs
May 29 19:19:45.492: INFO: Got endpoints: latency-svc-9s9wz [747.106325ms]
May 29 19:19:45.506: INFO: Created: latency-svc-wq8zt
May 29 19:19:45.543: INFO: Got endpoints: latency-svc-z5lhv [749.564617ms]
May 29 19:19:45.558: INFO: Created: latency-svc-dmbqn
May 29 19:19:45.593: INFO: Got endpoints: latency-svc-knj6m [749.255305ms]
May 29 19:19:45.606: INFO: Created: latency-svc-5zn9v
May 29 19:19:45.642: INFO: Got endpoints: latency-svc-8lpvj [743.363684ms]
May 29 19:19:45.663: INFO: Created: latency-svc-s5z9g
May 29 19:19:45.696: INFO: Got endpoints: latency-svc-dw7cf [754.624005ms]
May 29 19:19:45.708: INFO: Created: latency-svc-9s2rg
May 29 19:19:45.748: INFO: Got endpoints: latency-svc-zgdlx [755.444336ms]
May 29 19:19:45.765: INFO: Created: latency-svc-8fs5w
May 29 19:19:45.806: INFO: Got endpoints: latency-svc-q9wds [763.969855ms]
May 29 19:19:45.832: INFO: Created: latency-svc-nchdr
May 29 19:19:45.844: INFO: Got endpoints: latency-svc-f52kb [751.845201ms]
May 29 19:19:45.866: INFO: Created: latency-svc-dn5dx
May 29 19:19:45.896: INFO: Got endpoints: latency-svc-p22hj [747.354134ms]
May 29 19:19:45.930: INFO: Created: latency-svc-6hjnd
May 29 19:19:45.947: INFO: Got endpoints: latency-svc-2vjhv [750.840364ms]
May 29 19:19:45.984: INFO: Created: latency-svc-tn68t
May 29 19:19:45.998: INFO: Got endpoints: latency-svc-xrzls [750.292643ms]
May 29 19:19:46.030: INFO: Created: latency-svc-5lqsq
May 29 19:19:46.042: INFO: Got endpoints: latency-svc-chfdk [746.136288ms]
May 29 19:19:46.094: INFO: Created: latency-svc-fhr47
May 29 19:19:46.134: INFO: Got endpoints: latency-svc-px5xq [786.812708ms]
May 29 19:19:46.189: INFO: Got endpoints: latency-svc-2hdxm [795.350527ms]
May 29 19:19:46.189: INFO: Created: latency-svc-qpstp
May 29 19:19:46.193: INFO: Got endpoints: latency-svc-zxmrs [749.085398ms]
May 29 19:19:46.207: INFO: Created: latency-svc-mfqkm
May 29 19:19:46.218: INFO: Created: latency-svc-75pnp
May 29 19:19:46.243: INFO: Got endpoints: latency-svc-wq8zt [750.599555ms]
May 29 19:19:46.257: INFO: Created: latency-svc-2jm5s
May 29 19:19:46.302: INFO: Got endpoints: latency-svc-dmbqn [758.841062ms]
May 29 19:19:46.319: INFO: Created: latency-svc-pc67k
May 29 19:19:46.343: INFO: Got endpoints: latency-svc-5zn9v [749.623217ms]
May 29 19:19:46.357: INFO: Created: latency-svc-hrzfk
May 29 19:19:46.401: INFO: Got endpoints: latency-svc-s5z9g [758.24314ms]
May 29 19:19:46.455: INFO: Created: latency-svc-rs9nr
May 29 19:19:46.458: INFO: Got endpoints: latency-svc-9s2rg [761.927877ms]
May 29 19:19:46.474: INFO: Created: latency-svc-llhpf
May 29 19:19:46.491: INFO: Got endpoints: latency-svc-8fs5w [742.46565ms]
May 29 19:19:46.510: INFO: Created: latency-svc-pm4fn
May 29 19:19:46.545: INFO: Got endpoints: latency-svc-nchdr [738.774612ms]
May 29 19:19:46.558: INFO: Created: latency-svc-w7brd
May 29 19:19:46.593: INFO: Got endpoints: latency-svc-dn5dx [748.164862ms]
May 29 19:19:46.611: INFO: Created: latency-svc-f8vl8
May 29 19:19:46.644: INFO: Got endpoints: latency-svc-6hjnd [747.498538ms]
May 29 19:19:46.657: INFO: Created: latency-svc-k52xn
May 29 19:19:46.707: INFO: Got endpoints: latency-svc-tn68t [760.376019ms]
May 29 19:19:46.734: INFO: Created: latency-svc-6xqs6
May 29 19:19:46.744: INFO: Got endpoints: latency-svc-5lqsq [746.336994ms]
May 29 19:19:46.756: INFO: Created: latency-svc-t5zvs
May 29 19:19:46.804: INFO: Got endpoints: latency-svc-fhr47 [762.481197ms]
May 29 19:19:46.823: INFO: Created: latency-svc-wtgw5
May 29 19:19:46.866: INFO: Got endpoints: latency-svc-qpstp [731.609943ms]
May 29 19:19:46.903: INFO: Got endpoints: latency-svc-mfqkm [713.787978ms]
May 29 19:19:46.903: INFO: Created: latency-svc-rhsn5
May 29 19:19:46.916: INFO: Created: latency-svc-rz8dv
May 29 19:19:46.944: INFO: Got endpoints: latency-svc-75pnp [750.468548ms]
May 29 19:19:46.960: INFO: Created: latency-svc-gdxjt
May 29 19:19:46.998: INFO: Got endpoints: latency-svc-2jm5s [754.5271ms]
May 29 19:19:47.010: INFO: Created: latency-svc-xdgzn
May 29 19:19:47.043: INFO: Got endpoints: latency-svc-pc67k [740.560178ms]
May 29 19:19:47.064: INFO: Created: latency-svc-cwxl7
May 29 19:19:47.094: INFO: Got endpoints: latency-svc-hrzfk [750.551752ms]
May 29 19:19:47.107: INFO: Created: latency-svc-n5kmb
May 29 19:19:47.148: INFO: Got endpoints: latency-svc-rs9nr [747.512737ms]
May 29 19:19:47.177: INFO: Created: latency-svc-w8gcx
May 29 19:19:47.194: INFO: Got endpoints: latency-svc-llhpf [732.359171ms]
May 29 19:19:47.212: INFO: Created: latency-svc-4pnh7
May 29 19:19:47.242: INFO: Got endpoints: latency-svc-pm4fn [751.081671ms]
May 29 19:19:47.256: INFO: Created: latency-svc-lxjbp
May 29 19:19:47.293: INFO: Got endpoints: latency-svc-w7brd [748.735483ms]
May 29 19:19:47.306: INFO: Created: latency-svc-lr687
May 29 19:19:47.342: INFO: Got endpoints: latency-svc-f8vl8 [749.562914ms]
May 29 19:19:47.368: INFO: Created: latency-svc-b568c
May 29 19:19:47.396: INFO: Got endpoints: latency-svc-k52xn [751.815898ms]
May 29 19:19:47.419: INFO: Created: latency-svc-qrgfs
May 29 19:19:47.443: INFO: Got endpoints: latency-svc-6xqs6 [735.983305ms]
May 29 19:19:47.456: INFO: Created: latency-svc-g25lj
May 29 19:19:47.493: INFO: Got endpoints: latency-svc-t5zvs [748.469173ms]
May 29 19:19:47.507: INFO: Created: latency-svc-5bx5p
May 29 19:19:47.543: INFO: Got endpoints: latency-svc-wtgw5 [738.925516ms]
May 29 19:19:47.558: INFO: Created: latency-svc-rmjgv
May 29 19:19:47.593: INFO: Got endpoints: latency-svc-rhsn5 [727.26358ms]
May 29 19:19:47.606: INFO: Created: latency-svc-48d95
May 29 19:19:47.644: INFO: Got endpoints: latency-svc-rz8dv [740.951491ms]
May 29 19:19:47.657: INFO: Created: latency-svc-kmdls
May 29 19:19:47.695: INFO: Got endpoints: latency-svc-gdxjt [750.682955ms]
May 29 19:19:47.708: INFO: Created: latency-svc-x9kmx
May 29 19:19:47.748: INFO: Got endpoints: latency-svc-xdgzn [750.432345ms]
May 29 19:19:47.777: INFO: Created: latency-svc-csxtm
May 29 19:19:47.809: INFO: Got endpoints: latency-svc-cwxl7 [766.052329ms]
May 29 19:19:47.827: INFO: Created: latency-svc-wvzgs
May 29 19:19:47.842: INFO: Got endpoints: latency-svc-n5kmb [748.42397ms]
May 29 19:19:47.868: INFO: Created: latency-svc-l5pkn
May 29 19:19:47.892: INFO: Got endpoints: latency-svc-w8gcx [743.383482ms]
May 29 19:19:47.905: INFO: Created: latency-svc-vkwl5
May 29 19:19:47.942: INFO: Got endpoints: latency-svc-4pnh7 [747.62604ms]
May 29 19:19:47.959: INFO: Created: latency-svc-hxw8k
May 29 19:19:47.993: INFO: Got endpoints: latency-svc-lxjbp [750.640653ms]
May 29 19:19:48.043: INFO: Got endpoints: latency-svc-lr687 [749.387706ms]
May 29 19:19:48.093: INFO: Got endpoints: latency-svc-b568c [750.175035ms]
May 29 19:19:48.145: INFO: Got endpoints: latency-svc-qrgfs [749.045093ms]
May 29 19:19:48.192: INFO: Got endpoints: latency-svc-g25lj [748.68568ms]
May 29 19:19:48.243: INFO: Got endpoints: latency-svc-5bx5p [749.830722ms]
May 29 19:19:48.294: INFO: Got endpoints: latency-svc-rmjgv [750.158234ms]
May 29 19:19:48.351: INFO: Got endpoints: latency-svc-48d95 [757.388205ms]
May 29 19:19:48.392: INFO: Got endpoints: latency-svc-kmdls [748.203462ms]
May 29 19:19:48.453: INFO: Got endpoints: latency-svc-x9kmx [757.945325ms]
May 29 19:19:48.521: INFO: Got endpoints: latency-svc-csxtm [772.283261ms]
May 29 19:19:48.552: INFO: Got endpoints: latency-svc-wvzgs [743.306478ms]
May 29 19:19:48.662: INFO: Got endpoints: latency-svc-vkwl5 [770.210083ms]
May 29 19:19:48.662: INFO: Got endpoints: latency-svc-l5pkn [820.15045ms]
May 29 19:19:48.692: INFO: Got endpoints: latency-svc-hxw8k [749.778819ms]
May 29 19:19:48.692: INFO: Latencies: [28.809677ms 44.13925ms 55.343469ms 58.384383ms 72.46921ms 80.670016ms 89.966664ms 109.017376ms 130.866592ms 140.018235ms 153.444737ms 180.487148ms 201.018515ms 203.703116ms 211.194696ms 213.614286ms 213.654787ms 216.319387ms 220.510644ms 221.293673ms 222.302611ms 222.583222ms 223.660862ms 224.168381ms 224.225283ms 224.760303ms 225.024413ms 226.844681ms 227.64191ms 228.095727ms 228.666949ms 230.481216ms 230.788129ms 232.968509ms 234.049051ms 235.997423ms 236.075426ms 236.501141ms 240.805402ms 243.544406ms 246.395012ms 254.153202ms 284.610441ms 325.900484ms 336.707989ms 384.314068ms 427.357377ms 459.953796ms 492.174201ms 533.434643ms 570.751838ms 590.966894ms 616.807759ms 646.26136ms 662.363963ms 693.05661ms 713.787978ms 718.450555ms 720.592035ms 727.26358ms 728.63584ms 729.741978ms 730.420502ms 731.003925ms 731.609943ms 732.359171ms 735.983305ms 736.070317ms 736.559033ms 737.426767ms 738.774612ms 738.925516ms 739.092526ms 739.398339ms 740.560178ms 740.615788ms 740.951491ms 741.061504ms 742.334952ms 742.46565ms 742.746363ms 742.925473ms 742.942971ms 742.98397ms 743.306478ms 743.363684ms 743.383482ms 743.862409ms 744.069513ms 746.008685ms 746.017285ms 746.136288ms 746.336994ms 746.557505ms 747.106325ms 747.13603ms 747.354134ms 747.423542ms 747.498538ms 747.512737ms 747.571447ms 747.62604ms 748.164862ms 748.165565ms 748.203462ms 748.213571ms 748.377975ms 748.42397ms 748.469173ms 748.601486ms 748.641687ms 748.664886ms 748.68568ms 748.735483ms 748.800092ms 748.878996ms 749.044202ms 749.045093ms 749.080601ms 749.085398ms 749.233405ms 749.255305ms 749.365613ms 749.387706ms 749.401715ms 749.402015ms 749.562914ms 749.564617ms 749.620522ms 749.623217ms 749.759728ms 749.778819ms 749.822727ms 749.830722ms 749.944736ms 750.065139ms 750.158234ms 750.175035ms 750.191645ms 750.292643ms 750.432345ms 750.44435ms 750.468548ms 750.523357ms 750.546356ms 750.551752ms 750.599555ms 750.640653ms 750.673359ms 750.682955ms 750.840364ms 750.856567ms 750.95977ms 751.081671ms 751.099179ms 751.177182ms 751.352688ms 751.373584ms 751.532191ms 751.589092ms 751.815898ms 751.845201ms 751.952807ms 752.165514ms 752.499027ms 753.328862ms 753.36486ms 753.594971ms 753.87458ms 754.5271ms 754.624005ms 755.444336ms 755.446339ms 756.129364ms 757.388205ms 757.945325ms 758.24314ms 758.403248ms 758.841062ms 759.283783ms 759.999408ms 760.376019ms 761.927877ms 762.481197ms 763.132329ms 763.969855ms 764.261169ms 765.252008ms 766.052329ms 766.769963ms 767.996209ms 769.282357ms 770.210083ms 770.278694ms 772.283261ms 783.484688ms 786.812708ms 795.019818ms 795.350527ms 820.15045ms]
May 29 19:19:48.693: INFO: 50 %ile: 747.571447ms
May 29 19:19:48.693: INFO: 90 %ile: 759.999408ms
May 29 19:19:48.693: INFO: 99 %ile: 795.350527ms
May 29 19:19:48.693: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:19:48.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4267" for this suite.
May 29 19:20:14.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:20:14.837: INFO: namespace svc-latency-4267 deletion completed in 26.138875333s

• [SLOW TEST:39.918 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:20:14.837: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:20:20.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2608" for this suite.
May 29 19:21:10.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:21:11.003: INFO: namespace kubelet-test-2608 deletion completed in 50.087144325s

• [SLOW TEST:56.166 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:21:11.003: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 29 19:21:11.046: INFO: Waiting up to 5m0s for pod "pod-ead16acb-8246-11e9-b32a-4ae2266202a5" in namespace "emptydir-9408" to be "success or failure"
May 29 19:21:11.049: INFO: Pod "pod-ead16acb-8246-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.43189ms
May 29 19:21:13.053: INFO: Pod "pod-ead16acb-8246-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006540074s
May 29 19:21:15.057: INFO: Pod "pod-ead16acb-8246-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010237436s
May 29 19:21:17.060: INFO: Pod "pod-ead16acb-8246-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013738884s
STEP: Saw pod success
May 29 19:21:17.060: INFO: Pod "pod-ead16acb-8246-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:21:17.063: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-ead16acb-8246-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 19:21:17.081: INFO: Waiting for pod pod-ead16acb-8246-11e9-b32a-4ae2266202a5 to disappear
May 29 19:21:17.085: INFO: Pod pod-ead16acb-8246-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:21:17.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9408" for this suite.
May 29 19:21:23.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:21:23.168: INFO: namespace emptydir-9408 deletion completed in 6.080624611s

• [SLOW TEST:12.165 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:21:23.169: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 19:21:23.225: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2138ebd-8246-11e9-b32a-4ae2266202a5" in namespace "projected-5096" to be "success or failure"
May 29 19:21:23.231: INFO: Pod "downwardapi-volume-f2138ebd-8246-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.783615ms
May 29 19:21:25.234: INFO: Pod "downwardapi-volume-f2138ebd-8246-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009202333s
May 29 19:21:27.238: INFO: Pod "downwardapi-volume-f2138ebd-8246-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012442737s
May 29 19:21:29.241: INFO: Pod "downwardapi-volume-f2138ebd-8246-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01610895s
STEP: Saw pod success
May 29 19:21:29.241: INFO: Pod "downwardapi-volume-f2138ebd-8246-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:21:29.244: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-f2138ebd-8246-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 19:21:29.264: INFO: Waiting for pod downwardapi-volume-f2138ebd-8246-11e9-b32a-4ae2266202a5 to disappear
May 29 19:21:29.267: INFO: Pod downwardapi-volume-f2138ebd-8246-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:21:29.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5096" for this suite.
May 29 19:21:35.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:21:35.368: INFO: namespace projected-5096 deletion completed in 6.096930995s

• [SLOW TEST:12.199 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:21:35.369: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 19:21:35.409: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:21:41.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7136" for this suite.
May 29 19:22:31.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:22:31.540: INFO: namespace pods-7136 deletion completed in 50.087259533s

• [SLOW TEST:56.172 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:22:31.541: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 19:22:31.583: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ad25294-8247-11e9-b32a-4ae2266202a5" in namespace "downward-api-7634" to be "success or failure"
May 29 19:22:31.591: INFO: Pod "downwardapi-volume-1ad25294-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.668985ms
May 29 19:22:33.595: INFO: Pod "downwardapi-volume-1ad25294-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011316197s
May 29 19:22:35.599: INFO: Pod "downwardapi-volume-1ad25294-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015024606s
May 29 19:22:37.602: INFO: Pod "downwardapi-volume-1ad25294-8247-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018612905s
STEP: Saw pod success
May 29 19:22:37.602: INFO: Pod "downwardapi-volume-1ad25294-8247-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:22:37.605: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-1ad25294-8247-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 19:22:37.619: INFO: Waiting for pod downwardapi-volume-1ad25294-8247-11e9-b32a-4ae2266202a5 to disappear
May 29 19:22:37.622: INFO: Pod downwardapi-volume-1ad25294-8247-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:22:37.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7634" for this suite.
May 29 19:22:43.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:22:43.724: INFO: namespace downward-api-7634 deletion completed in 6.09893583s

• [SLOW TEST:12.182 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:22:43.725: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
May 29 19:22:43.782: INFO: Waiting up to 5m0s for pod "client-containers-2217be48-8247-11e9-b32a-4ae2266202a5" in namespace "containers-1528" to be "success or failure"
May 29 19:22:43.785: INFO: Pod "client-containers-2217be48-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407589ms
May 29 19:22:45.788: INFO: Pod "client-containers-2217be48-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005671252s
May 29 19:22:47.792: INFO: Pod "client-containers-2217be48-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009305423s
May 29 19:22:49.794: INFO: Pod "client-containers-2217be48-8247-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01218176s
STEP: Saw pod success
May 29 19:22:49.794: INFO: Pod "client-containers-2217be48-8247-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:22:49.797: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod client-containers-2217be48-8247-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 19:22:49.817: INFO: Waiting for pod client-containers-2217be48-8247-11e9-b32a-4ae2266202a5 to disappear
May 29 19:22:49.825: INFO: Pod client-containers-2217be48-8247-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:22:49.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1528" for this suite.
May 29 19:22:55.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:22:55.917: INFO: namespace containers-1528 deletion completed in 6.086979482s

• [SLOW TEST:12.192 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:22:55.917: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
May 29 19:22:55.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-6507'
May 29 19:22:56.356: INFO: stderr: ""
May 29 19:22:56.356: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
May 29 19:22:57.360: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:22:57.360: INFO: Found 0 / 1
May 29 19:22:58.360: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:22:58.360: INFO: Found 0 / 1
May 29 19:22:59.359: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:22:59.359: INFO: Found 0 / 1
May 29 19:23:00.360: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:23:00.360: INFO: Found 0 / 1
May 29 19:23:01.360: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:23:01.360: INFO: Found 1 / 1
May 29 19:23:01.360: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 29 19:23:01.363: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:23:01.363: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 29 19:23:01.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 logs redis-master-rqfht redis-master --namespace=kubectl-6507'
May 29 19:23:01.458: INFO: stderr: ""
May 29 19:23:01.458: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 May 19:23:00.182 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 May 19:23:00.182 # Server started, Redis version 3.2.12\n1:M 29 May 19:23:00.182 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 May 19:23:00.182 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 29 19:23:01.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 log redis-master-rqfht redis-master --namespace=kubectl-6507 --tail=1'
May 29 19:23:01.544: INFO: stderr: ""
May 29 19:23:01.544: INFO: stdout: "1:M 29 May 19:23:00.182 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 29 19:23:01.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 log redis-master-rqfht redis-master --namespace=kubectl-6507 --limit-bytes=1'
May 29 19:23:01.631: INFO: stderr: ""
May 29 19:23:01.631: INFO: stdout: " "
STEP: exposing timestamps
May 29 19:23:01.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 log redis-master-rqfht redis-master --namespace=kubectl-6507 --tail=1 --timestamps'
May 29 19:23:01.714: INFO: stderr: ""
May 29 19:23:01.714: INFO: stdout: "2019-05-29T19:23:00.182763104Z 1:M 29 May 19:23:00.182 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 29 19:23:04.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 log redis-master-rqfht redis-master --namespace=kubectl-6507 --since=1s'
May 29 19:23:04.314: INFO: stderr: ""
May 29 19:23:04.314: INFO: stdout: ""
May 29 19:23:04.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 log redis-master-rqfht redis-master --namespace=kubectl-6507 --since=24h'
May 29 19:23:04.397: INFO: stderr: ""
May 29 19:23:04.398: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 May 19:23:00.182 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 May 19:23:00.182 # Server started, Redis version 3.2.12\n1:M 29 May 19:23:00.182 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 May 19:23:00.182 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
May 29 19:23:04.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete --grace-period=0 --force -f - --namespace=kubectl-6507'
May 29 19:23:04.477: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 29 19:23:04.477: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 29 19:23:04.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6507'
May 29 19:23:04.570: INFO: stderr: "No resources found.\n"
May 29 19:23:04.570: INFO: stdout: ""
May 29 19:23:04.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -l name=nginx --namespace=kubectl-6507 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 29 19:23:04.643: INFO: stderr: ""
May 29 19:23:04.643: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:23:04.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6507" for this suite.
May 29 19:23:26.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:23:26.729: INFO: namespace kubectl-6507 deletion completed in 22.082915397s

• [SLOW TEST:30.812 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:23:26.729: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:23:33.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3472" for this suite.
May 29 19:24:03.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:24:03.881: INFO: namespace replication-controller-3472 deletion completed in 30.090060628s

• [SLOW TEST:37.152 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:24:03.881: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-51dc63bb-8247-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 19:24:03.927: INFO: Waiting up to 5m0s for pod "pod-configmaps-51dcd044-8247-11e9-b32a-4ae2266202a5" in namespace "configmap-228" to be "success or failure"
May 29 19:24:03.935: INFO: Pod "pod-configmaps-51dcd044-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.139737ms
May 29 19:24:05.939: INFO: Pod "pod-configmaps-51dcd044-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011765833s
May 29 19:24:07.943: INFO: Pod "pod-configmaps-51dcd044-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015445912s
May 29 19:24:09.946: INFO: Pod "pod-configmaps-51dcd044-8247-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019000015s
STEP: Saw pod success
May 29 19:24:09.946: INFO: Pod "pod-configmaps-51dcd044-8247-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:24:09.949: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-51dcd044-8247-11e9-b32a-4ae2266202a5 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 19:24:09.971: INFO: Waiting for pod pod-configmaps-51dcd044-8247-11e9-b32a-4ae2266202a5 to disappear
May 29 19:24:09.973: INFO: Pod pod-configmaps-51dcd044-8247-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:24:09.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-228" for this suite.
May 29 19:24:15.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:24:16.058: INFO: namespace configmap-228 deletion completed in 6.081691622s

• [SLOW TEST:12.177 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:24:16.058: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-591fe92f-8247-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 19:24:16.113: INFO: Waiting up to 5m0s for pod "pod-configmaps-59205692-8247-11e9-b32a-4ae2266202a5" in namespace "configmap-322" to be "success or failure"
May 29 19:24:16.118: INFO: Pod "pod-configmaps-59205692-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.854928ms
May 29 19:24:18.121: INFO: Pod "pod-configmaps-59205692-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007942623s
May 29 19:24:20.125: INFO: Pod "pod-configmaps-59205692-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011422066s
May 29 19:24:22.128: INFO: Pod "pod-configmaps-59205692-8247-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014335815s
STEP: Saw pod success
May 29 19:24:22.128: INFO: Pod "pod-configmaps-59205692-8247-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:24:22.130: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-59205692-8247-11e9-b32a-4ae2266202a5 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 19:24:22.153: INFO: Waiting for pod pod-configmaps-59205692-8247-11e9-b32a-4ae2266202a5 to disappear
May 29 19:24:22.157: INFO: Pod pod-configmaps-59205692-8247-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:24:22.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-322" for this suite.
May 29 19:24:28.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:24:28.247: INFO: namespace configmap-322 deletion completed in 6.086611472s

• [SLOW TEST:12.188 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:24:28.247: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 29 19:24:42.323: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:42.323: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:42.449: INFO: Exec stderr: ""
May 29 19:24:42.449: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:42.449: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:42.564: INFO: Exec stderr: ""
May 29 19:24:42.564: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:42.564: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:42.680: INFO: Exec stderr: ""
May 29 19:24:42.680: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:42.680: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:42.801: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 29 19:24:42.801: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:42.802: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:42.916: INFO: Exec stderr: ""
May 29 19:24:42.916: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:42.916: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:43.053: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 29 19:24:43.053: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:43.053: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:43.168: INFO: Exec stderr: ""
May 29 19:24:43.168: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:43.168: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:43.288: INFO: Exec stderr: ""
May 29 19:24:43.288: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:43.288: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:43.402: INFO: Exec stderr: ""
May 29 19:24:43.402: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-35 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 29 19:24:43.402: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
May 29 19:24:43.520: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:24:43.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-35" for this suite.
May 29 19:25:33.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:25:33.621: INFO: namespace e2e-kubelet-etc-hosts-35 deletion completed in 50.09718918s

• [SLOW TEST:65.374 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:25:33.622: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 19:25:33.668: INFO: Waiting up to 5m0s for pod "downwardapi-volume-875a52be-8247-11e9-b32a-4ae2266202a5" in namespace "downward-api-3624" to be "success or failure"
May 29 19:25:33.677: INFO: Pod "downwardapi-volume-875a52be-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.40593ms
May 29 19:25:35.681: INFO: Pod "downwardapi-volume-875a52be-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013347867s
May 29 19:25:37.685: INFO: Pod "downwardapi-volume-875a52be-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017235641s
May 29 19:25:39.689: INFO: Pod "downwardapi-volume-875a52be-8247-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021362466s
STEP: Saw pod success
May 29 19:25:39.689: INFO: Pod "downwardapi-volume-875a52be-8247-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:25:39.692: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-875a52be-8247-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 19:25:39.708: INFO: Waiting for pod downwardapi-volume-875a52be-8247-11e9-b32a-4ae2266202a5 to disappear
May 29 19:25:39.710: INFO: Pod downwardapi-volume-875a52be-8247-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:25:39.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3624" for this suite.
May 29 19:25:45.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:25:45.811: INFO: namespace downward-api-3624 deletion completed in 6.097732609s

• [SLOW TEST:12.189 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:25:45.812: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 19:25:45.878: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ea0bfa1-8247-11e9-b32a-4ae2266202a5" in namespace "projected-5541" to be "success or failure"
May 29 19:25:45.885: INFO: Pod "downwardapi-volume-8ea0bfa1-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.785009ms
May 29 19:25:47.894: INFO: Pod "downwardapi-volume-8ea0bfa1-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015636809s
May 29 19:25:49.898: INFO: Pod "downwardapi-volume-8ea0bfa1-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019761235s
May 29 19:25:51.902: INFO: Pod "downwardapi-volume-8ea0bfa1-8247-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023181071s
STEP: Saw pod success
May 29 19:25:51.902: INFO: Pod "downwardapi-volume-8ea0bfa1-8247-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:25:51.904: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-8ea0bfa1-8247-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 19:25:51.922: INFO: Waiting for pod downwardapi-volume-8ea0bfa1-8247-11e9-b32a-4ae2266202a5 to disappear
May 29 19:25:51.924: INFO: Pod downwardapi-volume-8ea0bfa1-8247-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:25:51.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5541" for this suite.
May 29 19:25:57.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:25:58.022: INFO: namespace projected-5541 deletion completed in 6.09529242s

• [SLOW TEST:12.211 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:25:58.023: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 29 19:25:58.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-9786'
May 29 19:26:00.449: INFO: stderr: ""
May 29 19:26:00.449: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 29 19:26:01.453: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:26:01.453: INFO: Found 0 / 1
May 29 19:26:02.452: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:26:02.452: INFO: Found 0 / 1
May 29 19:26:03.453: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:26:03.453: INFO: Found 0 / 1
May 29 19:26:04.452: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:26:04.452: INFO: Found 0 / 1
May 29 19:26:05.453: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:26:05.453: INFO: Found 1 / 1
May 29 19:26:05.453: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 29 19:26:05.455: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:26:05.455: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 29 19:26:05.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 patch pod redis-master-r5gp8 --namespace=kubectl-9786 -p {"metadata":{"annotations":{"x":"y"}}}'
May 29 19:26:05.543: INFO: stderr: ""
May 29 19:26:05.543: INFO: stdout: "pod/redis-master-r5gp8 patched\n"
STEP: checking annotations
May 29 19:26:05.546: INFO: Selector matched 1 pods for map[app:redis]
May 29 19:26:05.546: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:26:05.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9786" for this suite.
May 29 19:26:27.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:26:27.691: INFO: namespace kubectl-9786 deletion completed in 22.142015251s

• [SLOW TEST:29.668 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:26:27.691: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 19:26:27.779: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a79a53b4-8247-11e9-9319-000d3afdb8b2", Controller:(*bool)(0xc0027e181e), BlockOwnerDeletion:(*bool)(0xc0027e181f)}}
May 29 19:26:27.786: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a798d7fe-8247-11e9-9319-000d3afdb8b2", Controller:(*bool)(0xc0027e19c6), BlockOwnerDeletion:(*bool)(0xc0027e19c7)}}
May 29 19:26:27.789: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a79983a1-8247-11e9-9319-000d3afdb8b2", Controller:(*bool)(0xc002528586), BlockOwnerDeletion:(*bool)(0xc002528587)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:26:32.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-963" for this suite.
May 29 19:26:38.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:26:38.884: INFO: namespace gc-963 deletion completed in 6.083931586s

• [SLOW TEST:11.194 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:26:38.885: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0529 19:27:18.953902      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 29 19:27:18.953: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:27:18.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-634" for this suite.
May 29 19:27:24.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:27:25.037: INFO: namespace gc-634 deletion completed in 6.080677593s

• [SLOW TEST:46.152 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:27:25.037: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
May 29 19:27:25.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 api-versions'
May 29 19:27:26.428: INFO: stderr: ""
May 29 19:27:26.428: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:27:26.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7300" for this suite.
May 29 19:27:32.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:27:32.515: INFO: namespace kubectl-7300 deletion completed in 6.083252862s

• [SLOW TEST:7.478 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:27:32.516: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5975
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5975
STEP: Creating statefulset with conflicting port in namespace statefulset-5975
STEP: Waiting until pod test-pod will start running in namespace statefulset-5975
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5975
May 29 19:27:38.596: INFO: Observed stateful pod in namespace: statefulset-5975, name: ss-0, uid: d15702d2-8247-11e9-9319-000d3afdb8b2, status phase: Pending. Waiting for statefulset controller to delete.
May 29 19:27:38.780: INFO: Observed stateful pod in namespace: statefulset-5975, name: ss-0, uid: d15702d2-8247-11e9-9319-000d3afdb8b2, status phase: Failed. Waiting for statefulset controller to delete.
May 29 19:27:38.785: INFO: Observed stateful pod in namespace: statefulset-5975, name: ss-0, uid: d15702d2-8247-11e9-9319-000d3afdb8b2, status phase: Failed. Waiting for statefulset controller to delete.
May 29 19:27:38.790: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5975
STEP: Removing pod with conflicting port in namespace statefulset-5975
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5975 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 29 19:27:44.821: INFO: Deleting all statefulset in ns statefulset-5975
May 29 19:27:44.823: INFO: Scaling statefulset ss to 0
May 29 19:28:04.850: INFO: Waiting for statefulset status.replicas updated to 0
May 29 19:28:04.852: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:28:04.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5975" for this suite.
May 29 19:28:10.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:28:10.958: INFO: namespace statefulset-5975 deletion completed in 6.094665682s

• [SLOW TEST:38.442 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:28:10.959: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-e5217a1e-8247-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 19:28:11.004: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e521e447-8247-11e9-b32a-4ae2266202a5" in namespace "projected-5753" to be "success or failure"
May 29 19:28:11.015: INFO: Pod "pod-projected-configmaps-e521e447-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.060039ms
May 29 19:28:13.018: INFO: Pod "pod-projected-configmaps-e521e447-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013937832s
May 29 19:28:15.022: INFO: Pod "pod-projected-configmaps-e521e447-8247-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017180452s
May 29 19:28:17.025: INFO: Pod "pod-projected-configmaps-e521e447-8247-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020880149s
STEP: Saw pod success
May 29 19:28:17.025: INFO: Pod "pod-projected-configmaps-e521e447-8247-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:28:17.028: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-configmaps-e521e447-8247-11e9-b32a-4ae2266202a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 19:28:17.041: INFO: Waiting for pod pod-projected-configmaps-e521e447-8247-11e9-b32a-4ae2266202a5 to disappear
May 29 19:28:17.043: INFO: Pod pod-projected-configmaps-e521e447-8247-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:28:17.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5753" for this suite.
May 29 19:28:23.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:28:23.129: INFO: namespace projected-5753 deletion completed in 6.083526586s

• [SLOW TEST:12.171 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:28:23.129: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 29 19:28:30.215: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:28:30.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1508" for this suite.
May 29 19:29:04.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:29:04.333: INFO: namespace replicaset-1508 deletion completed in 34.087639432s

• [SLOW TEST:41.204 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:29:04.334: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 29 19:29:04.373: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 29 19:29:04.378: INFO: Waiting for terminating namespaces to be deleted...
May 29 19:29:04.380: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-11779686-vmss000000 before test
May 29 19:29:04.385: INFO: keyvault-flexvolume-k242s from kube-system started at 2019-05-29 17:43:47 +0000 UTC (1 container statuses recorded)
May 29 19:29:04.385: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 19:29:04.385: INFO: azure-ip-masq-agent-lgffz from kube-system started at 2019-05-29 17:43:50 +0000 UTC (1 container statuses recorded)
May 29 19:29:04.385: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 19:29:04.385: INFO: azure-cni-networkmonitor-jtwxq from kube-system started at 2019-05-29 17:43:46 +0000 UTC (1 container statuses recorded)
May 29 19:29:04.385: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 19:29:04.385: INFO: kube-proxy-k8hqh from kube-system started at 2019-05-29 17:43:50 +0000 UTC (1 container statuses recorded)
May 29 19:29:04.385: INFO: 	Container kube-proxy ready: true, restart count 0
May 29 19:29:04.385: INFO: sonobuoy-systemd-logs-daemon-set-270e25e81e454394-4gl5v from heptio-sonobuoy started at 2019-05-29 17:50:08 +0000 UTC (2 container statuses recorded)
May 29 19:29:04.385: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 29 19:29:04.385: INFO: 	Container systemd-logs ready: true, restart count 1
May 29 19:29:04.385: INFO: blobfuse-flexvol-installer-kmkt7 from kube-system started at 2019-05-29 17:43:49 +0000 UTC (1 container statuses recorded)
May 29 19:29:04.385: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 19:29:04.385: INFO: metrics-server-6bf85bb69b-88c8p from kube-system started at 2019-05-29 17:43:50 +0000 UTC (1 container statuses recorded)
May 29 19:29:04.385: INFO: 	Container metrics-server ready: true, restart count 1
May 29 19:29:04.385: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-29 17:49:58 +0000 UTC (1 container statuses recorded)
May 29 19:29:04.385: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 29 19:29:04.385: INFO: sonobuoy-e2e-job-ab3403e7f77e4843 from heptio-sonobuoy started at 2019-05-29 17:50:08 +0000 UTC (2 container statuses recorded)
May 29 19:29:04.385: INFO: 	Container e2e ready: true, restart count 0
May 29 19:29:04.385: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-088af24c-8248-11e9-b32a-4ae2266202a5 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-088af24c-8248-11e9-b32a-4ae2266202a5 off the node k8s-pool1-11779686-vmss000000
STEP: verifying the node doesn't have the label kubernetes.io/e2e-088af24c-8248-11e9-b32a-4ae2266202a5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:29:16.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6003" for this suite.
May 29 19:29:34.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:29:34.533: INFO: namespace sched-pred-6003 deletion completed in 18.076982329s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:30.199 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:29:34.533: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 29 19:29:34.594: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8692,SelfLink:/api/v1/namespaces/watch-8692/configmaps/e2e-watch-test-watch-closed,UID:16f3bd98-8248-11e9-9319-000d3afdb8b2,ResourceVersion:20900,Generation:0,CreationTimestamp:2019-05-29 19:29:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 29 19:29:34.594: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8692,SelfLink:/api/v1/namespaces/watch-8692/configmaps/e2e-watch-test-watch-closed,UID:16f3bd98-8248-11e9-9319-000d3afdb8b2,ResourceVersion:20901,Generation:0,CreationTimestamp:2019-05-29 19:29:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 29 19:29:34.603: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8692,SelfLink:/api/v1/namespaces/watch-8692/configmaps/e2e-watch-test-watch-closed,UID:16f3bd98-8248-11e9-9319-000d3afdb8b2,ResourceVersion:20902,Generation:0,CreationTimestamp:2019-05-29 19:29:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 29 19:29:34.603: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8692,SelfLink:/api/v1/namespaces/watch-8692/configmaps/e2e-watch-test-watch-closed,UID:16f3bd98-8248-11e9-9319-000d3afdb8b2,ResourceVersion:20903,Generation:0,CreationTimestamp:2019-05-29 19:29:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:29:34.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8692" for this suite.
May 29 19:29:40.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:29:40.690: INFO: namespace watch-8692 deletion completed in 6.084425707s

• [SLOW TEST:6.157 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:29:40.690: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-7504
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7504 to expose endpoints map[]
May 29 19:29:40.753: INFO: Get endpoints failed (2.487506ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May 29 19:29:41.756: INFO: successfully validated that service multi-endpoint-test in namespace services-7504 exposes endpoints map[] (1.005648156s elapsed)
STEP: Creating pod pod1 in namespace services-7504
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7504 to expose endpoints map[pod1:[100]]
May 29 19:29:45.799: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.038358909s elapsed, will retry)
May 29 19:29:46.804: INFO: successfully validated that service multi-endpoint-test in namespace services-7504 exposes endpoints map[pod1:[100]] (5.043641503s elapsed)
STEP: Creating pod pod2 in namespace services-7504
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7504 to expose endpoints map[pod1:[100] pod2:[101]]
May 29 19:29:50.855: INFO: Unexpected endpoints: found map[1b3b24eb-8248-11e9-9319-000d3afdb8b2:[100]], expected map[pod1:[100] pod2:[101]] (4.044468184s elapsed, will retry)
May 29 19:29:51.862: INFO: successfully validated that service multi-endpoint-test in namespace services-7504 exposes endpoints map[pod1:[100] pod2:[101]] (5.052244139s elapsed)
STEP: Deleting pod pod1 in namespace services-7504
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7504 to expose endpoints map[pod2:[101]]
May 29 19:29:51.885: INFO: successfully validated that service multi-endpoint-test in namespace services-7504 exposes endpoints map[pod2:[101]] (17.754158ms elapsed)
STEP: Deleting pod pod2 in namespace services-7504
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7504 to expose endpoints map[]
May 29 19:29:52.898: INFO: successfully validated that service multi-endpoint-test in namespace services-7504 exposes endpoints map[] (1.00953422s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:29:52.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7504" for this suite.
May 29 19:30:14.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:30:15.012: INFO: namespace services-7504 deletion completed in 22.091122425s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:34.322 seconds]
[sig-network] Services
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:30:15.012: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 19:30:15.051: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:30:21.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3261" for this suite.
May 29 19:31:11.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:31:11.305: INFO: namespace pods-3261 deletion completed in 50.108819065s

• [SLOW TEST:56.293 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:31:11.305: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-50a06848-8248-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 19:31:11.355: INFO: Waiting up to 5m0s for pod "pod-secrets-50a0e533-8248-11e9-b32a-4ae2266202a5" in namespace "secrets-49" to be "success or failure"
May 29 19:31:11.368: INFO: Pod "pod-secrets-50a0e533-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.399164ms
May 29 19:31:13.371: INFO: Pod "pod-secrets-50a0e533-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016087072s
May 29 19:31:15.373: INFO: Pod "pod-secrets-50a0e533-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018731347s
May 29 19:31:17.376: INFO: Pod "pod-secrets-50a0e533-8248-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021558399s
STEP: Saw pod success
May 29 19:31:17.376: INFO: Pod "pod-secrets-50a0e533-8248-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:31:17.379: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-secrets-50a0e533-8248-11e9-b32a-4ae2266202a5 container secret-volume-test: <nil>
STEP: delete the pod
May 29 19:31:17.395: INFO: Waiting for pod pod-secrets-50a0e533-8248-11e9-b32a-4ae2266202a5 to disappear
May 29 19:31:17.398: INFO: Pod pod-secrets-50a0e533-8248-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:31:17.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-49" for this suite.
May 29 19:31:23.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:31:23.494: INFO: namespace secrets-49 deletion completed in 6.093109225s

• [SLOW TEST:12.188 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:31:23.495: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
May 29 19:31:23.540: INFO: Waiting up to 5m0s for pod "var-expansion-57e465b3-8248-11e9-b32a-4ae2266202a5" in namespace "var-expansion-7997" to be "success or failure"
May 29 19:31:23.548: INFO: Pod "var-expansion-57e465b3-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.310207ms
May 29 19:31:25.552: INFO: Pod "var-expansion-57e465b3-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012093117s
May 29 19:31:27.556: INFO: Pod "var-expansion-57e465b3-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015595044s
May 29 19:31:29.560: INFO: Pod "var-expansion-57e465b3-8248-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019309549s
STEP: Saw pod success
May 29 19:31:29.560: INFO: Pod "var-expansion-57e465b3-8248-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:31:29.562: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod var-expansion-57e465b3-8248-11e9-b32a-4ae2266202a5 container dapi-container: <nil>
STEP: delete the pod
May 29 19:31:29.582: INFO: Waiting for pod var-expansion-57e465b3-8248-11e9-b32a-4ae2266202a5 to disappear
May 29 19:31:29.585: INFO: Pod var-expansion-57e465b3-8248-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:31:29.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7997" for this suite.
May 29 19:31:35.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:31:35.669: INFO: namespace var-expansion-7997 deletion completed in 6.080205329s

• [SLOW TEST:12.174 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:31:35.669: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-bqq8
STEP: Creating a pod to test atomic-volume-subpath
May 29 19:31:35.721: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bqq8" in namespace "subpath-5132" to be "success or failure"
May 29 19:31:35.729: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.335349ms
May 29 19:31:37.732: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011756421s
May 29 19:31:39.736: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01531507s
May 29 19:31:41.739: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 6.018149059s
May 29 19:31:43.742: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 8.021821653s
May 29 19:31:45.747: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 10.025952938s
May 29 19:31:47.750: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 12.02950477s
May 29 19:31:49.753: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 14.032770761s
May 29 19:31:51.757: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 16.036512743s
May 29 19:31:53.761: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 18.040055488s
May 29 19:31:55.764: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 20.043262991s
May 29 19:31:57.768: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 22.047316701s
May 29 19:31:59.772: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Running", Reason="", readiness=true. Elapsed: 24.051630793s
May 29 19:32:01.776: INFO: Pod "pod-subpath-test-configmap-bqq8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.055742049s
STEP: Saw pod success
May 29 19:32:01.776: INFO: Pod "pod-subpath-test-configmap-bqq8" satisfied condition "success or failure"
May 29 19:32:01.779: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-subpath-test-configmap-bqq8 container test-container-subpath-configmap-bqq8: <nil>
STEP: delete the pod
May 29 19:32:01.793: INFO: Waiting for pod pod-subpath-test-configmap-bqq8 to disappear
May 29 19:32:01.795: INFO: Pod pod-subpath-test-configmap-bqq8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bqq8
May 29 19:32:01.795: INFO: Deleting pod "pod-subpath-test-configmap-bqq8" in namespace "subpath-5132"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:32:01.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5132" for this suite.
May 29 19:32:07.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:32:07.895: INFO: namespace subpath-5132 deletion completed in 6.095235351s

• [SLOW TEST:32.225 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:32:07.895: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 19:32:07.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-725b0031-8248-11e9-b32a-4ae2266202a5" in namespace "downward-api-8898" to be "success or failure"
May 29 19:32:07.949: INFO: Pod "downwardapi-volume-725b0031-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.134964ms
May 29 19:32:09.952: INFO: Pod "downwardapi-volume-725b0031-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014419371s
May 29 19:32:11.955: INFO: Pod "downwardapi-volume-725b0031-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017594046s
May 29 19:32:13.959: INFO: Pod "downwardapi-volume-725b0031-8248-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021618429s
STEP: Saw pod success
May 29 19:32:13.960: INFO: Pod "downwardapi-volume-725b0031-8248-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:32:13.962: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-725b0031-8248-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 19:32:13.975: INFO: Waiting for pod downwardapi-volume-725b0031-8248-11e9-b32a-4ae2266202a5 to disappear
May 29 19:32:13.978: INFO: Pod downwardapi-volume-725b0031-8248-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:32:13.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8898" for this suite.
May 29 19:32:19.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:32:20.066: INFO: namespace downward-api-8898 deletion completed in 6.085486435s

• [SLOW TEST:12.171 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:32:20.067: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 19:32:20.115: INFO: (0) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.861744ms)
May 29 19:32:20.120: INFO: (1) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.639693ms)
May 29 19:32:20.123: INFO: (2) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.079128ms)
May 29 19:32:20.127: INFO: (3) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.827559ms)
May 29 19:32:20.131: INFO: (4) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.289078ms)
May 29 19:32:20.134: INFO: (5) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.340539ms)
May 29 19:32:20.138: INFO: (6) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.547947ms)
May 29 19:32:20.141: INFO: (7) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.12153ms)
May 29 19:32:20.144: INFO: (8) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.840819ms)
May 29 19:32:20.148: INFO: (9) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.61035ms)
May 29 19:32:20.151: INFO: (10) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.985024ms)
May 29 19:32:20.154: INFO: (11) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.973524ms)
May 29 19:32:20.157: INFO: (12) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.203133ms)
May 29 19:32:20.160: INFO: (13) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.170932ms)
May 29 19:32:20.164: INFO: (14) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.509046ms)
May 29 19:32:20.168: INFO: (15) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.947564ms)
May 29 19:32:20.171: INFO: (16) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.307937ms)
May 29 19:32:20.175: INFO: (17) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.629351ms)
May 29 19:32:20.178: INFO: (18) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.11263ms)
May 29 19:32:20.181: INFO: (19) /api/v1/nodes/k8s-pool1-11779686-vmss000000/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.192433ms)
[AfterEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:32:20.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7407" for this suite.
May 29 19:32:26.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:32:26.269: INFO: namespace proxy-7407 deletion completed in 6.085524983s

• [SLOW TEST:6.202 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:32:26.270: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 29 19:32:26.311: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 29 19:32:26.316: INFO: Waiting for terminating namespaces to be deleted...
May 29 19:32:26.318: INFO: 
Logging pods the kubelet thinks is on node k8s-pool1-11779686-vmss000000 before test
May 29 19:32:26.324: INFO: sonobuoy-systemd-logs-daemon-set-270e25e81e454394-4gl5v from heptio-sonobuoy started at 2019-05-29 17:50:08 +0000 UTC (2 container statuses recorded)
May 29 19:32:26.324: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 29 19:32:26.324: INFO: 	Container systemd-logs ready: true, restart count 1
May 29 19:32:26.324: INFO: blobfuse-flexvol-installer-kmkt7 from kube-system started at 2019-05-29 17:43:49 +0000 UTC (1 container statuses recorded)
May 29 19:32:26.324: INFO: 	Container blobfuse-flexvol-installer ready: true, restart count 0
May 29 19:32:26.324: INFO: metrics-server-6bf85bb69b-88c8p from kube-system started at 2019-05-29 17:43:50 +0000 UTC (1 container statuses recorded)
May 29 19:32:26.324: INFO: 	Container metrics-server ready: true, restart count 1
May 29 19:32:26.324: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-29 17:49:58 +0000 UTC (1 container statuses recorded)
May 29 19:32:26.324: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 29 19:32:26.324: INFO: sonobuoy-e2e-job-ab3403e7f77e4843 from heptio-sonobuoy started at 2019-05-29 17:50:08 +0000 UTC (2 container statuses recorded)
May 29 19:32:26.324: INFO: 	Container e2e ready: true, restart count 0
May 29 19:32:26.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 29 19:32:26.324: INFO: keyvault-flexvolume-k242s from kube-system started at 2019-05-29 17:43:47 +0000 UTC (1 container statuses recorded)
May 29 19:32:26.324: INFO: 	Container keyvault-flexvolume ready: true, restart count 0
May 29 19:32:26.324: INFO: azure-ip-masq-agent-lgffz from kube-system started at 2019-05-29 17:43:50 +0000 UTC (1 container statuses recorded)
May 29 19:32:26.324: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
May 29 19:32:26.324: INFO: azure-cni-networkmonitor-jtwxq from kube-system started at 2019-05-29 17:43:46 +0000 UTC (1 container statuses recorded)
May 29 19:32:26.324: INFO: 	Container azure-cnms ready: true, restart count 0
May 29 19:32:26.324: INFO: kube-proxy-k8hqh from kube-system started at 2019-05-29 17:43:50 +0000 UTC (1 container statuses recorded)
May 29 19:32:26.324: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15a33e415657f7d4], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:32:27.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9696" for this suite.
May 29 19:32:33.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:32:33.427: INFO: namespace sched-pred-9696 deletion completed in 6.082424364s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.158 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:32:33.428: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-8193429b-8248-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 19:32:33.481: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-81944329-8248-11e9-b32a-4ae2266202a5" in namespace "projected-8922" to be "success or failure"
May 29 19:32:33.494: INFO: Pod "pod-projected-configmaps-81944329-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.379414ms
May 29 19:32:35.497: INFO: Pod "pod-projected-configmaps-81944329-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015459669s
May 29 19:32:37.501: INFO: Pod "pod-projected-configmaps-81944329-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019762148s
May 29 19:32:39.505: INFO: Pod "pod-projected-configmaps-81944329-8248-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023365773s
STEP: Saw pod success
May 29 19:32:39.505: INFO: Pod "pod-projected-configmaps-81944329-8248-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:32:39.507: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-configmaps-81944329-8248-11e9-b32a-4ae2266202a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 29 19:32:39.535: INFO: Waiting for pod pod-projected-configmaps-81944329-8248-11e9-b32a-4ae2266202a5 to disappear
May 29 19:32:39.539: INFO: Pod pod-projected-configmaps-81944329-8248-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:32:39.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8922" for this suite.
May 29 19:32:45.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:32:45.634: INFO: namespace projected-8922 deletion completed in 6.091874471s

• [SLOW TEST:12.207 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:32:45.634: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 29 19:32:45.687: INFO: Waiting up to 5m0s for pod "pod-88dad4a5-8248-11e9-b32a-4ae2266202a5" in namespace "emptydir-3112" to be "success or failure"
May 29 19:32:45.693: INFO: Pod "pod-88dad4a5-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.981047ms
May 29 19:32:47.696: INFO: Pod "pod-88dad4a5-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009583666s
May 29 19:32:49.702: INFO: Pod "pod-88dad4a5-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015669261s
May 29 19:32:51.706: INFO: Pod "pod-88dad4a5-8248-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018849711s
STEP: Saw pod success
May 29 19:32:51.706: INFO: Pod "pod-88dad4a5-8248-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:32:51.708: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-88dad4a5-8248-11e9-b32a-4ae2266202a5 container test-container: <nil>
STEP: delete the pod
May 29 19:32:51.735: INFO: Waiting for pod pod-88dad4a5-8248-11e9-b32a-4ae2266202a5 to disappear
May 29 19:32:51.738: INFO: Pod pod-88dad4a5-8248-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:32:51.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3112" for this suite.
May 29 19:32:57.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:32:57.821: INFO: namespace emptydir-3112 deletion completed in 6.080221916s

• [SLOW TEST:12.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:32:57.821: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-901d937f-8248-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume configMaps
May 29 19:32:57.869: INFO: Waiting up to 5m0s for pod "pod-configmaps-901df310-8248-11e9-b32a-4ae2266202a5" in namespace "configmap-8139" to be "success or failure"
May 29 19:32:57.880: INFO: Pod "pod-configmaps-901df310-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.212764ms
May 29 19:32:59.884: INFO: Pod "pod-configmaps-901df310-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014843629s
May 29 19:33:01.888: INFO: Pod "pod-configmaps-901df310-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019181298s
May 29 19:33:03.893: INFO: Pod "pod-configmaps-901df310-8248-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023681849s
STEP: Saw pod success
May 29 19:33:03.893: INFO: Pod "pod-configmaps-901df310-8248-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:33:03.895: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-configmaps-901df310-8248-11e9-b32a-4ae2266202a5 container configmap-volume-test: <nil>
STEP: delete the pod
May 29 19:33:03.909: INFO: Waiting for pod pod-configmaps-901df310-8248-11e9-b32a-4ae2266202a5 to disappear
May 29 19:33:03.917: INFO: Pod pod-configmaps-901df310-8248-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:33:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8139" for this suite.
May 29 19:33:09.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:33:10.002: INFO: namespace configmap-8139 deletion completed in 6.082281741s

• [SLOW TEST:12.181 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:33:10.002: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
May 29 19:33:10.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 create -f - --namespace=kubectl-8555'
May 29 19:33:10.432: INFO: stderr: ""
May 29 19:33:10.432: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 19:33:10.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8555'
May 29 19:33:10.538: INFO: stderr: ""
May 29 19:33:10.538: INFO: stdout: "update-demo-nautilus-p2lcf update-demo-nautilus-sl5x6 "
May 29 19:33:10.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-p2lcf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:10.618: INFO: stderr: ""
May 29 19:33:10.618: INFO: stdout: ""
May 29 19:33:10.618: INFO: update-demo-nautilus-p2lcf is created but not running
May 29 19:33:15.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8555'
May 29 19:33:15.703: INFO: stderr: ""
May 29 19:33:15.703: INFO: stdout: "update-demo-nautilus-p2lcf update-demo-nautilus-sl5x6 "
May 29 19:33:15.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-p2lcf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:15.791: INFO: stderr: ""
May 29 19:33:15.791: INFO: stdout: ""
May 29 19:33:15.791: INFO: update-demo-nautilus-p2lcf is created but not running
May 29 19:33:20.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8555'
May 29 19:33:20.877: INFO: stderr: ""
May 29 19:33:20.877: INFO: stdout: "update-demo-nautilus-p2lcf update-demo-nautilus-sl5x6 "
May 29 19:33:20.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-p2lcf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:20.960: INFO: stderr: ""
May 29 19:33:20.960: INFO: stdout: "true"
May 29 19:33:20.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-p2lcf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:21.036: INFO: stderr: ""
May 29 19:33:21.037: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 19:33:21.037: INFO: validating pod update-demo-nautilus-p2lcf
May 29 19:33:21.043: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 19:33:21.043: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 19:33:21.043: INFO: update-demo-nautilus-p2lcf is verified up and running
May 29 19:33:21.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-sl5x6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:21.122: INFO: stderr: ""
May 29 19:33:21.122: INFO: stdout: "true"
May 29 19:33:21.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-nautilus-sl5x6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:21.192: INFO: stderr: ""
May 29 19:33:21.193: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 29 19:33:21.193: INFO: validating pod update-demo-nautilus-sl5x6
May 29 19:33:21.198: INFO: got data: {
  "image": "nautilus.jpg"
}

May 29 19:33:21.198: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 29 19:33:21.198: INFO: update-demo-nautilus-sl5x6 is verified up and running
STEP: rolling-update to new replication controller
May 29 19:33:21.200: INFO: scanned /root for discovery docs: <nil>
May 29 19:33:21.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8555'
May 29 19:33:47.783: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 29 19:33:47.783: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 29 19:33:47.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8555'
May 29 19:33:47.868: INFO: stderr: ""
May 29 19:33:47.868: INFO: stdout: "update-demo-kitten-2lrqp update-demo-kitten-grg5l "
May 29 19:33:47.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-kitten-2lrqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:47.947: INFO: stderr: ""
May 29 19:33:47.947: INFO: stdout: "true"
May 29 19:33:47.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-kitten-2lrqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:48.025: INFO: stderr: ""
May 29 19:33:48.025: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 29 19:33:48.025: INFO: validating pod update-demo-kitten-2lrqp
May 29 19:33:48.033: INFO: got data: {
  "image": "kitten.jpg"
}

May 29 19:33:48.034: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 29 19:33:48.034: INFO: update-demo-kitten-2lrqp is verified up and running
May 29 19:33:48.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-kitten-grg5l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:48.117: INFO: stderr: ""
May 29 19:33:48.117: INFO: stdout: "true"
May 29 19:33:48.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 get pods update-demo-kitten-grg5l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8555'
May 29 19:33:48.211: INFO: stderr: ""
May 29 19:33:48.211: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 29 19:33:48.211: INFO: validating pod update-demo-kitten-grg5l
May 29 19:33:48.216: INFO: got data: {
  "image": "kitten.jpg"
}

May 29 19:33:48.216: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 29 19:33:48.216: INFO: update-demo-kitten-grg5l is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:33:48.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8555" for this suite.
May 29 19:34:10.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:34:10.324: INFO: namespace kubectl-8555 deletion completed in 22.104533722s

• [SLOW TEST:60.321 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:34:10.325: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-643.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-643.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 29 19:34:18.420: INFO: DNS probes using dns-643/dns-test-bb54ae90-8248-11e9-b32a-4ae2266202a5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:34:18.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-643" for this suite.
May 29 19:34:24.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:34:24.535: INFO: namespace dns-643 deletion completed in 6.091666428s

• [SLOW TEST:14.211 seconds]
[sig-network] DNS
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:34:24.535: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 29 19:34:24.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3ccffd1-8248-11e9-b32a-4ae2266202a5" in namespace "downward-api-8718" to be "success or failure"
May 29 19:34:24.588: INFO: Pod "downwardapi-volume-c3ccffd1-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.160993ms
May 29 19:34:26.592: INFO: Pod "downwardapi-volume-c3ccffd1-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010864462s
May 29 19:34:28.596: INFO: Pod "downwardapi-volume-c3ccffd1-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01528064s
May 29 19:34:30.600: INFO: Pod "downwardapi-volume-c3ccffd1-8248-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01880266s
STEP: Saw pod success
May 29 19:34:30.600: INFO: Pod "downwardapi-volume-c3ccffd1-8248-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:34:30.602: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downwardapi-volume-c3ccffd1-8248-11e9-b32a-4ae2266202a5 container client-container: <nil>
STEP: delete the pod
May 29 19:34:30.620: INFO: Waiting for pod downwardapi-volume-c3ccffd1-8248-11e9-b32a-4ae2266202a5 to disappear
May 29 19:34:30.622: INFO: Pod downwardapi-volume-c3ccffd1-8248-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:34:30.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8718" for this suite.
May 29 19:34:36.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:34:36.708: INFO: namespace downward-api-8718 deletion completed in 6.082100049s

• [SLOW TEST:12.173 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:34:36.709: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-cb0f2587-8248-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 19:34:36.761: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cb0f89c1-8248-11e9-b32a-4ae2266202a5" in namespace "projected-2783" to be "success or failure"
May 29 19:34:36.768: INFO: Pod "pod-projected-secrets-cb0f89c1-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.771217ms
May 29 19:34:38.772: INFO: Pod "pod-projected-secrets-cb0f89c1-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01125025s
May 29 19:34:40.776: INFO: Pod "pod-projected-secrets-cb0f89c1-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015009174s
May 29 19:34:42.779: INFO: Pod "pod-projected-secrets-cb0f89c1-8248-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018672574s
STEP: Saw pod success
May 29 19:34:42.780: INFO: Pod "pod-projected-secrets-cb0f89c1-8248-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:34:42.783: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-secrets-cb0f89c1-8248-11e9-b32a-4ae2266202a5 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 19:34:42.798: INFO: Waiting for pod pod-projected-secrets-cb0f89c1-8248-11e9-b32a-4ae2266202a5 to disappear
May 29 19:34:42.804: INFO: Pod pod-projected-secrets-cb0f89c1-8248-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:34:42.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2783" for this suite.
May 29 19:34:48.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:34:48.891: INFO: namespace projected-2783 deletion completed in 6.084213555s

• [SLOW TEST:12.183 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:34:48.891: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-d2526d3f-8248-11e9-b32a-4ae2266202a5
STEP: Creating a pod to test consume secrets
May 29 19:34:48.951: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d2538e7a-8248-11e9-b32a-4ae2266202a5" in namespace "projected-9223" to be "success or failure"
May 29 19:34:48.954: INFO: Pod "pod-projected-secrets-d2538e7a-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477642ms
May 29 19:34:50.958: INFO: Pod "pod-projected-secrets-d2538e7a-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007259863s
May 29 19:34:52.962: INFO: Pod "pod-projected-secrets-d2538e7a-8248-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010837756s
May 29 19:34:54.965: INFO: Pod "pod-projected-secrets-d2538e7a-8248-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014311524s
STEP: Saw pod success
May 29 19:34:54.965: INFO: Pod "pod-projected-secrets-d2538e7a-8248-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:34:54.968: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod pod-projected-secrets-d2538e7a-8248-11e9-b32a-4ae2266202a5 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 29 19:34:54.986: INFO: Waiting for pod pod-projected-secrets-d2538e7a-8248-11e9-b32a-4ae2266202a5 to disappear
May 29 19:34:54.989: INFO: Pod pod-projected-secrets-d2538e7a-8248-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:34:54.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9223" for this suite.
May 29 19:35:01.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:35:01.075: INFO: namespace projected-9223 deletion completed in 6.082700021s

• [SLOW TEST:12.183 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:35:01.075: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5950.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5950.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5950.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5950.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5950.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5950.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 29 19:35:09.206: INFO: DNS probes using dns-5950/dns-test-d99bebe3-8248-11e9-b32a-4ae2266202a5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:35:09.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5950" for this suite.
May 29 19:35:15.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:35:15.322: INFO: namespace dns-5950 deletion completed in 6.093884853s

• [SLOW TEST:14.247 seconds]
[sig-network] DNS
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:35:15.322: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 29 19:35:21.946: INFO: Successfully updated pod "annotationupdatee21a3b85-8248-11e9-b32a-4ae2266202a5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:35:23.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3767" for this suite.
May 29 19:35:45.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:35:46.124: INFO: namespace projected-3767 deletion completed in 22.15565657s

• [SLOW TEST:30.802 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:35:46.124: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 29 19:35:46.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1962'
May 29 19:35:46.280: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 29 19:35:46.280: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 29 19:35:46.294: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-nzz76]
May 29 19:35:46.294: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-nzz76" in namespace "kubectl-1962" to be "running and ready"
May 29 19:35:46.299: INFO: Pod "e2e-test-nginx-rc-nzz76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.723891ms
May 29 19:35:48.303: INFO: Pod "e2e-test-nginx-rc-nzz76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009245996s
May 29 19:35:50.307: INFO: Pod "e2e-test-nginx-rc-nzz76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012735341s
May 29 19:35:52.310: INFO: Pod "e2e-test-nginx-rc-nzz76": Phase="Running", Reason="", readiness=true. Elapsed: 6.016305971s
May 29 19:35:52.310: INFO: Pod "e2e-test-nginx-rc-nzz76" satisfied condition "running and ready"
May 29 19:35:52.310: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-nzz76]
May 29 19:35:52.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 logs rc/e2e-test-nginx-rc --namespace=kubectl-1962'
May 29 19:35:52.419: INFO: stderr: ""
May 29 19:35:52.419: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
May 29 19:35:52.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-947231839 delete rc e2e-test-nginx-rc --namespace=kubectl-1962'
May 29 19:35:52.502: INFO: stderr: ""
May 29 19:35:52.502: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:35:52.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1962" for this suite.
May 29 19:36:14.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:36:14.587: INFO: namespace kubectl-1962 deletion completed in 22.081528252s

• [SLOW TEST:28.463 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:36:14.588: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 29 19:36:14.630: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May 29 19:36:25.664: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:36:25.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1845" for this suite.
May 29 19:36:31.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:36:31.755: INFO: namespace pods-1845 deletion completed in 6.086090857s

• [SLOW TEST:17.168 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:36:31.756: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:36:37.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8911" for this suite.
May 29 19:36:43.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:36:43.975: INFO: namespace emptydir-wrapper-8911 deletion completed in 6.124442494s

• [SLOW TEST:12.220 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:36:43.976: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 29 19:36:44.047: INFO: (0) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.807753ms)
May 29 19:36:44.050: INFO: (1) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.98252ms)
May 29 19:36:44.053: INFO: (2) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.250531ms)
May 29 19:36:44.056: INFO: (3) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.095524ms)
May 29 19:36:44.059: INFO: (4) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.179327ms)
May 29 19:36:44.068: INFO: (5) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.510742ms)
May 29 19:36:44.076: INFO: (6) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.643507ms)
May 29 19:36:44.081: INFO: (7) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.549423ms)
May 29 19:36:44.086: INFO: (8) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.941999ms)
May 29 19:36:44.090: INFO: (9) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.865555ms)
May 29 19:36:44.093: INFO: (10) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.194128ms)
May 29 19:36:44.097: INFO: (11) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.267231ms)
May 29 19:36:44.100: INFO: (12) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.267131ms)
May 29 19:36:44.103: INFO: (13) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.059723ms)
May 29 19:36:44.106: INFO: (14) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.304933ms)
May 29 19:36:44.110: INFO: (15) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.133226ms)
May 29 19:36:44.112: INFO: (16) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.641406ms)
May 29 19:36:44.115: INFO: (17) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.771411ms)
May 29 19:36:44.118: INFO: (18) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.895916ms)
May 29 19:36:44.121: INFO: (19) /api/v1/nodes/k8s-pool1-11779686-vmss000000:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.997421ms)
[AfterEach] version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:36:44.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4113" for this suite.
May 29 19:36:50.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:36:50.203: INFO: namespace proxy-4113 deletion completed in 6.080020955s

• [SLOW TEST:6.228 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 29 19:36:50.204: INFO: >>> kubeConfig: /tmp/kubeconfig-947231839
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 29 19:36:50.247: INFO: Waiting up to 5m0s for pod "downward-api-1aa01040-8249-11e9-b32a-4ae2266202a5" in namespace "downward-api-1245" to be "success or failure"
May 29 19:36:50.250: INFO: Pod "downward-api-1aa01040-8249-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340293ms
May 29 19:36:52.258: INFO: Pod "downward-api-1aa01040-8249-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011133028s
May 29 19:36:54.261: INFO: Pod "downward-api-1aa01040-8249-11e9-b32a-4ae2266202a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01405191s
May 29 19:36:56.265: INFO: Pod "downward-api-1aa01040-8249-11e9-b32a-4ae2266202a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017578702s
STEP: Saw pod success
May 29 19:36:56.265: INFO: Pod "downward-api-1aa01040-8249-11e9-b32a-4ae2266202a5" satisfied condition "success or failure"
May 29 19:36:56.268: INFO: Trying to get logs from node k8s-pool1-11779686-vmss000000 pod downward-api-1aa01040-8249-11e9-b32a-4ae2266202a5 container dapi-container: <nil>
STEP: delete the pod
May 29 19:36:56.283: INFO: Waiting for pod downward-api-1aa01040-8249-11e9-b32a-4ae2266202a5 to disappear
May 29 19:36:56.285: INFO: Pod downward-api-1aa01040-8249-11e9-b32a-4ae2266202a5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 29 19:36:56.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1245" for this suite.
May 29 19:37:02.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 29 19:37:02.381: INFO: namespace downward-api-1245 deletion completed in 6.092792475s

• [SLOW TEST:12.177 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.2-beta.0.85+66049e3b21efe1/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMay 29 19:37:02.381: INFO: Running AfterSuite actions on all nodes
May 29 19:37:02.381: INFO: Running AfterSuite actions on node 1
May 29 19:37:02.381: INFO: Skipping dumping logs from cluster

Ran 204 of 3585 Specs in 6373.669 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3381 Skipped PASS

Ginkgo ran 1 suite in 1h46m14.91591202s
Test Suite Passed
